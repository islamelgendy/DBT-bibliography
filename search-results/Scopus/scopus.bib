
@ARTICLE{Xiong2022,
author={Xiong, J. and Ye, H. and Pei, W. and Kong, L. and Huo, Q. and Han, Y.},
title={A monitoring and diagnostics method based on FPGA-digital twin for power electronic transformer},
journal={Electric Power Systems Research},
year={2022},
volume={210},
doi={10.1016/j.epsr.2022.108111},
art_number={108111},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131461626&doi=10.1016%2fj.epsr.2022.108111&partnerID=40&md5=dc2d0048572a56f53351fc37dfa7f38b},
affiliation={Institute of Electrical Engineering, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Power electronic transformer (PET) has received extensive attention and research for its flexible control and diverse applications. However, it is difficult to achieve real-time monitoring and diagnostics through traditional digital simulation software as well as hardware offline testing. In this paper, a real-time field-programmable gate array-digital twin (FPGA-DT) technique is developed for the monitoring and diagnostics. A novel method based on the FPGA-DT, is also proposed to analyze, detect and identify the PET open-circuit (O/C) faults. This method is performed through collecting, calculating and evaluating different fault characteristics between actual and DT systems. Then, the fault identification logic is executed and analyzed in the FPGA environment. The effectiveness and correctness of the proposed method is tested and validated by performing blocked mode and four-type O/C faults on AC side, HB, DAB and DC side of the PET. © 2022 Elsevier B.V.},
author_keywords={Digital twin (DT);  FPGA-DT platform;  Monitoring and diagnostics;  Open-circuit (O/C) faults;  Power electronic transformer (PET)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lin2022,
author={Lin, B. and Wang, S. and Wen, M. and Mao, X.},
title={Context-Aware Code Change Embedding for Better Patch Correctness Assessment},
journal={ACM Transactions on Software Engineering and Methodology},
year={2022},
volume={31},
number={3},
doi={10.1145/3505247},
art_number={51},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130752121&doi=10.1145%2f3505247&partnerID=40&md5=896d9827ba64bf2c9e1c335c5f5a6100},
affiliation={National University of Defense Technology, Changsha, China; Huazhong University of Science and Technology, Wuhan, China},
abstract={Despite the capability in successfully fixing more and more real-world bugs, existing Automated Program Repair (APR) techniques are still challenged by the long-standing overfitting problem (i.e., a generated patch that passes all tests is actually incorrect). Plenty of approaches have been proposed for automated patch correctness assessment (APCA). Nonetheless, dynamic ones (i.e., those that needed to execute tests) are time-consuming while static ones (i.e., those built on top of static code features) are less precise. Therefore, embedding techniques have been proposed recently, which assess patch correctness via embedding token sequences extracted from the changed code of a generated patch. However, existing techniques rarely considered the context information and program structures of a generated patch, which are crucial for patch correctness assessment as revealed by existing studies. In this study, we explore the idea of context-Aware code change embedding considering program structures for patch correctness assessment. Specifically, given a patch, we not only focus on the changed code but also take the correlated unchanged part into consideration, through which the context information can be extracted and leveraged. We then utilize the AST path technique for representation where the structure information from AST node can be captured. Finally, based on several pre-defined heuristics, we build a deep learning based classifier to predict the correctness of the patch. We implemented this idea as Cache and performed extensive experiments to assess its effectiveness. Our results demonstrate that Cache can (1) perform better than previous representation learning based techniques (e.g., Cache relatively outperforms existing techniques by 6%, 3%, and 16%, respectively under three diverse experiment settings), and (2) achieve overall higher performance than existing APCA techniques while even being more precise than certain dynamic ones including PATCH-SIM (92.9% vs. 83.0%). Further results reveal that the context information and program structures leveraged by Cache contributed significantly to its outstanding performance. © 2022 Association for Computing Machinery.},
author_keywords={Automated program repair;  deep learning;  patch correctness},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xie2022,
author={Xie, X. and Li, T. and Wang, J. and Ma, L. and Guo, Q. and Juefei-Xu, F. and Liu, Y.},
title={NPC: Neuron Path Coverage via Characterizing Decision Logic of Deep Neural Networks},
journal={ACM Transactions on Software Engineering and Methodology},
year={2022},
volume={31},
number={3},
doi={10.1145/3490489},
art_number={47},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130728492&doi=10.1145%2f3490489&partnerID=40&md5=4a2b36fff06c0dab5095153ffb174652},
affiliation={Singapore Management University, 50 Stamford Road, Singapore, 178899, Singapore; Nanyang Technological University, 50 NANYANG AVENUE, Singapore, 639798, Singapore; Kyushu University, Japan; Alibaba Group, 525 Almanor Ave, Sunnyvale, CA  94085, United States; Sci-Tech University, Zhejiang, China},
abstract={Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality. © 2022 Association for Computing Machinery.},
author_keywords={Deep learning testing;  model interpretation;  testing coverage criteria},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pandey2022,
author={Pandey, A. and Mishra, R.K. and Pandey, G.},
title={Investigating Exhaust Emissions from In-Use Passenger Cars: Exploratory Analysis and Policy Outlook},
journal={Journal of Environmental Engineering (United States)},
year={2022},
volume={148},
number={7},
doi={10.1061/(ASCE)EE.1943-7870.0002015},
art_number={04022035},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130727373&doi=10.1061%2f%28ASCE%29EE.1943-7870.0002015&partnerID=40&md5=2dc0bc97d3fa87ca7e465a260039f8ea},
affiliation={Dept. of Environmental Engineering, Delhi Technological Univ, Delhi, 110 042, India; Dept. of Civil Engineering, Madan Mohan Malaviya Univ. of Technology, Gorakhpur, 273 010, India},
abstract={Most developing countries still have various makes of petrol-driven cars dominating the overall passenger vehicle fleet. In such countries, the emission certification policy for in-use vehicles remains an area of concern, making the inspection and maintenance (I/M) program less effective. A thorough investigation of the exhaust emissions from such cars is required to explore and address this concern. This paper provides insights into the effects of vehicle variables on tailpipe emission parameters from an exclusively larger and heterogeneous dataset of in-use cars (n=1,580). Results showed that both vehicle variables such as age, mileage, emissions norm, and maintenance category, and two engine variables, i.e., aspiration type and fuel mixing conditions, had a significant and direct influence on tailpipe parameters, namely, CO, HC, CO2, O2, λ, and AFR (carbon monoxide, hydrocarbon, carbon dioxide, oxygen, lambda and air-fuel ratio respectively). Stronger correlations were found for the relatively larger (considering age, R2 for COidle=0.88, HCidle=0.73, λf.idle=0.74, AFRf.idle=0.73 and considering mileage, R2 for COidle=0.75, HCidle=0.67, λf.idle=0.62, AFRf.idle=0.61 for the whole dataset) and diverse make-wise (R2 values fared even better, 0.87-0.93 for CO and 0.69-0.77 for HC) data collected during the study. The present research provides a first-hand and comprehensive analysis of the effects of the stringency of emission norms and maintenance category on the exhaust emissions from in-use cars. The polynomial emission equations generated by this study can reliably predict the emission levels for CO and HC based on the age and/or mileage of cars. Further, the results recommend revised policies to upgrade the existing emission certification infrastructure and phasing out policy of cars. © 2022 American Society of Civil Engineers.},
author_keywords={Developing countries;  Emission compliance;  Idle testing;  Inspection/maintenance (I/M) program;  Maintenance category;  Tailpipe emission;  Vehicle variables},
document_type={Article},
source={Scopus},
}

@ARTICLE{Marculescu2022,
author={Marculescu, B. and Zhang, M. and Arcuri, A.},
title={On the Faults Found in REST APIs by Automated Test Generation},
journal={ACM Transactions on Software Engineering and Methodology},
year={2022},
volume={31},
number={3},
doi={10.1145/3491038},
art_number={41},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130688588&doi=10.1145%2f3491038&partnerID=40&md5=2e9af2e9b85872e84920878c08e52b41},
affiliation={Kristiania University College, Oslo, PB 1190, Norway; Kristiania University College, Oslo Metropolitan University, Oslo, 0107, Norway},
abstract={RESTful web services are often used for building a wide variety of enterprise applications. The diversity and increased number of applications using RESTful APIs means that increasing amounts of resources are spent developing and testing these systems. Automation in test data generation provides a useful way of generating test data in a fast and efficient manner. However, automated test generation often results in large test suites that are hard to evaluate and investigate manually.This article proposes a taxonomy of the faults we have found using search-based software testing techniques applied on RESTful APIs. The taxonomy is a first step in understanding, analyzing, and ultimately fixing software faults in web services and enterprise applications. We propose to apply a density-based clustering algorithm to the test cases evolved during the search to allow a better separation between different groups of faults. This is needed to enable engineers to highlight and focus on the most serious faults.Tests were automatically generated for a set of eight case studies, seven open-source and one industrial. The test cases generated during the search are clustered based on the reported last executed line and based on the error messages returned, when such error messages were available. The tests were manually evaluated to determine their root causes and to obtain additional information.The article presents a taxonomy of the faults found based on the manual analysis of 415 faults in the eight case studies and proposes a method to support the classification using clustering of the resulting test cases. © 2022 Association for Computing Machinery.},
author_keywords={automated Software Testing;  automated Test Generation;  evolutionary Algorithms;  Search-based software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mao20224393,
author={Mao, X. and Li, X. and Huang, Y. and Shi, J. and Zhang, Y.},
title={Programmable Logic Controllers Past Linear Temporal Logic for Monitoring Applications in Industrial Control Systems},
journal={IEEE Transactions on Industrial Informatics},
year={2022},
volume={18},
number={7},
pages={4393-4405},
doi={10.1109/TII.2021.3123194},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118594033&doi=10.1109%2fTII.2021.3123194&partnerID=40&md5=ce97972cde5332f7a3410454b38b4ca4},
affiliation={Software Engineering Institute, East China Normal University, Shanghai, 20000, China; National Trusted Embedded Software Engineering Technology Research Center, East China Normal University, Shanghai, 20000, China},
abstract={Programmable logic controllers (PLC), which are widely applied in modern industrial control systems (ICS), work as the controller of sensors and actuators in ICS. These systems require strict correctness, especially for safety-critical systems. Currently, increasingly ICS move to 'come online' scenarios to enhance cyber-physical features, but it makes them more vulnerable due to acquiring increased interconnection accompanied by weakening physical isolation. Moreover, with the more complex controlling environment, such as hundreds of more I/O points and more diverse field buses, the incorrect executions of PLC might cause the failure of the overall ICS. In this article, we examine how the security and safety of running PLC could be enhanced in both developing and deploying stages of ICS. We propose a novel application of runtime verification to guarantee the security and safety of real-world ICS. As a variant of temporal logic, PLC past linear temporal logic (PPLTL) is proposed to specify the security and safety properties of PLC. Using PPLTL, we synthesize monitors to improve the PLC program's security and safety as a partner of testing and static verification. Our monitors provide twofold processing in a nonintrusive manner: One is filtering abnormal input data before invading the original programs, the other is double-checking the output signals before driving the actuators. We use several case studies and benchmarks to demonstrate the efficiency of the approach. The empirical results show that the time overhead and memory occupation are tiny. © 2005-2012 IEEE.},
author_keywords={Industrial control system (ICS);  international electrotechnical commission (IEC) 61131-3 standard;  programmable logic controller (PLC);  runtime verification (RV);  temporal logic},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nguyen2022,
author={Nguyen, D.-L. and Thai, D.-K. and Tran, N.-T. and Ngo, T.-T. and Le, H.-V.},
title={Confined compressive behaviors of high-performance fiber-reinforced concrete and conventional concrete with size effect},
journal={Construction and Building Materials},
year={2022},
volume={336},
doi={10.1016/j.conbuildmat.2022.127382},
art_number={127382},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129569489&doi=10.1016%2fj.conbuildmat.2022.127382&partnerID=40&md5=4ca78d7dc9d36aef96ee23896e450889},
affiliation={Faculty of Civil Engineering, Ho Chi Minh City University of Technology and Education, No.01 Vo Van Ngan St, Thu Duc City, Ho Chi Minh City, Viet Nam; Department of Civil and Environmental Engineering, Sejong University, No.209 Neungdong-ro, Gwangjin-gu, Seoul, 05006, South Korea; Faculty of Civil Engineering, Ho Chi Minh City University of Transport, No.02 Vo Oanh St, Ward 25, Binh Thanh District, Ho Chi Minh City, Viet Nam; Faculty of Civil Engineering, Thuyloi University, No.175 Tay Son St, Dong Da District, Ha Noi, Viet Nam; Faculty of Civil Engineering, Hanoi University of Mining and Geology, No.18 Vien Street, Duc Thang Ward, Bac Tu Liem District, Ha Noi, Viet Nam},
abstract={In this article paper, the specimens’ size effect on passively confined compressive behavior of high-performance fiber-reinforced concrete (HPFRC) and conventional concrete (CC) were investigated through an experimental test program. All compressive specimens were cylinder-shaped using steel/uPVC cover and HPFRC/CC core. The thicknesses of uPVC covers were 0, 3.2, and 3.8 mm while those of steel covers were 0, 1.4, and 1.8 mm. For the size effect of HPFRC, three different heights were designed to prepare specimens, which are 200, 400, and 600 mm with the same inner diameter of 114 mm. For the size effect of CC, the different-sized specimens were employed diversified by the heights of 200 and 600 mm with the same inner diameter of 114 mm. The specimens were subjected to compressive loading at the only concrete core and their engineering relationships between stress and strain were performed. The specimens with a thicker cover generally produced a higher compressive strength compared to the specimens that have the same dimensions as the covered ones, regardless of a concrete type. The effect of specimen height on confined compressive strength could be obviously observed for both HPFRC and CC. For HPFRC series, the unconfined HPFRC specimens produced the lower significance of size effect than the confined. The steel covers were more effective than uPVC covers for confinement of HPFRC cores in enhancing strength. Asides from those, the effect of confinement on compressive strength was also analytically analyzed and discussed through Johnston's failure criterion as well as the size effect laws of Weibull and Bažant. © 2022 Elsevier Ltd},
author_keywords={Concrete core;  Confined concrete;  Size effect;  Steel tube;  uPVC tube},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alenezi20221426,
author={Alenezi, M. and Basit, H.A. and Beg, M.A. and Shaukat, M.S.},
title={Synthesizing secure software development activities for linear and agile lifecycle models},
journal={Software - Practice and Experience},
year={2022},
volume={52},
number={6},
pages={1426-1453},
doi={10.1002/spe.3072},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123887479&doi=10.1002%2fspe.3072&partnerID=40&md5=15e8815f67619eafec6889bce980be46},
affiliation={College of Computer & Information Sciences, Prince Sultan University, Riyadh, Saudi Arabia; Department of Computer Science and Information Technology, Lahore Leads University, Lahore, Pakistan; Independent Researcher, Islamabad, Pakistan},
abstract={Application security is an important concern, and security activities to support software development lifecycle processes, such as specification, design, implementation, and testing are increasingly in need. Despite the plethora of knowledge available for secure software development in online and books, software systems are seldom secure as developers lack security knowledge. The primary reason for this paradox is the diversity and overwhelming nature of the available security knowledge. In this article, we propose to synthesize the well-known secure software development practices for both linear and agile lifecycle models. Using the MediaWiki platform, we make this knowledge available to software developers and designers from a single source. © 2022 John Wiley & Sons Ltd.},
author_keywords={knowledge management;  software engineering;  software security;  wiki platform},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bhansali2022140,
author={Bhansali, S. and Aris, A. and Acar, A. and Oz, H. and Uluagac, A.S.},
title={A First Look at Code Obfuscation for WebAssembly},
journal={WiSec 2022 - Proceedings of the 15th ACM Conference on Security and Privacy in Wireless and Mobile Networks},
year={2022},
pages={140-145},
doi={10.1145/3507657.3528560},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130771928&doi=10.1145%2f3507657.3528560&partnerID=40&md5=a313a2cd6586326bdc24ef8aa59818dd},
affiliation={Cyber-Physical Systems Security Lab, Department of Electrical and Computer Engineering, Florida International University, Miami, FL, United States; Georgia Institute of Technology, Atlanta, GA, United States},
abstract={WebAssembly (Wasm) has seen a lot of attention lately as it spreads through the mobile computing domain and becomes the new standard for performance-oriented web development. It has diversified its uses far beyond just web applications by acting as an execution environment for mobile agents, containers for IoT devices, and enabling new serverless approaches for edge computing. Within the numerous uses of Wasm, not all of them are benign. With the rise of Wasm-based cryptojacking malware, analyzing Wasm applications has been a hot topic in the literature, resulting in numerous Wasm-based cryptojacking detection systems. Many of these methods rely on static analysis, which traditionally can be circumvented through obfuscation. However, the feasibility of the obfuscation techniques for Wasm programs has never been investigated thoroughly. In this paper, we address this gap and perform the first look at code obfuscation for Wasm. We apply numerous obfuscation techniques to Wasm programs, and test their effectiveness in producing a fully obfuscated Wasm program. Particularly, we obfuscate both benign Wasm-based web applications and cryptojacking malware instances and feed them into a state-of-the-art Wasm cryptojacking detector to see if current Wasm analysis methods can be subverted with obfuscation. Our analysis shows that obfuscation can be highly effective and can cause even a state-of-the-art detector to misclassify the obfuscated Wasm samples. © 2022 ACM.},
author_keywords={cryptojacking;  obfuscation;  wasm;  webassembly},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gomez-Perez2022,
author={Gomez-Perez, S.L. and Zhang, Y. and Byrne, C. and Wakefield, C. and Geesey, T. and Sclamberg, J. and Peterson, S.},
title={Concordance of Computed Tomography Regional Body Composition Analysis Using a Fully Automated Open-Source Neural Network versus a Reference Semi-Automated Program with Manual Correction},
journal={Sensors},
year={2022},
volume={22},
number={9},
doi={10.3390/s22093357},
art_number={3357},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128767753&doi=10.3390%2fs22093357&partnerID=40&md5=70a2237ac8d7ef278b3cacde5cb41e95},
affiliation={Department of Clinical Nutrition, Rush University, Chicago, IL  60612, United States; Rush Bioinformatics and Biostatistics Core, Rush University Medical Center, Chicago, IL  60612, United States; Department of Kinesiology and Nutrition, University of Illinois at Chicago, Chicago, IL  60612, United States; Department of Internal Medicine, Brooke Army Medical Center, Fort Sam HoustonTX  78234, United States; Department of Diagnostic Radiology and Nuclear Medicine, Rush University Medical Center, Chicago, IL  60612, United States},
abstract={Quick, efficient, fully automated open-source programs to segment muscle and adipose tissues from computed tomography (CT) images would be a great contribution to body composition research. This study examined the concordance of cross-sectional areas (CSA) and densities for muscle, visceral adipose tissue (VAT), subcutaneous adipose tissue (SAT), and intramuscular adipose tissue (IMAT) from CT images at the third lumbar (L3) between an automated neural network (test method) and a semi-automatic human-based program (reference method). Concordance was further evaluated by disease status, sex, race/ethnicity, BMI categories. Agreement statistics applied included Lin’s Concordance (CCC), Spearman correlation coefficient (SCC), Sorensen dice-similarity coefficient (DSC), and Bland–Altman plots with limits of agreement (LOA) within 1.96 standard deviation. A total of 420 images from a diverse cohort of patients (60.35 ± 10.92 years; body mass index (BMI) of 28.77 ± 7.04 kg/m2; 55% female; 53% Black) were included in this study. About 30% of patients were healthy (i.e., received a CT scan for acute illness or pre-surgical donor work-up), while another 30% had a diagnosis of colorectal cancer. The CCC, SCC, and DSC estimates for muscle, VAT, SAT were all greater than 0.80 (&gt;0.80 indicates good performance). Agreement analysis by diagnosis showed good performance for the test method except for critical illness (DSC 0.65–0.87). Bland–Altman plots revealed narrow LOA suggestive of good agreement despite minimal proportional bias around the zero-bias line for muscle, SAT, and IMAT CSA. The test method shows good performance and almost perfect concordance for L3 muscle, VAT, SAT, and IMAT per DSC estimates, and Bland–Altman plots even after stratification by sex, race/ethnicity, and BMI categories. Care must be taken to assess the density of the CT images from critically ill patients before applying the automated neural network (test method). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={adipose tissue;  agreement;  artificial intelligence;  automated segmentation;  body composition;  computed tomography;  muscle;  validation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tempel2022,
author={Tempel, S. and Herdt, V. and Drechsler, R.},
title={SymEx-VP: An open source virtual prototype for OS-agnostic concolic testing of IoT firmware},
journal={Journal of Systems Architecture},
year={2022},
volume={126},
doi={10.1016/j.sysarc.2022.102456},
art_number={102456},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127474981&doi=10.1016%2fj.sysarc.2022.102456&partnerID=40&md5=09da9b698230080b672d786d1b7a2191},
affiliation={Institute of Computer Science, University of Bremen, Bremen, Germany; Cyber-Physical Systems, DFKI GmbH, Bremen, Germany},
abstract={Constrained Internet of Things (IoT) devices with limited computing resource are increasingly employed in security critical areas. Therefore, it is important for the firmware of these devices to be tested sufficiently. On non-constrained conventional devices, dynamic testing techniques (e.g. fuzzing, symbolic execution, or concolic testing) are successfully utilized to discover critical bugs in tested software. Unfortunately, the diverse ecosystem and the dependence on low-level details of a wide range of peripherals makes it difficult to use these techniques in the IoT context. In order to address these challenges, we present SymEx-VP an open source emulation-based approach for concolic testing of IoT firmware. SymEx-VP is a virtual prototype for RISC-V hardware platforms and allows concolic testing of RISC-V machine code. To support a wide range of different peripherals, SymEx-VP utilizes SystemC, a hardware modeling language for C++. By employing a SystemC extension mechanism, SymEx-VP can inject concolic inputs into the emulated firmware through the memory-mapped I/O peripheral interface of existing SystemC peripheral models. This allows us to support different operating systems and libraries used in the IoT with minimal integration effort. We provide an extensive description of SymEx-VP, illustrate peripheral modeling and firmware testing using it by example, and perform tests with four operating systems to demonstrate the advantages of our OS-agnostic firmware testing method. © 2022 The Authors},
author_keywords={Concolic testing;  Internet of things;  RISC-V;  SystemC;  Virtual prototyping},
document_type={Article},
source={Scopus},
}

@ARTICLE{AftabJilani2022,
author={Aftab Jilani, A. and Sherin, S. and Ijaz, S. and Zohaib Iqbal, M. and Uzair Khan, M.},
title={Deriving and evaluating a fault model for testing data science applications},
journal={Journal of Software: Evolution and Process},
year={2022},
volume={34},
number={5},
doi={10.1002/smr.2449},
art_number={e2449},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126361168&doi=10.1002%2fsmr.2449&partnerID=40&md5=3cdc6ecc89f6f54c3fe47a0ab41f7662},
affiliation={Quest Lab, Department of Computer Science, National University of Computer and Emerging Sciences, Islamabad, Pakistan; UAV-Dependability Lab, National Center of Robotics & Automation (NCRA), Islamabad, Pakistan},
abstract={Data science (DS) applications not only suffer from traditional software faults but may also suffer from data-specific and model-related faults. Fault models play an important role in evaluating and designing tests for testing DS applications. The existing fault models do not consider DS specific faults. In this study, we built a fault model DS applications. We investigate the faults by using diverse approaches: (i) a multi-vocal literature survey of published literature, (ii) semi-structured interviews of industry experts. The Multi-vocal study allows us to synthesize the existing knowledge from researchers and practitioners. Qualitative data from semi-structured interviews provide us with insights into the nature of faults encountered by practitioners. We combine the results of (i) and (ii) to derive a detailed fault model. The developed fault model is further validated through a quantitative survey of industry practitioners, and the respondents were asked to identify the faults from our proposed fault model that they have experienced and classify those faults based on their severity as perceived by practitioners and its frequency. The results show that practitioners consider prediction bias and model decay as the most severe faults while data sampling and splitting faults along with feature engineering faults are the most frequent. © 2022 John Wiley & Sons, Ltd.},
author_keywords={data science;  fault model;  software testing;  testing machine learning applications},
document_type={Article},
source={Scopus},
}

@ARTICLE{Su20221115,
author={Su, T. and Fan, L. and Chen, S. and Liu, Y. and Xu, L. and Pu, G. and Su, Z.},
title={Why My App Crashes? Understanding and Benchmarking Framework-Specific Exceptions of Android Apps},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={4},
pages={1115-1137},
doi={10.1109/TSE.2020.3013438},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128871403&doi=10.1109%2fTSE.2020.3013438&partnerID=40&md5=f62c5b038788da75077acd1a6b4d1de6},
affiliation={Department of Computer Science, Eth Zurich, Zurich, 8092, Switzerland; College of Cyber Science, Nankai Univerisity, Tianjin, China; Nanyang Technological University, Singapore, 639798, Singapore; College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798, Singapore; Department of Computer Science and Engineering, New York University Shanghai, Shanghai, 200122, China},
abstract={Mobile apps have become ubiquitous. Ensuring their correctness and reliability is important. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to both developers and researchers. However, such studies are difficult and yet to be carried out - this work fills this gap. We collected 16,245 and 8,760 unique exceptions from 2,486 open-source and 3,230 commercial Android apps, respectively, and observed that the exceptions thrown from Android framework (termed 'framework-specific exceptions') account for the majority. With one-year effort, we (1) extensively investigated these framework-specific exceptions, and (2) further conducted an online survey of 135 professional app developers about how they analyze, test, reproduce and fix these exceptions. Specifically, we aim to understand the framework-specific exceptions from several perspectives: (i) their characteristics (e.g., manifestation locations, fault taxonomy), (ii) the developers' testing practices, (iii) existing bug detection techniques' effectiveness, (iv) their reproducibility and (v) bug fixes. To enable follow-up research (e.g., bug understanding, detection, localization and repairing), we further systematically constructed, DroidDefects, the first comprehensive and largest benchmark of Android app exception bugs. This benchmark contains 33 reproducible exceptions (with test cases, stack traces, faulty and fixed app versions, bug types, etc.), and 3,696 ground-truth exceptions (real faults manifested by automated testing tools), which cover the apps with different complexities and diverse exception types. Based on our findings, we also built two prototype tools: Stoat+, an optimized dynamic testing tool, which quickly uncovered three previously-unknown, fixed crashes in Gmail and Google+; ExLocator, an exception localization tool, which can locate the root causes of specific exception types. Our dataset, benchmark and tools are publicly available on https://github.com/tingsu/droiddefects. © 1976-2012 IEEE.},
author_keywords={android applications;  bug reproducibility;  empirical study;  exception analysis;  Mobile applications;  software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Melegati2022,
author={Melegati, J. and Guerra, E. and Wang, X.},
title={HyMap: Eliciting hypotheses in early-stage software startups using cognitive mapping},
journal={Information and Software Technology},
year={2022},
volume={144},
doi={10.1016/j.infsof.2021.106807},
art_number={106807},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122334888&doi=10.1016%2fj.infsof.2021.106807&partnerID=40&md5=8fa059ad29b5218310fe83a7745a74fa},
affiliation={Free University of Bozen-Bolzano, Piazza Domenicani 3, Bolzano, Italy},
abstract={Context: Software startups develop innovative, software-intensive products. Given the uncertainty associated with such an innovative context, experimentation, an approach based on validating assumptions about the software product through data obtained from diverse techniques, like A/B tests or interviews, is valuable for these companies. Relying on data rather than opinions reduces the chance of developing unnecessary products or features, improving the likelihood of success, especially in early development stages, when implementing unnecessary features represents a higher risk for companies’ survival. Nevertheless, researchers have argued that the lack of clearly defined practices led to limited adoption of experimentation. Since the first step of the approach is to define hypotheses, testable statements about the software product features, based on which software development teams will create experiments, eliciting hypotheses is a natural first step to develop practices. Objective: We aim to develop a systematic technique for identifying hypotheses in early-stage software startups to support experimentation in these companies and, consequently, improve their software products. Methods: We followed a Design Science approach consisting of an artifact construction process, divided in three phases, and an evaluation within three startups. Results: We developed the HyMap, a hypotheses elicitation technique based on cognitive mapping. It consists of a process conducted by a facilitator using pre-defined questions, supported by a visual language to depict a cognitive map representing the founder's understanding of the product. Our evaluation showed that founders perceived the artifacts as clear, easy to use, and useful leading to hypotheses and facilitating their idea's visualization. Conclusion: From a theoretical perspective, our study provides a better understanding of the guidance founders use to develop their startups and, from a practical point of view, a technique to identify hypotheses in early-stage software startups. © 2022 Elsevier B.V.},
author_keywords={Experimentation;  Hypotheses elicitation;  Hypotheses engineering;  Software startups},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tan2022,
author={Tan, A.J.J. and Chong, C.Y. and Aleti, A.},
title={E-SC4R: Explaining Software Clustering for Remodularisation},
journal={Journal of Systems and Software},
year={2022},
volume={186},
doi={10.1016/j.jss.2021.111162},
art_number={111162},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121611444&doi=10.1016%2fj.jss.2021.111162&partnerID=40&md5=3fc21df665dd81fdf14c5d9138a20ca9},
affiliation={School of Information Technology, Monash University Malaysia, Jalan Lagoon Selatan, Bandar Sunway, Subang Jaya, Selangor, 47500, Malaysia; Faculty of Information Technology, Monash University, Clayton, 3168, VIC, Australia},
abstract={Maintenance of existing software requires a large amount of time for comprehending the source code. The architecture of a software, however, may not be clear to maintainers if up-to-date documentations are not available. Software clustering is often used as a remodularisation and architecture recovery technique to help recover a semantic representation of the software design. Due to the diverse domains, structure, and behaviour of software systems, the suitability of different clustering algorithms for different software systems are not investigated thoroughly. Research that introduce new clustering techniques usually validate their approaches on a specific domain, which might limit its generalisability. If the chosen test subjects could only represent a narrow perspective of the whole picture, researchers might risk not being able to address the external validity of their findings. This work aims to fill this gap by introducing a new approach, Explaining Software Clustering for Remodularisation (E-SC4R), to evaluate the effectiveness of different software clustering approaches. This work focuses on hierarchical clustering and Bunch clustering algorithms and provides information about their suitability according to the features of the software, which as a consequence, enables the selection of the most suitable algorithm and configuration that can achieve the best MoJoFM value from our existing pool of choices for a particular software system. The E-SC4R framework is tested on 30 open-source software systems with varying sizes and domains, and demonstrates that it can characterise both the strengths and weaknesses of the analysed software clustering algorithms using software features extracted from the code. The proposed approach also provides a better understanding of the algorithms’ behaviour by showing a 2D representation of the effectiveness of clustering techniques on the feature space generated through the application of dimensionality reduction techniques. © 2021 Elsevier Inc.},
author_keywords={Architecture recovery;  Feature extraction;  Footprint visualisation;  Software clustering;  Software remodularisation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Dodge2022191,
author={Dodge, J. and Anderson, A.A. and Olson, M. and Dikkala, R. and Burnett, M.},
title={How Do People Rank Multiple Mutant Agents?},
journal={International Conference on Intelligent User Interfaces, Proceedings IUI},
year={2022},
pages={191-211},
doi={10.1145/3490099.3511115},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127754207&doi=10.1145%2f3490099.3511115&partnerID=40&md5=9e71570308a3f7fce2093f14398440ba},
affiliation={Oregon State University, Corvallis, OR, United States},
abstract={Faced with several AI-powered sequential decision-making systems, how might someone choose on which to rely? For example, imagine car buyer Blair shopping for a self-driving car, or developer Dillon trying to choose an appropriate ML model to use in their application. Their first choice might be infeasible (i.e., too expensive in money or execution time), so they may need to select their second or third choice. To address this question, this paper presents: 1) Explanation Resolution, a quantifiable direct measurement concept; 2) a new XAI empirical task to measure explanations: "the Ranking Task"; and 3) a new strategy for inducing controllable agent variations - Mutant Agent Generation. In support of those main contributions, it also presents 4) novel explanations for sequential decision-making agents; 5) an adaptation to the AAR/AI assessment process; and 6) a qualitative study around these devices with 10 participants to investigate how they performed the Ranking Task on our mutant agents, using our explanations, and structured by AAR/AI. From an XAI researcher perspective, just as mutation testing can be applied to any code, mutant agent generation can be applied to essentially any neural network for which one wants to evaluate an assessment process or explanation type. As to an XAI user's perspective, the participants ranked the agents well overall, but showed the importance of high explanation resolution for close differences between agents. The participants also revealed the importance of supporting a wide diversity of explanation diets and agent "test selection"strategies. © 2022 ACM.},
author_keywords={After-Action Review;  Explainable AI},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2022295,
author={Liu, Y. and Zhang, X.-Y.},
title={Adaptive Random Testing for Multiagent Path Finding Systems},
journal={IEEE Transactions on Reliability},
year={2022},
volume={71},
number={1},
pages={295-308},
doi={10.1109/TR.2022.3146323},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126331873&doi=10.1109%2fTR.2022.3146323&partnerID=40&md5=cee5f10713f72330ab65f10f5fa0b7af},
affiliation={Beihang University, Beijing, 100191, China; National Institute of Informatics, Tokyo, 101-8430, Japan},
abstract={The multiagent path finding (MAPF) problem identifies the scheduling of multiple agents simultaneously, such that all of them can reach their targets efficiently. To date, MAPF systems have been assigned important tasks such as traffics and warehouses. It is essential to conduct testing for MAPF systems to detect potential failures. Namely, in an MAPF system, a test case is a specific MAPF scenario, including the initial locations of the agents and the environment for these agents to play in. By testing, we intend to find the scenarios (i.e., test cases) whose executions reveal failures. Testing MAPF systems is challenging due to the complexity of its input and the interactions among multiple agents. This article proposes the testing approach based on the adaptive random testing (ART) for MAPF systems. ART aims to generate new test cases far from the already executed ones. Particularly, to calculate the distance between each pair of test cases, we introduce two metrics, the initial density distribution and the destination density distribution, to characterize the distribution of the agents' initial and destination nodes, respectively. Benefit from ART, the diversity of the information generated during testing can be improved. Experimental results show that compared with the random testing, our approach can detect more diverse failure-revealing scenarios. © 1963-2012 IEEE.},
author_keywords={Adaptive random testing (ART);  Density measurement;  Multiagent path finding (MAPF);  Multiagent systems;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2022291,
author={Gao, Z. and Zhang, H. and Yao, Y. and Xiao, J. and Zeng, S. and Ge, G. and Wang, Y. and Ullah, A. and Reviriego, P.},
title={Soft Error Tolerant Convolutional Neural Networks on FPGAs with Ensemble Learning},
journal={IEEE Transactions on Very Large Scale Integration (VLSI) Systems},
year={2022},
volume={30},
number={3},
pages={291-302},
doi={10.1109/TVLSI.2021.3138491},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123366333&doi=10.1109%2fTVLSI.2021.3138491&partnerID=40&md5=b5a1abb4b8d8fcf652eb800c04e87004},
affiliation={School of Electrical and Information Engineering, Tianjin University, Tianjin, 300072, China; Tianjin International Engineering Institute, Tianjin University, Tianjin, 300072, China; School of Electronic Engineering, Tsinghua University, Beijing, 100084, China; Department of Electronics Engineering, University of Engineering and Technology Peshawar, Abbottabad, 220101, Pakistan; Department of Telematic Engineering, Universidad Carlos Iii de Madrid, Leganés, 28911, Spain},
abstract={Convolutional neural networks (CNNs) are widely used in computer vision and natural language processing. Field-programmable gate arrays (FPGAs) are popular accelerators for CNNs. However, if used in critical applications, the reliability of FPGA-based CNNs becomes a priority because FPGAs are prone to suffer soft errors. Traditional protection schemes, such as triple modular redundancy (TMR), introduce a large overhead, which is not acceptable in resource-limited platforms. This article proposes to use an ensemble of weak CNNs to build a robust classifier with low cost. To have a group of base CNNs with low complexity and balanced similarity and diversity, residual neural networks (ResNets) with different layers (20/32/44/56) are combined in the ensemble system to replace a single strong ResNet 110. In addition, a robust combiner is designed based on the reliability evaluation of a single ResNet. Single ResNets with different layers and different ensemble schemes are implemented on the FPGA accelerator based on Xilinx Zynq 7000 SoC. The reliability of the ensemble systems is evaluated based on a large-scale fault injection platform and compared with that of the TMR-protected ResNet 110 and ResNet 20. Experiment results show that the proposed ensembles could effectively improve the system reliability when suffering soft errors with an overhead much lower than TMR. © 1993-2012 IEEE.},
author_keywords={Convolutional neural networks (CNNs);  ensemble;  fault injection;  field-programmable gate array (FPGA) accelerator;  soft error tolerance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Almulla2022,
author={Almulla, H. and Gay, G.},
title={Learning how to search: generating effective test cases through adaptive fitness function selection},
journal={Empirical Software Engineering},
year={2022},
volume={27},
number={2},
doi={10.1007/s10664-021-10048-8},
art_number={38},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122924823&doi=10.1007%2fs10664-021-10048-8&partnerID=40&md5=acd8ea7d236f42ca67e901e7ed627caf},
affiliation={University of South Carolina, Columbia, SC, United States; Chalmers and the University of Gothenburg, Gothenburg, Sweden},
abstract={Search-based test generation is guided by feedback from one or more fitness functions—scoring functions that judge solution optimality. Choosing informative fitness functions is crucial to meeting the goals of a tester. Unfortunately, many goals—such as forcing the class-under-test to throw exceptions, increasing test suite diversity, and attaining Strong Mutation Coverage—do not have effective fitness function formulations. We propose that meeting such goals requires treating fitness function identification as a secondary optimization step. An adaptive algorithm that can vary the selection of fitness functions could adjust its selection throughout the generation process to maximize goal attainment, based on the current population of test suites. To test this hypothesis, we have implemented two reinforcement learning algorithms in the EvoSuite unit test generation framework, and used these algorithms to dynamically set the fitness functions used during generation for the three goals identified above. We have evaluated our framework, EvoSuiteFIT, on a set of Java case examples. EvoSuiteFIT techniques attain significant improvements for two of the three goals, and show limited improvements on the third when the number of generations of evolution is fixed. Additionally, for two of the three goals, EvoSuiteFIT detects faults missed by the other techniques. The ability to adjust fitness functions allows strategic choices that efficiently produce more effective test suites, and examining these choices offers insight into how to attain our testing goals. We find that adaptive fitness function selection is a powerful technique to apply when an effective fitness function does not already exist for achieving a testing goal. © 2022, The Author(s).},
author_keywords={Automated test generation;  Hyperheuristic search;  Reinforcement learning;  Search-based test generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kumar2022,
author={Kumar, A. and Patel, N. and Gupta, N. and Gupta, V.},
title={Design, analysis and implementation of electronically interfaced photovoltaic system using ARM Cortex-M4 microcontroller},
journal={Computers and Electrical Engineering},
year={2022},
volume={98},
doi={10.1016/j.compeleceng.2022.107701},
art_number={107701},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122628286&doi=10.1016%2fj.compeleceng.2022.107701&partnerID=40&md5=6d25a49674dab72294c88560774854cc},
affiliation={Birla Institute of Technology, Mesra, Ranchi, Jharkhand, India; Research Institute of Sciences and Engineering (RISE), University of Sharjah, United Arab Emirates; Malaviya National Institute of Technology JaipurRajasthan, India},
abstract={This research article presents a detailed documentation regarding test-bench development of Grid Interfaced Solar Photovoltaic (GIPV) system. The proposed GIPV system is developed using Voltage Source Converter (VSC) and DC-DC boost converter. The GIPV test-bench is developed using various auxiliary circuits including voltage/current measuring circuit, signal conditioning circuit, phase-shifting circuit, complementary pulse generation circuit, and driver circuit etc. These auxiliary circuits are designed in express printed circuit board software and fabricated in laboratory to develop a complete GIPV system test-bench. A combination of modified Synchronous Reference Frame (SRF) scheme and Proportional Resonant (PR) controller is proposed for current reference generation. Amplitude adaptivity based SRF-Phase Locked Loop (SRF-PLL) is presented to precisely detect the phase-angle of grid voltage under distorted and unbalanced conditions. The prototype is implemented using a commercially cheap ARM Cortex-M4 based STM32F407VGT6 microcontroller. This microcontroller has diversified peripherals for flexible target set-up with faster computation capability and a sophisticated user interface having TCP/IP connectivity. The control structure is developed in waijung environment for automatic code generation. The effectiveness of the proposed control algorithm is tested by numerical simulations in MATLAB/Simulink. Finally, practicality of the proposed GIPV system is experimentally demonstrated using ARM based microcontroller under various operating conditions. © 2022 Elsevier Ltd},
author_keywords={Arm Cortex M4 microcontroller;  DSTATCOM;  Photovoltaic energy conversion;  Power quality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Coviello2022193,
author={Coviello, C. and Romano, S. and Scanniello, G. and Antoniol, G.},
title={GASSER: A Multi-Objective Evolutionary Approach for Test Suite Reduction},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2022},
volume={32},
number={2},
pages={193-225},
doi={10.1142/S0218194022500085},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127579323&doi=10.1142%2fS0218194022500085&partnerID=40&md5=97e667d9fa597f42aabca2f6a8b001e8},
affiliation={University of Basilicata, Department of Mathematics, Computer Science and Economics, Viale dell'Ateneo Lucano 10, Potenza, 85100, Italy; University of Salerno, Department of Computer Science, Via Giovanni Paolo II, Fisciano, 84084, Italy; Polytechnique Montreal, Department of Computer Engineering and Software Engineering, 2500 Chem. de Polytechnique, Montreal, QC  H3T 1J4, Canada},
abstract={Regression testing is a practice that ensures a System Under Test (SUT) still works as expected after changes have been implemented. The simplest approach for regression testing is Retest-all, which consists of re-executing the entire Test Suite (TS) on the changed version of the SUT. Retest-all could be expensive in case a SUT and its TS grow in size and, if resources are insufficient, its application could be impracticable. A Test Suite Reduction (TSR) approach aims to overcome these issues by reducing the size of TSs, while preserving their fault-detection capability. In this paper, we introduce and validate an approach for TSR based on a multi-objective evolutionary algorithm, namely, Non-dominated Sorting Genetic Algorithm II (NSGA-II). This approach seeks to reduce TSs by maximizing both statement coverage and diversity of test cases of the reduced TSs, while minimizing the size of the reduced TSs. We named this approach Genetic Algorithm for teSt SuitE Reduction (GASSER). To assess GASSER, we conducted an experiment on 19 versions of four software systems from a public dataset - i.e. Software-artifact Infrastructure Repository (SIR). We compared GASSER with nine baseline approaches. The comparison was based on the size of the reduced TSs and their fault-detection capability. The most important take-away result is that GASSER, as compared with the baseline approaches, reduces more the size of the TSs with a non-significant effect on their fault-detection capability. The results of our empirical assessment suggest that the application of multi-objective evolutionary algorithms and, in particular, NSGA-II might represent a viable means to deal with TSR. © 2022 World Scientific Publishing Company.},
author_keywords={genetic algorithm;  Regression testing;  test suite reduction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Motwani2022637,
author={Motwani, M. and Soto, M. and Brun, Y. and Just, R. and Le Goues, C.},
title={Quality of Automated Program Repair on Real-World Defects},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={2},
pages={637-661},
doi={10.1109/TSE.2020.2998785},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125483909&doi=10.1109%2fTSE.2020.2998785&partnerID=40&md5=8d2db681875e8e0fa3d7ad2f9d007769},
affiliation={Manning College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, MA  01003-9264, United States; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA  15213, United States; Paul G. Allen School of Computer Science and Engineering, University of Washington, Seattle, WA  98195-2350, United States},
abstract={Automated program repair is a promising approach to reducing the costs of manual debugging and increasing software quality. However, recent studies have shown that automated program repair techniques can be prone to producing patches of low quality, overfitting to the set of tests provided to the repair technique, and failing to generalize to the intended specification. This paper rigorously explores this phenomenon on real-world Java programs, analyzing the effectiveness of four well-known repair techniques, GenProg, Par, SimFix, and TrpAutoRepair, on defects made by the projects' developers during their regular development process. We find that: (1) When applied to real-world Java code, automated program repair techniques produce patches for between 10.6 and 19.0 percent of the defects, which is less frequent than when applied to C code. (2) The produced patches often overfit to the provided test suite, with only between 13.8 and 46.1 percent of the patches passing an independent set of tests. (3) Test suite size has an extremely small but significant effect on the quality of the patches, with larger test suites producing higher-quality patches, though, surprisingly, higher-coverage test suites correlate with lower-quality patches. (4) The number of tests that a buggy program fails has a small but statistically significant positive effect on the quality of the produced patches. (5) Test suite provenance, whether the test suite is written by a human or automatically generated, has a significant effect on the quality of the patches, with developer-written tests typically producing higher-quality patches. And (6) the patches exhibit insufficient diversity to improve quality through some method of combining multiple patches. We develop JaRFly, an open-source framework for implementing techniques for automatic search-based improvement of Java programs. Our study uses JaRFly to faithfully reimplement GenProg and TrpAutoRepair to work on Java code, and makes the first public release of an implementation of Par. Unlike prior work, our study carefully controls for confounding factors and produces a methodology, as well as a dataset of automatically-generated test suites, for objectively evaluating the quality of Java repair techniques on real-world defects. © 1976-2012 IEEE.},
author_keywords={Automated program repair;  Defects4J;  Gen Prog;  Java;  Objective quality measure;  Par;  Patch quality;  Trp Auto Repair},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mathew2022,
author={Mathew, R. and Mehbodniya, A. and Ambalgi, A.P. and Murali, M. and Sahay, K.B. and Babu, D.V.},
title={In a virtual power plant, a blockchain-based decentralized power management solution for home distributed generation},
journal={Sustainable Energy Technologies and Assessments},
year={2022},
volume={49},
doi={10.1016/j.seta.2021.101731},
art_number={101731},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119090342&doi=10.1016%2fj.seta.2021.101731&partnerID=40&md5=9326419b1f8d2f25b8c30c8e7ac77ff3},
affiliation={Mukesh Patel School of Technology Management and Engineering, NMIMS (Deemed-to-be) University Mumbai, India; Department of Electronics and Communications Engineering, Kuwait College of Science and Technology (KCST), Doha Area, 7th Ring Road, Kuwait; Electronics Department, Mangalore University, Mangalore, Karnataka, India; Department of EEE, KSRM College of Engineering, Kadapa, Andhra Pradesh, India; Department of Electrical Engineering, Madan Mohan Malaviya University of Technology, Gorakhpur, India; Department of Electronics & Communication Engineering, Aarupadai Veedu Institute of Technology, Vinayaka Mission's Research Foundation, Paiyanoor 603 104Tamil Nadu, India},
abstract={Introduction of Distributed Energy Resources (DER), including regional renewable resources, energy storage, including controlled applications; has a tremendous disruptive and transformative impact on the centralized power grid. The inclusion of DERs is generally accepted to require a paradigmatic shift to the decentralized power grid with electronic power inverters. Energy has used blockchain technology as a simple payment mechanism for customers; however, it has not improved business process performance. The Virtual Power Plant (VPP) is a potential model for integrating SCRs into the electrical grid. Create a blockchain-based VPP management system throughout this work to allow a diverse set of transitive energy behaviors between home customers in a VPP with renewable, electricity generation, and storage systems. Customers can engage through a VPP to trade energy for mutually beneficial purposes and provide network infrastructure, including power, backup, and smart grids. Develop a decentralized optimization algorithm to improve customer power planning, marketing, and Internet services while respecting their independence and privacy. Then, managing the power of VPP, the researchers create a network of sample blockchain and apply the various algorithms to it. The designers tested the viability and efficiency of our program and blockchain system through tests using actual data. The blockchain-based VPP power management system reduces customer costs up to 39%, also reducing full system costs by 11%, according to modeling results. © 2021},
author_keywords={Blockchain;  Elegant grid;  Energy distribution;  Energy management;  Virtual power plant},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tong2022,
author={Tong, H. and Lu, W. and Xing, W. and Liu, B. and Wang, S.},
title={SHSE: A subspace hybrid sampling ensemble method for software defect number prediction},
journal={Information and Software Technology},
year={2022},
volume={142},
doi={10.1016/j.infsof.2021.106747},
art_number={106747},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117945474&doi=10.1016%2fj.infsof.2021.106747&partnerID=40&md5=9dd2a7f4d0d7c0b7cd0996e9e09f9932},
affiliation={School of Software Engineering, Beijing Jiaotong University, Beijing, 100044, China; School of Reliability and Systems Engineering, Science & Technology on Reliability & Environmental Engineering Laboratory, Beihang University, Beijing, 100191, China},
abstract={Context: Software defect number prediction (SDNP) helps allocate limited testing resources by ranking software modules according to the predicted defect numbers. However, the highly skewed distribution of defects greatly degrades the performance of SDNP models by preventing SDNP models from ranking software modules accurately. Objective: This paper introduces a novel subspace hybrid sampling ensemble (SHSE) method based on feature subspace construction, hybrid sampling, and ensemble learning for building high-performance SDNP models. Method: Specifically, we first construct a series of feature subspace to ensure the diversity of base learners. In each of feature subspace, we then use the proposed hybrid sampling method to balance the training subset without losing too much information and introducing lots of noisy data caused by only using undersampling or oversampling techniques. Finally, we train each base learner and combine them by using the proposed weighted ensemble strategy. Experiments are performed on 27 public defect datasets. We compare SHSE with five state-of-the-art resampling-based models and four zero-inflated/hurdle models in terms of the ranking performance measure fault-percentile-average (FPA). To demonstrate the effectiveness of SHSE, two statistical testing methods including Wilcoxon Signed-rank test and Scott–Knott Effect Size Difference test are utilized. Cliff's δ is also computed for quantifying the difference when there is significant difference between SHSE and each baseline. Results: The experimental results show that SHSE significantly outperforms the baselines and improves the performance over each baseline with as least medium effect size on most datasets. On average, SHSE improves the performance over the resampling-based methods by 8.7%∼14.4% and the zero-inflate/hurdle models by 10.3%∼15.2%. Conclusion: It can be concluded that SHSE is a more promising alternative for software defect number prediction. © 2021 Elsevier B.V.},
author_keywords={Ensemble learning;  Feature subspace;  Hybrid sampling for regression;  Imbalanced data;  Software defect number prediction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chapman2022437,
author={Chapman, B.V. and Liu, D. and Shen, Y. and Olamigoke, O.O. and Lakomy, D.S. and Barrera, A.M.G. and Stecklein, S.R. and Sawakuchi, G.O. and Bright, S.J. and Bedrosian, I. and Litton, J.K. and Smith, B.D. and Woodward, W.A. and Perkins, G.H. and Hoffman, K.E. and Stauder, M.C. and Strom, E.A. and Arun, B.K. and Shaitelman, S.F.},
title={Breast Radiation Therapy–Related Treatment Outcomes in Patients With or Without Germline Mutations on Multigene Panel Testing},
journal={International Journal of Radiation Oncology Biology Physics},
year={2022},
volume={112},
number={2},
pages={437-444},
doi={10.1016/j.ijrobp.2021.09.026},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117860454&doi=10.1016%2fj.ijrobp.2021.09.026&partnerID=40&md5=2d83e1f3a5268bc126b6b72c5765f86a},
affiliation={Departments of Radiation Oncology; Biostatistics; Breast Medical Oncology and Clinical Cancer Genetics; Radiation Physics, and; Breast Surgical Oncology, The University of Texas MD Anderson Cancer Center, Houston, TX, United States},
abstract={Purpose: Multigene panel testing has increased the detection of germline mutations in patients with breast cancer. The implications of using radiation therapy (RT) to treat patients with pathogenic variant (PV) mutations are not well understood and have been studied mostly in women with only BRCA1 or BRCA2 PVs. We analyzed oncologic outcomes and toxicity after adjuvant RT in a contemporary, diverse cohort of patients with breast cancer who underwent genetic panel testing. Methods and Materials: We retrospectively reviewed the records of 286 women with clinical stage I-III breast cancer diagnosed from 1995 to 2017 who underwent surgery, breast or chest wall RT with or without regional nodal irradiation, multigene panel testing, and evaluation at a large cancer center's genetic screening program. We evaluated rates of overall survival, locoregional recurrence, disease-specific death, and radiation-related toxicities in 3 groups: BRCA1/2 PV carriers, non-BRCA1/2 PV carriers, and patients without PV mutations. Results: PVs were detected in 25.2% of the cohort (12.6% BRCA1/2 and 12.6% non-BRCA1/2). The most commonly detected non-BRCA1/2 mutated genes were ATM, CHEK2, PALB2, CDH1, TP53, and PTEN. The median follow-up time for the entire cohort was 4.4 years (95% confidence interval, 3.8-4.9 years). No differences were found in overall survival, locoregional recurrence, or disease-specific death between groups (P > .1 for all). Acute and late toxicities were comparable across groups. Conclusion: Oncologic and toxicity outcomes after RT in women with PV germline mutations detected by multigene pane testing are similar to those in patients without detectable mutations, supporting the use of adjuvant RT as a standard of care when indicated. © 2021 Elsevier Inc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Huang2022,
author={Huang, R. and Chen, H. and Sun, W. and Towey, D.},
title={Candidate test set reduction for adaptive random testing: An overheads reduction technique},
journal={Science of Computer Programming},
year={2022},
volume={214},
doi={10.1016/j.scico.2021.102730},
art_number={102730},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117162737&doi=10.1016%2fj.scico.2021.102730&partnerID=40&md5=29bcefa6b8f27cf90dedb49a14a2ce5a},
affiliation={Faculty of Information Technology, Macau University of Science and Technology, Macau, 999078, China; School of Computer Science and Communication Engineering, Jiangsu University, Jiangsu, 212013, China; School of Big Data and Software Engineering, Chongqing University, Chongqing, 401331, China; School of Computer Science, University of Nottingham Ningbo ChinaZhejiang  315100, China},
abstract={Adaptive Random Testing (ART) is a family of testing techniques that were proposed as an enhancement of random testing (RT). ART achieves better failure-detection capability than RT by more evenly distributing test cases throughout the input domain. However, this process of selecting more diverse test cases incurs a heavy computational cost. In this paper, we propose a new ART method that improves on the efficiency of Fixed-Size-Candidate-Set ART (FSCS) by applying a test set reduction strategy. The proposed method, FSCS by Candidate Test Set Reduction (FSCS-CTSR), reduces the number of randomly generated candidate test cases, but supplements them with earlier, unused candidates that have lower similarity to the executed test cases. Simulations and experimental studies were conducted to examine the effectiveness and efficiency of the method, with the experimental results showing a comparable failure-detection effectiveness to FSCS, but with lower computational costs. © 2021 Elsevier B.V.},
author_keywords={Adaptive random testing;  FSCS;  Random testing;  Software testing;  Test set reduction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2022,
author={Zhang, R. and Kong, M. and Dong, B. and O'Neill, Z. and Cheng, H. and Hu, F. and Zhang, J.},
title={Development of a testing and evaluation protocol for occupancy sensing technologies in building HVAC controls: A case study of representative people counting sensors},
journal={Building and Environment},
year={2022},
volume={208},
doi={10.1016/j.buildenv.2021.108610},
art_number={108610},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120449122&doi=10.1016%2fj.buildenv.2021.108610&partnerID=40&md5=e11aa7e42606d1ee0f1c2adc5525e970},
affiliation={Well Living Lab, Delos Living LLC, 221 1st Ave SW, Rochester, MN  55902, United States; Department of Mechanical and Aerospace Engineering, Syracuse University, Syracuse, NY, United States; Department of Mechanical Engineering, Texas A&M University, College Station, TX, United States; Taylor Engineers, Alameda, CA, United States; The University of Alabama, Tuscaloosa, AL, United States; Pacific Northwest National Laboratory, Richland, WA, United States},
abstract={Occupancy-based control (OBC) in smart buildings provides numerous potentials to improve building energy efficiency. To achieve effective OBC, the occupancy status of a space or a building needs to be well understood in terms of whether the space is occupied and how many people are in the space. Well-designed and selected people counting technologies are cornerstones for OBC. However, the study and application of such technologies in OBC are relatively new. There are few mature people counting sensors available on the market. In addition, there is a lack of standardized guidance to comprehensively test, evaluate and compare the performance of people counting sensors, which further limits the selection and application of them in OBC. Hence, an innovative testing protocol was developed to fill in the gap. This paper introduces the design of the protocol with eight diversities and discusses case studies that evaluated four representative types of people counting sensors following the proposed protocol. It is found that the protocol can effectively guide a comprehensive evaluation of the selected sensors and lead to informative findings on the sensor performance. The test results can provide insights and suggestions not only for sensor developers to improve the sensor hardware and software design but also for heating, ventilation, and air-conditioning (HVAC) system designers and building managers to select the proper people counting sensors for OBC design. The test results on the sensor accuracy can also support additional evaluations of integrated OBC and building system operation. It is anticipated that the developed methodology can offer guidance on the test and evaluation of other occupancy sensing technologies. © 2021 The Authors},
author_keywords={Building system;  Occupancy-based control;  People counting;  Sensor performance},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Marín2022,
author={Marín, B. and Vos, T.E.J. and Paiva, A.C.R. and Fasolino, A.R. and Snoeck, M.},
title={ENACTEST - European Innovation Alliance for Testing Education},
journal={CEUR Workshop Proceedings},
year={2022},
volume={3144},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131255272&partnerID=40&md5=23524a140850922df11d79fe0b9fbe51},
affiliation={Universitat Politècnica de València (UPV), Camino de Vera s/n, Valencia, 46021, Spain; Open Universiteit (OU), Netherlands; Faculty of Engineering, University of Porto, INESC TEC, Rua Dr. Roberto Frias, s/n, Porto, 4200-465, Portugal; Università degli Studi di Napoli Federico II, DIETI, Via Claudio 21, Italy; KU Leuven, Naamsestraat 69, box 3500, Leuven, 3000, Belgium},
abstract={Testing software is very important, but not done well, resulting in problematic and erroneous software applications. The cause radicates from a skills mismatch between what is needed in industry, the learning needs of students, and the way testing is currently being taught at higher and vocational education institutes. The goal of this project is to identify and design seamless teaching materials for testing that are aligned with industry and learning needs. To represent the entire socio-economic environment that will benefit from the results, this project consortium is composed of a diverse set of partners ranging from universities to small enterprises. The project starts with research in sensemaking and cognitive models when doing and learning testing. Moreover, a study will be done to identify the needs of industry for training and knowledge transfer processes for testing. Based on the outcomes of this research and the study, we will design and develop capsules on teaching software testing including the instructional materials that take into account the cognitive models of students and the industry needs. Finally, we will validate these teaching testing capsules developed during the project. © 2021 The Authors.},
author_keywords={cognitive models;  education;  knowledge transfer;  software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Prajapati2022,
author={Prajapati, A.},
title={Software module clustering using grid-based large-scale many-objective particle swarm optimization},
journal={Soft Computing},
year={2022},
doi={10.1007/s00500-022-07182-w},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131083272&doi=10.1007%2fs00500-022-07182-w&partnerID=40&md5=d600093579a282309cb0d00e25198e92},
affiliation={Department of CSE& IT, Jaypee Institute of Information Technology, Noida, India},
abstract={There are huge numbers of real-world optimization problems, which often contain a large number of decision variables (n > 100) and objective functions (m > 3). Such optimization problems are generally regarded as large-scale many-objective optimization problems (LSMaOPs). Although a variety of search-based optimization algorithms have been proposed to solve various types of synthetic and real-world LSMaOPs, the problems of producing a well-distributed approximation of the Pareto front remain challenging. In this work, we propose a grid-based large-scale many-objective particle swarm optimization, namely GLMPSO, for solving the LSMaOPs, i.e., large-scale many-objective software module clustering problems (LMSMCPs). To balance the convergence and diversity of the GLMPSO, a grid-based ranking strategy and angle-based selection strategy are employed at different stages of the selection process. To balance the exploration and exploitation of the solution space, and avoid GLMPSO getting stuck in local minima, we use the center-based velocity computation. To test the effectiveness of the proposed GLMPSO, it is applied over nine LMSMCPs of software module clustering and the obtained results are compared with five existing approaches. The comparative results demonstrate that the proposed approach is more effective and has significant advantages over existing approaches. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Grid-based criteria;  Many-objective optimization;  Software clustering;  Two-archive optimization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tang2022,
author={Tang, Y. and Ren, Z. and Jiang, H. and Qiao, L. and Liu, D. and Zhou, Z. and Kong, W.},
title={Detecting Compiler Bugs Via a Deep Learning-Based Framework},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2022},
doi={10.1142/S0218194022500206},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131061013&doi=10.1142%2fS0218194022500206&partnerID=40&md5=2c5a28893025753803aca13f5b8ef5f2},
affiliation={School of Software, Dalian University of Technology, No. 2, Linggong Road, Ganjingzi District, Liaoning Province Dalian City, China; Dut Artificial Intelligence Institute, Dalian, China; Key Laboratory of Safety-Critical Software, Ministry of Industry and Information Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Beijing Institute of Control Engineering, No. 104, Youyi Rd. Haidian District, Beijing, China},
abstract={Compiler testing is the most widely used way to assure compiler quality. However, since compilers require a large number of sophisticated test programs as inputs, the existing approaches in compiler testing still have a limited capability in generating both syntactically valid and diverse test programs. In this paper, we propose DeepGen, a deep learning-based approach to support compiler testing through the inference of a generative model for compiler inputs. First, DeepGen trains a Transformer-XL model based on a large corpus of seed programs, and uses the trained model to generate syntactically valid programs. Then, DeepGen adopts a sampling strategy in the inference phase to generate diverse test programs. Finally, DeepGen leverages differential testing on the generated programs to discover compiler bugs. We have evaluated DeepGen over two popular C++ compilers GCC and LLVM, and the results confirm the effectiveness of our approach. DeepGen detects 35.29%, 53.33%, and 187.50% more bugs than three existing approaches, i.e. DeepSmith, DeepFuzz, and Csmith, respectively. In addition, 30.43% bugs detected by DeepGen are not detected by other approaches. Furthermore, DeepGen has successfully detected 38 bugs in the latest development versions of GCC and LLVM; 21 of them have been confirmed/fixed by the developers. © 2022 World Scientific Publishing Company.},
author_keywords={Compiler testing;  machine learning;  program generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Balos2022,
author={Balos, C.J. and Luszczek, P. and Osborn, S. and Willenbring, J. and Yang, U.M.},
title={Challenges of and Opportunities for a Large Diverse Software Team},
journal={Computing in Science and Engineering},
year={2022},
pages={1-10},
doi={10.1109/MCSE.2022.3172873},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130834682&doi=10.1109%2fMCSE.2022.3172873&partnerID=40&md5=7019c82d03dd469ece69ad968b1055a2},
affiliation={Lawrence Livermore National Laboratory; University of Tennessee Knoxville; Sandia National Laboratories},
abstract={A large software team consisting of members with different expertise, skillsets, personalities, ethnicities, and involving collaboration on a large and complex software product presents many technical and cultural challenges, but also provides unique opportunities. In this article, we discuss the essential issues we faced when successfully transforming a collection of various independently developed software libraries into one large integrated product: the eXtreme-scale scientific Software Development Kit (xSDK). We argue it is just as important to pay attention to cultural challenges, such as establishment of reliable communication channels that considers, among others, differences in personalities and backgrounds as well as overcoming geographical separation and time-zone distribution when collaborating, as technical challenges. Finally, we discuss opportunities stemming from participating in a large diverse software team, such as increased internal expertise, variety of skillsets, broadened connections to external experts, and access to a larger pool of ideas or solutions. IEEE},
author_keywords={Computers;  Cultural differences;  Interoperability;  Libraries;  Software;  Sustainable development;  Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tu2022,
author={Tu, H. and Jiang, H. and Zhou, Z. and Tang, Y. and Ren, Z. and Qiao, L. and Jiang, L.},
title={Detecting C++ Compiler Front-End Bugs via Grammar Mutation and Differential Testing},
journal={IEEE Transactions on Reliability},
year={2022},
pages={1-15},
doi={10.1109/TR.2022.3171220},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130473202&doi=10.1109%2fTR.2022.3171220&partnerID=40&md5=a8645f084b01b22b27c4e266274bb3f1},
affiliation={School of Software, Dalian University of Technology, Dalian 188065 China, and also with the School of Computing and Information Systems, Singapore Management University Singapore 116024; School of Software, Dalian University of Technology, Dalian 116024 China, also with and Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province China, and also with the DUT Artificial Intelligence, Dalian 116024 China; Beijing Institute of Control Engineering, Beijing 100190 China; School of Computing and Information Systems, Singapore Management University Singapore 188065},
abstract={C++ is a widely used programming language and the C++ front-end is a critical part of a C++ compiler. Although many techniques have been proposed to test compilers, few studies are devoted to detecting bugs in C++ compiler. In this study, we take the first step to detect bugs in C++ compiler front-ends. To do so, two main challenges need to be addressed, namely, the acquisition of test programs that are more likely to trigger bugs in compiler front-ends and the bug identification from complicated compiler outputs. In this article, we propose a novel framework named Ccoft to detect bugs in C++ compiler front-ends. To address the first challenge, Ccoft implements a practical program generator. The generator first transforms C++ grammars into a flexible structured format and then utilizes an equal-chance selection (ECS) strategy to conduct structure-aware grammar mutation to generate diverse C++ programs. Next, Ccoft employs a set of differential testing strategies to identify various kinds of bugs in C++ compiler front-ends by comparing complex outputs emitted by C++ compilers, thus tackling the second challenge. Empirical evaluation results over two mainstream compilers (i.e., GCC and Clang) show that Ccoft greatly improves two state-of-the-art approaches (i.e., Dharma and Grammarinator) by 135&#x0025; and 111&#x0025; in terms of the numbers of detected bugs, respectively. By running Ccoft for three months, we have successfully reported 136 bugs for two C++ compilers, of which 78 (57 confirmed, assigned, or fixed) for GCC and 58 (10 confirmed or fixed) for Clang. IEEE},
author_keywords={Automated testing;  C++ languages;  compiler defect;  compiler testing;  Computer bugs;  front-end;  Grammar;  Program processors;  reliability;  Software;  software testing;  Syntactics;  Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Din2022,
author={Din, F. and Khalid, S. and Fayaz, M. and Gwak, J. and Zamli, K.Z. and Mashwani, W.K.},
title={Fuzzy Adaptive Teaching Learning-Based Optimization for Solving Unconstrained Numerical Optimization Problems},
journal={Mathematical Problems in Engineering},
year={2022},
volume={2022},
doi={10.1155/2022/2221762},
art_number={2221762},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129964216&doi=10.1155%2f2022%2f2221762&partnerID=40&md5=9405bd2c97f63ace282aecb613676928},
affiliation={Department Of Computer Science And It, University Of Malakand, Kpk, Pakistan; Department Of Computer Science, University Of Central Asia, Naryn, Kyrgyzstan; Department Of Software, Korea National University Of Transportation, Chungju, 27469, South Korea; Department Of Biomedical Engineering, Korea National University Of Transportation, Chungju, 27469, South Korea; Department Of Ai Robotics Engineering, Korea National University Of Transportation, Chungju, 27469, South Korea; Department Of It And Energy Convergence (BK21 FOUR), Korea National University Of Transportation, Chungju, 27469, South Korea; Faculty Of Computing, Universiti Malaysia Pahang, Pahang Darul Makmur, Pekan, 26600, Malaysia; Faculty Of Science And Technology, Universitas Airlangga, C Campus JI. Dr. H. Soekamo, Mulyorejo, Surabaya, 60115, Indonesia; Institute Of Numerical Sciences, Kohat University Of Science And Technology, Kpk, Pakistan},
abstract={Teaching learning-based optimization is one of the widely accepted metaheuristic algorithms inspired by teaching and learning within classrooms. It has successfully addressed several real-world optimization problems, but it may still be trapped in local optima and may suffer from the problem of premature convergence in the case of solving some challenging optimization problems. To overcome these drawbacks and to achieve an appropriate percentage of exploitation and exploration, this study presents a new modified teaching learning-based optimization algorithm called the fuzzy adaptive teaching learning-based optimization algorithm. The proposed fuzzy adaptive teaching learning-based optimization algorithm uses three measures from the search space, namely, quality measure, diversification measure, and intensification measure. As the 50-50 probabilities for exploitation and exploration in the basic teaching learning-based optimization algorithm may be counterproductive, the Mamdani-type fuzzy inference system of the new algorithm takes these measures as a crisp inputs and generates selection as crisp output to choose either exploitation or exploration based on the current search requirement. This fuzzy-based adaptive selection helps to adequately balance global search or exploration and local search or exploitation operations during the search process as these operations are intrinsically dynamic. The performance of the fuzzy adaptive teaching learning-based optimization is evaluated against other metaheuristic algorithms including basic teaching learning-based optimization on 23 unconstrained global test functions. Moreover, adaptive teaching learning-based optimization is used to search for near-optimal values for the four parameters of the COCOMO II model, which are then tested for validity on a software project of NASA. Analysis and comparison of the obtained results indicate the efficiency and competitiveness of the proposed algorithm in addressing unconstrained continuous optimization tasks. © 2022 Fakhrud Din et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rath2022,
author={Rath, S.K. and Sahu, M. and Das, S.P. and Mohapatra, S.K.},
title={Hybrid Software Reliability Prediction Model Using Feature Selection and Support Vector Classifier},
journal={2022 International Conference on Emerging Smart Computing and Informatics, ESCI 2022},
year={2022},
doi={10.1109/ESCI53509.2022.9758339},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129683368&doi=10.1109%2fESCI53509.2022.9758339&partnerID=40&md5=069e2008c52b5b4deb679bb8b73f1cb4},
affiliation={C. V. Raman Global University, Dept. of Computer Science Engineering, Odisha, Bhubaneswar, India; Birla Global University, Dept. of Computer Science Engineering, Odisha, Bhubaneswar, India; Chitkara University Institute of Engineering and Technology, Chitkara University, Punjab, India},
abstract={The primary purpose of the software industry is to provide high-quality software. Software system failure is caused by faulty software components. The goal of reliable software is to reduce the amount of software programme failures. Software defect prediction is a crucial aspect of developing high-quality software. One can predict software failures by implement essential prediction metrics and previous fault information. A good software fault prediction model makes testing easier while also improving the quality and consistency of software. For defect prediction systems based on diverse parameters, several methodologies have been proposed. However, none of the models meet the criteria for software reliability defect prediction. So in this article we proposed a hybrid software reliability model using feature selection and support vector classifier. In terms of software reliability defect prediction, the provided methodology is acceptable for different software metrics with experimental approvals utilizing a standard dataset. In the methodology, the NASA Metrics Data Program datasets are used for real-time verification and validation. © 2022 IEEE.},
author_keywords={prediction defect model;  quality software;  software fault prediction Introduction;  software metrics;  Support vector machines},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yandrapally2022,
author={Yandrapally, R.K. and Mesbah, A.},
title={Fragment-Based Test Generation For Web Apps},
journal={IEEE Transactions on Software Engineering},
year={2022},
doi={10.1109/TSE.2022.3171295},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129586706&doi=10.1109%2fTSE.2022.3171295&partnerID=40&md5=8efa386f475e53a1a71432a395d95265},
affiliation={Electrical and Computer Engineering, The University of British Columbia Faculty of Applied Science, 120487 Vancouver, British Columbia, Canada},
abstract={Automated model-based test generation presents a viable alternative to the costly manual test creation currently employed for regression testing of web apps. However, existing model inference techniques rely on threshold-based whole-page comparison to establish state equivalence, which cannot reliably identify near-duplicate web pages in modern web apps. Consequently, existing techniques produce inadequate models for dynamic web apps, and fragile test oracles, rendering the generated regression test suites ineffective. We propose a model-based test generation technique, FRAGGEN, that eliminates the need for thresholds, by employing a novel state abstraction based on page fragmentation to establish state equivalence. FRAGGEN also uses fine-grained page fragment analysis to diversify state exploration and generate reliable test oracles. Our evaluation shows that FRAGGEN outperforms existing whole-page techniques by detecting more near- duplicates, inferring better web app models and generating test suites that are better suited for regression testing. On a dataset of 86,165 state-pairs, FRAGGEN detected 123% more near-duplicates on average compared to whole-page techniques. The crawl models inferred by FRAGGEN have 62% more precision and 70% more recall on average. FRAGGEN also generates reliable regression test suites with test actions that have nearly 100% success rate on the same version of the web app even if the execution environment is varied. The test oracles generated by FRAGGEN can detect 98.7% of the visible changes in web pages while being highly robust, making them suitable for regression testing. IEEE},
author_keywords={Analytical models;  Load modeling;  Test pattern generators;  Testing;  Uniform resource locators;  Visualization;  Web pages},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reiz2022105,
author={Reiz, A. and Fellmann, M. and Sinervo, L.-M. and Heinonen, A.},
title={Selecting the Right Software for Supporting Participatory Budgeting in Local Government – Reviewing Suitable Solutions},
journal={Communications in Computer and Information Science},
year={2022},
volume={1529 CCIS},
pages={105-118},
doi={10.1007/978-3-031-04238-6_9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128938963&doi=10.1007%2f978-3-031-04238-6_9&partnerID=40&md5=509ae154ba8cad9b411be3eb0cdeffa0},
affiliation={Rostock University, Rostock, 18051, Germany; Faculty of Management and Business, Tampere University, Tampere, 33014, Finland; LAB University of Applied Sciences, Lahti, Finland},
abstract={Participatory Budgeting (PB) empowers the constituents to decide on how to spend a part of the public money. The citizens create proposals, which are then (if they are within the given rules of the PB) voted on by the public. It is believed that PBs strengthen democracy and increase the efficiency of public spending. Information and communication technology (ICT) can support these PB initiatives. There are several software solutions available for implementing a PB. However, picking the right solution is far from an easy task as the solutions are as diverse as the needs and possible requirements of administrations. This paper scrutinizes different solutions in aiming to provide support for aspiring municipalities in selecting the right PB software. The following work sheds light on the differences between the available software solutions. First, we shortly describe the applications and then lay out the tested capabilities of the software. Afterward, we show the fulfillment level of these capabilities and present an excel tool for making individual, informed decisions. The paper is concluded with a description of the tool selection process in two Finnish municipalities. © 2022, Springer Nature Switzerland AG.},
author_keywords={ICT;  Participatory budgeting;  Review;  Software analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Irfan2022,
author={Irfan, A. and Khan, M.G. and Amin, A.A. and Mohsin, S.A. and Adnan, M. and Zulfiqar, A.},
title={Model-Based Design, HIL Testing, and Rapid Control Prototyping of a Low-Cost POC Quadcopter with Stability Analysis and Control},
journal={Complexity},
year={2022},
volume={2022},
doi={10.1155/2022/1492170},
art_number={1492170},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128299128&doi=10.1155%2f2022%2f1492170&partnerID=40&md5=8e1dbbc6ceb80d07576c173892b17c1c},
affiliation={Department of Electrical Engineering, National University of Computer and Emerging Sciences, Chiniot-Faisalabad Campus, Islamabad, 44000, Pakistan},
abstract={Unmanned aerial vehicles (UAVs), particularly quadcopters, have several medical, agriculture, surveillance, and security applications. However, the use of this innovative technology for civilian applications is still very limited in low-income countries due to the high cost, whereas low-cost controllers available in the market are often tuned using the hit and trial approach and are limited for specific applications. This paper addresses this issue and presents a novel proof of concept (POC) low-cost quadcopter UAV design approach using a systematic Model-Based Design (MBD) method for mathematical modeling, simulation, real-time testing, and prototyping. The quadcopter dynamic model is developed, and controllers are designed using Proportional Integral, and Derivative (PID), Pole Placement, and Linear Quadratic Regulator (LQR) control strategies. The stability of the controllers is also checked using Lyapunov stability analysis. For verification and validation (V&V) of the design, Software-in-the-Loop, Processor-in-the-Loop, Hardware-in-the-loop testing, and Rapid Control Prototyping have been performed. The V&V methods of the MBD approach showed practically valid results with a stable flight of the quadcopter prototype. The proposed low-cost POC quadcopter design approach can be easily modified to have enhanced features, and quadcopters with different design parameters can be assembled using this approach for a diverse range of applications. © 2022 Abdullah Irfan et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hoffmann2022,
author={Hoffmann, M. and Mendez, D. and Fagerholm, F. and Luckhardt, A.},
title={The human side of Software Engineering Teams: an investigation of contemporary challenges},
journal={IEEE Transactions on Software Engineering},
year={2022},
doi={10.1109/TSE.2022.3148539},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124763511&doi=10.1109%2fTSE.2022.3148539&partnerID=40&md5=32ebd8e9ac6f8e09c5eea4f3f839a667},
affiliation={Software and System Engineering, TUM, 9184 Munchen, Bavaria, Germany, (e-mail: marcohoffmann2002@web.de); Software Engineering Research Lab, Blekinge Institute of Technology, 4206 Karlskrona, Blekinge, Sweden, (e-mail: daniel.mendez@bth.se); Department of Computer Science, Blekinge Institute of Technology, 4206 Karlskrona, Blekinge, Sweden, (e-mail: fabian.fagerholm@aalto.fi); Software and System Engineering, TUM, 9184 Munchen, Bavaria, Germany, (e-mail: luckhardt@fortiss.org)},
abstract={There have been numerous recent calls for research on the human side of software engineering and its impact on project success. An analysis of which challenges in software engineering teams are most relevant and frequent is still missing. As teams are more international, it is more frequent that their members have different personal values as well as different communication habits. Additionally, virtual team setups (working geographically separated, remote communication using digital tools and frequently changing team members) are increasingly prevalent. We designed a survey instrument and asked respondents to assess the frequency and criticality of a set of challenges, both within teams as well as between teams and clients. For the team challenges, we asked if mitigation measures were already in place to tackle the challenge. Respondents were also asked to provide information about their team setup. The survey included an instrument to measure Schwartz human values. The survey was first piloted and then distributed to professionals working in software engineering teams. In this article, we report on the results obtained from 192 survey respondents. We present a set of challenges that takes the survey feedback into account and introduce two categories of challenges; inter-personal and intra-personal. We found no evidence for links between personality values and challenges. We found some significant links between the number of distinct nationalities in a team and certain challenges. We found evidence that a higher degree of virtualization leads to an increase of the frequency of some human challenges. We present a set of human challenges in software engineering that can be used for further research on causes and mitigation measures, which serves as our starting point for a theory about causes of contemporary human challenges in software engineering teams. Our findings warrants further research on human challenges in software engineering and gather more evidence and test countermeasures, such as whether the employment of virtual reality software incorporating facial expressions and movements can help establish a less detached way of communication. IEEE},
author_keywords={Cultural differences;  Diversity;  Human Challenges;  Human factors;  Human Values;  Productivity;  Software;  Software engineering;  Software Engineering;  Survey Research;  Virtual groups;  Virtual Teams;  Virtualization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2022,
author={Chen, L. and Wu, J. and Yang, H. and Zhang, K.},
title={Does PageRank apply to service ranking in microservice regression testing?},
journal={Software Quality Journal},
year={2022},
doi={10.1007/s11219-021-09579-6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123484468&doi=10.1007%2fs11219-021-09579-6&partnerID=40&md5=5e9b85c7413d39e98d724baa58be6820},
affiliation={School of Computer Science and Engineering, Beijing University of Aeronautics and Astronautics, Beijing, 100191, China},
abstract={Regression testing is required in each development iteration of microservice systems. Test case prioritization, which improves the fault detection rate by optimizing the execution order of test cases, is one of the main techniques to optimize regression testing. Existing test case prioritization techniques mainly rely on artifacts such as codes and system models, which are limited to microservice systems with service autonomy, development method diversity, and large service scale. This paper proposes a test case prioritization approach based on service ranking referred to as TCP-SR. TCP-SR ranks the services based on API gateway logs. The weights of test cases are calculated with the result of service ranking, which could be used to order test cases with single-objective and multi-objective strategies. To evaluate the effectiveness of TCP-SR, the empirical study based on four microservice systems is presented. The results show that the fault detection rate of TCP-SR is almost twice as high as that of the random prioritization technique, and almost the same as the prioritization technique based on WS-BPEL but requires much less prioritization time cost. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={API gateway;  Microservice;  PageRank;  Regression testing;  Test case prioritization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Menendez2022295,
author={Menendez, H.D. and Boreale, M. and Gorla, D. and Clark, D.},
title={Output Sampling for Output Diversity in Automatic Unit Test Generation},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={1},
pages={295-308},
doi={10.1109/TSE.2020.2987377},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123183897&doi=10.1109%2fTSE.2020.2987377&partnerID=40&md5=09e017de477f100b23dcb41ab5e5ca5f},
affiliation={Department of Computer Science, Middlesex University London, London, NW4 4BT, United Kingdom; University of Florence, FI, Firenze, 50121, Italy; Department of Computer Science, University of Rome Sapienza, Roma, 00100, Italy; Department of Computer Science, University College London, London, WC1E 6BT, United Kingdom},
abstract={Diverse test sets are able to expose bugs that test sets generated with structural coverage techniques cannot discover. Input-diverse test set generators have been shown to be effective for this, but also have limitations: e.g., they need to be complemented with semantic information derived from the Software Under Test. We demonstrate how to drive the test set generation process with semantic information in the form of output diversity. We present the first totally automatic output sampling for output diversity unit test set generation tool, called OutGen. OutGen transforms a program into an SMT formula in bit-vector arithmetic. It then applies universal hashing in order to generate an output-based diverse set of inputs. The result offers significant diversity improvements when measured as a high output uniqueness count. It achieves this by ensuring that the test set's output probability distribution is uniform, i.e., highly diverse. The use of output sampling, as opposed to any of input sampling, CBMC, CAVM, behaviour diversity or random testing improves mutation score and bug detection by up to 4150 and 963 percent respectively on programs drawn from three different corpora: the R-project, SIR and CodeFlaws. OutGen test sets achieve an average mutation score of up to 92 percent, and 70 percent of the test sets detect the defect. Moreover, OutGen is the only automatic unit test generation tool that is able to detect bugs on the real number C functions from the R-project. © 1976-2012 IEEE.},
author_keywords={OutGen;  output diversity;  output sampling;  SMT solver;  Unit testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Qian2022123,
author={Qian, N.},
title={3D Jewelry Design System Based on K-means Algorithm},
journal={Lecture Notes on Data Engineering and Communications Technologies},
year={2022},
volume={102},
pages={123-129},
doi={10.1007/978-981-16-7466-2_13},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121376088&doi=10.1007%2f978-981-16-7466-2_13&partnerID=40&md5=567af906dd3b22cf01a8c80d3ef2a17c},
affiliation={Fashion Design School, Shanghai Institute of Visual Art, Shanghai, China},
abstract={In the context of Industry 4.0, the inevitable trend of the development of smart manufacturing zombie manufacturing is the inevitable direction of the transformation and upgrading of traditional industries. The distinguishing characteristics of intelligent manufacturing are informatization, networking, automation, integration, and digitization. It is the deep penetration of information technology and Internet technology in the industrial field. Based on the traditional jewelry design method can no longer meet the diverse aesthetic needs of the public, the emergence of algorithm technology as a new technology provides more possibilities and greater development space for jewelry art. This article aims to study a 3D jewelry design system based on K-means algorithm. Based on the analysis of the basic principles of 3D technology, the advantages of K-means algorithm in jewelry design, and the optimization of 3D model details, a jewelry 3D design system is designed. Including modules such as user interface, login module and jewelry attribute selection, the system was finally tested. The test results show that the system is in the testing phase and strictly follows the software testing process. The entire testing process is an iterative process, and the functional test finally has no bugs. It is found that the performance meets the current needs of users and choices. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Design system;  Jewelry design;  K-means algorithm;  Three-dimensional design},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mohamed202271,
author={Mohamed, Z.S. and Dash, S.P.},
title={An Explorative Study on Material Feasibility for Relief Shelter for Refugees},
journal={Lecture Notes in Civil Engineering},
year={2022},
volume={202},
pages={71-95},
doi={10.1007/978-981-16-6978-1_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119354036&doi=10.1007%2f978-981-16-6978-1_6&partnerID=40&md5=7649fe81098cb684b67119fac984c6b9},
affiliation={Manipal School of Architecture and Planning, MAHE, Manipal, Udupi, Karnataka, India},
abstract={There has been an unprecedented growth in the refugees’ crisis due to humanitarian emergencies like countries’ political situation, climate change, and related natural disasters. The report of United Nations High Commissioner for refugees 2018 stated that there has been a rapid increase in the displaced population from 43.3 million to 70.8 million between 2009 and 2018. In this regard, the role of refugee shelters plays a salient role in providing housing to these homeless. International discourse has increasingly focused on the unique way in which displaced people live in refugee shelters. The material for construction is the major challenge to accommodate the number of people with varied weather conditions. Hence, it is imperative to explore different kinds of materials that can be appropriate in the process of designing sustainable refugee shelters. The aim of the paper is to explore different kinds of materials that are competent in designing a viable shelter without compromising with the comfort and aspirations of the users. The various materials were selected based on diverse parameters; a model was developed utilizing the material which was simulated (using Sefaira software) to test how effectively the material would serve in the given region. The results aided in interpreting the optimum ranges within the dwelling for adequate thermal comfort of the users. This study, therefore, suggests recommendations to support the design of spatial and architectural solutions for shelter design for refugees with suitable material selection during the emergency approach. © 2022, The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.},
author_keywords={Refugee shelters;  Refugees’ crisis;  Shelter design;  Thermal comfort;  Viable shelter},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Marques2022,
author={Marques, H. and Laranjeiro, N. and Bernardino, J.},
title={Injecting software faults in Python applications: The OpenStack case study},
journal={Empirical Software Engineering},
year={2022},
volume={27},
number={1},
doi={10.1007/s10664-021-10047-9},
art_number={20},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118689455&doi=10.1007%2fs10664-021-10047-9&partnerID=40&md5=c3406de66580de74d62c62bf837d1fc4},
affiliation={Polytechnic of Coimbra, Coimbra Institute of Engineering (ISEC), Coimbra, Portugal; University of Coimbra, Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, Coimbra, Portugal},
abstract={Software fault injection techniques have been largely used as means for evaluating the dependability of systems in presence of certain types of faults. Despite the large diversity of tools offering the possibility of emulating the presence of software faults, there is little practical support for emulating the presence of software faults in Python applications, which are increasingly being used to support business critical cloud services. In this paper, we present FIT4Python, a tool for injecting software faults in Python code and then use it, in a mutation testing campaign, to analyse the effectiveness of OpenStack’s test suite against new probable software faults. We begin by analysing the types of faults affecting Nova Compute, the core component of OpenStack. We use our tool to emulate the presence of new faults in Nova Compute API to understand how well OpenStack’s battery of unit, functional, and integration tests cover these new, but probable, situations. Results show clear limitations in the effectiveness of OpenStack developers’ test suites, with many cases of injected faults passing undetected through all three types of tests and that nearly half of the analysed problems could be detected with trivial changes or additions to the unit tests. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Bug reports;  Dependability evaluation;  Fault injection;  Mutation testing;  Orthogonal defect classification;  Software faults},
document_type={Article},
source={Scopus},
}

@ARTICLE{Odili2022160,
author={Odili, J.B. and Nasser, A.B. and Noraziah, A. and Wahab, M.H.A. and Ahmed, M.},
title={African Buffalo Optimization Algorithm Based T-Way Test Suite Generation Strategy for Electronic-Payment Transactions},
journal={Lecture Notes in Networks and Systems},
year={2022},
volume={299},
pages={160-174},
doi={10.1007/978-3-030-82616-1_15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113467798&doi=10.1007%2f978-3-030-82616-1_15&partnerID=40&md5=bfb6e06b0d1c1d3e76d4abbab7669a2f},
affiliation={Faculty of Science and Science Education, Anchor University Lagos, Lagos, Nigeria; Faculty of Computing, Universiti Malaysia Pahang, Kuantan, 26300, Malaysia; Centre for Software Development and Integrated Computing, Pekan, Malaysia; Advanced Communication Research Center, Universiti Tun Hussein Onn Malaysia, Parit Raja, Malaysia},
abstract={The use of meta-heuristics in Combinatorial Interaction Testing (CIT) is becoming more and more popular due to their effectiveness and efficiency over the traditional methods especially in authenticating electronic payment (e-payment) transactions. Concomitantly, over the past two decades, there has been a rise both in the development of metaheuristics and their application to diverse theoretical and practical areas including CIT in e-payments. In the implementation of t-way strategies (the t is used to represent the interaction strength), mixed results have been reported; some very exciting but, in other cases, the performance of metaheuristics has been, to say the least, below par. This mixed trend has led many researchers to explore alternate ways of improving the effectiveness and efficiency of metaheuristics in CIT, hence this study. It must be emphasized, however, that available literature indicates that no particular metaheuristic testing strategy has had consistent superior performance over the others in diverse testing environments and configurations. The need for effectiveness, therefore, necessitates the need for algorithm hybridization to deploy only the component parts of algorithms that have been proven to enhance overall search capabilities while at the same time eliminating the demerits of particular algorithms in the hybridization procedure. In this paper, therefore, a hybrid variant of the African Buffalo Optimization (ABO) algorithm is proposed for CIT. Four hybrid variants of the ABO are proposed through a deliberate improvement of the ABO with four algorithmic components. Experimental procedures indicate that the hybridization of the ABO with these algorithmic components led to faster convergence and greater effectiveness superior to the outcomes of existing techniques, thereby placing the algorithm among the best when compared with other methods/techniques. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={African Buffalo Optimization;  Combinatorial problem;  e-payment system;  Optimization techniques;  Software testing;  T-way testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Franco2021,
author={Franco, A. and Moernaut, J. and Schneider-Muntau, B. and Strasser, M. and Gems, B.},
title={Triggers and consequences of landslide-induced impulse waves – 3D dynamic reconstruction of the Taan Fiord 2015 tsunami event},
journal={Engineering Geology},
year={2021},
volume={294},
doi={10.1016/j.enggeo.2021.106384},
art_number={106384},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115910573&doi=10.1016%2fj.enggeo.2021.106384&partnerID=40&md5=caec0d38b6a2f9ae4633c6496c170492},
affiliation={Unit of Hydraulic Engineering, University of Innsbruck, Technikerstraße 13, Innsbruck, 6020, Austria; Institute of Geology, University of Innsbruck, Innrain 52f, Innsbruck, 6020, Austria; Unit of Geotechnical and Tunnel Engineering, University of Innsbruck, Technikerstraße 13, Innsbruck, 6020, Austria},
abstract={Natural multi-hazards as landslide-induced tsunamis require a multi-disciplinary approach to analyze the cascade effects that pose a significant threat to mountain communities and the surrounding territory. This paper comprises a detailed study of both the landslide evolution and the wave dynamics of the October 2015 Taan Fiord (Alaska) tsunami event, which represents a highly valuable case study for generic methodology development of single code applications using the numerical software Flow3D and testing its applicability for cascading wave hazard evaluation. First, a geomorphological analysis of the unstable slope is performed by elaborating diverse digital elevation models, where a significant vertical displacement of −90 m is observed before the final collapse, and the influence of listric faults within the slide body results in a bulging of the glacier at the toe. Data from time-series analyses suggests that the glacier retreat (and the reduction of local buttresses) critically destabilized the slope leading to the October 2015 catastrophic failure. The reconstructed landslide volume is estimated to be 49.4 Mm3, where 26 Mm3 entered the fiord and triggered the tsunami. Second, wave dynamics are recreated with Flow3D. Both dense fluid and granular media models are used and compared to verify their performance in initiating the impulse wave, where a measured impact speed ranging between 32 and 49 ms−1 triggers a maximum wave amplitude of about 95–99 m. the maximum run-up of 193 m at the Hoof Hill Fan is recreated with both approaches, but general overestimations (about 9–12%) compared to the observations, in the impact area, are computed for the inundation process. A good approximation of the observed run-up along the entire length of the fiord is found for wave propagation models using the granular media approach. Beyond the Taan-Fiord case study and for evaluation of cascading landslide-induced hydraulic hazard in other settings, this work points out (i) the necessity of using a high temporal resolution of digital elevation models to analyze the multi-stage slope failure and to properly estimate the landslide volume, (ii) the applicability of the applied numerical models to reproduce the wave dynamics of a landslide-induced tsunami event on one code only, and (iii) how these models can be adopted to develop hazard maps related to potential wave hazards in natural basins. © 2021 The Author(s)},
author_keywords={Flow3D;  Granular media;  Landslide-induced impulse wave;  Multi-hazards;  Wave dynamics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Singh2021,
author={Singh, S.P. and Dhiman, G. and Tiwari, P. and Jhaveri, R.H.},
title={A soft computing based multi-objective optimization approach for automatic prediction of software cost models},
journal={Applied Soft Computing},
year={2021},
volume={113},
doi={10.1016/j.asoc.2021.107981},
art_number={107981},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118154933&doi=10.1016%2fj.asoc.2021.107981&partnerID=40&md5=bc6710c1940e597c15939e36e33d4fe7},
affiliation={CSED, Bundelkhand Institute of Engineering and Technology Jhansi, Jhansi, UP, India; Department of Computer Science, Government Bikram College of Commerce, Patiala, India; Department of Computer Science, Aalto University, Espoo, Finland; Department of Computer Science & Engineering, Pandit Deendayal Energy University, Gandhinagar, India},
abstract={This paper tries to extend the idea of single-objective differential evolution (DE) algorithm to a multi-objective algorithm. Most of the existing algorithms face the problem of diversity loss and convergence rate. In this paper, we propose a novel multi-objective DE algorithm to deal with this problem. In the validation process, the proposed method is validated in two steps. Firstly, the new homeostasis factor-based mutation operator incorporates multi-objective differential evolution algorithms (MODE). In this method, we use the Pareto optimality principle. We incorporate a new adaptive-based mutation operator (MODE) to create more diversity and enhance convergence rate among candidate solutions which provide better solutions to help the evolution. The effectiveness of the proposed method is evaluated on eight benchmarks of bi-objective and tri-objective test functions. Our proposed method performed well compared to the latest variants of multi-objective evolutionary algorithms (MOEAs). Secondly, the proposed method is used for an application-based test by applying it for software cost estimation. This method also incorporates multi-objective parameters, i.e., two objectives-based software cost estimation and three objectives-based software cost estimation. The proposed approach achieves better results in most software projects in terms of reducing effort and minimum error. © 2021 Elsevier B.V.},
author_keywords={Adaptation;  Multiobjective differential evolution;  Multiobjective evolutionary algorithms;  Optimization;  Software cost estimation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Carpenter2021,
author={Carpenter, C.M. and Frank, D.N. and Williamson, K. and Arbet, J. and Wagner, B.D. and Kechris, K. and Kroehl, M.E.},
title={tidyMicro: a pipeline for microbiome data analysis and visualization using the tidyverse in R},
journal={BMC Bioinformatics},
year={2021},
volume={22},
number={1},
doi={10.1186/s12859-021-03967-2},
art_number={41},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100273274&doi=10.1186%2fs12859-021-03967-2&partnerID=40&md5=6fdc017b70a37878e63a21e8746674bf},
affiliation={Department of Biostatistics and Informatics, Colorado School of Public Health, University of Colorado Anschutz Medical Campus, Aurora, CO, United States; Division of Infectious Diseases, Department of Medicine, University of Colorado Anschutz Medical Campus, Denver, CO, United States},
abstract={Background: The drive to understand how microbial communities interact with their environments has inspired innovations across many fields. The data generated from sequence-based analyses of microbial communities typically are of high dimensionality and can involve multiple data tables consisting of taxonomic or functional gene/pathway counts. Merging multiple high dimensional tables with study-related metadata can be challenging. Existing microbiome pipelines available in R have created their own data structures to manage this problem. However, these data structures may be unfamiliar to analysts new to microbiome data or R and do not allow for deviations from internal workflows. Existing analysis tools also focus primarily on community-level analyses and exploratory visualizations, as opposed to analyses of individual taxa. Results: We developed the R package “tidyMicro” to serve as a more complete microbiome analysis pipeline. This open source software provides all of the essential tools available in other popular packages (e.g., management of sequence count tables, standard exploratory visualizations, and diversity inference tools) supplemented with multiple options for regression modelling (e.g., negative binomial, beta binomial, and/or rank based testing) and novel visualizations to improve interpretability (e.g., Rocky Mountain plots, longitudinal ordination plots). This comprehensive pipeline for microbiome analysis also maintains data structures familiar to R users to improve analysts’ control over workflow. A complete vignette is provided to aid new users in analysis workflow. Conclusions: tidyMicro provides a reliable alternative to popular microbiome analysis packages in R. We provide standard tools as well as novel extensions on standard analyses to improve interpretability results while maintaining object malleability to encourage open source collaboration. The simple examples and full workflow from the package are reproducible and applicable to external data sets. © 2021, The Author(s).},
author_keywords={Microbiome;  Open source;  Pipeline;  R;  Tidyverse;  Visualization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jakobs2021847,
author={Jakobs, M.-C.},
title={CoVeriTest: interleaving value and predicate analysis for test-case generation},
journal={International Journal on Software Tools for Technology Transfer},
year={2021},
volume={23},
number={6},
pages={847-851},
doi={10.1007/s10009-020-00572-1},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087622370&doi=10.1007%2fs10009-020-00572-1&partnerID=40&md5=c22d1a6abffe02bbf7c88a2771dac021},
affiliation={LMU Munich, Munich, Germany; TU Darmstadt, Darmstadt, Germany},
abstract={Verification techniques are well-suited for automatic test-case generation. They basically need to check the reachability of every test goal and generate test cases for all reachable goals. This is also the basic idea of our CoVeriTest submission. However, the set of test goals is not fixed in CoVeriTest , instead we can configure the set of test goals. For Test-Comp’19, we support the set of all __VERIFIER_error() calls as well as the set of all branches. Thus, we can deal with the two test specifications considered in Test-Comp’19. Since the tasks in Test-Comp are diverse and verification techniques have different strengths and weaknesses, we also do not stick to a single verification technique, but use a hybrid approach that combines multiple techniques. More concrete, CoVeriTest interleaves different verification techniques and allows to configure the cooperation (i.e., information exchange and time limits). To choose from a large set of verification techniques, CoVeriTest is integrated into the analysis framework CPAchecker. For the competition, we interleave CPAchecker’s value and predicate analysis and let both analyses resume their analysis performed in the previous iteration. © 2020, The Author(s).},
author_keywords={Cooperative verification;  CPAchecker;  Model checking;  Software testing;  Test-case generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Madeja2021575,
author={Madeja, M. and Porubän, J. and Bačíková, M. and Sulír, M. and Juhár, J. and Chodarev, S. and Gurbáľ, F.},
title={AUTOMATING TEST CASE IDENTIFICATION in JAVA OPEN SOURCE PROJECTS on GITHUB},
journal={Computing and Informatics},
year={2021},
volume={40},
number={3},
pages={575-605},
doi={10.31577/cai_2021_3_575},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121786949&doi=10.31577%2fcai_2021_3_575&partnerID=40&md5=756fb81ecc52db5b52a5f79a25f5e9cb},
affiliation={Department of Computers and Informatics, Faculty of Electrical Engineering and Informatics, Technical University of Košice, Letná 9, Košice, 042 00, Slovakia},
abstract={Software testing is one of the very important Quality Assurance (QA) components. A lot of researchers deal with the testing process in terms of tester motivation and how tests should or should not be written. However, it is not known from the recommendations how the tests are written in real projects. In this paper, the following was investigated: (i) the denotation of the word "test" in different natural languages; (ii) whether the number of occurrences of the word "test" correlates with the number of test cases; and (iii) what testing frameworks are mostly used. The analysis was performed on 38 GitHub open source repositories thoroughly selected from the set of 4.3M GitHub projects. We analyzed 20 340 test cases in 803 classes manually and 170 k classes using an automated approach. The results show that: (i) there exists a weak correlation (r = 0.655) between the number of occurrences of the word "test" and the number of test cases in a class; (ii) the proposed algorithm using static file analysis correctly detected 97% of test cases; (iii) 15% of the analyzed classes used main() function whose represent regular Java programs that test the production code without using any third-party framework. The identification of such tests is very complex due to implementation diversity. The results may be leveraged to more quickly identify and locate test cases in a repository, to understand practices in customized testing solutions, and to mine tests to improve program comprehension in the future. © 2021 Slovak Academy of Sciences. All rights reserved.},
author_keywords={GitHub;  Java testing;  Open-source projects;  Program comprehension;  Test smells;  Testing practices},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nagy2021351,
author={Nagy, S. and Nguyen-Tuong, A. and Hiser, J.D. and Davidson, J.W. and Hicks, M.},
title={Same Coverage, Less Bloat: Accelerating Binary-only Fuzzing with Coverage-preserving Coverage-guided Tracing},
journal={Proceedings of the ACM Conference on Computer and Communications Security},
year={2021},
pages={351-365},
doi={10.1145/3460120.3484787},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119382970&doi=10.1145%2f3460120.3484787&partnerID=40&md5=ae6004dbfe9457bd888ed347c3de26be},
affiliation={Virginia Tech, Blacksburg, VA, United States; University of Virginia, Charlottesville, VA, United States},
abstract={Coverage-guided fuzzing's aggressive, high-volume testing has helped reveal tens of thousands of software security flaws. While executing billions of test cases mandates fast code coverage tracing, the nature of binary-only targets leads to reduced tracing performance. A recent advancement in binary fuzzing performance is Coverage-guided Tracing (CGT), which brings orders-of-magnitude gains in throughput by restricting the expense of coverage tracing to only when new coverage is guaranteed. Unfortunately, CGT suits only a basic block coverage granularity - -yet most fuzzers require finer-grain coverage metrics: edge coverage and hit counts. It is this limitation which prohibits nearly all of today's state-of-the-art fuzzers from attaining the performance benefits of CGT. This paper tackles the challenges of adapting CGT to fuzzing's most ubiquitous coverage metrics. We introduce and implement a suite of enhancements that expand CGT's introspection to fuzzing's most common code coverage metrics, while maintaining its orders-of-magnitude speedup over conventional always-on coverage tracing. We evaluate their trade-offs with respect to fuzzing performance and effectiveness across 12 diverse real-world binaries (8 open- and 4 closed-source). On average, our coverage-preserving CGT attains near-identical speed to the present block-coverage-only CGT, UnTracer; and outperforms leading binary- and source-level coverage tracers QEMU, Dyninst, RetroWrite, and AFL-Clang by 2 - 24x, finding more bugs in less time. © 2021 ACM.},
author_keywords={binaries;  code coverage;  fuzzing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{He20212526,
author={He, J. and Sivanrupan, G. and Tsankov, P. and Vechev, M.},
title={Learning to Explore Paths for Symbolic Execution},
journal={Proceedings of the ACM Conference on Computer and Communications Security},
year={2021},
pages={2526-2540},
doi={10.1145/3460120.3484813},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119345865&doi=10.1145%2f3460120.3484813&partnerID=40&md5=fdede948f78a6a3ef98fea8ced2ce772},
affiliation={ETH Zurich, Zurich, Switzerland},
abstract={Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states. In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in KLEE, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer AFL to find more paths and security violations. © 2021 ACM.},
author_keywords={fuzzing;  machine learning;  program testing;  symbolic execution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{He20212229,
author={He, X. and Xie, X. and Li, Y. and Sun, J. and Li, F. and Zou, W. and Liu, Y. and Yu, L. and Zhou, J. and Shi, W. and Huo, W.},
title={SoFi: Reflection-Augmented Fuzzing for JavaScript Engines},
journal={Proceedings of the ACM Conference on Computer and Communications Security},
year={2021},
pages={2229-2242},
doi={10.1145/3460120.3484823},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119329474&doi=10.1145%2f3460120.3484823&partnerID=40&md5=5e1cd39551d7e945a4aab9361bd41a42},
affiliation={Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Cyber Security, University of Chinese Academy of Sciences, Beijing, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; Software Engineering Application Technology Lab, Huawei Technologies, Shenzhen, China; Renmin University of China, Beijing, China},
abstract={JavaScript engines have been shown prone to security vulnerabilities, which can lead to serious consequences due to their popularity. Fuzzing is an effective testing technique to discover vulnerabilities. The main challenge of fuzzing JavaScript engines is to generate syntactically and semantically valid inputs such that deep functionalities can be explored. However, due to the dynamic nature of JavaScript and the special features of different engines, it is quite challenging to generate semantically meaningful test inputs. We observed that state-of-the-art semantic-aware JavaScript fuzzers usually require manually written rules to analyze the semantics for a JavaScript engine, which is labor-intensive, incomplete and engine-specific. Moreover, the error rate of generated test cases is still high. Another challenge is that existing fuzzers cannot generate new method calls that are not included in the initial seed corpus or pre-defined rules, which limits the bug-finding capability. To this end, we propose a novel semantic-aware fuzzing technique named SoFi. To guarantee the validity of the generated test cases, SoFi adopts a fine-grained program analysis to identify available variables and infer types of these variables for the mutation. Moreover, an automatic repair strategy is proposed to repair syntax/semantic errors in invalid test cases. To improve the exploration capability of SoFi, we propose a reflection-based analysis to identify unseen attributes and methods of objects, which are further used in the mutation. With fine-grained analysis and reflection-based augmentation, SoFi can generate more valid and diverse test cases. Besides, SoFi is general in different JavaScript engines without any manual configuration (e.g., the grammar rules). The evaluation results have shown that SoFi outperforms state-of-the-art techniques in generating semantically valid inputs, improving code coverage and detecting more bugs. SoFi discovered 51 bugs in popular JavaScript engines, 28 of which have been confirmed or fixed by the developers and 10 CVE IDs have been assigned. © 2021 ACM.},
author_keywords={fuzzing;  vulnerability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Manes20212312,
author={Manes, V.J.M. and Han, H. and Han, C. and Cha, S.K. and Egele, M. and Schwartz, E.J. and Woo, M.},
title={The Art, Science, and Engineering of Fuzzing: A Survey},
journal={IEEE Transactions on Software Engineering},
year={2021},
volume={47},
number={11},
pages={2312-2331},
doi={10.1109/TSE.2019.2946563},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073569517&doi=10.1109%2fTSE.2019.2946563&partnerID=40&md5=f0b4773f046af619aa62cf9bb0058a5b},
affiliation={KAIST Cyber Security Research Center, Daejeon, South Korea; KAIST, Daejeon, South Korea; Naver Corp., Daejeon, South Korea; Boston University, Boston, MA  02215, United States; Software Engineering Institute, Carnegie Mellon University, Pittsburgh, PA  15213, United States},
abstract={Among the many software testing techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective. © 1976-2012 IEEE.},
author_keywords={automated software testing;  fuzz testing;  fuzzing;  Software security},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2021,
title={Onward! 2021 - Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, co-located with SPLASH 2021},
journal={Onward! 2021 - Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, co-located with SPLASH 2021},
year={2021},
page_count={151},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119051165&partnerID=40&md5=50fc1267dbb8444139897b8e04be08e7},
abstract={The proceedings contain 11 papers. The topics discussed include: Dala: a simple capability-based dynamic language design for data race-freedom; programming with neural surrogates of programs; natural language-guided programming; kotlin coroutines: design and implementation; motivating complexity understanding by profiling energy usage; towards self-adaptable languages; programming as architecture, design, and urban planning; the kingdoms of objects and values; let a thousand flowers bloom: on the uses of diversity in software testing; and angelic and demonic visitation: school memories.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Groce2021136,
author={Groce, A.},
title={Let a thousand flowers bloom: On the uses of diversity in software testing},
journal={Onward! 2021 - Proceedings of the 2021 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software, co-located with SPLASH 2021},
year={2021},
pages={136-144},
doi={10.1145/3486607.3486772},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119048357&doi=10.1145%2f3486607.3486772&partnerID=40&md5=2c162eeb8b14a9476c78f78a9280954d},
affiliation={Northern Arizona University, United States},
abstract={Software testing is hard, and a testing problem is composed of many sub-problems with different, often conflicting, solutions. Like many real-world problems, it admits no single optimal solution, but requires dexterity, and the opportunistic combination of many partial solutions. Exploration and experiment, even by practitioners, are important in real-world critical testing efforts. An important set of research results in the field endorse and codify the value of diversity in test generation. However, our current approaches to evaluating research results arguably cut against this fundamental reality: while effective testing may need true diversity, combining many partial answers, the iron logic of the research results section often imposes a totalizing vision where authors must at least pretend to present a monolithic, unitary solution, a new "king of the hill." © 2021 ACM.},
author_keywords={ensemble methods;  research evaluation methods;  software testing;  swarm testing;  test diversity;  test length},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schwartz2021,
author={Schwartz, Y. and Godoy-Shimizu, D. and Korolija, I. and Dong, J. and Hong, S.M. and Mavrogianni, A. and Mumovic, D.},
title={Developing a Data-driven school building stock energy and indoor environmental quality modelling method},
journal={Energy and Buildings},
year={2021},
volume={249},
doi={10.1016/j.enbuild.2021.111249},
art_number={111249},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111014198&doi=10.1016%2fj.enbuild.2021.111249&partnerID=40&md5=b579e175c453446be5540ae21c615b76},
affiliation={UCL IEDE – University College London, Institute for Environmental Design and Engineering, United Kingdom},
abstract={The school building sector has a pivotal role to play in the transition to a low carbon UK economy. School buildings are responsible for 15% of the country's public sector carbon emissions, with space heating currently making up the largest proportion of energy use and associated costs in schools. Children spend a large part of their waking life in school buildings. There is substantial evidence that poor indoor air quality and thermal discomfort can have detrimental impacts on the performance, wellbeing and health of schoolchildren and school staff. Maintaining high indoor environmental quality whilst reducing energy demand and carbon emissions in schools is challenging due to the unique operational characteristics of school environments, e.g. high and intermittent occupancy densities or changes in occupancy patterns throughout the year. Furthermore, existing data show that 81% of the school building stock in England was constructed before 1976. Challenges facing the ageing school building stock may be exacerbated in the context of ongoing and future climate change. In recent decades, building stock modelling has been widely used to quantify and evaluate the current and future energy and indoor environmental quality performance of large numbers of buildings at the neighbourhood, city, regional or national level. Building stock models commonly use building archetypes, which aim to represent the diversity of building stocks through frequently occurring building typologies. The aim of this paper is to introduce the Data dRiven Engine for Archetype Models of Schools (DREAMS), a novel, data-driven, archetype-based school building stock modelling framework. DREAMS enables the detailed representation of the school building stock in England through the statistical analysis of two large scale and highly detailed databases provided by the UK Government: (i) the Property Data Survey Programme (PDSP) from the Department for Education (DfE), and (ii) Display Energy Certificates (DEC). In this paper, the development of 168 building archetypes representing 9,551 primary schools in England is presented. The energy consumption of the English primary school building stock was modelled for a typical year under the current climate using the widely tested and applied building performance software EnergyPlus. For the purposes of modelling validation, the DREAMS space heating demand predictions were compared against average measured energy consumption of the schools that were represented by each archetype. It was demonstrated that the simulated fossil-thermal energy consumption of a typical primary school in England was only 7% higher than measured energy consumption (139 kWh/m2/y simulated, compared to 130 kWh/m2/y measured). The building stock model performs better at predicting the energy performance of naturally ventilated buildings, which constitute 97% of the stock, than that of mechanically ventilated ones. The framework has also shown capabilities in predicting energy consumption on a more localised scale. The London primary school building stock was examined as a case study. School building stock modelling frameworks such as DREAMS can be powerful tools that aid decision-makers to quantify and evaluate the impact of a wide range of building stock-level policies, energy efficiency interventions and climate change scenarios on school energy and indoor environmental performance. © 2021 The Author(s)},
author_keywords={Energy benchmarking;  Energy Consumption in Schools;  Energy Efficiency;  IEQ in Schools;  School Stock Modelling;  Schools in England},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abrecht2021,
author={Abrecht, S. and Gauerhof, L. and Gladisch, C. and Groh, K. and Heinzemann, C. and Woehrle, M.},
title={Testing deep learning-based visual perception for automated driving},
journal={ACM Transactions on Cyber-Physical Systems},
year={2021},
volume={5},
number={4},
doi={10.1145/3450356},
art_number={37},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116015581&doi=10.1145%2f3450356&partnerID=40&md5=7fe6a18118c9d7e10ce3d791abf1e244},
affiliation={Robert Bosch GmbH, Robert-Bosch-Campus 1, Renningen, 71272, Germany},
abstract={Due to the impressive performance of deep neural networks (DNNs) for visual perception, there is an increased demand for their use in automated systems. However, to use deep neural networks in practice, novel approaches are needed, e.g., for testing. In this work, we focus on the question of how to test deep learning-based visual perception functions for automated driving. Classical approaches for testing are not sufficient: A purely statistical approach based on a dataset split is not enough, as testing needs to address various purposes and not only average case performance. Additionally, a complete specification is elusive due to the complexity of the perception task in the open context of automated driving. In this article, we review and discuss existing work on testing DNNs for visual perception with a special focus on automated driving for test input and test oracle generation as well as test adequacy. We conclude that testing of DNNs in this domain requires several diverse test sets. We show how such tests sets can be constructed based on the presented approaches addressing different purposes based on the presented methods and identify open research questions. © 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={automated driving;  autonomous driving;  computer vision;  deep learning;  perception;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nguyen20213801,
author={Nguyen, T. and Xu, X. and Henry, F. and Liao, R.-L. and Sarwer, M.G. and Karczewicz, M. and Chao, Y.-H. and Xu, J. and Liu, S. and Marpe, D. and Sullivan, G.J.},
title={Overview of the Screen Content Support in VVC: Applications, Coding Tools, and Performance},
journal={IEEE Transactions on Circuits and Systems for Video Technology},
year={2021},
volume={31},
number={10},
pages={3801-3817},
doi={10.1109/TCSVT.2021.3074312},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104677034&doi=10.1109%2fTCSVT.2021.3074312&partnerID=40&md5=714e9ab819d024add2efec47f12e320b},
affiliation={Department of Video Communication and Applications, Fraunhofer Institute for Telecommunications-Heinrich Hertz Institute, Berlin, Germany; Tencent, Palo Alto, CA, United States; Orange Labs, Issy-les-Moulineaux, France; Alibaba, Sunnyvale, CA, United States; Qualcomm Technologies Inc., San Diego, CA, United States; Bytedance Inc., San Diego, CA, United States; Microsoft, Redmond, WA, United States},
abstract={In an increasingly connected world, consumer video experiences have diversified away from traditional broadcast video into new applications with increased use of non-camera-captured content such as computer screen desktop recordings or animations created by computer rendering, collectively referred to as screen content. There has also been increased use of graphics and character content that is rendered and mixed or overlaid together with camera-generated content. The emerging Versatile Video Coding (VVC) standard, in its first version, addresses this market change by the specification of low-level coding tools suitable for screen content. This is in contrast to its predecessor, the High Efficiency Video Coding (HEVC) standard, where highly efficient screen content support is only available in extension profiles of its version 4. This paper describes the screen content support and the five main low-level screen content coding tools in VVC: Transform skip residual coding (TSRC), block-based differential pulse-code modulation (BDPCM), intra block copy (IBC), adaptive color transform (ACT), and the palette mode. The specification of these coding tools in the first version of VVC enables the VVC reference software implementation (VTM) to achieve average bit-rate savings of about 41% to 61% relative to the HEVC test model (HM) reference software implementation using the Main 10 profile for 4:2:0 screen content test sequences. Compared to the HM using the Screen-Extended Main 10 profile and the same 4:2:0 test sequences, the VTM provides about 19% to 25% bit-rate savings. The same comparison with 4:4:4 test sequences revealed bit-rate savings of about 13% to 27% for Y'C-{B}C-{R} and of about 6% to 14% for R'G'B' screen content. Relative to the HM without the HEVC version 4 screen content coding extensions, the bit-rate savings for 4:4:4 test sequences are about 33% to 64% for Y'C-{B}C-{R} and 43% to 66% for R'G'B' screen content. © 1991-2012 IEEE.},
author_keywords={ACT;  BDPCM;  H.266;  HEVC;  IBC;  palette coding;  screen content coding;  TSRC;  video coding;  VTM;  VVC},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ferreira2021,
author={Ferreira, F. and Vale, G. and Diniz, J.P. and Figueiredo, E.},
title={Evaluating T-wise testing strategies in a community-wide dataset of configurable software systems},
journal={Journal of Systems and Software},
year={2021},
volume={179},
doi={10.1016/j.jss.2021.110990},
art_number={110990},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106269170&doi=10.1016%2fj.jss.2021.110990&partnerID=40&md5=b588a76ee06011838fe6c1ecc8d7d3c5},
affiliation={Federal University of Minas Gerais, Belo Horizonte, Brazil; Saarland University, Saarbrücken, Germany},
abstract={Configurable software systems allow developers to maintain a unique platform and address a diversity of deployment contexts and usages. Testing configurable systems is essential because configurations that fail may potentially hurt users and degrade the project reputation. As extensively testing all valid configurations is infeasible in practice, several testing strategies have been proposed to recommend an optimal sample of configurations able to find most existing faults. However, up to now, we could not find studies comparing testing strategies with a community-wide dataset. Aiming at (i) comparing sampling testing strategies and (ii) understanding the location of faults, we use a community-wide dataset from the literature and compare suggested configurations from variations of five t-wise testing strategies (e.g., ICPL-T2, Chvatal-T4, and IncLing-T2). This comparison aims to find which strategies are faster, more comprehensive, effective on identifying faults, time-efficient, and coverage-efficient in this community-wide dataset and the reasons why a strategy fared better in one investigated property. Complementary, we investigate the dispersion of faults over classes and features from the dataset. As a result, we found that the dispersion of faults are usually concentrated in a few classes and features. Furthermore, fault-prone classes and features are distinguishable from classes and features safe of faults. Overall, we believe that with our results practitioners acquire the necessary knowledge to choose a testing strategy that best fits their needs. Moreover, researchers and tool builders are served with a bunch of opportunities to improve existing testing strategies and tools. For instance, they may incorporate information from fault-prone classes and features when selecting configurations to be tested in their testing strategies. © 2021},
author_keywords={Feature interactions;  Software faults;  T-wise sampling strategies;  Testing configurable systems},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hussain20219433,
author={Hussain, T. and Amin, S. and Zabit, U. and Ayguadé, E.},
title={Implementation of a high-accuracy phase unwrapping algorithm using parallel-hybrid programming approach for displacement sensing using self-mixing interferometry},
journal={Journal of Supercomputing},
year={2021},
volume={77},
number={9},
pages={9433-9453},
doi={10.1007/s11227-021-03634-6},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100752247&doi=10.1007%2fs11227-021-03634-6&partnerID=40&md5=dad9beddd07e543d29a905f8b46f7522},
affiliation={Riphah International University Islamabad, Islamabad, Pakistan; National University of Sciences and Technology (NUST), Islamabad, Pakistan; Barcelona Supercomputing Center, Barcelona, Spain},
abstract={Phase unwrapping is an integral part of multiple algorithms with diverse applications. Detailed phase unwrapping is also necessary for achieving high-accuracy metric sensing using laser feedback-based self-mixing interferometry (SMI). Among SMI specific phase unwrapping approaches, a technique called Improved Phase Unwrapping Method (IPUM) provides the highest accuracy. However, due to its complex, sequential, and compute-intensive nature, this method requires a high-performance computing architecture, capable of scalable parallel processing so that such a high-accuracy algorithm can be used for high-bandwidth sensing applications. In this work, the existing sequential IPUM C program is parallelized by using hybrid OpenMP/MPI (Open Multi-Processing/Message Passing Interface) parallel programming models and tested on Barcelona Supercomputing Center Nord-III Supercomputer. The computational performance of the proposed parallel-hybrid IPUM algorithm is compared with existing IPUM sequential code by executing multi-core and uni-core processor architecture, respectively. While comparing the performance of sequential IPUM with the parallel-hybrid IPUM algorithm on 16 nodes of Nord-III supercomputer, the results show that the parallel-hybrid algorithm gets 345.9x times performance improvement as compared to IPUM’s standard, sequential implementation on a single node system. The results show that the parallel-hybrid version of IPUM gives a scalable performance for different target velocities and a different number of processing cores. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.},
author_keywords={HPC;  Interferometry;  Parallel processing;  Phase unwrapping;  Supercomputing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Erickson20213302,
author={Erickson, J. and Baker, J. and Barrett, S. and Brady, C. and Brower, M. and Carbonell, R. and Charlebois, T. and Coffman, J. and Connell-Crowley, L. and Coolbaugh, M. and Fallon, E. and Garr, E. and Gillespie, C. and Hart, R. and Haug, A. and Nyberg, G. and Phillips, M. and Pollard, D. and Qadan, M. and Ramos, I. and Rogers, K. and Schaefer, G. and Walther, J. and Lee, K.},
title={End-to-end collaboration to transform biopharmaceutical development and manufacturing},
journal={Biotechnology and Bioengineering},
year={2021},
volume={118},
number={9},
pages={3302-3312},
doi={10.1002/bit.27688},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100122820&doi=10.1002%2fbit.27688&partnerID=40&md5=9bb3f933c513061af2ce576c048728a1},
affiliation={National Institute for Innovation in Manufacturing Biopharmaceuticals, Newark, DE, United States; Office of Biotechnology Products (OBP), Center for Drug Evaluation and Research (CDER), U.S. Food and Drug Administration, Silver Spring, MD, United States; Global CMC Development, Sanofi, Framingham, MA, United States; Biologics MS&T, Bristol-Myers Squibb, Devens, MA, United States; Biologics Process Research and Development, Merck & Co., Inc., Kenilworth, NJ, United States; National Institute for Innovation in Manufacturing Biopharmaceuticals, Raleigh, NC, United States; BioTx Pharmaceutical Sciences, Pfizer, Andover, MA, United States; Biopharmaceutical Development, AstraZeneca, Gaithersburg, MD, United States; Process Design, Just-Evotec Biologics, Seattle, WA, United States; Manufacturing Science and Technology, Drug Substance, Genentech, Inc., Oceanside, CA, United States; Process Development, Amgen, Cambridge, MA, United States; Next Generation Processing R&D, MilliporeSigma, Bedford, MA, United States; Sartorius Corporate Research, Sartorius, Boston, MA, United States; Biologics Research and Development, Eli Lilly and Company, Indianapolis, IN, United States; Material Measurement Laboratory and Office of Advanced Manufacturing, National Institute of Standards and Technology, Gaithersburg, MD, United States; API Large Molecule BioTherapeutics Development, Janssen R&D, Malvern, PA, United States},
abstract={An ambitious 10-year collaborative program is described to invent, design, demonstrate, and support commercialization of integrated biopharmaceutical manufacturing technology intended to transform the industry. Our goal is to enable improved control, robustness, and security of supply, dramatically reduced capital and operating cost, flexibility to supply an extremely diverse and changing portfolio of products in the face of uncertainty and changing demand, and faster product development and supply chain velocity, with sustainable raw materials, components, and energy use. The program is organized into workstreams focused on end-to-end control strategy, equipment flexibility, next generation technology, sustainability, and a physical test bed to evaluate and demonstrate the technologies that are developed. The elements of the program are synergistic. For example, process intensification results in cost reduction as well as increased sustainability. Improved robustness leads to less inventory, which improves costs and supply chain velocity. Flexibility allows more products to be consolidated into fewer factories, reduces the need for new facilities, simplifies the acquisition of additional capacity if needed, and reduces changeover time, which improves cost and velocity. The program incorporates both drug substance and drug product manufacturing, but this paper will focus on the drug substance elements of the program. © 2021 The Authors. Biotechnology and Bioengineering published by Wiley Periodicals LLC.},
author_keywords={biopharmaceutical;  continuous bioprocess;  factory of the future;  innovation;  manufacturing;  process intensification;  technology},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhou2021,
author={Zhou, Q.},
title={Research on the spatial perception evaluation of settlement streets based on the concept of sustainability},
journal={IOP Conference Series: Earth and Environmental Science},
year={2021},
volume={831},
number={1},
doi={10.1088/1755-1315/831/1/012044},
art_number={012044},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112536386&doi=10.1088%2f1755-1315%2f831%2f1%2f012044&partnerID=40&md5=3dfe847a8a6df2601305a5cc4ad34dd0},
affiliation={School of Landscape Architecture, Central South University of Forestry Science and Technology, Changsha, China},
abstract={Nowadays, the perception of the street space environment in urban residential areas is receiving more and more attention, and a sick urban space environment can make environmental participants feel their safety is threatened. By redesigning the street space environment, the safety of the street and the perception of the environment can be improved. Taking Guitang Road as a typical residential street in Yuhua District of Changsha City, we use Depthmap software to analyze the degree of connectivity, integration and selection of street axes, and apply the reliability validity test, principal component analysis and multiple linear regression analysis in SPSS software based on the statistical results of the questionnaire to quantitatively study the statistical data and explore the main physical Environmental factors. Based on the six categories of spatial elements in CPTED theory, we propose a design strategy to reduce the fear of victimization in residential street space, so as to improve the street space environment and sustainable development. The aim is to promote urban planning and environmental sustainability to carry each other, in order to make urban planning diverse and stable through rational planning and construction of new environmental elements. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ebadi2021103,
author={Ebadi, H. and Moghadam, M.H. and Borg, M. and Gay, G. and Fontes, A. and Socha, K.},
title={Efficient and Effective Generation of Test Cases for Pedestrian Detection-Search-based Software Testing of Baidu Apollo in SVL},
journal={Proceedings - 3rd IEEE International Conference on Artificial Intelligence Testing, AITest 2021},
year={2021},
pages={103-110},
doi={10.1109/AITEST52744.2021.00030},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118804446&doi=10.1109%2fAITEST52744.2021.00030&partnerID=40&md5=5f94af29d0c96b99a57164c0907b95e2},
affiliation={Infotiv AB, Gothenburg, Sweden; RISE Research Institutes of Sweden, Västerås, Sweden; RISE Research Institutes of Sweden, Lund, Sweden; Chalmers and the University of Gothenburg, Gothenburg, Sweden},
abstract={With the growing capabilities of autonomous vehicles, there is a higher demand for sophisticated and pragmatic quality assurance approaches for machine learning-enabled systems in the automotive AI context. The use of simulation-based prototyping platforms provides the possibility for early-stage testing, enabling inexpensive testing and the ability to capture critical corner-case test scenarios. Simulation-based testing properly complements conventional on-road testing. However, due to the large space of test input parameters in these systems, the efficient generation of effective test scenarios leading to the unveiling of failures is a challenge. This paper presents a study on testing pedestrian detection and emergency braking system of the Baidu Apollo autonomous driving platform within the SVL simulator. We propose an evolutionary automated test generation technique that generates failure-revealing scenarios for Apollo in the SVL environment. Our approach models the input space using a generic and flexible data structure and benefits a multi-criteria safety-based heuristic for the objective function targeted for optimization. This paper presents the results of our proposed test generation technique in the 2021 IEEE Autonomous Driving AI Test Challenge. In order to demonstrate the efficiency and effectiveness of our approach, we also report the results from a baseline random generation technique. Our evaluation shows that the proposed evolutionary test case generator is more effective at generating failure-revealing test cases and provides higher diversity between the generated failures than the random baseline. © 2021 IEEE.},
author_keywords={Advanced Driver Assistance Systems;  Automotive Simulators;  Evolutionary Algorithm;  Pedestrian Detection;  Search-Based Test Generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2021128,
author={Nguyen, V. and Huber, S. and Gambi, A.},
title={SALVO: Automated Generation of Diversified Tests for Self-driving Cars from Existing Maps},
journal={Proceedings - 3rd IEEE International Conference on Artificial Intelligence Testing, AITest 2021},
year={2021},
pages={128-135},
doi={10.1109/AITEST52744.2021.00033},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118781490&doi=10.1109%2fAITEST52744.2021.00033&partnerID=40&md5=8936904007f1d7a04f45820a41fcfd84},
affiliation={University of Passau, Passau, Germany},
abstract={Simulation-based tests are more cost-effective and less dangerous than field tests; hence, they are becoming the norm for thoroughly testing self-driving cars in virtual environments. High-quality simulation-based testing requires physically accurate computer simulations, detailed and photo-realistic maps, and systematic approaches for generating tests. Moreover, since creating detailed maps is a manual process, they are expensive, and testers should avoid wasting such valuable resources, for example, by generating irrelevant test cases. To address those issues, we propose SALVO a fully automated approach to identify quantifiably diverse and critical driving scenarios that can be instantiated on existing high-definition maps and implement them in an industrial driving simulator as executable test cases. The evaluation of SALVO in the context of the 2021 IEEE Autonomous Driving AI Test Challenge showed that it could analyze maps of different complexity and identify many critical driving scenarios in minutes. Furthermore, the tests SALVO generated stressed a state-of-Art self-driving car software in quantifiably diverse ways and exposed issues with its implementation. © 2021 IEEE.},
author_keywords={Autonomous Vehicles;  Diversity;  Feature Space;  Software-in-The-loop;  Test Case Generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Strandberg202183,
author={Strandberg, P.E. and Frasheri, M. and Enoiu, E.P.},
title={Ethical AI-Powered Regression Test Selection},
journal={Proceedings - 3rd IEEE International Conference on Artificial Intelligence Testing, AITest 2021},
year={2021},
pages={83-84},
doi={10.1109/AITEST52744.2021.00025},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118765881&doi=10.1109%2fAITEST52744.2021.00025&partnerID=40&md5=6def368603386b7c1afe4c5bc3e3a6e4},
affiliation={Mälardalen University, Sweden; Aarhus University, Denmark},
abstract={Test automation is common in software development; often one tests repeatedly to identify regressions. If the amount of test cases is large, one may select a subset and only use the most important test cases. The regression test selection (RTS) could be automated and enhanced with Artificial Intelligence (AI-RTS). This however could introduce ethical challenges. While such challenges in AI are in general well studied, there is a gap with respect to ethical AI-RTS. By exploring the literature and learning from our experiences of developing an industry AI-RTS tool, we contribute to the literature by identifying three challenges (assigning responsibility, bias in decision-making and lack of participation) and three approaches (explicability, supervision and diversity). Additionally, we provide a checklist for ethical AI-RTS to help guide the decision-making of the stakeholders involved in the process. © 2021 IEEE.},
author_keywords={artificial intelligence;  computer ethics;  regression test selection;  test automation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dreyzin2021405,
author={Dreyzin, V.E. and Logvinov, D.I. and Grimov, A.A. and Varganov, V.V.},
title={Reliability Assurance of Neutron Flux Measurements},
journal={Measurement Techniques},
year={2021},
volume={64},
number={5},
pages={405-413},
doi={10.1007/s11018-021-01946-w},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117124199&doi=10.1007%2fs11018-021-01946-w&partnerID=40&md5=e0b590a1b8377846461f68004d383fba},
affiliation={Southwest State University, Kursk, Russian Federation; LLC NEORADTECH, Kursk, Russian Federation},
abstract={In the paper, the necessity for carrying out spectrometric measurements of neutron fluxes in the nuclear industry in order to accurately estimate radiation density and dose characteristics is justified in terms of the inadequacy of current metrological and methodological support for both spectrometric and radiometric measurements of random neutron fluxes. The authors briefly describe the concept of a real-time multidetector neutron spectrometer along with results obtained from a prototype study. In order to increase the reliability of neutron measurements, an innovative concept for verifying a real-time multidetector neutron spectrometer is proposed. This concept involves using several dozens of reference fields exhibiting diverse and precisely known spectra, rather than a single flux, whose spectrum is unknown. The developed testing and verification system is used to carry out a verification. Reference neutron fields produced by this system to exhibit various energetic spectral forms are mathematically modeled. The spectra of the generated reference neutron fields are computed using a system simulated by Monte Carlo software during laboratory tests of a prototype real-time multidetector neutron spectrometer-dosimeter. It is proposed to train a neural network embedded in the multidetector neutron spectrometer-dosimeter on an extended set of basic spectra, which, in addition to the reference field spectra, includes neutron flux spectra reliably known from literature descriptions. A procedure for forming a set of the simulated realizations of training and verification samples used in the neural network training is presented. It is shown that energy errors of the proposed real-time multidetector neutron spectrometer-dosimeter can be avoided during neutron flux measurements, even when performing verification in the reference fields exhibiting a limited variety of spectral shapes. The paper justifies the possibility of minimizing the energy errors of the existing neutron radiometer-dosimeters in the course of the verification performed in the neutron reference fields of the developed testing and verification system. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={measurement;  neutron radiation;  reference fields;  spectrum;  verification system},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sabogal2021143,
author={Sabogal, S. and George, A.},
title={A Methodology for Evaluating and Analyzing FPGA-Accelerated, Deep-Learning Applications for Onboard Space Processing},
journal={Proceedings - 2021 IEEE Space Computing Conference, SCC 2021},
year={2021},
pages={143-154},
doi={10.1109/SCC49971.2021.00022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116738793&doi=10.1109%2fSCC49971.2021.00022&partnerID=40&md5=1f70fea0af9cc940a88fa46d2fc06ce7},
affiliation={Nsf Center for Space, High-Performance, and Resilient Computing (SHREC), University of Pittsburgh, Pittsburgh, PA, United States},
abstract={Due to continued innovations in onboard data analysis and spacecraft autonomy, enabled by deep learning (DL), modern spacecraft require dependable, high-performance computers to process onboard an immense volume of raw sensor data into actionable information to formulate critical decisions autonomously. To enable compute-intensive DL algorithms, commercial-off-the-shelf processors, including FPGAs and system-on-chips, are often employed for their superior performance, energy-efficiency, and affordability compared to traditional radiation-hardened alternatives; however, these processors are highly susceptible to radiation-induced single-event effects (SEEs) that can degrade the dependability of DL applications. Researchers have created a diverse collection of DL models that perform a variety of tasks useful for Earth-observation missions. However, due to characteristic differences between models and accelerators, their tradeoffs can vary in terms of accuracy, area, performance, energy-efficiency, and dependability, which are factors crucial for resource-constrained and mission-critical systems. To select the optimal DL solution that maximizes inference performance, conserves onboard resources, and satisfies mission dependability requirements, a methodology is required to evaluate and compare the tradeoffs between competing options. In this paper, we propose a methodology for evaluating and analyzing the tradeoffs of FPGA-accelerated DL models, including a hierarchical fault-injection approach to accelerate the characterization of SEE susceptibility of DL solutions in terms of well-established dependability metrics. Furthermore, we identify performance and dependability trends, analyze the impact of SEEs on the inference accuracy, and predict design fault rates for near-Earth orbital environments. To demonstrate the versatility of our methodology, we evaluate and analyze four semantic-segmentation models accelerated on four Xilinx Deep-Learning Processing Unit accelerators. © 2021 IEEE.},
author_keywords={deep learning;  fault injection;  FPGA;  semantic segmentation;  single-event effects;  space computing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Huang2021651,
author={Huang, R. and Sun, W. and Chen, T.Y. and Ng, S. and Chen, J.},
title={Identification of Failure Regions for Programs with Numeric Inputs},
journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
year={2021},
volume={5},
number={4},
pages={651-667},
doi={10.1109/TETCI.2020.3013713},
art_number={9165789},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111092983&doi=10.1109%2fTETCI.2020.3013713&partnerID=40&md5=30939ffed17ae00b5a9589a6e43c82a1},
affiliation={School of Computer Science and Communication Engineering, And Jiangsu Key Laboratory of Security Technology for Industrial Cyberspace, Jiangsu University, Zhenjiang, China; School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, VIC, Australia},
abstract={Failure region, where failure-causing inputs reside, has provided many insights to enhance testing effectiveness of many testing methods. Failure region may also provide some important information to support other processes such as software debugging. When a testing method detects a software failure, indicating that a failure-causing input is identified, the next important question is about how to identify the failure region based on this failure-causing input, i.e., Identification of Failure Regions (IFR). In this paper, we introduce a new IFR strategy, namely Search for Boundary (SB), to identify an approximate failure region of a numeric input domain. SB attempts to identify additional failure-causing inputs that are as close to the boundary of the failure region as possible. To support SB, we provide a basic procedure, and then propose two methods, namely Fixed-orientation Search for Boundary (FSB) and Diverse-orientation Search for Boundary (DSB). In addition, we implemented an automated experimentation platform to integrate these methods. In the experiments, we evaluated the proposed SB methods using a series of simulation studies and empirical studies with different types of failure regions. The results show that our methods can effectively identify a failure region, within the limited testing resources. © 2017 IEEE.},
author_keywords={failure-based testing;  identification of failure region (IFR);  Software debugging;  software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2021,
author={Wang, X. and Kuang, B. and Liu, P. and Hu, W. and Li, Y.},
title={Dynamic performance investigation on the hydraulic-suspended passive shutdown subassembly of SFR},
journal={Annals of Nuclear Energy},
year={2021},
volume={158},
doi={10.1016/j.anucene.2021.108244},
art_number={108244},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103668580&doi=10.1016%2fj.anucene.2021.108244&partnerID=40&md5=4e66b9c303fdec1e45f1cfb88d4024cb},
affiliation={School of Mechanical Engineering, Shanghai Jiao Tong University, Dongchuan Rd. 800, Shanghai, 200240, China; China Institute of Atomic Energy, P. O. Box 275, Beijing, 102413, China},
abstract={Hydraulic suspended passive shutdown subassembly (HS-PSS) belongs to one of the diverse passive safety devices for further enhance safety for pool-type sodium-cooled fast reactor (SFR). The hydraulic moving body (HMB) in the PSS with neutron absorbers inside was designed to insert into the core automatically within a stipulated time when the mainstream below a critical flow rate. The dynamic displacement performance of the HMB is of particular importance to ensure the safety of SFR especially during the Unprotected loss of flow accident (ULOF). In this paper, the mathematical modeling related to the falling process, the hydraulic characteristics of HMB under different flows are detailed. A series result of the sensitivity analysis of the factors affecting HMB performance, such as the flow rate and the size of some key structures, are obtained through calculation. A PSS-mockup was tested at the hydraulic (water) test bench and the calculation program was validated by the experimental results. © 2021 Elsevier Ltd},
author_keywords={Dynamic performance;  Hydraulic moving body;  Hydraulic test;  Hydraulic-suspended passive shutdown subassembly;  Mathematical model;  Sodium cooled fast reactor},
document_type={Article},
source={Scopus},
}

@ARTICLE{Papadopoulos20211528,
author={Papadopoulos, A.V. and Versluis, L. and Bauer, A. and Herbst, N. and Kistowski, J.V. and Ali-Eldin, A. and Abad, C.L. and Amaral, J.N. and Tuma, P. and Iosup, A.},
title={Methodological Principles for Reproducible Performance Evaluation in Cloud Computing},
journal={IEEE Transactions on Software Engineering},
year={2021},
volume={47},
number={8},
pages={1528-1543},
doi={10.1109/TSE.2019.2927908},
art_number={8758926},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069901345&doi=10.1109%2fTSE.2019.2927908&partnerID=40&md5=9c299235e7d1348845564486d78ca306},
affiliation={Mälardalen University, Västerås, Sweden; Vrije Universiteit Amsterdam, Amsterdam, Netherlands; University of Würzburg, Würzburg, Germany; Umeå University, Umeå, Sweden; Escuela Superior Politecnica Del Litoral, Guayaquil, Ecuador; University of Alberta, Edmonton, AB, Canada; Charles University, Prague, Czech Republic; Vrije Universiteit Amsterdam, Amsterdam, Netherlands},
abstract={The rapid adoption and the diversification of cloud computing technology exacerbate the importance of a sound experimental methodology for this domain. This work investigates how to measure and report performance in the cloud, and how well the cloud research community is already doing it. We propose a set of eight important methodological principles that combine best-practices from nearby fields with concepts applicable only to clouds, and with new ideas about the time-accuracy trade-off. We show how these principles are applicable using a practical use-case experiment. To this end, we analyze the ability of the newly released SPEC Cloud IaaS benchmark to follow the principles, and showcase real-world experimental studies in common cloud environments that meet the principles. Last, we report on a systematic literature review including top conferences and journals in the field, from 2012 to 2017, analyzing if the practice of reporting cloud performance measurements follows the proposed eight principles. Worryingly, this systematic survey and the subsequent two-round human reviews, reveal that few of the published studies follow the eight experimental principles. We conclude that, although these important principles are simple and basic, the cloud community is yet to adopt them broadly to deliver sound measurement of cloud environments. © 1976-2012 IEEE.},
author_keywords={Experimental evaluation;  experimentation;  observation study},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Antoine2021,
author={Antoine, A.J., Jr. and Obenland, C.A. and Ramirez, R. and Barr, C. and Cushing, M. and Nichol, C.A.},
title={Using Science Concepts in a Mathematics Professional Development Program to Improve Student's Standardized Test Scores},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124550148&partnerID=40&md5=afadd982312750d36feb5fef8b5d8d04},
affiliation={Rice University Office of STEM Engagement; Rice University, United States; Rice University, United States},
abstract={Algebra I, formally described as a course covering expressions, functions, inequalities, systems of equations, exponents, real numbers and polynomials, has been known as a predictor of student success and college readiness. With this in mind, there is an urgent need to improve student performance in these courses. This paper describes a unique teacher professional development program that has the goal of increasing student success in mathematics using inquiry and team-based pedagogical strategies and integrating other STEM subject-area concepts into mathematics classrooms. In this article, we describe this regional, year-long professional development program composed of mathematics and science teacher pairs from the same campus. The program was designed to pair educators across content areas to discuss and implement best practices for teaching a curriculum aligning mathematics and science Texas Essential Knowledge and Skills (TEKS). The cohort of 79 8th and 9th grade mathematics and science teachers received over 100 hours of training throughout the year with the primary goals of providing resources and training, connecting mathematics and science classrooms, and providing teachers with team-building, mentoring and support. The educator cohort experienced both collaborative and inquiry-based, grade-level and content-specific professional development sessions. Teacher participants responded favorably to the program and that resulted in significant changes in teacher and student outcomes. We evaluated the impact of the program on teachers through interviews, pre and post content tests, along with surveys. Several themes were apparent in the evaluation including an appreciation for learning how grade-level science themes can provide meaningful context for mathematics instruction and how peer-observation and mentoring opportunities are imperative for teachers of all levels of experience. Participating teachers had significant gains in both leadership assessments (total gain of 18.8%, p<0.05) and Algebra content post-tests (gain of 13%, p<0.05) using the Diagnostic Mathematics Assessments for Middle School Teachers (DTAMS). They also reported growth in readiness to teach core mathematics standards, teach mathematics to diverse student populations and implement inquiry-based mathematics techniques. Student evaluation was primarily conducted through an analysis of student performance on state mathematics assessments. Employing the Texas Education Research Center (EdRC) database, which is a repository of Texas Education Agency (TEA) data, we have found that students of participating teachers had a significant increase in their performance on the Algebra I mandated state assessment test as compared to a well-matched group of comparison students. The mean score on the state Algebra I test for students of these teachers was 4065.3 (N=2456), in contrast to the mean scores of the comparison student group of 3954.7 (N=24,560). It is interesting to note that this correlates to participating teachers' students having an average score in the “Meets Expectations” category while the comparison group fell in the “Approaches Expectations” category as defined by the Texas Education Agency (TEA). This report will provide a practical groundwork for crafting cross-curricular professional development opportunities that lead to increased teacher self-efficacy and student achievement on standardized mathematics assessments. © American Society for Engineering Education, 2021},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stwalley2021,
author={Stwalley, C.S. and Stwalley, R.M., III and Booth-Womack, V.L. and Baldwin, G.L. and LaRose, S.},
title={Using Board Spectrum Technological Projects to Introduce Diverse Student Populations to Biological & Agricultural Engineering},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124534724&partnerID=40&md5=63010b642cb5d6822694fa410cc7d072},
affiliation={Purdue University, West Lafayette, United States; Purdue University, United States},
abstract={This paper is a work-in-progress, focused on the utilization of the Rising Scholars Program to introduce minority students to experiential engineering projects within Agricultural and Biological Engineering. Traditional admissions processes at top institutions predominately utilize standardized test scores when comparing student applications. The equity of these high-stakes tests most severely affects students of low socioeconomic status (SES). The NSF-sponsored program, Rising Scholars: Web of Support used as an Indicator of Success in Engineering, was created to investigate whether alternative admission criteria could be used to identify low-SES applicants who would excel within STEM fields in higher education, even if they did not have the superior standardized testing metrics preferred by current admissions processes. The students underwent a pre-selection process to determine their eligibility. The overall experience was designed to enhance student connectivity within the collegiate environment. The Gallup-Purdue Index (2014) found that feeling supported and having learning experiences that illustrated learned principles produced a graduate who would be engaged in their work. The Rising Scholar (RS) program utilized a prescribed path through college designed to enhance these features. These positive experiences are exemplified by the Purdue Agricultural and Biological Engineering (ABE) department and how they approach the overall educational process. Faculty are motivated in their teaching, research, and extension efforts by a focus on meeting the world's grand challenges, in which most college students are also highly interested. The Rising Scholars Program utilized the Vertically Integrated Projects model to introduce their students to real-life projects at the freshman and sophomore level, which could potentially be continued on into graduate school. Several of the RS students have worked with the Purdue ABE Hog Cooling Pad Project and these students have conducted research, prototyping, and design modifications on the pad. They have participated in five experimental bench tests of the design and four consecutive live animal studies related to the pad performance. Within these experiments, Rising Scholars students were able to work on real-life projects, with real-world impact. The preliminary hypothesis question is: Are future graduates of the Rising Scholars Program more likely to thrive in all areas of well-being due to their collegiate experiences?. © American Society for Engineering Education, 2021},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schill2021,
author={Schill, S.A. and Bielefeldt, A.R.},
title={Mentoring Correlates to Characteristics of University K12 Outreach Programs: Survey Findings (Fundamental)},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124530876&partnerID=40&md5=eade3df3f57f90cc93ca2475bb35f379},
affiliation={University of Colorado, Boulder, United States},
abstract={Effective mentorship between faculty and undergraduate students has been recognized by the National Academies as an avenue to address issues of diversity and identity in Science, Technology, Engineering, and Mathematics (STEM). Mentoring relationships may also form in other contexts, such as between undergraduates and K-12 students in K-12 STEM outreach programs. A survey was administered to university faculty/staff who coordinate K-12 STEM outreach programs to obtain a pool of respondents and facilitate interview selection in a larger phenomenographic study. This paper presents the results from the survey, and focuses on developing a better understanding of mentoring in K-12 STEM outreach programs through the research question, Do K-12 STEM outreach program characteristics differ between programs that are and are not believed to foster mentoring relationships between university and K-12 students? The survey yielded useful responses from 61 program coordinators representing 131 K-12 STEM outreach programs. Tests for association between individual program characteristics and program coordinators' beliefs about mentoring in their program(s) and a binomial logistic regression model were carried out using IBM SPSS 26. The most significant program characteristics were found to be having the goal “Improve K-12 student learning in STEM,” the program time (i.e., Day, Summer, Academic Year OR Year), and the level of cohort experience among college students. A discussion as to why these characteristics differ between programs that are and are not believed to foster mentoring relationships is included, and future qualitative work in the larger study will provide more insight. © American Society for Engineering Education, 2021},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stwalley2021,
author={Stwalley, C.S. and Stwalley, R.M., III and Baldwin, G.L. and Booth-Womack, V.L. and LaRose, S.},
title={Value of Experiential Experiences for Diverse Student Populations within Engineering Disciplines},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124510170&partnerID=40&md5=cdef6fe9868959a31288de70f71c35b1},
affiliation={Purdue University, West Lafayette, United States},
abstract={Traditional admissions processes at top institutions predominately utilize standardized test scores when comparing student applications. The equity of these high-stakes tests most severely affects students of low socioeconomic status (SES). The NSF-sponsored program, Rising Scholars: Web of Support used as an Indicator of Success in Engineering, was created to investigate whether alternative admission criteria could be used to identify low-SES applicants who would excel within STEM fields in higher education, even if they did not have the superior standardized testing metrics preferred by current admissions process. The quality of the student's support networks and their readiness for higher education as determined by an in-person interview with the selection committee were used as input data for a Web of Support characterization model to predict a student's likely collegiate success at the matriculation point. There were three cohorts with a total of 21 students chosen for the program during their entry to the university which included applicants of low-SES and under-represented minority status. A significant programmatic element for these students was their involvement in experiential activities through pre-existing programs in the institution. It was reasonably assumed that the Rising Scholars student population could be positively influenced toward long-term educational commitment through experiential activities providing realistic views of professional activity. The prescribed collegiate path for these students contained an experiential educational element for each summer between admission and graduation. A summer research project with a faculty-directed laboratory before the sophomore year and a self-directed research project prior to the junior year were used to build project management experience, along with a paid, external internship in a professional organization likely to hire within the student's major. Based upon the limited data collected so far, the researchers seem to have been conclusively demonstrated that a structured, 'high-touch' program with a heavy experiential component can successfully move low-SES students with STEM inclinations through a highly ranked institution. Counselling to reduce the anxiety surrounding the collegiate process for first generation students and some form of scholarship support to reduce the financial burden are both crucial underlying elements to this program's success, but the importance of hands-on, experiential activities that help the student visualize their professional career cannot be under-estimated. © American Society for Engineering Education, 2021},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Panebianco2021,
author={Panebianco, C.J. and Iatridis, J.C. and Weiser, J.},
title={Development of an At-home Metal Corrosion Laboratory Experiment for STEM Outreach in Biomaterials During the Covid-19 Pandemic},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124507647&partnerID=40&md5=a3feca23df38008fbdb7cba1f1a760a7},
affiliation={Icahn School of Medicine, Mount Sinai, United States; The Cooper Union},
abstract={Due to the coronavirus disease 2019 (COVID-19) pandemic, many universities and outreach programs have switched to online learning platforms, which inhibits students from completing formative hands-on experiments. To address this, we developed a series of at-home experiments for undergraduate engineering students and adapted one of these experiments for outreach purposes. This experiment was well received by middle school students in the Young Eisner Scholars (YES) Program and resulted in significant learning gains by pre/post-test assessment. Additionally, students showed enhanced attitudes toward science after completing their at-home experiments, as measured by pre/post-surveys. These results motivate the use of similar at-home experiments with virtual instruction to remotely teach engineering concepts to diverse, underserved communities during the COVID-19 pandemic and beyond. © American Society for Engineering Education, 2021},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ladele2021258,
author={Ladele, O. and Baxter, J. and van der Werf, P. and Gilliland, J.A.},
title={Familiarity breeds acceptance: Predictors of residents’ support for curbside food waste collection in a city with green bin and a city without},
journal={Waste Management},
year={2021},
volume={131},
pages={258-267},
doi={10.1016/j.wasman.2021.06.010},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109367286&doi=10.1016%2fj.wasman.2021.06.010&partnerID=40&md5=e97e73b601a570434522cb8326937ee4},
affiliation={Department of Geography and Environment, University of Western Ontario, London, ON, Canada},
abstract={Food waste remains a high priority greenhouse gas (GHG) emission problem and household curbside collection – green bin1 - with mass treatment is often adopted as a viable solution for GHG reduction. The aim of this study is to explore attitudinal and situational predictors of support for residential curbside green bin programs. Using responses to 517 household surveys from the mid-sized Canadian cities of London, Ontario (proposed green bin program) and Kitchener-Waterloo (KW), Ontario (operating green bin program for 10+ years) comparison of means t-tests, correlations and linear regression are used to test five hypotheses derived from the food waste and waste diversion literatures that predict green bin support: situational factors, current food wasting, theory of planned behaviour attitudes, concern that green bin encourages food wasting, and concern that food waste ends up in the garbage regardless of green bin. There is some support for all five hypotheses. Residents in Kitchener-Waterloo were significantly more supportive (83%) than those in London (65%). While residents in both communities are supportive because of the perceived convenience and environmental benefits of the green bin, the number of regression model predictors is greater in London (16 compared to 9 for Kitchener-Waterloo). The findings overall suggest sustained municipal education at the implementation stage may lead to positive resident habituation to green bin and thus, durable public buy-in. © 2021},
author_keywords={Comparative;  Convenience;  Curbside;  Food waste;  Green bin;  TPB},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Groce2021658,
author={Groce, A. and Grieco, G.},
title={Echidna-parade: A tool for diverse multicore smart contract fuzzing},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={658-661},
doi={10.1145/3460319.3469076},
art_number={3469076},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111463354&doi=10.1145%2f3460319.3469076&partnerID=40&md5=60253c2a174f9cfe8b3a41fbb80e6ff6},
affiliation={Northern Arizona University, United States; Trail of Bits, United States},
abstract={Echidna is a widely used fuzzer for Ethereum Virtual Machine (EVM) compatible blockchain smart contracts that generates transaction sequences of calls to smart contracts. While Echidna is an essentially single-threaded tool, it is possible for multiple Echidna processes to communicate by use of a shared transaction sequence corpus. Echidna provides a very large variety of configuration options, since each smart contract may be best-tested by a non-default configuration, and different faults or coverage targets within a single contract may also have differing ideal configurations. This paper presents echidna-parade, a tool that provides pushbutton multicore fuzzing using Echidna as an underlying fuzzing engine, and automatically provides sophisticated diversification of configurations. Even without using multiple cores, echidna-parade can improve the effectiveness of fuzzing with Echidna, due to the advantages provided by multiple types of test configuration diversity. Using echidna-parade with multiple cores can produce significantly better results than Echidna, in less time. © 2021 ACM.},
author_keywords={Fuzzing;  Smart contracts;  Swarm testing;  Test diversity;  Test length},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2021607,
author={Gao, H. and Cheng, S. and Xue, Y. and Zhang, W.},
title={A lightweight framework for function name reassignment based on large-scale stripped binaries},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={607-619},
doi={10.1145/3460319.3464804},
art_number={3464804},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111455207&doi=10.1145%2f3460319.3464804&partnerID=40&md5=bb68b93671b9ae738d8dacff6913dcbb},
affiliation={CAS Key Laboratory of Electro-magnetic Space Information, University of Science and Technology of China, Hefei, China; School of Computer Science and Technology, University of Science and Technology of China, Hefei, China},
abstract={Software in the wild is usually released as stripped binaries that contain no debug information (e.g., function names). This paper studies the issue of reassigning descriptive names for functions to help facilitate reverse engineering. Since the essence of this issue is a data-driven prediction task, persuasive research should be based on sufficiently large-scale and diverse data. However, prior studies can only be based on small-scale datasets because their techniques suffer from heavyweight binary analysis, making them powerless in the face of big-size and large-scale binaries. This paper presents the Neural Function Rename Engine (NFRE), a lightweight framework for function name reassignment that utilizes both sequential and structural information of assembly code. NFRE uses fine-grained and easily acquired features to model assembly code, making it more effective and efficient than existing techniques. In addition, we construct a large-scale dataset and present two data-preprocessing approaches to help improve its usability. Benefiting from the lightweight design, NFRE can be efficiently trained on the large-scale dataset, thereby having better generalization capability for unknown functions. The comparative experiments show that NFRE outperforms two existing techniques by a relative improvement of 32% and 16%, respectively, while the time cost for binary analysis is much less. © 2021 ACM.},
author_keywords={Binary Analysis;  Neural Networks;  Reverse Engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Srivastava2021244,
author={Srivastava, P. and Payer, M.},
title={Gramatron: Effective grammar-aware fuzzing},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={244-256},
doi={10.1145/3460319.3464814},
art_number={3464814},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111430498&doi=10.1145%2f3460319.3464814&partnerID=40&md5=15c60f437d9554713aac480f1891f4ae},
affiliation={Purdue University, United States; EPFL, Switzerland},
abstract={Fuzzers aware of the input grammar can explore deeper program states using grammar-aware mutations. Existing grammar-aware fuzzers are ineffective at synthesizing complex bug triggers due to: (i) grammars introducing a sampling bias during input generation due to their structure, and (ii) the current mutation operators for parse trees performing localized small-scale changes. Gramatron uses grammar automatons in conjunction with aggressive mutation operators to synthesize complex bug triggers faster. We build grammar automatons to address the sampling bias. It restructures the grammar to allow for unbiased sampling from the input state space. We redesign grammar-aware mutation operators to be more aggressive, i.e., perform large-scale changes. Gramatron can consistently generate complex bug triggers in an efficient manner as compared to using conventional grammars with parse trees. Inputs generated from scratch by Gramatron have higher diversity as they achieve up to 24.2% more coverage relative to existing fuzzers. Gramatron makes input generation 98% faster and the input representations are 24% smaller. Our redesigned mutation operators are 6.4× more aggressive while still being 68% faster at performing these mutations. We evaluate Gramatron across three interpreters with 10 known bugs consisting of three complex bug triggers and seven simple bug triggers against two Nautilus variants. Gramatron finds all the complex bug triggers reliably and faster. For the simple bug triggers, Gramatron outperforms Nautilus four out of seven times. To demonstrate Gramatron's effectiveness in the wild, we deployed Gramatron on three popular interpreters for a 10-day fuzzing campaign where it discovered 10 new vulnerabilities. © 2021 Owner/Author.},
author_keywords={Dynamic software analysis;  Fuzzing;  Grammar-aware},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lyu2021270,
author={Lyu, Y. and Volokh, S. and Halfond, W.G.J. and Tripp, O.},
title={SAND: A static analysis approach for detecting SQL antipatterns},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={270-282},
doi={10.1145/3460319.3464818},
art_number={3464818},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111429937&doi=10.1145%2f3460319.3464818&partnerID=40&md5=51e76bac84707f8c6a64f893750d2bdd},
affiliation={Amazon, United States; University of Southern California, United States},
abstract={Local databases underpin important features in many mobile applications, such as responsiveness in the face of poor connectivity. However, failure to use such databases correctly can lead to high resource consumption or even security vulnerabilities. We present SAND, an extensible static analysis approach that checks for misuse of local databases, also known as SQL antipatterns, in mobile apps. SAND features novel abstractions for common forms of application/database interactions, which enables concise and precise specification of the antipatterns that SAND checks for. To validate the efficacy of SAND, we have experimented with a diverse suite of 1,000 Android apps. We show that the abstractions that power SAND allow concise specification of all the known antipatterns from the literature (12-74 LOC), and that the antipatterns are modeled accurately (99.4-100% precision). As for performance, SAND requires on average 41 seconds to complete a scan on a mobile app. © 2021 ACM.},
author_keywords={Database;  Mobile applications;  Performance;  Security},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sun2021204,
author={Sun, J. and Su, T. and Li, J. and Dong, Z. and Pu, G. and Xie, T. and Su, Z.},
title={Understanding and finding system setting-related defects in Android apps},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={204-215},
doi={10.1145/3460319.3464806},
art_number={3464806},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111417520&doi=10.1145%2f3460319.3464806&partnerID=40&md5=e4d9d9e759e4ba5a43185f69f6f04e24},
affiliation={East China Normal University, China; National University of Singapore, Singapore; Peking University, China; ETH Zurich, Switzerland},
abstract={Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first empirical study to understand the characteristics of these setting-related defects (in short as "setting defects"), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over three person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate their impact, root causes, and consequences. We find that setting defects have a wide, diverse impact on apps' correctness, and the majority of these defects (≈70.7%) cause non-crash (logic) failures, and thus could not be automatically detected by existing app testing techniques due to the lack of strong test oracles. Motivated and guided by our study, we propose setting-wise metamorphic fuzzing, the first automated testing approach to effectively detect setting defects without explicit oracles. Our key insight is that an app's behavior should, in most cases, remain consistent if a given setting is changed and later properly restored, or exhibit expected differences if not restored. We realize our approach in SetDroid, an automated, end-to-end GUI testing tool, for detecting both crash and non-crash setting defects. SetDroid has been evaluated on 26 popular, open-source apps and detected 42 unique, previously unknown setting defects in 24 apps. To date, 33 have been confirmed and 21 fixed. We also apply SetDroid on five highly popular industrial apps, namely WeChat, QQMail, TikTok, CapCut, and AlipayHK, all of which each have billions of monthly active users. SetDroid successfully detects 17 previously unknown setting defects in these apps' latest releases, and all defects have been confirmed and fixed by the app vendors. The majority of SetDroid-detected defects (49 out of 59) cause non-crash failures, which could not be detected by existing testing tools (as our evaluation confirms). These results demonstrate SetDroid's strong effectiveness and practicality. © 2021 ACM.},
author_keywords={Android;  Empirical study;  Setting;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2021103,
author={Zhang, L. and Zhang, Y. and Zhang, M.},
title={Efficient white-box fairness testing through gradient search},
journal={ISSTA 2021 - Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2021},
pages={103-114},
doi={10.1145/3460319.3464820},
art_number={3464820},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111416008&doi=10.1145%2f3460319.3464820&partnerID=40&md5=4095e806105d68281a1d239513506743},
affiliation={East China Normal University, Shanghai, China; Singapore Management University, Singapore, Singapore},
abstract={Deep learning (DL) systems are increasingly deployed for autonomous decision-making in a wide range of applications. Apart from the robustness and safety, fairness is also an important property that a well-designed DL system should have. To evaluate and improve individual fairness of a model, systematic test case generation for identifying individual discriminatory instances in the input space is essential. In this paper, we propose a framework EIDIG for efficiently discovering individual fairness violation. Our technique combines a global generation phase for rapidly generating a set of diverse discriminatory seeds with a local generation phase for generating as many individual discriminatory instances as possible around these seeds under the guidance of the gradient of the model output. In each phase, prior information at successive iterations is fully exploited to accelerate convergence of iterative optimization or reduce frequency of gradient calculation. Our experimental results show that, on average, our approach EIDIG generates 19.11% more individual discriminatory instances with a speedup of 121.49% when compared with the state-of-the-art method and mitigates individual discrimination by 80.03% with a limited accuracy loss after retraining. © 2021 ACM.},
author_keywords={Fairness testing;  Neural networks;  Software bias;  Test case generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Langdon20211683,
author={Langdon, W.B. and Petke, J. and Clark, D.},
title={Dissipative polynomials},
journal={GECCO 2021 Companion - Proceedings of the 2021 Genetic and Evolutionary Computation Conference Companion},
year={2021},
pages={1683-1691},
doi={10.1145/3449726.3463147},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111035351&doi=10.1145%2f3449726.3463147&partnerID=40&md5=344338ae4faa22f2cfff903957dac7b0},
affiliation={University College London, Department of Computer Science, London, United Kingdom},
abstract={Limited precision floating point computer implementations of large polynomial arithmetic expressions are nonlinear and dissipative. They are not reversible (irreversible, lack conservation), lose information, and so are robust to perturbations (anti-fragile) and resilient to fluctuations. This gives a largely stable locally flat evolutionary neutral fitness search landscape. Thus even with a large number of test cases, both large and small changes deep within software typically have no effect and are invisible externally. Shallow mutations are easier to detect but their RMS error need not be simple. © 2021 ACM.},
author_keywords={correctness attraction;  diversity;  entropy;  evolvability;  genetic programming;  information funnels;  information loss;  introns;  mutational robustness;  neutral networks;  sbse;  software robustness;  software testing;  theory of bloat},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang2021175,
author={Yang, S. and Sun, S. and Liu, X. and Zhang, Y. and Li, W. and Li, Q. and Zhang, W.},
title={Calorific Value and Proximate Analysis of Carbonized Materials Derived from 24 Bamboo Speaes [24种竹材炭化热值与工业分析]},
journal={Linye Kexue/Scientia Silvae Sinicae},
year={2021},
volume={57},
number={7},
pages={175-183},
doi={10.11707/j.1001-7488.20210719},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114618435&doi=10.11707%2fj.1001-7488.20210719&partnerID=40&md5=34efd066b18e9274042797591a6a5756},
affiliation={College of Chemistry and Materials Engineering, Zhejiang A & F University, Hangzhou, 311300, China; Key Laboratory of Bamboo Research of Zhejiang, Zhejiag Academy of Forestry, Hangzhou, 310023, China; International Center for Bamboo and Rattan, Beijing, 100102, China},
abstract={【Objective】The relationships were explored between caloric values of bamboo charcoal and different species, ages and parts of bamboos under the same carbonization process, calorific values of bamboo charcoal were correlatively analyzed with their ashes, volatiles and fixed carbon contents, and the deduced empirical formula was also applied for calorific values calculation in order to provide guidance and reference for the industrialization and energy utilization of bamboo wood. 【Method】The calorific values of bamboo charcoals carbonized from various bamboo species were taken as the objects of this study. 24 bamboo species grown in Lin'an, Zhejiang Province, and 2- to 13-year-old moso bamboo grown in Anji, Zhejiang Province, were collected and carbonized under the same process. Applying the single-factor-control method, the calorific values and proximate analysis of bamboo charcoals which were carbonized by middle parts of 24 bamboo species and moso bamboos(2-13 years), and top, middle and base parts of 4-,5-,6-year-old moso bamboos were tested, respectively. The effects of species, ages and parts of bamboo on the calorific value and fixed carbon, volatile, ash content were studied. Test data were summarized to find whether the calorific value was significantly correlated with fixed carbon, volatile and ash content by SPSS software. The empirical formula was deduced based on the relationships between the carbonization temperature(T), calorific value(Q) and fixed carbon(C).【Result】The calorific values of 24 kinds of bamboo charcoal were distributed in the range of 27.94-32.98 kJ•g-1, the mean value was 31.10 kJ•g-1, the standard deviation was 1.11; the fixed carbon content was 75.35%-92.59%, the mean value was 85.87%, and the standard deviation was 3.65; the ash content was 3.34%-15.98%, the mean value was 7.21%, and the average volatile content was 6.91%. The calorific value of the charcoal produced from 2- to 13-year-old bamboo was distributed in the range of 30.93-33.81 kJ•g-1, and the standard deviation of the fixed carbon content, ash content and volatile content were all less than 5, respectively. For 4-, 5- and 6-year-old bamboo carbonized materials, the absolute difference of calorific value of each part was below 1.38 kJ•g-1, and the relative difference was under 3%. The high calorific value of bamboo charcoal was positively correlated with fixed carbon content, but negatively correlated with ash content. According to the relationships between the carbonization temperature, calorific value and fixed carbon of bamboo charcoal, the deduced empirical formulas were as follows: Q=0.001 8C2-0.111C+28.099(R2=0.72),C=26.934lnT-93.122(R2=0.88).【Conclusion】 1) The diversity exists between calorific value and proximate analysis of bamboo charcoals carbonized from different bamboo species, which depends on the structural characteristics of bamboo species. The calorific value of carbonized materials of the same bamboo species at the base is higher than that at the middle and top parts, which is attributed to the higher lignin content at the base. In addition, bamboo ages and growth parts have no significant influences on calorific value and proximate analysis. 2) There is a linear relationship between the calorific value of bamboo charcoal and the fixed carbon and ash content, of which the calorific value is positively correlated with fixed carbon content and negatively correlated with ash content. The deduced empirical formulas Q=0.001 8C2-0.111C+28.099(R2=0.72) and C=26.934lnT-93.122(R2=0.88). © 2021, Editorial Department of Scientia Silvae Sinicae. All right reserved.},
author_keywords={Bamboo age;  Bamboo part;  Bamboo species;  Calorific values;  Empirical formula;  Proximate analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chien2021589,
author={Chien, H.-Y. and Huang, C.-Y. and Fang, C.-C.},
title={Applying slicing-based testability transformation to improve test data generation with symbolic execution},
journal={International Journal of Performability Engineering},
year={2021},
volume={17},
number={7},
pages={589-599},
doi={10.23940/ijpe.21.07.p3.589599},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112658205&doi=10.23940%2fijpe.21.07.p3.589599&partnerID=40&md5=11be6650d5ebeb744a0d7b67d7297b2b},
affiliation={Institue of Information Systems and Applications, National Tsing Hua University, Hsinchu, 300044, Taiwan; Department of Computer Science, National Tsing Hua University, Hsinchu, 300044, Taiwan},
abstract={Symbolic execution techniques are widely adopted in diverse fields, including software quality analysis, software defect detection and so on. Symbolic execution engines assist people to apply the technique on exploiting and test data generation conveniently. However, symbolic execution is quite hard to scale to large and complicated programs which have massive paths and conditional statements due to path explosion. Moreover, for the development cycle of software systems, the necessity to generate test data frequently and rapidly makes them tough to apply symbolic execution technique on software testing. In this paper, we propose three modes of slicing-based testability transformations to change the semantics of programs while maintaining or improving the code coverage of generated test data by the symbolic execution tool: KLEE. The concept of these testability transformations is in decreasing the execution load of KLEE through shortening execution paths in programs. Our proposed testability transformation Mode 1 (TTM1) slices programs with an automated program slicing tool and takes functions in the program as the slicing criterion. On the other hand, our proposed testability transformations Mode 2 (TTM2) and Mode 3 (TTM3) slice programs manually and limit the max depth of paths in programs to a specific value as the slicing criterion. Experimental results show TTM1 could decrease 4.5% of solver queries while increasing 6.6% of code coverage on average. TTM2 could decrease 7.7% of solver queries while increasing 5.4% of coverage on average. TTM3 could decrease 17.6% of solver queries while increasing 3.6% of coverage on average. It can be observed that though the semantics of programs change due to slicing, the coverage of test data generated by KLEE could maintain the same or even become higher. Our finding also indicates that there is room to design slicing-based testability transformations to improve quality of test generation with symbolic execution. © 2021 Totem Publisher, Inc. All rights reserved.},
author_keywords={Automated test generation;  KLEE;  Program slicing;  Symbolic execution;  Testability transformation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Menéndez2021,
author={Menéndez, H.D. and Jahangirova, G. and Sarro, F. and Tonella, P. and Clark, D.},
title={Diversifying Focused Testing for Unit Testing},
journal={ACM Transactions on Software Engineering and Methodology},
year={2021},
volume={30},
number={4},
doi={10.1145/3447265},
art_number={44},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112081320&doi=10.1145%2f3447265&partnerID=40&md5=402001c564e76cc21560601175706987},
affiliation={Middlesex University London, The Burroughs, Hendon, London, NW4 4BG, United Kingdom; Università della Svizzera Italiana, Via Buffi, Lugano, 6900, Switzerland; University College London, Gower Street, London, WC1E 6BT, United Kingdom},
abstract={Software changes constantly, because developers add new features or modifications. This directly affects the effectiveness of the test suite associated with that software, especially when these new modifications are in a specific area that no test case covers. This article tackles the problem of generating a high-quality test suite to cover repeatedly a given point in a program, with the ultimate goal of exposing faults possibly affecting the given program point. Both search-based software testing and constraint solving offer ready, but low-quality, solutions to this: Ideally, a maximally diverse covering test set is required, whereas search and constraint solving tend to generate test sets with biased distributions. Our approach, Diversified Focused Testing (DFT), uses a search strategy inspired by GödelTest. We artificially inject parameters into the code branching conditions and use a bi-objective search algorithm to find diverse inputs by perturbing the injected parameters, while keeping the path conditions still satisfiable. Our results demonstrate that our technique, DFT, is able to cover a desired point in the code at least 90% of the time. Moreover, adding diversity improves the bug detection and the mutation killing abilities of the test suites. We show that DFT achieves better results than focused testing, symbolic execution, and random testing by achieving from 3% to 70% improvement in mutation score and up to 100% improvement in fault detection across 105 software subjects. © 2021 ACM.},
author_keywords={DFT;  diversity;  focused testing;  GödelTest;  Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khanna202181,
author={Khanna, M. and Chauhan, N. and Sharma, D.K. and Singh, L.K.},
title={A multi-objective approach for test suite reduction during testing of web applications: A search-based approach},
journal={International Journal of Applied Metaheuristic Computing},
year={2021},
volume={12},
number={3},
pages={81-122},
doi={10.4018/IJAMC.2021070104},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110857797&doi=10.4018%2fIJAMC.2021070104&partnerID=40&md5=7f67af47fe2986693bebc4aa381e9ec4},
affiliation={Hindustan College of Science and Technology, Mathura, India; YMCA University of Science and Technology, Faridabad, India; GLA University, Mathura, India},
abstract={During the development and maintenance phases of evolving software, new test cases would be needed for the verification of the accuracy of the modifications as well as for new functionalities leading to an increase in the size of the test suite. Various related objectives are to be kept in mind while reducing the original test suite by removing redundancy and generating a practical representative set of the unique test cases, some of which may need to be maximized and the remaining ones minimized. This paper presents a multi-objective approach for the test suite reduction problem in which one objective is to be minimized and the remaining two maximized. In this study, experiments were performed on diverse versions of four web applications. Various state-of-the-art algorithms and their updated versions were compared with non-dominated sorting genetic algorithm-II (NSGA-II) for performance evaluation. Based on experimental findings, it was concluded that NSGA-II outperforms all other algorithms; moreover, the algorithm attempts to satisfy all the objectives without compromising coverage. Copyright © 2021, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.},
author_keywords={Greedy Algorithms;  Heuristics-Based Software Testing;  Multi-Objective Optimization NSGA-II;  Test Suite Reduction;  Web Application Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Peng2021,
author={Peng, Z. and Lin, X. and Simon, M. and Niu, N.},
title={Unit and regression tests of scientific software: A study on SWMM},
journal={Journal of Computational Science},
year={2021},
volume={53},
doi={10.1016/j.jocs.2021.101347},
art_number={101347},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104071997&doi=10.1016%2fj.jocs.2021.101347&partnerID=40&md5=d9b772ece86a90c03d848f6b85ecbf3a},
affiliation={Department of Electrical Engineering and Computer Science, University of Cincinnati, Cincinnati, OH  45221, United States; U.S. EPA Office of Research and Development, Center Water Infrastructure Division, Cincinnati, OH  45268, United States},
abstract={Testing helps assure software quality by executing a program and uncovering bugs. Scientific software developers often find it challenging to carry out systematic and automated testing due to reasons like inherent model uncertainties and complex floating-point computations. Extending the recent work on analyzing the unit tests written by the developers of the Storm Water Management Model (SWMM) [32], we report in this paper the investigation of both unit and regression tests of SWMM. The results show that the 2953 unit tests of SWMM have a 39.7% statement-level code coverage and a 82.4% user manual coverage. Meanwhile, an examination of 58 regression tests of SWMM shows a 44.9% statement-level code coverage and a near 100% user manual coverage. We also observe a “getter-setter-getter” testing pattern from the SWMM unit tests, and suggest a diversified way of executing regression tests. © 2021 Elsevier B.V.},
author_keywords={Regression testing;  Scientific software;  Storm Water Management Model (SWMM);  Test coverage;  Unit testing;  User manual},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bardin2021,
author={Bardin, S. and Kosmatov, N. and Marcozzi, M. and Delahaye, M.},
title={Specify and measure, cover and reveal: A unified framework for automated test generation},
journal={Science of Computer Programming},
year={2021},
volume={207},
doi={10.1016/j.scico.2021.102641},
art_number={102641},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102972622&doi=10.1016%2fj.scico.2021.102641&partnerID=40&md5=ca5ea7ef564a2aa2964c9d8639cdd1c6},
affiliation={Université Paris-Saclay, CEA, List, Palaiseau, France; Thales Research & Technology, Palaiseau, France; Imperial College London, United Kingdom; DGA, Bruz, France},
abstract={Automatic test input generation (ATG) is a major topic in software engineering, analysis and security. In this paper, we bridge the gap between state-of-the-art white-box ATG techniques, especially Dynamic Symbolic Execution, and the diversity of test objectives that they may be used to cover in practice, including many of those defined by common source-code coverage criteria. We define a new coverage specification mechanism, called labels, for specifying test objectives, and prove it to be both expressive and amenable to efficient automation. We present an efficient approach for detecting – revealing – infeasible (i.e. uncoverable) test objectives expressed as labels. We demonstrate that measuring the achieved coverage can be efficiently performed for labels. Finally, we propose an innovative extension of DSE resulting in an efficient support for label coverage, while the existing naive approach induces an exponential blow-up of the search space. Experiments show that our ATG technique yields very significant savings and confirm the interest of infeasible label detection, enabling to lift DSE to label coverage with only a slight overhead. Overall, we show that label coverage provides the basis of a rich framework allowing one to express and handle test objectives from various contexts in an efficient and generic manner. To illustrate this framework, we describe LTEST, an all-in-one testing toolset based on labels and used in the industry, which offers automatic program annotation, ATG, coverage measurement and detection of infeasible test objectives. © 2021 Elsevier B.V.},
author_keywords={Coverage criteria;  Software testing;  Static analysis;  Symbolic execution;  Test generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rabin2021,
author={Rabin, M.R.I. and Bui, N.D.Q. and Wang, K. and Yu, Y. and Jiang, L. and Alipour, M.A.},
title={On the generalizability of Neural Program Models with respect to semantic-preserving program transformations},
journal={Information and Software Technology},
year={2021},
volume={135},
doi={10.1016/j.infsof.2021.106552},
art_number={106552},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101620511&doi=10.1016%2fj.infsof.2021.106552&partnerID=40&md5=62be7f0961c03479aee7fcf3f7e7e4be},
affiliation={University of Houston, United States; Visa Research, United States; Singapore Management University, Singapore; The Open University, United Kingdom},
abstract={Context: With the prevalence of publicly available source code repositories to train deep neural network models, neural program models can do well in source code analysis tasks such as predicting method names in given programs that cannot be easily done by traditional program analysis techniques. Although such neural program models have been tested on various existing datasets, the extent to which they generalize to unforeseen source code is largely unknown. Objective: Since it is very challenging to test neural program models on all unforeseen programs, in this paper, we propose to evaluate the generalizability of neural program models with respect to semantic-preserving transformations: a generalizable neural program model should perform equally well on programs that are of the same semantics but of different lexical appearances and syntactical structures. Method: We compare the results of various neural program models for the method name prediction task on programs before and after automated semantic-preserving transformations. We use three Java datasets of different sizes and three state-of-the-art neural network models for code, namely code2vec, code2seq, and GGNN, to build nine such neural program models for evaluation. Results: Our results show that even with small semantically preserving changes to the programs, these neural program models often fail to generalize their performance. Our results also suggest that neural program models based on data and control dependencies in programs generalize better than neural program models based only on abstract syntax trees (ASTs). On the positive side, we observe that as the size of the training dataset grows and diversifies the generalizability of correct predictions produced by the neural program models can be improved too. Conclusion: Our results on the generalizability of neural program models provide insights to measure their limitations and provide a stepping stone for their improvement. © 2021 Elsevier B.V.},
author_keywords={Code representation;  Generalizability;  Model evaluation;  Neural models;  Program transformation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pereira2021167,
author={Pereira, P.R.M. and Costa, F.W.D. and Bolfe, E.L. and MacArringe, L. and Botelho, A.C.},
title={Comparison of classification algorithms of images for the mapping of the land covering in tasso fragoso municipality, Brazil},
journal={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
year={2021},
volume={5},
number={3},
pages={167-173},
doi={10.5194/isprs-annals-V-3-2021-167-2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113147235&doi=10.5194%2fisprs-annals-V-3-2021-167-2021&partnerID=40&md5=d7babcb2871f6b2fc3e6e1d5a2f6f9c4},
affiliation={University of Campinas - Unicamp, Graduate Programme in Geography, Campinas - SP, Brazil; Faculdade de Ciências e Tecnologia - UNESP PP, Presidente Prudente - SP, Brazil; Brazilian Agricultural Research Corporation, Embrapa Informática Agropecuária, Campinas - SP, Brazil},
abstract={One of the main applications of satellite images is the characterization of terrestrial coverage, that from the use of classification techniques, allows the monitoring of spatial transformations of the terrestrial surface, this process being directly associated with the potential of classifiers to differentiate the most diverse data present in the images, a fundamental aspect for the use of remote sensing data. This article evaluates the performance of different classification algorithms in the mapping classes of land use and land cover in medium resolution images from the Landsat 8 program, the test area of this test corresponds to the Municipality of Tasso Fragoso (State Maranhão - Brazil), stands out for a typical vegetation cover of the Cerrado Biome, presents similar spectral patterns that induce high difficulty of class differentiation automatically. In this paper, were analyzed the machine learning algorithms C5.0 and Random Forest in comparison to traditional classification algorithms being the Minimum Distance and the Spectral Angle Mapper. The best results were generated by Random Forest with 90% accuracy and Kappa of 0.861, followed by the C5.0 algorithm. Traditional algorithms, on the other hand, presented a lower precision rate with global accuracy, not exceeding 75% of accuracy and Kappa varying between 0.507 and 0.627. The accuracy of the producer showed that all the algorithms, in major or minor tendency presented difficulties in to differentiate the areas, with rates of mistakes varying between 25 and 75%, being the main, the confusion with pastoral areas. © 2021 Copernicus GmbH. All rights reserved.},
author_keywords={Cerrado Biome;  Digital Classification;  Landsat 8;  Maranhão State;  Performance Indexes},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2021,
author={Li, G. and Chen, S. and Dai, J. and Zhou, T. and Zhang, L.},
title={Numerical simulation of hydraulic transients in hydropower station with complicated water conveyance system},
journal={IOP Conference Series: Earth and Environmental Science},
year={2021},
volume={774},
number={1},
doi={10.1088/1755-1315/774/1/012142},
art_number={012142},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108590004&doi=10.1088%2f1755-1315%2f774%2f1%2f012142&partnerID=40&md5=9d221553d98f3080ba060f86f45ec7c9},
affiliation={Power China Huadong Engineering Corporation Limited, Zhejiang, China},
abstract={With the construction of Jinping II hydropower station with extra-large diversion tunnel, Baihetan hydropower station with complicated tailwater system, Yangfanggou hydropower station with complicated surge chamber, High requirements are put forward for the calculation and analysis of hydraulic transient process. In the water transmission and power generation system design of these projects, the following technical problems was solved by Power China Huadong Engineering Corporation Limited: simulation of variable impedance coefficient of differential surge chamber, interface tracking analysis of free-surface-pressure flow system, narrow upper chamber open channel effect and simulation of trigeminy surge chamber. The solutions to those technical problems and were summarized, and the hydraulic transients simulation and calculation software of complex waterway system - Hysim was developed. Based on the field test of Jinping II Hydropower Station, Baihetan hydropower station and Yangfanggou hydropower station, the simulation calculations were carried out. The accuracy of the hydraulic transients simulation software is verified by comparing the inversion results with the test values. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bian20211,
author={Bian, Z. and Blot, A. and Petke, J.},
title={Refining Fitness Functions for Search-Based Program Repair},
journal={Proceedings - 2021 IEEE/ACM International Workshop on Automated Program Repair, APR 2021},
year={2021},
pages={1-8},
doi={10.1109/APR52552.2021.00008},
art_number={9474534},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111104139&doi=10.1109%2fAPR52552.2021.00008&partnerID=40&md5=6df26009acf46623ac0de1bbe216d15e},
affiliation={University College London, London, United Kingdom},
abstract={Debugging is a time-consuming task for software engineers. Automated Program Repair (APR) has proved successful in automatically fixing bugs for many real-world applications. Search-based APR generates program variants that are then evaluated on the test suite of the original program, using a fitness function. In the vast majority of search-based APR work only the Boolean test case result is taken into account when evaluating the fitness of a program variant. We pose that more fine-grained fitness functions could lead to a more diverse fitness landscape, and thus provide better guidance for the APR search algorithms. We thus present 2Phase, a fitness function that also incorporates the output of test case failures, and compare it with ARJAe, that shares the same principles, and the standard fitness, that only takes the Boolean test case result into consideration. We conduct the comparison on 16 buggy programs from the QuixBugs benchmark using the Gin genetic improvement framework. The results show no significant difference in the performance of all three fitness functions considered. However, Gin was able to find 8 correct fixes, more than any of the APR tools in the recent QuixBugs study. © 2021 IEEE.},
author_keywords={Fitness Function;  Genetic Improvement;  Genetic Programming;  Program Repair;  Software Engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sinha20211829,
author={Sinha, P. and Singh, V.K. and Bohra, A. and Kumar, A. and Reif, J.C. and Varshney, R.K.},
title={Genomics and breeding innovations for enhancing genetic gain for climate resilience and nutrition traits},
journal={Theoretical and Applied Genetics},
year={2021},
volume={134},
number={6},
pages={1829-1843},
doi={10.1007/s00122-021-03847-6},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106268904&doi=10.1007%2fs00122-021-03847-6&partnerID=40&md5=a4252a3550378cae0d0231717baa5848},
affiliation={International Crops Research Institute for the Semi-Arid Tropics (ICRISAT), Hyderabad, India; International Rice Research Institute (IRRI), IRRI South Asia Hub, ICRISAT, Hyderabad, India; ICAR- Indian Institute of Pulses Research (IIPR), Kanpur, India; Leibniz Institute of Plant Genetics and Crop Plant Research (IPK), Gatersleben, Germany; State Agricultural Biotechnology Centre, Centre for Crop and Food Innovation, Food Futures Institute, Murdoch University, Murdoch, WA, Australia},
abstract={Key message: Integrating genomics technologies and breeding methods to tweak core parameters of the breeder’s equation could accelerate delivery of climate-resilient and nutrient rich crops for future food security. Abstract: Accelerating genetic gain in crop improvement programs with respect to climate resilience and nutrition traits, and the realization of the improved gain in farmers’ fields require integration of several approaches. This article focuses on innovative approaches to address core components of the breeder’s equation. A prerequisite to enhancing genetic variance (σ2g) is the identification or creation of favorable alleles/haplotypes and their deployment for improving key traits. Novel alleles for new and existing target traits need to be accessed and added to the breeding population while maintaining genetic diversity. Selection intensity (i) in the breeding program can be improved by testing a larger population size, enabled by the statistical designs with minimal replications and high-throughput phenotyping. Selection priorities and criteria to select appropriate portion of the population too assume an important role. The most important component of breeder′s equation is heritability (h2). Heritability estimates depend on several factors including the size and the type of population and the statistical methods. The present article starts with a brief discussion on the potential ways to enhance σ2g in the population. We highlight statistical methods and experimental designs that could improve trait heritability estimation. We also offer a perspective on reducing the breeding cycle time (t), which could be achieved through the selection of appropriate parents, optimizing the breeding scheme, rapid fixation of target alleles, and combining speed breeding with breeding programs to optimize trials for release. Finally, we summarize knowledge from multiple disciplines for enhancing genetic gains for climate resilience and nutritional traits. © 2021, The Author(s).},
document_type={Review},
source={Scopus},
}

@ARTICLE{Su2021537,
author={Su, Z. and Zhang, G. and Yue, F. and Zhan, D. and Li, M. and Li, B. and Yao, X.},
title={Enhanced Constraint Handling for Reliability-Constrained Multiobjective Testing Resource Allocation},
journal={IEEE Transactions on Evolutionary Computation},
year={2021},
volume={25},
number={3},
pages={537-551},
doi={10.1109/TEVC.2021.3055538},
art_number={9340399},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100479441&doi=10.1109%2fTEVC.2021.3055538&partnerID=40&md5=da150aaa2bcd8e5c52c479dfa9cb2e5a},
affiliation={School of Computer Science and Information Engineering, Hefei University of Technology, Hefei, China; CERCIA, School of Computer Science, University of Birmingham, Birmingham, United Kingdom},
abstract={The multiobjective testing resource allocation problem (MOTRAP) is how to efficiently allocate the finite testing time to various modules, with the aim of optimizing system reliability, testing cost, and testing time simultaneously. To deal with this problem, a common approach is to use multiobjective evolutionary algorithms (MOEAs) to seek a set of tradeoff solutions between the three objectives. However, such a tradeoff set may contain a substantial proportion of solutions with very low reliability level, which consume lots of computational resources but may be valueless to the software project manager. In this article, a MOTRAP model with a prespecified reliability is first proposed. Then, new lower bounds on the testing time invested in different modules are theoretically deduced from the necessary condition for the achievement of the given reliability, based on which an exact algorithm for determining the new lower bounds is presented. Moreover, several enhanced constraint-handling techniques (ECHTs) derived from the new bounds are successively developed to be combined with MOEAs to correct and reduce the constraint violation. Finally, the proposed ECHTs are evaluated in comparison with various state-of-the-art constraint-solving approaches. The comparative results demonstrate that the proposed ECHTs can work well with MOEAs, make the search focus on the feasible region of the prespecified reliability, and provide the software project manager with better and more diverse, satisfactory choices in test planning. © 1997-2012 IEEE.},
author_keywords={Constraint handling;  evolutionary algorithms (EAs);  multiobjective testing resource allocation;  reliability constraint},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sahin2021806,
author={Sahin, O. and Akay, B. and Karaboga, D.},
title={Archive-based multi-criteria Artificial Bee Colony algorithm for whole test suite generation},
journal={Engineering Science and Technology, an International Journal},
year={2021},
volume={24},
number={3},
pages={806-817},
doi={10.1016/j.jestch.2020.12.011},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100408094&doi=10.1016%2fj.jestch.2020.12.011&partnerID=40&md5=77bc9bb78d85eecf10f5d4993d773d2b},
affiliation={Erciyes University, The Department of Computer Engineering, Melikgazi, Kayseri  38039, Turkey; King Abdulaziz University, Faculty of Computing and Information Technology, Department of Information Systems, Jeddah, Saudi Arabia},
abstract={Testing an object-oriented software is harder than testing a structural program due to inheritance, states, behaviour of different objects, association, and polymorphism properties of the object-oriented software, which is established based on classes. The classes should be carefully tested using predefined test cases to optimize coverage goals. Because generating test cases manually is labour-intensive, search-based software testing methods that maximize coverage criteria by a search algorithm can be used to generate test cases automatically. Artificial Bee Colony (ABC) algorithm is a powerful search tool that performs balanced exploration and exploitation. In this study, two new multi-criteria and combinatorial ABC algorithms using mutation and crossover operators are proposed to generate a test suite that maximizes a fitness function combining various goals for object-oriented software. First, an archive-based ABC algorithm is proposed to keep covered targets in the archive to use available search resource efficiently. Second, the archive-based ABC algorithm is improved to bring more diversity in the population. To investigate the effect of the archive, the basic ABC algorithm and the archive-based ABC algorithms are compared on a set of classes from SF110 corpus. To validate efficiency of the proposed methods, they are compared to state-of-the-art evolutionary test suite generators and random testing methodology on the classes divided into difficulty levels based on their weight method class values. The experimental results reveal that introducing an archive into the ABC algorithm provides fast convergence compared to the basic ABC and the improved archive-based ABC produces higher coverage for the problems with both low and high difficulty levels. © 2020 Karabuk University},
author_keywords={Archive-based Artificial Bee Colony;  Artificial Bee Colony;  Genetic algorithm;  Software testing;  Test suite generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Deng2021,
author={Deng, T. and Robinson, W.N.},
title={Changes in emergent software development routines: The moderation effects of routine diversity},
journal={International Journal of Information Management},
year={2021},
volume={58},
doi={10.1016/j.ijinfomgt.2020.102306},
art_number={102306},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099317698&doi=10.1016%2fj.ijinfomgt.2020.102306&partnerID=40&md5=669802fa6784ba17b0eb4a88f9d1c92e},
affiliation={Department of Business Information & Analytics, Daniels College of Business, University of Denver, 2101 S. University Blvd, Denver, CO, United States; Department of Computer Information Systems, J. Mack Robinson College of Business, Georgia State University, 35 Broad Street, Atlanta, GA, United States},
abstract={Research into the role of routine change has been conducted in relatively stable and structured organizational settings. This study extends the current understanding of routine change to more fluid organizational forms. Drawing on the literature in organizational routines and routine change, we develop and test a model that captures the dynamic relationships among routine change, routine diversity, and project performance in the context of open-source software development. By sequence-mining the digital trace data of OSS projects, we show that routine change reduces project popularity; however, this effect is mitigated as routine diversity increases. Thus, routine diversity provides a coping mechanism for participants to adapt to changing routines. This study reveals the dynamics in open-source development routines and their effects on project popularity, which can apply to various other fluid organizational forms. © 2021 Elsevier Ltd},
author_keywords={Coordination;  Fluid organizations;  Open-source software development;  Organizational routines;  Project popularity;  Sequence analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20211259,
author={Wang, J. and Wang, S. and Chen, J. and Menzies, T. and Cui, Q. and Xie, M. and Wang, Q.},
title={Characterizing Crowds to Better Optimize Worker Recommendation in Crowdsourced Testing},
journal={IEEE Transactions on Software Engineering},
year={2021},
volume={47},
number={6},
pages={1259-1276},
doi={10.1109/TSE.2019.2918520},
art_number={8721154},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067033110&doi=10.1109%2fTSE.2019.2918520&partnerID=40&md5=b01caa903d2be83bd1487392746308b3},
affiliation={Laboratory for Internet Software Technologies, State Key Laboratory of Computer Sciences, Institute of Software Chinese Academy of Sciences, Beijing, China; York University, Toronto, ON, Canada; Department of Computer Science, North Carolina State University, Raleigh, NC, United States; Bytedance Inc., Beijing, China; Damo Academy of Alibaba Group, Beijing, China},
abstract={Crowdsourced testing is an emerging trend, in which test tasks are entrusted to the online crowd workers. Typically, a crowdsourced test task aims to detect as many bugs as possible within a limited budget. However not all crowd workers are equally skilled at finding bugs; Inappropriate workers may miss bugs, or report duplicate bugs, while hiring them requires nontrivial budget. Therefore, it is of great value to recommend a set of appropriate crowd workers for a test task so that more software bugs can be detected with fewer workers. This paper first presents a new characterization of crowd workers and characterizes them with testing context, capability, and domain knowledge. Based on the characterization, we then propose Multi-Objective Crowd wOrker recoMmendation approach (MOCOM), which aims at recommending a minimum number of crowd workers who could detect the maximum number of bugs for a crowdsourced testing task. Specifically, MOCOM recommends crowd workers by maximizing the bug detection probability of workers, the relevance with the test task, the diversity of workers, and minimizing the test cost. We experimentally evaluate MOCOM on 532 test tasks, and results show that MOCOM significantly outperforms five commonly-used and state-of-the-art baselines. Furthermore, MOCOM can reduce duplicate reports and recommend workers with high relevance and larger bug detection probability; because of this it can find more bugs with fewer workers. © 1976-2012 IEEE.},
author_keywords={crowd worker recommendation;  Crowdsourced testing;  multi-objective optimization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hawisa2021496,
author={Hawisa, K.A. and Jalboub, M.K. and Alganga, M.A.},
title={Determination of the Suitable Value of Resonance Point for Best Operation of Thyristor Controlled Series Capacitor},
journal={2021 IEEE 1st International Maghreb Meeting of the Conference on Sciences and Techniques of Automatic Control and Computer Engineering, MI-STA 2021 - Proceedings},
year={2021},
pages={496-500},
doi={10.1109/MI-STA52233.2021.9464446},
art_number={9464446},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113600951&doi=10.1109%2fMI-STA52233.2021.9464446&partnerID=40&md5=be4218c43280a6c220c2293e8346a6ba},
affiliation={Azzawia University, Faculty of Engineering, Dept. Os Electrical and Electronic, Azzawia, Libyan Arab Jamahiriya; Azzawia Higher Institute of Science and Technology, Dept. of Electrical and Electronic Engineering, Azzawia, Libyan Arab Jamahiriya},
abstract={Electricity generation is experiencing a remarkable diversion in recent years, where some effects drive it to such that, for example, electricity demand due to increasing of using electrical and electronic appliances, Market forces where the large companies are competing to increase their shares in the electrical aspect, and also the shortage of the natural resources is one of the most effective aspects for this diversion. With a view to increasing the power system transfer capabilities, a Flexible Ac transmission system (FACTS) could be a solution with its different categories. One of the categories is called Thyristors controlled series capacitor (TCSC). The TCSC is attached to the network in series, where the overall effective transmission line impedance can be controlled in order to increase the transfer power capability.From the three different operation modes, a Vernier mode was chosen, and because of the thyristor theory of how current pass within the thyristor controlled reactor (TCR) branch, the RMS value of the TCR current can vary and leads the equivalent impedance of TCR to change its inductance value from the basic value to infinity.In this paper, choosing the suitable value for the resonance point in Vernier mode, which determines the range of inductive and capacitive region of the TCSC, was considered.To simulate that, the IEEE 14 bus test system was applied in the Neplan program. And the simulation results showed great advances in power transfer capabilities as well as bus voltages. © 2021 IEEE.},
author_keywords={FACTS;  IEEE -14 bus test system and Neplan program;  RMS;  TCR;  TCSC;  Vernier mode},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{He2021410,
author={He, P. and Meister, C. and Su, Z.},
title={Testing machine translation via referential transparency},
journal={Proceedings - International Conference on Software Engineering},
year={2021},
pages={410-422},
doi={10.1109/ICSE43902.2021.00047},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115701240&doi=10.1109%2fICSE43902.2021.00047&partnerID=40&md5=6e1f4a0c41f84563f06bd797e43154db},
affiliation={Department of Computer Science, ETH Zurich, Switzerland},
abstract={Machine translation software has seen rapid progress in recent years due to the advancement of deep Neural Networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying Neural Networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic. © 2021 IEEE.},
author_keywords={Machine translation;  Metamorphic testing;  Referential transparency;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2021397,
author={Wang, Z. and You, H. and Chen, J. and Zhang, Y. and Dong, X. and Zhang, W.},
title={Prioritizing test inputs for deep neural networks via mutation analysis},
journal={Proceedings - International Conference on Software Engineering},
year={2021},
pages={397-409},
doi={10.1109/ICSE43902.2021.00046},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111447656&doi=10.1109%2fICSE43902.2021.00046&partnerID=40&md5=d7fa07eae4a63264d5f380d8a9d581ab},
affiliation={College O F Intelligence and Computing, Tianjin University, Tianjin, China; Information and Network Center, Tianjin University, Tianjin, China},
abstract={Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA. © 2021 IEEE.},
author_keywords={Deep Learning Testing;  Deep Neural Network;  Label;  Mutation;  Test Prioritization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Castellano202136,
author={Castellano, E. and Cetinkaya, A. and Thanh, C.H. and Klikovits, S. and Zhang, X. and Arcaini, P.},
title={Frenetic at the SBST 2021 Tool Competition},
journal={Proceedings - 2021 IEEE/ACM 14th International Workshop on Search-Based Software Testing, SBST 2021},
year={2021},
pages={36-37},
doi={10.1109/SBST52555.2021.00016},
art_number={9476234},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111127135&doi=10.1109%2fSBST52555.2021.00016&partnerID=40&md5=0d88fd3ba5d5bad646b811b1fa9c1c4b},
affiliation={National Institute of Informatics, Tokyo, Japan},
abstract={Frenetic is a genetic approach that leverages a curvature-based road representation. Given an autonomous driving agent, the goal of Frenetic is to generate roads where the agent fails to stay within its lane. In other words, Frenetic tries to minimize the 'out of bound distance', which is the distance between the car and either edge of the lane if the car is within the lane, and proceeds to negative values once the car drives off. This work resembles classic aspects of genetic algorithms such as mutations and crossover, but introduces some nuances aiming at improving diversity of the generated roads. © 2021 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Panichella202120,
author={Panichella, S. and Gambi, A. and Zampetti, F. and Riccio, V.},
title={SBST Tool Competition 2021},
journal={Proceedings - 2021 IEEE/ACM 14th International Workshop on Search-Based Software Testing, SBST 2021},
year={2021},
pages={20-27},
doi={10.1109/SBST52555.2021.00011},
art_number={9476243},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109206901&doi=10.1109%2fSBST52555.2021.00011&partnerID=40&md5=042cfc2e2f096f9d616144fc7080968b},
affiliation={Zurich University of Applied Science (ZHAW), Zurich, Switzerland; University of Passau, Passau, Germany; University of Sannio, Benevento, Italy; Software Institute-USI, Lugano, Switzerland},
abstract={We report on the organization, challenges, and results of the ninth edition of the Java Unit Testing Competition as well as the first edition of the Cyber-Physical Systems Testing Tool Competition. Java Unit Testing Competition. This year, five tools, Randoop, UtBot, Kex, Evosuite, and EvosuiteDSE, were executed on a benchmark with (i) new classes under test, selected from three open-source software projects, and (ii) the set of classes from three projects considered in the eighth edition. We relied on an improved Docker infrastructure to execute the tools and the subsequent coverage and mutation analysis. Given the high number of participants, we considered only two time budgets for test case generation: Thirty seconds and two minutes. Cyber-Physical Systems Testing Tool Competition. Five tools, Deeper, Frenetic, GABExplore, GAB Exploit, and Swat, competed on testing self-driving car software by generating simulation-based tests using our new testing infrastructure. We considered two experimental settings to study test generators' transitory and asymptotic behaviors and evaluated the tools' test generation effectiveness and the exposed failures' diversity. This paper describes our methodology, the statistical analysis of the results together with the contestant tools, and the challenges faced while running the competition experiments. © 2021 IEEE.},
author_keywords={Automated Test Generation;  Competition;  Java;  Self driving car software;  Simulation based Testing;  Tools;  Unit Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xiucai2021,
author={Xiucai, Z. and Tao, Y. and Wenan, H. and Guofeng, Z.},
title={Component design of word report generation based on LabWindows/CVI},
journal={Journal of Physics: Conference Series},
year={2021},
volume={1884},
number={1},
doi={10.1088/1742-6596/1884/1/012009},
art_number={012009},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105508670&doi=10.1088%2f1742-6596%2f1884%2f1%2f012009&partnerID=40&md5=394295981a4e1474b4c91e13ab425145},
affiliation={Ceyear Technologies Co., Ltd, Qingdao, 266555, China},
abstract={In order to realize the automation and diversification of word report generation business in the field of test and measurement, this paper first proposes a report generation collaborative processing mechanism based on a series of functional components through the demand analysis of report generation business and the design of software function allocation. Then, based on the development resources provided by LabWindows/CVI, this paper discusses the realization ways of the bookmark management and the automatic report generation based on word template. Finally, the practical effect of the word report generation business is given. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tan202162,
author={Tan, M. and Zhang, K. and Wu, X. and Liu, H.},
title={Experimental study on large particle solid-liquid two-phase flow in a centrifugal pump [泵内大颗粒固液两相流流动试验]},
journal={Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
year={2021},
volume={37},
number={8},
pages={62-67},
doi={10.11975/j.issn.1002-6819.2021.08.007},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108725032&doi=10.11975%2fj.issn.1002-6819.2021.08.007&partnerID=40&md5=e5536808e2afb8f79a0d8eb532398305},
affiliation={Research Center of Fluid Mechanical Engineering and Technology, Jiangsu University, Zhenjiang, 212013, China; School of Energy and Power Engineering, Jiangsu University, Zhenjiang, 212013, China},
abstract={As important equipment for solid material transportation, solid-liquid two-phase flow pump has been widely used in many fields and the medium was complex and diverse. Therefore, the internal flow characteristics, particle motion and collision of the solid-liquid two-phase flow in the pump have always been the focus of research. At present, the pump with small-scale solid phase has been studied well, but research on the mechanism of coarse particle solid-liquid two-phase flow in the pump is still insufficient. A single-stage single-suction cantilever centrifugal pump was selected as the research object and the high-speed photographic test method was used in the paper to study the movement rules on large diameter spherical particles in the solid-liquid two-phase flow pump, the ability to pass the pump and the collision law between the particles and the tongue. Through the processing of the test results by Motion Studio and MATLAB software, the movement laws of particles with different diameters and volume fractions in the solid-liquid two-phase pump was obtained. The experiment results indicated that the relative motion trajectories of spherical particles with different diameters all have a tendency to move toward the back of the blade at the impeller inlet. During the movement, it gradually deviates from the back of the blade and approaches the pressure side of the next blade. When the particle diameter is 6 mm, the relative motion trajectory is the longest, and when the particle diameter is 8 mm, the relative motion trajectory is the shortest. The passing pump time of spherical particles with different diameters mainly concentrated between 0.25-0.55 s. The average passing pump time of particles with diameters of 8 mm and 10 mm decreases by 15.15% and 11.03% respectively compared with that of particles with diameters of 6 mm. The relative motion trajectories of the three volume fractions of particles in the impeller basically coincide with each other, and they also tend to close to the back of the blade at the inlet of the impeller. With the passage of time, the particles gradually approach from the back of the blade to the pressure side of the next blade. But at the impeller outlet area, the particle outlet position is closest to the blade pressure surface when the volume concentration is 3%, the position where the particle flows out the impeller is farthest from the blade pressure surface when the volume concentration is 1%. The passing pump time of spherical particles with different volume fractions is mainly concentrated between 0.35-0.55 s. The average passing pump time of particles with the volume fraction is 3% and 5% increased by 4.38% and 3.21% respectively compared with that when the volume fraction of particles is 1%. When the volume fraction of particles is 1%, 3% and 5%, the collision probability between particles and tongue is 0.5%, 0.69% and 0.9% respectively. When the particle diameter was 6 mm, 8 mm and 10 mm, the probability of particle collision in the tongue region is 0.69%, 0.63% and 0.55% respectively. The research results can provide a reference for the structural design and anti-wear research of large particle two-phase flow pump. © 2021, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.},
author_keywords={Centrifugal pump;  Experiment;  High speed photography;  Large particles;  Solid-liquid two-phase flow},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jiang20211438,
author={Jiang, X. and Liu, H.},
title={Sustainable intelligent guiding and diversified integration under the background of data modeling},
journal={Proceedings - 5th International Conference on Computing Methodologies and Communication, ICCMC 2021},
year={2021},
pages={1438-1441},
doi={10.1109/ICCMC51019.2021.9418337},
art_number={9418337},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106030713&doi=10.1109%2fICCMC51019.2021.9418337&partnerID=40&md5=9b1fee4da7350d976d7049bc04aa00bd},
affiliation={Jiangxi University of Applied Science, School of Education and Physical Education, Nanchang, 330000, China; Nanchang University, School of Science and Technology, Nanchang, 330000, China},
abstract={At present, with the gradual application of information technology, manufacturing industry is experiencing a round of technological innovation. As a regional industrial park, many enterprises are faced with the opportunity of transformation. Because of their small scale, these enterprises are short of manufacturing resources, professional talents and knowledge. The emergence of manufacturing service platform can integrate resources from different channels, provide enterprises with services such as production, design and intellectual support, and improve manufacturing efficiency. Based on a brief analysis of the connotation of knowledge fusion and transformation in complex problem solving and the demand for collaborative innovation, this paper proposes a metaphor cognitive strategy and cognitive method of multiple integration according to its characteristics, and designs a data modeling method based on intelligent guidance and multiple theory. Based on the traffic intelligent guidance system, this paper uses Proteus Software to simulate and test some modules, including data acquisition module, time display module and indicator module, and uses electronic components and related materials to make each module and model. © 2021 IEEE.},
author_keywords={CAN Bus Design;  Data Modeling;  Intelligent Guidance;  Multi Theory;  Sustainability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rahmawati2021,
author={Rahmawati, D. and Dewi, A.K. and Endah Prasetya, A.W.},
title={Selection and evaluation of agronomic character of high temperature tolerant mutant gogo rice (Oryza sativa L) in nursery phase},
journal={IOP Conference Series: Earth and Environmental Science},
year={2021},
volume={672},
number={1},
doi={10.1088/1755-1315/672/1/012002},
art_number={012002},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104251924&doi=10.1088%2f1755-1315%2f672%2f1%2f012002&partnerID=40&md5=494f8cc0ac79c7d8801ef87f3fc0289f},
affiliation={Departement of Agricultural Production, State Polytechnic of Jember, Indonesia; Research and Development Center for Isotope and Radiation Technology Batan, Indonesia},
abstract={Increasing global temperatures due to climate change have a negative impact on crops. The aim of this research is to study the selection and evaluation of agronomic characters between mutant lines and comparison varieties against high temperature stress in the nursery phase. This research was conducted from September to December 2019 in the PAIR-BATAN (Isotope and Radiation Application Center-National Nuclear Technology Agency) greenhouse. This study used a qualitative descriptive method with a high temperature stress score according to the International Rice Testing Program (IRTP). The genetic material used was 64 numbers of the third generation Situgintung mutant lines tested and the comparison varieties were the N22 (high temperature tolerant) and IR52 (high temperature susceptible) variety and the original Situgintung variety. The results showed that of the 64 lines of the third generation of upland rice Situgintung, 15 lines were selected that had high temperature stress tolerance, namely 1 vulnerable line and 14 highly susceptible lines. The recovery score showed that there were 15 lines that were able to regrow after high temperature stress and had no different characters of agronomic diversity. Furthermore, to obtain homogeneous varieties of situgintung, purification and observation of the selected lines can be carried out. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dneprovskaya202178,
author={Dneprovskaya, N.V. and Shevtsova, I.V.},
title={Methodological Approach to Use of Web Content by Small Business},
journal={Scientific and Technical Information Processing},
year={2021},
volume={48},
number={2},
pages={78-86},
doi={10.3103/S0147688221020040},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114692588&doi=10.3103%2fS0147688221020040&partnerID=40&md5=67d8cd7f7be1aba275816db16b27389a},
affiliation={Plekhanov Russian University of Economics, Moscow, 117997, Russian Federation; Lomonosov Moscow State University, Moscow, 119991, Russian Federation; HSE University, Moscow, 101000, Russian Federation},
abstract={Abstract: Digitization encourages accumulation of web content, an important resource of economic activity. Despite high level of development of technology of work with web content, the need for significant expenses restricts its use by small businesses. Web content characterized by absence of structure, diversity of sources, and high speed of data flow, is included in the concept of “Big Data”, efficient work with which requires access to financial, computing, and labor resources. The developed and tested methodological approach to use of web content, taking into account capabilities of small business, enables a specialist in any subject area to upload textual information, convert it into a database, and analyze it using widespread or public domain software. © 2021, Allerton Press, Inc.},
author_keywords={digital economy;  digitalization of society;  small business;  text analysis automation;  web content},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Flemstrom2021351,
author={Flemstrom, D. and Jonsson, H. and Enoiu, E.P. and Afzal, W.},
title={Industrial Scale Passive Testing with T-EARS},
journal={Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation, ICST 2021},
year={2021},
pages={351-361},
doi={10.1109/ICST49551.2021.00047},
art_number={9438550},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107966580&doi=10.1109%2fICST49551.2021.00047&partnerID=40&md5=5a30ccc6791b7af35ef6a910b2cd355d},
affiliation={Mälardalen University, Sweden; Mälardalen University, Bombardier Transportation Ab, Sweden},
abstract={Passive testing continuously observes the system or system execution logs without any interference or instrumentation to test diverse combinations of functions, resulting in a more thorough evaluation over time. However, reaching a working solution to passive testing is not without challenges. While there have been some efforts to extract information from system requirements to create passive test cases, to our knowledge, no such efforts are mature enough to be applied in a real, industrial safety-critical context. Our passive testing approach uses the Timed Easy Approach to Requirements Syntax (T-EARS) specification language and its accompanying tool-chain. This study reports challenges and solutions to introducing system-level passive testing for a vehicular safety-critical system through industrial data analysis, including 116 safety-related requirements. Our results show that passive testing using the T-EARS language and its tool-chain can be used for system-level testing in an industrial setting for 64% of the studied requirements. We identified several sources of false positive results and show how to tune test cases to reduce such false positives systematically. Finally, we show the requirement coverage achieved by a manual test session and that passive testing using T-EARS can find a set of injected faults that are considered hard to find with other test techniques. © 2021 IEEE.},
author_keywords={passive testing;  software testing;  test case automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Afzal2021263,
author={Afzal, A. and Katz, D.S. and Le Goues, C. and Timperley, C.S.},
title={Simulation for Robotics Test Automation: Developer Perspectives},
journal={Proceedings - 2021 IEEE 14th International Conference on Software Testing, Verification and Validation, ICST 2021},
year={2021},
pages={263-274},
doi={10.1109/ICST49551.2021.00036},
art_number={9438553},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106379446&doi=10.1109%2fICST49551.2021.00036&partnerID=40&md5=caf2505f588f8f6686bb4d69d312a018},
affiliation={Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={Robotics simulation plays an important role in the design, development, and verification and validation of robotics systems. Simulation represents a potentially cheaper, safer, and more reliable alternative to the widely used practice of manual field testing, and introduces valuable opportunities for extensive test automation. The goal of this paper is to develop a principled understanding of the ways robotics developers use simulation in their testing processes and the challenges they face in doing so. This understanding can guide the improvement of simulators and testing techniques for modern robotics development.To that end, we conduct a survey of 82 robotics developers from a diversity of backgrounds, addressing the current capabilities and limits of simulation in practice. We find that simulation is used by 84% of our participants for testing, and that many participants want to use simulation as part of their test automation. Using qualitative and quantitative research methods, we identify 10 high-level challenges that impede developers from using simulation for manual and automated testing and in general. These challenges include the gap between simulation and reality, a lack of reproducibility, and considerable resource costs associated with simulation. Finally, we outline ways in which simulators can be improved for use as a means of verification and validation and ways that the software engineering community can contribute to these improvements. © 2021 IEEE.},
author_keywords={empirical study;  practitioner survey;  robotics simulation challenges;  robotics testing;  simulation testing;  testing challenges},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nuh2021,
author={Nuh, J.A. and Koh, T.W. and Baharom, S. and Osman, M.H. and Kew, S.N.},
title={Performance evaluation metrics for multi-objective evolutionary algorithms in search-based software engineering: Systematic literature review},
journal={Applied Sciences (Switzerland)},
year={2021},
volume={11},
number={7},
doi={10.3390/app11073117},
art_number={3117},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103552991&doi=10.3390%2fapp11073117&partnerID=40&md5=c2baaf374612ba5acbd852f76199471e},
affiliation={Faculty of Computer Science and Information Technology, Universiti Putra Malaysia (UPM), Serdang, 43400, Malaysia; Faculty of Social Sciences and Humanities, Universiti TeknologiMalaysia, Skudai, 80130, Malaysia},
abstract={Many recent studies have shown that various multi-objective evolutionary algorithms have been widely applied in the field of search-based software engineering (SBSE) for optimal solutions. Most of them either focused on solving newly re-formulated problems or on proposing new approaches, while a number of studies performed reviews and comparative studies on the performance of proposed algorithms. To evaluate such performance, it is necessary to consider a number of performance metrics that play important roles during the evaluation and comparison of investigated algorithms based on their best-simulated results. While there are hundreds of performance metrics in the literature that can quantify in performing such tasks, there is a lack of systematic review conducted to provide evidence of using these performance metrics, particularly in the software engineering problem domain. In this paper, we aimed to review and quantify the type of performance metrics, number of objectives, and applied areas in software engineering that reported in primary studies-this will eventually lead to inspiring the SBSE community to further explore such approaches in depth. To perform this task, a formal systematic review protocol was applied for planning, searching, and extracting the desired elements from the studies. After considering all the relevant inclusion and exclusion criteria for the searching process, 105 relevant articles were identified from the targeted online databases as scientific evidence to answer the eight research questions. The preliminary results show that remarkable studies were reported without considering performance metrics for the purpose of algorithm evaluation. Based on the 27 performance metrics that were identified, hypervolume, inverted generational distance, generational distance, and hypercube-based diversity metrics appear to be widely adopted in most of the studies in software requirements engineering, software design, software project management, software testing, and software verification. Additionally, there are increasing interest in the community in re-formulating many objective problems with more than three objectives, yet, currently are dominated in re-formulating two to three objectives. © 2021 by the authors.},
author_keywords={Manyobjective evolutionary algorithms;  Multi-objective evolutionary algorithms;  Performance metrics;  Search-based software engineering},
document_type={Review},
source={Scopus},
}

@ARTICLE{Xu20212534,
author={Xu, C. and Yang, X. and He, Z. and Qiu, J. and Gao, H.},
title={Precise Positioning of Circular Mark Points and Transistor Components in Surface Mounting Technology Applications},
journal={IEEE Transactions on Industrial Informatics},
year={2021},
volume={17},
number={4},
pages={2534-2544},
doi={10.1109/TII.2020.2999023},
art_number={9104889},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099518898&doi=10.1109%2fTII.2020.2999023&partnerID=40&md5=42d08b1a463dbb88d2c6e1cf3eb3ec3d},
affiliation={Research Institute of Intelligent Control and Systems, The Department of Control Science and Engineering, Harbin Institute of Technology, Harbin, 150001, China},
abstract={The visual inspection algorithms are the core of automatic optical inspection system on surface mounting machines. This article is concerned with the development of precise positioning algorithms for circular mark points and transistor (TR) components on surface mounting devices. To handle nonuniform illumination or occlusion of other components, a polar coordinate transform and smoothness selection based circular mark point location method is proposed. The TR components are fundamental chips in electronic products and have various package types. The illumination changes, background disturbance, and the diversity of package types have imposed great challenges on the development of the uniform algorithm for detection and location of TR components. To deal with these issues, the 1-D integral image based TR component detection and location algorithm is proposed and the coordinates and orientation of the component are calculated simultaneously. The efficiency of the proposed methods is tested on real images and compared with classical Hough transform method, commercial algorithms on SMT482 device, and two methods of Halcon software. © 2005-2012 IEEE.},
author_keywords={Circular mark point;  detection and location;  surface mounting technology (SMT);  transistor (TR) component;  visual inspection algorithms},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ibias2021,
author={Ibias, A. and Núñez, M. and Hierons, R.M.},
title={Using mutual information to test from Finite State Machines: Test suite selection},
journal={Information and Software Technology},
year={2021},
volume={132},
doi={10.1016/j.infsof.2020.106498},
art_number={106498},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098462490&doi=10.1016%2fj.infsof.2020.106498&partnerID=40&md5=4f42e188fa8eccbf01869e89be57a9c5},
affiliation={Instituto de Tecnología del Conocimiento, Universidad Complutense de Madrid, Madrid, Spain; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom},
abstract={Context: Mutual Information is an information theoretic measure designed to quantify the amount of similarity between two random variables ranging over two sets. In this paper, we adapt this concept and show how it can be used to select a good test suite to test from a Finite State Machine (FSM) based on a maximise diversity approach. Objective: The main goal of this paper is to use Mutual Information in order to select test suites to test from FSMs and evaluate whether we obtain better results, concerning the quality of the selected test suite, than current state-of-the-art measures. Method: First, we defined our scenario. We considered the case where we receive two (or more) test suites and we have to choose between them. We were interested in this scenario because it is a recurrent case in regression testing. Second, we defined our notion based on Mutual Information: Biased Mutual Information. Finally, we carried out experiments in order to evaluate the measure. Results: We obtained experimental evidence that demonstrates the potential value of the measure. We also showed that the time needed to compute the measure is negligible when compare to the time needed to apply extra testing. We compared our measure with a state-of-the-art test selection measure and showed that our proposal outperforms it. Finally, we have compared our measure with a notion of transition coverage. Our experiments showed that our measure is slightly worse than transition coverage, as expected, but its computation is 10 times faster. Conclusion: Our experiments showed that Biased Mutual Information is a good measure for selecting test suites, outperforming the current state-of-the-art measure, and having a (negative) correlation to fault coverage. Therefore, we can conclude that our new measure can be used to select the test suite that is likely to find more faults. As a result, it has the potential to be used to automate test generation. © 2020 Elsevier B.V.},
author_keywords={Finite State Machines;  Formal approaches to testing;  Information Theory;  Mutual information},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Deshmukh20211048,
author={Deshmukh, Y.S. and Kumar, P. and Karan, R. and Singh, S.K.},
title={Breast Cancer Detection-Based Feature Optimization Using Firefly Algorithm and Ensemble Classifier},
journal={Proceedings - International Conference on Artificial Intelligence and Smart Systems, ICAIS 2021},
year={2021},
pages={1048-1054},
doi={10.1109/ICAIS50930.2021.9395788},
art_number={9395788},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105002768&doi=10.1109%2fICAIS50930.2021.9395788&partnerID=40&md5=cbea742263103975cef543c9b7c809d3},
affiliation={Madhyanchal Professional University, Department of Computer Science and Engineering, Bhopal, India},
abstract={The accurate detection of the malignant cell of breast cancer, decrease the rate of mortality of women around the world. The process of feature optimization, increase the probability of feature selection and mapping of data. This paper proposed ensemble-based classifier for detection of breast cancer on an early stage. The proposed ensemble classifier support vector machine is a base classifier, and the other is boost classifier. The firefly algorithm reduces the variance of breast cancer features for the selection of feature components for the classification algorithm. The most dominated work is the extraction of features of breast cancer image. For the extraction of features applied wavelet packet transform, wavelet packet transform overcome the limitation of wavelet transform and increase the diversity of feature extraction process. The proposed algorithm implemented in MATLAB software and tested with the very reputed breast cancer image dataset, MIAS and DDSM. The proposed algorithm's performance measured with standard parameters such as accuracy, specificity, sensitivity, and MCC. The evaluated results indicate that the proposed algorithm is better than DWPT, SVM and BMC. The increasing ratio of the classification algorithm is 8% instead of existing algorithms. © 2021 IEEE.},
author_keywords={component; Breast cancer;  Detection;  Ensemble;  Firefly;  MATLAB (key words);  SVM;  WPT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kireev2021,
author={Kireev, A. and Sklifus, Y. and Kireeva, M. and Ignatev, O.},
title={Automated Calculations of the Standardless Method for Adjusting the Time Corrected Gain Function of Ultrasonic Test Solutions for Railway Rolling Stock},
journal={ACM International Conference Proceeding Series},
year={2021},
doi={10.1145/3487757.3490935},
art_number={3490935},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125372085&doi=10.1145%2f3487757.3490935&partnerID=40&md5=04695c644b71055d2facce0242bda4f7},
affiliation={Lugansk State University Volodymyr Dahl, Lugansk, Ukraine; Rostov State Transport University, Rostov-on-Don, Russian Federation},
abstract={As means of transportation, the railway is gaining more and more importance, and the given fact has inspired a wide array of innovations, including the use of mobile devices [1] with calibration reflectors for ultrasonic diagnostics of railway transport parts. However, this method needs to be improved since it has a number of disadvantages including a decrease in the amplitude of the echo signal from the reference reflectors due to low manufacturing accuracy that may lead to a decline in testing reliability. With this in mind, we tried to validate that a standardless method for adjusting the time corrected gain function significantly increases the reliability of ultrasonic testing of railway rolling stock parts. We have also devised novel software to automate calculations of the standard-less method for adjusting the time corrected gain function of ultrasonic devices. This software al-lows automating the calculations of the standardless method when using different types of artificial reflectors as references and considering the attenuation of an ultrasonic wave along diverse reflective surfaces. The given software makes it possible to improve the reliability of ultrasonic testing as well as to increase the adjustment speed of ultrasonic equipment, which will result in reducing cost per ultrasonic diagnostics. © 2021 ACM.},
author_keywords={pulse-echo method;  Rail transport parts;  standardless method;  ultrasonic testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Siegel2021,
author={Siegel, K. and Woodard, M. and Poland, D. and Bartels, A.},
title={Lucy Ground Readiness Testing during a National Pandemic},
journal={IEEE Aerospace Conference Proceedings},
year={2021},
volume={2021-March},
doi={10.1109/AERO50100.2021.9438271},
art_number={9438271},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111396869&doi=10.1109%2fAERO50100.2021.9438271&partnerID=40&md5=656d09d5677fc0878c947c32acc6ad56},
affiliation={General Dynamics Mission Systems (GDMS), Seabrook, MD  20706, United States},
abstract={The Lucy mission is slated to launch in the fall of 2021 with the objective of making scientific observations of seven of Jupiter's Trojan asteroids. A complex system of ground based infrastructure is necessary to operate the spacecraft and process scientific data. This infrastructure is comprised of six ground elements. A significant verification and validation (VV) effort must be undertaken to ensure reliable functionality of this geographically diverse system; this effort takes the form of Ground Readiness Testing (GRT). The two year GRT campaign requires extensive detailed planning, preparation and coordination across multiple mission elements for execution of many individual tests. The network connectivity, hardware and software functionality and a myriad of integrated systems need to be fully functionally tested. The GRT Manager must outline all testing opportunities in accordance with mission requirements, and element requirements to properly test the Ground System (GS) prior to launch, and needs to consider each mission element's internal schedule for testing and delivery to verify requirements in a timely manner. During the early stages of performing GRTs, the COVID-19 pandemic forced the team to restrict work primarily to remote operations with restricted physical access to those systems. This restriction brought with it new challenges in the management and execution of the GRTs, which would not normally have arisen in other pre-launch environments. The pandemic challenged GRT management to rethink their approach to a successful schedule of testing. Challenges came in the form of communication, lack of real-time in-person troubleshooting, and decisions that take into account the safety of the team as well as the need for engineering progress to be made. For example, work on the actual GS hardware and some software needed for testing and troubleshooting has slowed. The team needs to determine the true need for going into the office in order to update, integrate, or fix any issues for testing. No longer can someone walk down the hall to chat, brainstorm or fix issues of concern quickly. When face-to-face coordination is no longer available, new ways of communication need to be put in place. Ensuring successful completion of the GRT campaign is different in both its management and execution due to the pandemic. Prior to March 2020, the Lucy GS was already coordinating meetings and whiteboard sessions across the country since the mission elements were spread across the U.S. Now the entire GS has more opportunity to meet from the safety of their own home. For many this relieved some of the stress associated with the work environment and allowed team members to be more productive in their work life, and active in their home life. Less time and money is spent traveling via car or plane to accomplish ground readiness testing. The paper will describe how the Lucy team met new challenges and took advantage of the unforeseen benefits while staying on track to complete the GRT campaign successfully ahead of the October 2021 launch. During the early stages of performing GRTs, the COVID-19 pandemic forced the team to restrict work primarily to remote operations with restricted physical access to those systems. This restriction brought with it new challenges in the management and execution of the GRTs, which would not normally have arisen in other pre-launch environments. The pandemic challenged GRT management to rethink their approach to a successful schedule of testing. Challenges came in the form of communication, lack of real-time in-person troubleshooting, and decisions that take into account the safety of the team as well as the need for engineering progress to be made. For example, work on the actual GS hardware and some software needed for testing and troubleshooting has slowed. The team needs to determine the true need for going into the office in order to update, integrate, or fix any issues for testing. No longer can someone walk down the hall to chat, brainstorm or fix issues of concern quickly. When face-to-face coordination is no longer available, new ways of communication need to be put in place. Ensuring successful completion of the GRT campaign is different in both its management and execution due to the pandemic. Prior to March 2020, the Lucy GS was already coordinating meetings and whiteboard sessions across the country since the mission elements were spread across the U.S. Now the entire GS has more opportunity to meet from the safety of their own home. For many this relieved some of the stress associated with the work environment and allowed team members to be more productive in their work life, and active in their home life. Less time and money is spent traveling via car or plane to accomplish ground readiness testing. The paper will describe how the Lucy team met new challenges and took advantage of the unforeseen benefits while staying on track to complete the GRT campaign successfully ahead of the October 2021 launch. Prior to March 2020, the Lucy GS was already coordinating meetings and whiteboard sessions across the country since the mission elements were spread across the U.S. Now the entire GS has more opportunity to meet from the safety of their own home. For many this relieved some of the stress associated with the work environment and allowed team members to be more productive in their work life, and active in their home life. Less time and money is spent traveling via car or plane to accomplish ground readiness testing. The paper will describe how the Lucy team met new challenges and took advantage of the unforeseen benefits while staying on track to complete the GRT campaign successfully ahead of the October 2021 launch. mission elements were spread across the U.S. Now the entire GS has more opportunity to meet from the safety of their own home. For many this relieved some of the stress associated with the work environment and allowed team members to be more productive in their work life, and active in their home life. Less time and money is spent traveling via car or plane to accomplish ground readiness testing. The paper will describe how the Lucy team met new challenges and took advantage of the unforeseen benefits while staying on track to complete the GRT campaign successfully ahead of the October 2021 launch. © 2021 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Coviello2021556,
author={Coviello, C. and Romano, S. and Scanniello, G. and Antoniol, G.},
title={Gasser},
journal={Proceedings - 2021 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2021},
year={2021},
pages={556-560},
doi={10.1109/SANER50967.2021.00065},
art_number={9426036},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106626720&doi=10.1109%2fSANER50967.2021.00065&partnerID=40&md5=963cd9a156bcd475e6b31c7d5d1fa296},
affiliation={University of Basilicata, Potenza, Italy; University of Bari, Bari, Italy; Polytechnique Montreal, Montreal, Canada},
abstract={Regression testing is an important activity that ensures a System Under Test (SUT) still works as expected after changes. Regression testing can be expensive in case of large Test Suites (TSs). Test Suite Reduction (TSR) approaches speed up regression testing by removing redundant test cases. These approaches can be classified as adequate or inadequate. Adequate approaches reduce TSs so that they completely preserve the test requirements (e.g., statement coverage) of the original TSs. Inadequate approaches produce reduced TSs that only partially preserve test requirements. An inadequate TSR approach is appealing when it leads to a higher reduction in TS size at the expense of a negligible loss in fault-detection capability. We defined an inadequate approach for TSR named GASSER (Genetic Algorithm for teSt SuitE Reduction). It is based on a multi-objective evolutionary algorithm, NSGA-II (Non-dominated Sorting Genetic Algorithm II). GASSER seeks to reduce TSs by maximizing both the statement coverage and diversity of test cases, and minimizing the size of the reduced TSs. We implemented GASSER in a Java prototype of a supporting tool and named it as the approach, namely GASSER. In this tooldemo paper, we present such a tool prototype as well as the results of a preliminary empirical study to assess the validity of both the approach and the tool prototype. A screen-cast of GASSER in action is available at https://youtu.be/20Uf1ugEvAQ. © 2021 IEEE.},
author_keywords={Genetic Algorithm;  Regression Testing;  Test Suite Reduction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Palak202139,
author={Palak and Gulia, P. and Gill, N.S.},
title={Optimized test case selection using scout-less hybrid artificial bee colony approach and crossover operator},
journal={International Journal of Engineering Trends and Technology},
year={2021},
volume={69},
number={3},
pages={39-45},
doi={10.14445/22315381/IJETT-V69I3P208},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103210392&doi=10.14445%2f22315381%2fIJETT-V69I3P208&partnerID=40&md5=cea37586d43bbab273d62f0099c03a4b},
affiliation={Department of Computer Science and Applications, Maharshi Dayanand University, India; Department of Computer Science and Applications, Maharshi Dayanand University, India; Department of Computer Science and Applications, Maharshi Dayanand University, India},
abstract={Efficient software testing depends on the quality of test cases that are capable of catching defects from every corner of the software and achieving higher coverage. In this article, a hybrid artificial bee colony optimization-based technique is proposed. The proposed approach defines the scout bee phase for abandoned solutions and incorporates features of a genetic algorithm for diversification. The proposed approach selects a minimal test suite with equivalent or better efficiency of its superset. It offers time and money-saving and contributes towards early product delivery. The proposed technique is assessed using five widely used programming problems and their mutants. When compared with similar existing techniques (i.e., Particle Swarm Optimization, Ant Colony Optimization, and Original Artificial Bee Colony) over various fitness ranges, the performance of the proposed approach shows better results and outperforms in terms of overall execution time and coverage. © 2021 Seventh Sense Research Group®.},
author_keywords={Artificial bee colony;  Genetic algorithm;  Software testing;  Swarm intelligence;  Test case selection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kang2021,
author={Kang, H.J. and Lo, D.},
title={Adversarial Specification Mining},
journal={ACM Transactions on Software Engineering and Methodology},
year={2021},
volume={30},
number={2},
doi={10.1145/3424307},
art_number={16},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101388218&doi=10.1145%2f3424307&partnerID=40&md5=a7c9f0d6a4bb45fb099da48dbcf44304},
affiliation={School of Information Systems, Singapore Management University, Singapore},
abstract={There have been numerous studies on mining temporal specifications from execution traces. These approaches learn finite-state automata (FSA) from execution traces when running tests. To learn accurate specifications of a software system, many tests are required. Existing approaches generalize from a limited number of traces or use simple test generation strategies. Unfortunately, these strategies may not exercise uncommon usage patterns of a software system. To address this problem, we propose a new approach, adversarial specification mining, and develop a prototype, Diversity through Counter-examples (DICE). DICE has two components: DICE-Tester and DICE-Miner. After mining Linear Temporal Logic specifications from an input test suite, DICE-Tester adversarially guides test generation, searching for counterexamples to these specifications to invalidate spurious properties. These counterexamples represent gaps in the diversity of the input test suite. This process produces execution traces of usage patterns that were unrepresented in the input test suite. Next, we propose a new specification inference algorithm, DICE-Miner, to infer FSAs using the traces, guided by the temporal specifications. We find that the inferred specifications are of higher quality than those produced by existing state-of-the-art specification miners. Finally, we use the FSAs in a fuzzer for servers of stateful protocols, increasing its coverage. © 2021 ACM.},
author_keywords={fuzzing;  search-based test generation;  Specification mining},
document_type={Article},
source={Scopus},
}

@ARTICLE{Thameur20211029,
author={Thameur, H.B. and Dayoub, I.},
title={Real-Time In-Lab Test of Eigenvalue-Based Spectrum Sensing Using USRP RIO SDR Boards},
journal={IEEE Communications Letters},
year={2021},
volume={25},
number={3},
pages={1029-1032},
doi={10.1109/LCOMM.2020.3037010},
art_number={9252926},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098750663&doi=10.1109%2fLCOMM.2020.3037010&partnerID=40&md5=9a591093d077631d2745aec29e48e5bf},
affiliation={Department of Opto-Acousto-Electronics (DOAE), Université Polytechnique Hauts-de-France, Valenciennes, 59313, France; Cnrs, Umr 8520 IEMN-DOAE, Isen, Centrale Lille, University of Lille, Valenciennes, 59313, France},
abstract={The diversity of prevailing and emerging cognitive radio (CR) applications makes CR an attractive domain to the researchers as well as the industry. The spectrum sensing (SS) is considered as the backbone of any autonomous CR. Thus, in this letter, we go one step further towards industrial integration of such CR feature by conducting in-lab test of a real communication scenario using a software defined radio (SDR) which is the universal software radio peripheral (USRP) platform. The eigenvalue-based SS (ESS) approaches have been, consistently, assessed in literature. Accordingly, in this work, we have proposed and tested a prototype integrating an ESS algorithm with the USRP (NI-2954R) reconfigurable I/O (RIO) devices. We have conducted real-time in-lab experiments. The proposed testbed has empirically demonstrated its capability to carry out a realistic communication scenario without loss of detection performances compared to software-based simulations. © 1997-2012 IEEE.},
author_keywords={Cognitive radio;  measurement;  multiple-antennas;  real-time;  SDR;  spectrum sensing;  testbed;  USRP RIO},
document_type={Article},
source={Scopus},
}

@ARTICLE{Han2021,
author={Han, D. and Li, G.},
title={Development of smart english classroom system based on FPGA software and hardware co-simulation test},
journal={Microprocessors and Microsystems},
year={2021},
volume={81},
doi={10.1016/j.micpro.2020.103774},
art_number={103774},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098740745&doi=10.1016%2fj.micpro.2020.103774&partnerID=40&md5=eee03efe6dd18bb2e775f73538744de8},
affiliation={University of York, York City, YO10 5DD, United Kingdom; Beijing Normal University, Beijing, 100875, China},
abstract={Improving English proficiency for professional students is a significant challenge in the AEC and the Renaissance. The problem found in the Polytechnic Gland Shipping Research Institute is the lack of time motivation and insufficient time for students to study outside the classroom lack of time. Therefore developed an Android-based “Technical English” application that supports PPN, English teaching, and learning processes. Development and Smart Classroom List (SCI) Verification. SCI is a learning environment tool that integrates existing technologies, including TROFLEI, TICI and CCEI (ergonomics). SCI is included to learn the functions, materials to describe flexibility, and experience factors in the smart classroom. The information technology detail used in some projects has also been adapted to the elegant classroom environment. Older classmates check the tool and reveal ten balances: body design, flexibility, and use of technology, learning data diversity, inquiry, collaboration, student drive, honesty and learning experience. The results of the Acid Ceasefire Agreement prove that is a simple tool for assessing the learning environment in a smart classroom. Student Differences, Flexibility, Technology Application, and Learning Data Factors There are four significant practical priority differences in the learning environment in a smart classroom. FPGA has received a lot of attention from the nuclear industry as an alternative platform for logical programming controllers for digitalization and control. The FPGA development software feature includes several steps for integration and refinement, and each such step requires a different simulation verification step. This research proposes an integrated software testing framework that simulates all the glitches created by FPGA software and evaluates whether it can work in general, using the standard Oracle programs for all glitches simultaneously. © 2020},
author_keywords={Fpga software;  Technical English;  TICI and CCEI;  TROFLEI},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gong202172,
author={Gong, L. and Wang, H. and Yang, Y. and Wang, Z. and Kang, C.},
title={A study on the impact effect of drift ice on the lining of a water tunnel [流冰对输水隧洞衬砌的撞击影响研究]},
journal={Zhendong yu Chongji/Journal of Vibration and Shock},
year={2021},
volume={40},
number={4},
pages={72-80},
doi={10.13465/j.cnki.jvs.2021.04.011},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102574434&doi=10.13465%2fj.cnki.jvs.2021.04.011&partnerID=40&md5=42845bcba853db8ef5da8e82618c526b},
affiliation={Department of Civil Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China; Institute of Water Diversion Engineering and Security of Water Transferring, Lanzhou Jiaotong University, Lanzhou, 730070, China},
abstract={In order to find the impact law of ice in water medium in different working conditions on the lining of a water tunnel, nonlinear finite element simulation of the impact of current ice and the water tunnel was carried out using the LS-DYNA software, and the corresponding indoor physical model test was verified with the geometric scale of Cl as 5. The results show that the impact effect of ice compression strength and thickness on tunnel lining respectively show approximate logarithmic function relationship and polynomial relationship; comprehensive analysis of the different simulated working conditions in this paper, it can be found that the effect of water medium and " effect of water cushion " are more obvious in the process of small flow ice impinging on the lining of the water supply tunnel, so these effects should be fully considered in the solution analysis. At the same time, the results obtained by simulation and experiment are basically consistent, indicating that the numerical simulation model adopted in this paper is accurate and reliable. © 2021, Editorial Office of Journal of Vibration and Shock. All right reserved.},
author_keywords={Drift ice;  Ice compression strength;  Impact effects;  Water diversion tunnel;  Water medium},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dabija20211,
author={Dabija, A. and Kluczek, M. and Zagajewski, B. and Raczko, E. and Kycko, M. and Al-Sulttani, A.H. and Tardà, A. and Pineda, L. and Corbera, J.},
title={Comparison of support vector machines and random forests for corine land cover mapping},
journal={Remote Sensing},
year={2021},
volume={13},
number={4},
pages={1-35},
doi={10.3390/rs13040777},
art_number={777},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101942532&doi=10.3390%2frs13040777&partnerID=40&md5=b9b0534774a9dcf4982ff277a0585b5d},
affiliation={Department of Geoinformatics, Cartography and Remote Sensing, Geomatics and Information Systems, Faculty of Geography and Regional Studies, University of Warsaw, Warszawa, 00-927, Poland; Catalan Earth Observation Centre, Cartographic and Geological Institute of Catalonia, Barcelona, E-08038, Spain},
abstract={Land cover information is essential in European Union spatial management, particularly that of invasive species, natural habitats, urbanization, and deforestation; there-fore, the need for accurate and objective data and tools is critical. For this purpose, the European Union’s flagship program, the Corine Land Cover (CLC), was created. Intensive works are currently being carried out to prepare a new version of CLC+ by 2024. The ge-ographical, climatic, and economic diversity of the European Union raises the challenge to verify various test areas’ methods and algorithms. Based on the Corine program’s pre-cise guidelines, Sentinel-2 and Landsat 8 satellite images were tested to assess classification accuracy and regional and spatial development in three varied areas of Catalonia, Poland, and Romania. The method is dependent on two machine learning algorithms, Random Forest (RF) and Support Vector Machine (SVM). The bias of classifications was reduced using an iterative of randomized training, test, and verification pixels. The ease of the implementation of the used algorithms makes reproducing the results possible and comparable. The results show that an SVM with a radial kernel is the best classifier, fol-lowed by RF. The high accuracy classes that can be updated and classes that should be redefined are specified. The methodology’s potential can be used by developers of CLC+ products as a guideline for algorithms, sensors, and the possibilities and difficulties of classifying different CLC classes. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={Braila;  Catalonia;  Corine;  Land cover mapping;  Random forest;  Support vector machine;  Warsaw},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tan2021,
author={Tan, Z. and Che, Y. and Xiao, L. and Hu, W. and Li, P. and Xu, J.},
title={Research of fatal car-to-pedestrian precrash scenarios for the testing of the active safety system in China},
journal={Accident Analysis and Prevention},
year={2021},
volume={150},
doi={10.1016/j.aap.2020.105857},
art_number={105857},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097351002&doi=10.1016%2fj.aap.2020.105857&partnerID=40&md5=cadceabd414798aadf3012773bce8f33},
affiliation={Xihua University, National Experiment Teaching Demonstration Center of Automotive Engineering, Chengdu, 610039, China; Chongqing Jiaotong University, College of Traffic & Transportation, Chongqing, 400074, China; Sichuan Xihua Jiaotong Forensic Science Center, Chengdu, 610039, China; State Administration for Market Regulation Defective Product Administrative Center (DPAC), Beijing, 100101, China},
abstract={Road safety remains a challenge with numerous Vulnerable Road Users (VRUs) suffering from injuries and death every year. Pedestrian protection using active safety systems, such as Automated Emergency Braking (AEB), is an effective measure to combat the situation. Furthermore, the perception of precrash scenarios plays an important role in active safety research. It is essential to understand and define precrash scenarios. This study aimed to apply the obtained typical car-to-pedestrian precrash scenarios from Chinese severely injured pedestrian traffic accidents to develop and test active safety systems. The National Automobile Accident In-Depth Investigation System (NAIS) recorded 467 cases from 2011 to 2018 in China, and 12 items were selected from the NAIS database as description variables for the precrash scenario. The items were divided into four categories: car, pedestrian, road, and environment. Group decision theory was applied to evaluate the importance of each variable in its category. A total of 34 basic scenarios were defined and obtained according to the extracted significant variables. These basic scenarios represented diverse fatal scenarios in China which are crucial for autonomous driving. The frequency distribution of the scenarios demonstrated that the top five scenarios covered 85.3 % of the total. Five scenarios were identified to have the common characteristic of cars going straight. Additionally, 13 detailed scenarios were obtained from the five basic scenarios by using cluster and frequency analyses. In contrast to the New Car Assessment Program (NCAP) test scenarios, weather and lighting conditions were considered in these 13 scenarios, and the driving speed before the crash were mostly distributed in the range of 40–80 km/h (20–60 km/h in the NCAP). Meanwhile, both walking and running were commonly recorded for pedestrians to cross the street from the nearside, compared with records of walking only to cross from the nearside in the NCAP. These results contribute to a reference for test scenarios of pedestrian AEB or Forward Collision Warning (FCW) in China. © 2020 Elsevier Ltd},
author_keywords={Automated emergency braking (AEB);  Autonomous driving;  Cluster analysis;  Pedestrian;  Test scenario;  Traffic accident},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sarsekov2021,
author={Sarsekov, A. and Al Kindi, S.A. and Albeshr, M. and Luo, Y. and Kamaletdinov, B. and Arali, V.B.},
title={Changing the Status Quo: Cases of Production Restoration in Inactive Offshore Oil Wells},
journal={Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference, ADIP 2021},
year={2021},
doi={10.2118/207524-MS},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127632860&doi=10.2118%2f207524-MS&partnerID=40&md5=09016f1e6379f44da5a20cd42fe746c7},
affiliation={Adnoc Offshore; Schlumberger},
abstract={The United Arab Emirates oil and gas reservoirs are continuously intersected with a growing number of horizontal wells and longer drains at varying bottomhole static temperatures. This results in a variety of naturally flowing and more challenging wells where stimulation is required for sustainable flow. Hence it became important to not only rely on plain acid systems for production gain, but to also include more sophisticated acid stimulation systems that can provide improved results in more challenging environments where plain acid may be found lacking. These results were recently achieved via the introduction of single-phase retarded acid (SPRA) as well as viscoelastic diverting acid (VEDA) in inactive wells offshore. The application of SPRA and VEDA was subsequent to extensive laboratory testing including core flow tests, solubility tests, and emulsion tendency testing to the performance of these blends against existing acid recipes such as plain HCl and polymer-based diverting acid. These tests proved that a combination of SPRA and VEDA would allow maximizing lateral coverage in heterogenous reservoirs due to the chemical diversion capabilities from thief zones without imposing further damage that polymer-based diverted acids may cause. The combined SPRA and VEDA would also enhance acid wormhole penetration due to the reduced rate of reaction caused by acid retardation. Such tests were supported with software simulations that provided acid dosage, pumping rate, and pumping method sensitives. Proposing SPRA and VEDA at higher pumping rates enabled the delivery of previously unattainable production influx at sustainable wellhead pressures. In addition, 28% acid content typically used for dolomitic reservoirs was considered unnecessary as 20% retarded acid proved sufficient in such environments. This allowed bullheading treatments, which was previously not possible due to the restriction on pumping 28% acid content across wellheads to avoid causing corrosive damage. Other treatment parameters such as volumes, rates, and acid/diverter sequence and ratio were also adjusted for optimal wormhole penetration across all zones using a fit-for-purpose carbonate matrix acidizing modeling software. The success of SPRA and VEDA was clear in post-treatment evaluation for the cases of previously shut-in wells. These wells were able to produce sustainably at the required tubinghead pressure (production line pressure) after unsuccessful attempts to flow prior to stimulation. The novelty of this paper is the assessment between legacy carbonate stimulation results in UAE using plain HCl acid and polymer-based diverting acid (PDA) and using SPRA and VEDA in shut-in or inactive wells. It also highlights the game-changing solutions that suit the increasing challenges observed in offshore inactive wells including well placement, lithology, bottomhole static temperature, and permeability contrast. © Copyright 2021, Society of Petroleum Engineers},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xiang202197,
author={Xiang, Y.},
title={An Empirical Analysis of the Effect of Chinese Enterprises' Internationalization on Performance: Taking Listed Companies as an Example},
journal={Proceedings - 2021 2nd International Conference on Big Data Economy and Information Management, BDEIM 2021},
year={2021},
pages={97-100},
doi={10.1109/BDEIM55082.2021.00028},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126908687&doi=10.1109%2fBDEIM55082.2021.00028&partnerID=40&md5=e0a51328253073ff8e38af7f9f2d119c},
affiliation={Wuhan Business University, College of Business Administration, Hubei, Wuhan, China},
abstract={As Chinese 'Going out' and 'Belt and Road' proposals along with the further development of reform and opening up, more and more Chinese enterprises are joining in the wave of globalization. They hope to find new profit growth points and expand diversified businesses through international development. However, there are many problems and challenges in the international of Chinese enterprises. What is the impact of enterprises' internationalization on performance? In order to test it, we select 621 Chinese manufacturing listed companies, uses state software for model building and data analysis. The result shows that there is a U -shaped relationship between enterprises' internationalization and performance, which means that we should take a long-term strategic view on the internationalization development of Chinese enterprises. © 2021 IEEE.},
author_keywords={International;  Listed Companies;  Performance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2021515,
author={Wang, B. and Guo, B. and Wang, Z.},
title={Has the development of cross-border e-commerce promoted the transformation and upgrading of the G20 export trade?: Based on the empirical analysis of the adjustment effect of the export trade diversification},
journal={Proceedings - 2021 2nd International Conference on Big Data Economy and Information Management, BDEIM 2021},
year={2021},
pages={515-519},
doi={10.1109/BDEIM55082.2021.00111},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126832586&doi=10.1109%2fBDEIM55082.2021.00111&partnerID=40&md5=66562822689d5fb778dc07b3a0307c2d},
affiliation={China Jiliang University, School of Economics and Management, Yiwu, China; China Jiliang University, School of Economics and Management, Hangzhou, China},
abstract={The rapid development of Internet technology has given birth to a series of e-commerce platforms, and e-commerce platforms that integrate big data and the Internet of Things have brought disruptive changes to traditional international trade. In the era of economic globalization, the role of cross-border e-commerce (hereinafter referred to as C - EC) in international trade is becoming more and more important, and it is also a new driving force for the transformation and upgrading of foreign trade and economic development of the G20 countries. Based on the analysis of the G20 C - EC and export trade development status, this paper constructs a measurement model of the relationship between the development of C - EC and the transition of export trade. Using the G20 panel data from 2000 to 2019 to calculate the technical complexity of export trade, we use SPSSAU software to perform regression analysis and test the moderating effect model, the empirical study explores the impact of the development of C - EC on the transformation and upgrading of export trade. The results show that the development of C - EC can not only significantly facilitate the transformation and upgrading of export trade, but also significantly enhance its boosting role under the adjustment of the export trade diversification index. © 2021 IEEE.},
author_keywords={Cross-border e-commerce;  Export technology complexity;  G20;  Trade facilitation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2021139,
author={Zhang, S. and Li, Y. and Yan, W. and Guo, Y. and Chen, X.},
title={Dependency-aware Form Understanding},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2021},
volume={2021-October},
pages={139-149},
doi={10.1109/ISSRE52982.2021.00026},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126392109&doi=10.1109%2fISSRE52982.2021.00026&partnerID=40&md5=4b0f5842ab44baaa7ff768f6705154b8},
affiliation={Peking University, MOE Key Lab of HCST, Dept of Computer Science, Beijing, China; Microsoft Research, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China},
abstract={Form understanding is an important task in many fields such as software testing, AI assistants, and improving accessibility. One key goal of understanding a complex set of forms is to identify the dependencies between form elements. However, it remains a challenge to capture the dependencies accurately due to the diversity of UI design patterns and the variety in development experiences. In this paper, we propose a deep-learning-based approach called DependEX, which integrates convolutional neural networks (CNNs) and transformers to help understand dependencies within forms. DependEX extracts semantic features from UI images using CNN-based models, captures contextual patterns using a multilayer transformer encoder module, and models dependencies between form elements using two embedding layers. We evaluate DependEX with a large-scale dataset from mobile Web applications. Experimental results show that our proposed model achieves over 92% accuracy in identifying dependencies between UI elements, which significantly outperforms other competitive methods, especially for heuristic-based methods. We also conduct case studies on automatic form filling and test case generation from natural language (NL) instructions, which demonstrates the applicability of our approach. © 2021 IEEE.},
author_keywords={Automatic form filling;  CNN;  Deep learning;  Dependencies;  Form understanding},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Singh2021339,
author={Singh, V.K. and Sharma, Y.},
title={Interfacing of Digital and Resolver Type Systems for Transmission of Heading Angle},
journal={Proceedings of the 8th International Conference on Signal Processing and Integrated Networks, SPIN 2021},
year={2021},
pages={339-344},
doi={10.1109/SPIN52536.2021.9565978},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126201873&doi=10.1109%2fSPIN52536.2021.9565978&partnerID=40&md5=985607e8e894d0f5a2ff7003eb56e284},
affiliation={Department of Electronics and Communication, Delhi Technological University, Delhi, India},
abstract={A marine vessel platform is fitted with sensors and equipments of diverse origin which are required to work in tandem in order to enhance its operation capability. However, due to incompatibilities in the data formats and protocols that are used, these equipments/sensors cannot be integrated directly. As the systems available are a mixture of digital and analog types, this paper details the design and development of Embedded Interfacing Solution for interfacing of Digital and Resolver type systems for transmission of Heading Angle. An embedded interfacing board was first designed, using Cadence OrCAD proprietary software, and subsequently implemented using microcontroller 8051, Digital to Resolver converter modules, low power operational amplifiers and other required components. This electronic embedded interfacing board converts the digital input containing heading angle into Analog Resolver type of output. This was tested using simulated heading angle created with the help of COM Port Toolkit software application and the output of this embedded interfacing solution was measured on Angle Position Indicator (API) and with the help of Oscilloscope. © 2021 IEEE},
author_keywords={Gyroscope;  Heading Angle;  Resolver;  RS422A;  Serial asynchronous;  UART},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{O'Toole2021,
author={O'Toole, S. and Sewell, C. and Mehrpouyan, H.},
title={IoT Security and Safety Testing Toolkits for Water Distribution Systems},
journal={2021 8th International Conference on Internet of Things: Systems, Management and Security, IOTSMS 2021},
year={2021},
doi={10.1109/IOTSMS53705.2021.9704886},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125666429&doi=10.1109%2fIOTSMS53705.2021.9704886&partnerID=40&md5=d29d2d7ad1ba82f523b1509cc3904405},
affiliation={Boise State University, Department of Computer Science, Boise, United States},
abstract={Due to the critical importance of Industrial Control Systems (ICS) to the operations of cities and countries, research into the security of critical infrastructure has become increasingly relevant and necessary. As a component of both the research and application sides of smart city development, accurate and precise modeling, simulation, and verification are key parts of a robust design and development tools that provide critical assistance in the prevention, detection, and recovery from abnormal behavior in the sensors, controllers, and actuators which make up a modern ICS system. However, while these tools have potential, there is currently a need for helper-tools to assist with their setup and configuration, if they are to be utilized widely. Existing state-of-the-art tools are often technically complex and difficult to customize for any given IoT/ICS processes. This is a serious barrier to entry for most technicians, engineers, researchers, and smart city planners, while slowing down the critical aspects of safety and security verification. To remedy this issue, we take a case study of existing simulation toolkits within the field of water management and expand on existing tools and algorithms with simplistic automated retrieval functionality using a much more in-depth and usable customization interface to accelerate simulation scenario design and implementation, allowing for customization of the cyber-physical network infrastructure and cyber attack scenarios. We additionally provide a novel in-tool-assessment of network's resilience according to graph theory path diversity. Further, we lay out a roadmap for future development and application of the proposed tool, including expansions on resiliency and potential vulnerability model checking, and discuss applications of our work to other fields relevant to the design and operation of smart cities. © 2021 IEEE.},
author_keywords={Critical infrastructure;  Industrial control systems;  Internet of things;  Operational technology;  Resilience;  Safety;  Simulation;  Smart cities;  Software development;  Toolkits},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2021593,
author={Wang, T. and Golubev, Y. and Smirnov, O. and Li, J. and Bryksin, T. and Ahmed, I.},
title={PyNose: A Test Smell Detector for Python},
journal={Proceedings - 2021 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021},
year={2021},
pages={593-605},
doi={10.1109/ASE51524.2021.9678615},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125505191&doi=10.1109%2fASE51524.2021.9678615&partnerID=40&md5=5f01f79415c145aeb2c16ddf7ae82883},
affiliation={University of California, Irvine, Irvine, CA, United States; JetBrains Research, Saint Petersburg State University, Saint Petersburg, Russian Federation},
abstract={Similarly to production code, code smells also occur in test code, where they are called test smells. Test smells have a detrimental effect not only on test code but also on the production code that is being tested. To date, the majority of the research on test smells has been focusing on programming languages such as Java and Scala. However, there are no available automated tools to support the identification of test smells for Python, despite its rapid growth in popularity in recent years. In this paper, we strive to extend the research to Python, build a tool for detecting test smells in this language, and conduct an empirical analysis of test smells in Python projects.We started by gathering a list of test smells from existing research and selecting test smells that can be considered language-agnostic or have similar functionality in Python's standard Unittest framework. In total, we identified 17 diverse test smells. Additionally, we searched for Python-specific test smells by mining frequent code change patterns that can be considered as either fixing or introducing test smells. Based on these changes, we proposed our own test smell called Suboptimal Assert. To detect all these test smells, we developed a tool called PYNOSE in the form of a plugin to PyCharm, a popular Python IDE. Finally, we conducted a large-scale empirical investigation aimed at analyzing the prevalence of test smells in Python code. Our results show that 98% of the projects and 84% of the test suites in the studied dataset contain at least one test smell. Our proposed Suboptimal Assert smell was detected in as much as 70.6% of the projects, making it a valuable addition to the list. © 2021 IEEE.},
author_keywords={Code change patterns;  Code smells;  Empirical studies;  Mining software repositories;  Python;  Test smells},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ribeiro20214824,
author={Ribeiro, M.T. and Wu, T. and Guestrin, C. and Singh, S.},
title={Beyond Accuracy: Behavioral Testing of NLP Models with CheckList (Extended Abstract)},
journal={IJCAI International Joint Conference on Artificial Intelligence},
year={2021},
pages={4824-4828},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125488885&partnerID=40&md5=185a81cae5d9e68edb845288d54cd5c6},
affiliation={Microsoft Research; University of Washington; University of California, Irvine, United States},
abstract={Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20211196,
author={Chen, Z. and Wang, R. and Xiang, J. and Yu, Y. and Xia, X. and Ji, S. and Xuan, Q. and Yang, X.},
title={Detecting Adversarial Samples with Graph-Guided Testing},
journal={Proceedings - 2021 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021},
year={2021},
pages={1196-1198},
doi={10.1109/ASE51524.2021.9678732},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125457338&doi=10.1109%2fASE51524.2021.9678732&partnerID=40&md5=3552a71e4573db161b6cc31e6ba91b9a},
affiliation={Zhejiang University of Technology, Institute of Cyberspace Security, Hangzhou, 310023, China; National University of Defense Technology, Changsha, 410073, China; Monash University, Melbourne, Australia; Zhejiang University, Hangzhou, 310023, China},
abstract={Deep Neural Networks (DNN) are known to be vulnerable to adversarial samples, the detection of which is crucial for the wide application of these DNN models. Recently, a number of deep testing methods in software engineering were proposed to find the vulnerability of DNN systems, and one of them, i.e., Model Mutation Testing (MMT), was used to successfully detect various adversarial samples generated by different kinds of adversarial attacks. However, the mutated models in MMT are always huge in number (e.g., over 100 models) and lack diversity (e.g., can be easily circumvented by high-confidence adversarial samples), which makes it less efficient in real applications and less effective in detecting high-confidence adversarial samples. In this study, we propose Graph-Guided Testing (GGT) for adversarial sample detection to overcome these aforementioned challenges. GGT generates pruned models with the guide of graph characteristics, each of them has only about 5% parameters of the mutated model in MMT, and graph guided models have higher diversity. The initial experiments on CIFAR10 validate that GGT performs much better than MMT with respect to both effectiveness and efficiency. © 2021 IEEE.},
author_keywords={Adversarial sample detection;  Deep learning testing;  Graph structure;  Model pruning;  Neural networks;  Whitebox testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2021104,
author={Chen, S. and Jin, S. and Xie, X.},
title={Testing Your Question Answering Software via Asking Recursively},
journal={Proceedings - 2021 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021},
year={2021},
pages={104-116},
doi={10.1109/ASE51524.2021.9678670},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125445964&doi=10.1109%2fASE51524.2021.9678670&partnerID=40&md5=78bb824f7f2071723c47a01dea7b67e8},
affiliation={Wuhan University, School of Computer Science, China},
abstract={Question Answering (QA) is an attractive and challenging area in NLP community. There are diverse algorithms being proposed and various benchmark datasets with different topics and task formats being constructed. Q&A software has also been widely used in daily human life now. However, current Q&A software is mainly tested in a reference-based paradigm, in which the expected outputs (labels) of test cases need to be annotated with much human effort before testing. As a result, neither the just-in-time test during usage nor the extensible test on massive unlabeled real-life data is feasible, which keeps the current testing of Q&A software from being flexible and sufficient. In this paper, we propose a method, qaAskeR, with three novel Metamorphic Relations for testing Q&A software. qaAskeR does not require the annotated labels but tests Q&A software by checking its behaviors on multiple recursively asked questions that are related to the same knowledge. Experimental results show that qaAskeR can reveal violations at over 80% of valid cases without using any preannotated labels. Diverse answering issues, especially the limited generalization on question types across datasets, are revealed on a state-of-the-art Q&A algorithm. © 2021 IEEE.},
author_keywords={Natural language processing;  Question answering;  Recursive metamorphic testing;  Testing and validation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sari2021416,
author={Sari, D.P. and Masruroh, N.A. and Asih, A.M.S.},
title={Factors Affecting Consumer Acquisition of Secondhand Smartphone in Indonesia},
journal={2021 IEEE International Conference on Industrial Engineering and Engineering Management, IEEM 2021},
year={2021},
pages={416-420},
doi={10.1109/IEEM50564.2021.9673092},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125400994&doi=10.1109%2fIEEM50564.2021.9673092&partnerID=40&md5=2c14a55c36199b489b5bb1134743b0dc},
affiliation={Universitas Gadjah Mada, Department of Mechanical and Industrial Engineering, Yogyakarta, Indonesia; Department of Industrial Engineering, Diponegoro University, Semarang, Indonesia},
abstract={Regulation plays an essential role in handling electronic waste. Indonesia's electronic waste management system is still at the lowest level because it is still limited to informal initiatives. When making regulations, we should consider informal channels by collaborating with formal channels. Informal channels are given space for secondhand product sales and repairs, while formal channels focus on waste collection and recycling. Secondhand products still have a potential market. This study aims to analyze the relationship between demographic variables to purchasing decisions of secondhand smartphones and the factors that influence the purchasing decisions of secondhand smartphones. The survey was conducted on 328 smartphone users in Indonesia, in which 143 of them are consumers of secondhand smartphone products. The data was processed using the chi-square test and multiple regression analysis with SPSS version 22 software. Data processing shows that purchasing decisions of secondhand smartphones are related to demographic variables, gender, and education level and are also significantly influenced by product price, income, and product quality. The product diversity factor does not significantly influence purchasing decisions. © 2021 IEEE.},
author_keywords={Chi-square;  Informal;  Multiple regression analysis;  Secondhand smartphone},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guerrero-Balaguera202167,
author={Guerrero-Balaguera, J.-D. and Condia, J.E.R. and Reorda, M.S.},
title={A Novel Compaction Approach for SBST Test Programs},
journal={Proceedings of the Asian Test Symposium},
year={2021},
volume={2021-November},
pages={67-72},
doi={10.1109/ATS52891.2021.00024},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124701521&doi=10.1109%2fATS52891.2021.00024&partnerID=40&md5=740c3cedc6b90596d4c1d64f6ae85b42},
affiliation={Politecnico di Torino, Department of Control and Computer Engineering, Torino, Italy},
abstract={In-field test of processor-based devices is a must when considering safety-critical systems (e.g., in robotics, aerospace, and automotive applications). During in-field testing, different solutions can be adopted, depending on the specific constraints of each scenario. In the last years, Self-Test Libraries (STLs) developed by IP or semiconductor companies became widely adopted. Given the strict constraints of in-field test, the size and time duration of a STL is a crucial parameter. This work introduces a novel approach to compress functional test programs belonging to an STL. The proposed approach is based on analyzing (via logic simulation) the interaction between the micro-architectural operation performed by each instruction and its capacity to propagate fault effects on any observable output, reducing the required fault simulations to only one. The proposed compaction strategy was validated by resorting to a RISC-V processor and several test programs stemming from diverse generation strategies. Results showed that the proposed compaction approach can reduce the length of test programs by up to 93.9% and their duration by up to 95%, with minimal effect on fault coverage. © 2021 IEEE.},
author_keywords={Functional Testing;  Software-Based Self-Test (SBST);  Test Compaction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kochanczyk2021,
author={Kochanczyk, W. and Chauhan, V.},
title={DESIGN OF A ROBOTIC VEHICLE FOR ASME STUDENT DESIGN COMPETITION 2021},
journal={ASME International Mechanical Engineering Congress and Exposition, Proceedings (IMECE)},
year={2021},
volume={9},
doi={10.1115/IMECE2021-72195},
art_number={V009T09A026},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124484979&doi=10.1115%2fIMECE2021-72195&partnerID=40&md5=0bc11f58cea8c1279aecd3a60bc4837e},
affiliation={Western New England University, Springfield, MA, United States},
abstract={Robotics is a very active and diverse field with new applications found daily for emerging designs. This year ASME SDC emphasizes one such application with small RC robotic vehicles. These vehicles are primarily cargo carriers with the ability to harness wind and solar renewable energy sources to recharge their limited battery. Analog to modern electric battery-powered cars the robot utilizes a single AAA battery which was the biggest challenge of the competition. To satisfy all the requirements posed by the competition, a brand new vehicle platform was developed including both wind and solar charging capabilities. As part of the development process a new drive train, control system, wind turbine, and robot frame was developed. For performance maximization, all outsourced, as well as custom-designed components, were extensively researched and tested. Using test data and CAD software the final design was created including all the components selected during testing. This produced a successful prototype satisfying all competition requirements and being accepted to take part in SDC 2021. The maximum score achieved by the robot reached 0.4488 points using the given scoring matrix, however, it did not qualify for the final rounds of the competition. This project indicated several ideas that may improve the performance of small robots, such as the effective use of solar panels, the creation of systems with low power requirements, as well as the use of RC toys to develop more complex robotic vehicles. Copyright © 2021 by ASME},
author_keywords={Low-Power Vehicles;  Robots Based on RC Toys;  Small RC Robots;  Small Wind Turbines;  Solar Powered Robots},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Faller2021,
author={Faller, T. and Scholl, P. and Paxian, T. and Becker, B.},
title={Towards SAT-Based SBST Generation for RISC-V Cores},
journal={2021 IEEE 22nd Latin American Test Symposium, LATS 2021},
year={2021},
doi={10.1109/LATS53581.2021.9651819},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124355417&doi=10.1109%2fLATS53581.2021.9651819&partnerID=40&md5=f691ee2b8e62ad0718b3d0d1a156cadc},
affiliation={University of Freiburg, Chair of Computer Architecture, Freiburg, 79110, Germany},
abstract={The increasing amount and diversity of System-On-a-Chip (SoC) devices with short development times pose numerous challenges. The RISC-V initiative targets this market with a free and open ISA that supports custom instruction set extensions and accelerators to adapt to application specific scenarios and meet varying constraints w.r.t. efficiency, security, safety and computational power. In this context we target an appropriate test strategy to find manufacturing defects during production, and moreover, to detect degradation in the field. An essential part of this strategy will be so-called Software-Based Self-Tests (SBST). Manually developing SBST programs is a tedious and time-consuming task that requires the expertise of a skilled engineer with detailed knowledge about the specific architecture of the processor at hand. In contrast we present a staggered SBST approach for the automatic creation of SBST programs for RISC-V architectures with the help of SAT-based test pattern generation. First experimental results to demonstrate the feasibility of our approach are provided by test generation results for two exemplary RISC-V processor, each in two variants. © 2021 IEEE.},
author_keywords={Automatic SBST;  Functional ATPG;  Microprocessor Test;  RISC-V;  Software-based Self-Test},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Elmishali202147,
author={Elmishali, A. and Sotto-Mayor, B. and Roshanski, I. and Sultan, A. and Kalech, M.},
title={BEIRUT: Repository Mining for Defect Prediction},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2021},
volume={2021-October},
pages={47-56},
doi={10.1109/ISSRE52982.2021.00018},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124318705&doi=10.1109%2fISSRE52982.2021.00018&partnerID=40&md5=81b4d659b74753fe10359110ff96a459},
affiliation={Ben-Gurion University of the Negev, Software and Information Systems Engineering, Be'er-Sheva, Israel},
abstract={Software Defect Prediction is an important activity used in the Testing Phase of the software development life cycle. Within the research of new defect prediction approaches and the selection of training sets for the classification task, different benchmarks have been analyzed in the literature. They provide several features and defective information over specific software archives. Therefore, they are commonly used in research to evaluate new approaches. However, the current benchmarks contain several limitations, such as lack of project variability, outdated benchmarks, single-version projects, a small number of projects and metrics, unavailable resources, poor usability, and non-extensible tools. Therefore, we introduce a novel tool Bgu rEpository mlning foR bUg predicIion (BEIRUT) for benchmark generation for defect prediction, composed of three main features: Given an open-source repository from GitHub, BEIRUT mines the software repository by (1) selecting the best $k$ versions, based on the defective rate of each version, (2) generating training sets and a testing set for defect prediction, composed of a large number of metrics and defective information extracted from each of the selected versions and (3) creating defect prediction models from those extracted metrics. In the end, BEIRUT extracts a diversified catalog of 644 metrics and the defective information from each component of $k$ versions, automatically selected based on the rate of defects in each version. They were collected from 512 different projects, starting from 2009. The tool is also supplemented with an easy-to-use web interface that provides a configurable selection of projects and metrics and an interface to manage the defect prediction tasks. Moreover, this tool is adapted to be extended with new projects and new extractors, introducing new metrics to the benchmark. The web service tool can be found at rps.ise.bgu.ac.il/beirut. © 2021 IEEE.},
author_keywords={Defect Prediction;  Open Source Metrics;  Repository Mining Tool;  Software Quality Metrics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zualkernan2021227,
author={Zualkernan, I.},
title={EXPLORING PREDICTING PERFORMANCE OF ENGINEERING STUDENTS USING DEEP LEARNING},
journal={18th International Conference on Cognition and Exploratory Learning in Digital Age, CELDA 2021},
year={2021},
pages={227-234},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124079678&partnerID=40&md5=acb33f71bcba9e0638e370465bf1c3f3},
affiliation={American University of Sharjah, United Arab Emirates},
abstract={A significant amount of research has gone into predicting student performance and many studies have been conducted to predict why students drop out. A variety of data including digital footprints, socio-economic data, financial data, and psychological aspects have been used to predict student performance at the test, course, or program level. Fairly good prediction results have been achieved using both traditional machine learning and more recently deep learning methods. While using diverse sets of data has achieved good results, this data is often difficult and expensive to collect, and may have privacy-related issues. This paper explores the extent to which only prior performance data readily available with registrars in most Universities can be used to predict student performance in future terms. Twenty term data from 789 students enrolled an engineering program at an American University were used to train long term short term (LSTM), Bi-directional LSTM and Gated Reference Units (GRU) models to predict student performance in future terms. The results are that all three types of models were able to reasonably predict the next term's performance (F1-score of about 0.70) regardless of the number of terms a student had spent the University. The models generally did not overfit. The prediction was reasonable until about trying to predict performance on seventh term in the future, but the performance dropped beyond this point primarily due to lack of sufficient data (F1-score of about 0.2). © 2021 Virtual Simulation Innovation Workshop, SIW 2021. All rights reserved.},
author_keywords={Deep learning;  Engineering;  GRU;  Higher education;  LSTM;  Performance prediction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Maly2021,
author={Maly, C. and Person, S. and Soh, L.-K.},
title={SE-First: A New Approach to Software Engineering Education},
journal={Proceedings - Frontiers in Education Conference, FIE},
year={2021},
volume={2021-October},
doi={10.1109/FIE49875.2021.9637325},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123870232&doi=10.1109%2fFIE49875.2021.9637325&partnerID=40&md5=7bd5b5e95df5c7eb7abea5c2166bcd56},
affiliation={University of Nebraska-Lincoln, Department of Computer Science and Engineering, Lincoln, United States},
abstract={In this Innovative Practice Full Paper, we note that the way software is developed has changed significantly in the past 50 years. Software developers today cannot just be good at writing code; they must also possess non-technical skills in order to work successfully within diverse teams and have an appreciation for the tools and processes needed to build and maintain complex systems. In this work, we describe a novel first-year Software Engineering-First (SE-First) curriculum that introduces students to the broader picture of software development while students learn fundamental computing concepts. To assess the effectiveness of our novel first-year curriculum, we compare students who completed the first-year software engineering curriculum with students who completed our traditional computer science curriculum. We assess student knowledge of computing concepts, and their self-efficacy. Initial results show that students who complete the first-year software engineering courses perform as well or better on the computing concepts test, they are more confident in their computing abilities and in the application of computing skills to their field, and they have a higher success rate in their first-year computing courses (i.e., fewer students drop the course and fewer students receive a D or F course grade) compared with students who complete the traditional first-year computing program. © 2021 IEEE.},
author_keywords={attitudes;  computational thinking;  curriculum;  education;  perceptions;  software engineering;  software engineering-first model},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gopal2021,
author={Gopal, B. and Cooper, S.},
title={Peer Instruction in Online Synchronous Software Engineering - Findings from fine-grained clicker data},
journal={Proceedings - Frontiers in Education Conference, FIE},
year={2021},
volume={2021-October},
doi={10.1109/FIE49875.2021.9637353},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123822161&doi=10.1109%2fFIE49875.2021.9637353&partnerID=40&md5=fd48def65acf080888dbca68ffd4c28e},
affiliation={University of Nebraska-Lincoln, Computer Science and Engineering, Lincoln, United States},
abstract={In this Research Full paper, we present the results of a replication study in a semester-long, sophomore-level software engineering course utilizing Peer Instruction (PI). PI is an active learning pedagogy with roots in STEM Education. In this study, we examine the relationship between student response data from in-class PI correctness and students' performance on quizzes and exams. We worked with a fully remote, synchronous course offered over Zoom. The study we replicated was with an honors cohort of students with a diversity of undergraduate majors, while we focused on a non-honors course containing computing-related majors. Our intervention design included a flipped-classroom approach for each class session with required readings, reading quizzes, followed by PI in class using online breakout rooms for peer discussion. Our course modules were heavily based on industry practices and knowledge from the workforce, across several varied modules that encompass the complete software development lifecycle, and were as follows: Software Process Models (SPM), Software Architecture (SA), Databases (DB), User Interface/user Experience (UI/UX), Software Testing (ST), and Continuous Integration (CI). Our data points for analysis with fine-grained PI student response data were two-fold: scores from weekly online quizzes, and a summative final exam, administered online through a course management system (CMS), at different points during the semester after the PI sessions. The online quizzes and the online exam were timed, closed book/notes, and conducted during class periods. We analyzed and classified individual student responses before and after each question in each module and attempted to create response patterns for each module. We correlated these response patterns with exam and quiz scores using ANOVA techniques, on a variety of questions including Parson's problems. We report overall correctness on each type of vote, track student response patterns from in-class to quizzes and the exam, and quantify absolute percentages of students that demonstrate longer-term learning from the PI process. Our results show that 58% of students exhibited cognitive gains across all modules during PI sessions. Students who learn in class from PI perform well on the quizzes and the final exam, indicating persistence of the knowledge gained during PI several weeks after the actual sessions. We also found that those who fail to learn from the PI process in the class perform worse on quizzes and the final exam. Our results were consistent across all modules. More significantly, we found PI to be an effective way to teach our software engineering course based on student learning before and after PI, in a completely virtual environment, a result unique to our study. Based on our results, we discuss the implications for software engineering education, both in-person and virtual. © 2021 IEEE.},
author_keywords={Active Learning;  Cognitive gains using Peer Instruction;  Fine-grained data analysis;  Online courses;  Peer Instruction;  Software Engineering Education;  student response data;  Virtual Instruction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu2021,
author={Xu, Y. and Liu, B. and Wang, X. and Zhang, H. and Zhang, Z.},
title={Research on an autonomous and controllable portable universal interface test platform},
journal={3rd International Conference on Industrial Artificial Intelligence, IAI 2021},
year={2021},
doi={10.1109/IAI53119.2021.9619437},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123798644&doi=10.1109%2fIAI53119.2021.9619437&partnerID=40&md5=3c0d56d4ef95322c3fa177c81eb2e5de},
affiliation={Jiangsu Automation Research Institute, Jiangsu, Lianyungang, China},
abstract={Industrial software testing including software development and debugging depends on the external input interface. The development, debugging and adaptation of interface software simulation consumes a lot of time. The process of software evaluation and self-test lack a portable general software testing equipment suitable for the industrial field, in order to greatly improve the testing efficiency, test integrity and adequacy. Therefore, it is urgent for the general interface generation platform to be transformed into high performance such as hardware, distributed, hardware interface adaptation, test task load and high real-time. In this paper, the overall design framework of portable general software test equipment is carried out, which includes the design and software development of the execution host, the software transformation of general control host and other research contents. At the same time, a portable general software testing equipment for complex industrial system software and multiple interfaces is developed. This platform can satisfy the diversity of complex industrial software system interfaces and the real-time requirements of special systems. It is expected to further promote the development of interface testing automation. © 2021 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ren2021102,
author={Ren, Z. and Fan, X. and Li, X. and Zhou, Z. and Jiang, H.},
title={Multi-objective Evolutionary Algorithm for String SMT Solver Testing},
journal={Proceedings - 2021 8th International Conference on Dependable Systems and Their Applications, DSA 2021},
year={2021},
pages={102-113},
doi={10.1109/DSA52907.2021.00019},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123470016&doi=10.1109%2fDSA52907.2021.00019&partnerID=40&md5=f5e797cc21b3808fe23c34da00dd4fe4},
affiliation={Dalian University of Technology, School of Software, Dalian, China},
abstract={String Satisfiability Modulo Theories (SMT) solver is widely used in academia and industry. The runtime efficiency of the solvers may have great impact on various software engineering tasks such as automated reasoning and formal verification. Although many studies have been conducted to test SMT solvers, they mainly focus on detecting soundness bugs of SMT solvers. In contrast, only very few studies concentrate on detecting performance defects of SMT solvers. Moreover, in the existing literatures, we observe two major barriers in generating test cases that trigger performance defects for string SMT solvers, i.e., the guidance information barrier and the diversity barrier.In this paper, we propose a multi-objective evolutionary algorithm, MulStringFuzz, to detect performance defects of string SMT solvers. The unique feature of MulStringFuzz lies in the combination of the multi-objective model and the diversity maintenance mechanism. To tackle the guidance information barrier, MulStringFuzz employs multiple objective functions, i.e., the running time, the code coverage, and the test case complexity to guide the test case generation. To tackle the diversity barrier, a tracing based crowding distance mechanism is proposed to ensure the diversity of generated test cases. Extensive experiments are conducted to evaluate the effectiveness of MulStringFuzz, and we investigate how each proposed mechanism contribute to the overall framework. The test cases generated by MulStringFuzz can cover nearly 5,000 more lines of code and trigger 3.25 times performance defects than StringFuzz, which shows that MulStringFuzz can effectively detect performance defect of the String SMT solver. © 2021 IEEE.},
author_keywords={Fuzz testing;  Multi-objective search algorithm;  String SMT Solvers},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kröber202146,
author={Kröber, C. and Hammel, K. and Schade, C. and Filz, N. and Dewitz, L.},
title={User Involvement for Application Development: Methods, Opportunities and Experiences from Three Different Academic Projects},
journal={Communications in Computer and Information Science},
year={2021},
volume={1501 CCIS},
pages={46-83},
doi={10.1007/978-3-030-93186-5_3},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123438822&doi=10.1007%2f978-3-030-93186-5_3&partnerID=40&md5=76c5ea6a1c7b48663d76faa46d81e545},
affiliation={Center for Open Digital Innovation and Participation, TU Dresden, Dresden, 01062, Germany; University of Applied Sciences, Kiepenheuerallee 5, Potsdam, 14469, Germany},
abstract={This paper introduces several tools and methods of user inquiry and usability testing by means of three academic software development projects. The projects have different objectives and user groups and address different issues. This makes it possible to show the diverse ways of involving users in the development of solutions and allows to evaluate which human-centered approach might be suitable for similar projects and problems. © 2021, Springer Nature Switzerland AG.},
author_keywords={Human-centered design;  Personas;  Prototype testing;  Usability;  UX},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kreutzer202158,
author={Kreutzer, P. and Kunze, T. and Philippsen, M.},
title={Test Case Reduction: A Framework, Benchmark, and Comparative Study},
journal={Proceedings - 2021 IEEE International Conference on Software Maintenance and Evolution, ICSME 2021},
year={2021},
pages={58-69},
doi={10.1109/ICSME52107.2021.00012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123382546&doi=10.1109%2fICSME52107.2021.00012&partnerID=40&md5=50fa2182c634b2147dae89aa63edee82},
affiliation={Programming Systems Group, Friedrich-Alexander University Erlangen-Nürnberg (FAU), Germany},
abstract={Given a program that triggers a bug in a compiler (or other kind of language processor), the goal of test case reduction is to cut away all code that is irrelevant for the bug, i.e., to generate a smaller program that still induces the bug. Research has proposed several language-agnostic reduction techniques that automatically reduce bug-inducing programs in arbitrary programming languages, but there is no large-scale, conclusive evaluation of these algorithms yet. Furthermore, the development of new algorithms is hampered by the unavailability of comparable implementations of previous techniques and of diverse test programs that trigger different bugs in real compilers. To close these gaps and to foster future research in this area, this paper makes three contributions: (1) A framework that includes efficient, fine-tuned implementations of 6 state-of-the-art reducers, (2) a diverse benchmark that comprises 321 fuzzer-generated programs in two programming languages that trigger 110 different bugs in real compilers, and (3) a comparative study that builds upon our framework and benchmark and compares the reduction techniques w.r.t. their effectiveness and efficiency. Our results show that there is no reduction technique yet that performs best across all test cases and languages. Our framework and benchmark are available online and we provide the necessary scripts and tools to replicate our study. © 2021 IEEE.},
author_keywords={benchmark;  compiler testing;  test case reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ferreira2021,
author={Ferreira, F. and Diniz, J.P. and Vale, G. and Figueiredo, E.},
title={On the Challenges for Creating a Test Suite for Configurable Software Systems},
journal={CIbSE 2021 - XXIV Ibero-American Conference on Software Engineering},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123289885&partnerID=40&md5=3a22aebec0679035e97071118d46a5c1},
affiliation={Federal University of Minas Gerais (UFMG); Saarland University, Saarbrücken, Germany},
abstract={Configurable software systems allow developers to maintain a unique platform and address a diversity of deployment contexts and usages. Testing configurable systems is essential because configurations that fail may potentially hurt users and degrade the project reputation. Considering a gap of knowledge on creating tests for configurable software systems, we propose a list of ten challenges faced when performing test suites for configurable systems and dealing with a test suite for our dataset systems. Our list includes, for instance, the challenges of testing high coupled classes and of determining metrics for measuring the quality of the test suite. Our results can be seen as lessons learned on creating tests for configurable systems and they aim at supporting researchers and practitioners on this activity for configurable systems. © CIbSE 2021. All rights reserved.},
author_keywords={Challenges;  Feature Interaction;  Software Faults;  Testing Configurable Systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Saiki2021247,
author={Saiki, K. and Ihara, A.},
title={Linkage of Similar Code Snippets Assessed in the Micro Benchmark Service jsPerf},
journal={Proceedings - IEEE 21st International Working Conference on Source Code Analysis and Manipulation, SCAM 2021},
year={2021},
pages={247-251},
doi={10.1109/SCAM52516.2021.00038},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123276098&doi=10.1109%2fSCAM52516.2021.00038&partnerID=40&md5=f0386a401c033e6a7dc3761edbfc920b},
affiliation={Wakayama University, Social Software Engineering Laboratory (SocSEL), Wakayama, Japan},
abstract={A benchmark is an action to assess performance (e.g., program execution time) by developers preparing and running several test cases over a long period. To reasonably assess the performance of method-level code snippets, developers could use a micro benchmark. Some micro benchmarks for JavaScript provide online web services (e.g., jsPerf and MeasureThat.net). Developers easily search code snippets with better performance in the micro benchmark service. Then, the developers will find many similar code snippets for different functions in the service because the micro benchmark service has a collection of versatile method-level code snippets. To find replaceable code snippets with better performance, we tackle to distinguish similar code snippets for different functions with more fine-grained size than method-level in micro benchmark services.This study proposes an approach to collect diverse code snippets using the similar function. The approach measures the similarity using Code2Vec between some code snippets assessed in the micro benchmark service, and find an appropriate threshold to associate with the code snippets using the similar function. Using the micro benchmark service jsPerf dataset that the authors collected, this study evaluates the usefulness of our approach. Specifically, we collect code snippets related to the most frequent topics "innerHTML vs removeChild"and "for vs forEach"assessed in jsPerf. Consequently, we find our approach achieves higher precision (98% and 92%) to identify diverse code snippets using the similar function. © 2021 IEEE.},
author_keywords={micro benchmark;  program analysis;  software quality;  source code classification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu2021,
author={Wu, S. and Qiu, K. and Zheng, Z.},
title={An Empirical Study on Environmental Factors for Reproducing Concurrent Software Failures},
journal={Proceedings - Annual Reliability and Maintainability Symposium},
year={2021},
volume={2021-May},
doi={10.1109/RAMS48097.2021.9605757},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123054505&doi=10.1109%2fRAMS48097.2021.9605757&partnerID=40&md5=9661d9de322ba5b030701fdeea5f31a5},
affiliation={Beihang University},
abstract={Mandelbug-caused failures are usually non-deterministic and hard to be reproduced. Trivedi and Grottke proposed the environmental-diversity-based fault tolerance method to mitigate the damages caused by Mandelbugs. However, the design of such fault tolerance methods requires the study on the fundamental research question: what are the environmental factors that can influence the reproducibility of Mandelbugs? Towards this issue, we have two main contributions in this work. First, a systematic framework is proposed for analyzing environmental factors for the concurrency bugs, a typical subtype of Mandelbug. Second, we conducted a two-stage empirical study by applying environmental factors in stress testing to accelerate the occurrence of concurrency bugs to verify the framework's effectiveness. After the correlation and variance analysis, we found that the environmental factors, such as sigload and syscall, can effectively influence the time-to-failure of concurrency-bug-caused failures. Besides, these empirical findings can benefit the application of environmental-diversity-based fault tolerance techniques. © 2021 IEEE.},
author_keywords={atomicity violation;  data race;  environmental factor;  order violation;  stress testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Goetz2021195,
author={Goetz, T.},
title={On-demand placement test options within a moodle environment},
journal={Proceedings of the European Conference on e-Learning, ECEL},
year={2021},
pages={195-202},
doi={10.34190/EEL.21.035},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121607214&doi=10.34190%2fEEL.21.035&partnerID=40&md5=7462a4d061739ec89d4ab31a1cb5a584},
affiliation={Hokusei Gakuen University, Sapporo, Japan},
abstract={Members of a private Liberal Arts College in Japan have administered a placement test for new, returning, and transfer students at the beginning of each academic year for more than ten years. The placement test until the onset of COVID-19 in 2020 had been a 57-item test composed exclusively of material from Cengage Learning for use with the World Link textbook series, the series of choice for all first-year students. Since 2020 and the onset of the COVID-19 pandemic, the placement test has changed from an in-house sit-down event to an online, On-Demand format. Audio questions were removed, and questions with meaningless Facility Index rankings were removed to be replaced with original items. The once 57 item test became a 40 item, smartphone-friendly test. The same 40 item test from 2020 was administered again in 2021 to a cohort of 504. Four hundred eighty-one took the test, leaving 23 non-participants to be placed manually. A mean score of 51.24% was observed with a more or less normal bell curve. Students spread across eight departments need placement in level-appropriate, uniformly sized classes. Recent years have shown that score clustering occurred where classes needed dividing. Clusters refer to identical scores that group students into subgroups making line-drawing a subjective, time-consuming task. The trouble score clustering had to be addressed given the time constraints for announcing class memberships and being ready to answer allegations of unfair or capricious approach to class membership creation. In answer to this, the test items were re-weighed from a uniform weight of 1.00 to weights within a set range (1.00-easy to 1.09-difficult) to ensure greater score diversity and hence ease with student ranking. The 2020 40 Item test's Facility Index was used as a guide for setting the weights for the 2021 test. This paper will share the process undertaken to avert score clustering and enable class creation in an informed, principled manner, all within a matter of hours from data download with benefit to all concerned. © the authors, 2021. All Rights Reserved.},
author_keywords={ESL program;  Leadership;  Matriculated students;  Placement procedure;  Placement test;  Ranking;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Creager2021286,
author={Creager, R. and Blackwood, J. and Pribyl, T. and Bassit, L. and Rao, A. and Greenleaf, M. and Frank, F. and Lam, W. and Ortlund, E. and Schinazi, R. and Greninger, A. and Cirrincione, M. and Gort, D. and Kennedy, E. and Samuta, A. and Shaw, M. and Walsh, B. and Lai, E.},
title={RADx Variant Task Force Program for Assessing the Impact of Variants on SARS-CoV-2 Molecular and Antigen Tests},
journal={IEEE Open Journal of Engineering in Medicine and Biology},
year={2021},
volume={2},
pages={286-290},
doi={10.1109/OJEMB.2021.3116490},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121059212&doi=10.1109%2fOJEMB.2021.3116490&partnerID=40&md5=5893da4ce8916be84e8e133d15442454},
affiliation={NaviDx, Newport Beach, CA, United States; Biocomx, Dana Point, CA, United States; Beck and Associates, Travelers Rest, SC, United States; Emory University, Atlanta, GA, United States; University of Washington, Seattle, WA, United States; ShoreFront Strategies, Holland, MI, United States; OOMVELT, Lakewood, OH, United States; Innovation Works, Pittsburgh, PA, United States; Innova Group, Alpharetta, GA, United States; Personalized Science, San Diego, CA, United States},
abstract={Goal: Monitoring the genetic diversity and emerging mutations of SARS-CoV-2 is crucial for understanding the evolution of the virus and assuring the performance of diagnostic tests, vaccines, and therapies against COVID-19. SARS-CoV-2 is still adapting to humans and, as illustrated by B.1.1.7 (Alpha) and B.1.617.2 (Delta), lineage dynamics are fluid, and strain prevalence may change radically in a matter of months. The National Institutes of Health's Rapid Acceleration of Diagnostics (RADxSM) initiative created a Variant Task Force to assess the impact of emerging SARS-CoV-2 variants on in vitro diagnostic testing. Working in tandem with clinical laboratories, the FDA, and the CDC, the Variant Task Force uses both in silico modeling and in vitro testing to determine the effect of SARS-CoV-2 mutations on diagnostic molecular and antigen tests. Here, we offer an overview of the approach and activities of the RADx Variant Task Force to ensure test performance against emerging SARS-CoV-2 lineages. © 2020 IEEE.},
author_keywords={COVID-19;  in vitro diagnostics;  mutations;  SARS-CoV-2;  variants of concern},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aldababsa2021,
author={Aldababsa, M. and Guven, E. and Akif Durmaz, M. and Goztepe, C. and Kurt, G.K. and Kucur, O.},
title={Unified Performance Analysis of Antenna Selection Schemes for Cooperative MIMO-NOMA with Practical Impairments},
journal={IEEE Transactions on Wireless Communications},
year={2021},
doi={10.1109/TWC.2021.3129307},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120853799&doi=10.1109%2fTWC.2021.3129307&partnerID=40&md5=c9b8265f0d20f82296161b724e5f674b},
affiliation={Department of Electrical and Electronics Engineering, Istanbul Gelisim University, 34310, Istanbul, Turkey. (e-mail: m.dababsa87@gmail.com); Department of Electronics and Communication Engineering, Istanbul Technical University, 34469, Istanbul/Turkey.; Department of Electronics Engineering, Gebze Technical University, 41400, Gebze/Kocaeli/Turkey.},
abstract={This paper presents a unified outage probability (OP) performance analysis of two hybrid antenna selection (AS) schemes, transmit antenna selection (TAS) and maximal ratio combining (MRC), and joint transmit and receive antenna selection (JTRAS) in multiple-input multiple-output non-orthogonal multiple access based downlink amplify-and-forward (AF) relaying network with channel estimation error (CEE) and feedback delay (FD). Since the communications in the first and second hops are kinds of single-user and multi-user communications, respectively the AS is done as optimal TAS/MRC or JTRAS is applied in the first hop while the suboptimal majority-based TAS/MRC or JTRAS is employed in the second hop. For both TAS/MRC and JTRAS schemes, the OP expressions are derived in single closed-form over Nakagami-m fading channels in the practical and ideal cases. Moreover, in the practical case, the lower bound OP expressions are found and at high signal-to-noise ratio (SNR) values, the OP reaches an error floor value, which means zero-diversity order. In the ideal case, asymptotic OP expressions are obtained in high SNR regime and demonstrate achievable non-zero diversity and array gains. Finally, through simulations and software-defined radio-based real-time tests, the accuracy of theoretical analysis is validated. IEEE},
author_keywords={Channel estimation error;  Diversity reception;  Feedback delay;  Joint transmit and receive antenna selection;  Maximal ratio combining;  Multiple-input multiple-output;  NOMA;  Non-orthogonal multiple access;  Outage probability;  Receiving antennas;  Relays;  Signal to noise ratio;  Software-defined radio-based real-time tests;  Transmit antenna selection;  Transmitting antennas;  Wireless communication},
document_type={Article},
source={Scopus},
}

@ARTICLE{Werth20211310,
author={Werth, O. and Sonneberg, M.-O. and Leyerer, M. and Breitner, M.H.},
title={Examining customers’ critical acceptance factors toward ridepooling services},
journal={Transportation Research Record},
year={2021},
volume={2675},
number={11},
pages={1310-1323},
doi={10.1177/03611981211026304},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120031659&doi=10.1177%2f03611981211026304&partnerID=40&md5=89ce9b0b9e4ff40a9c86b59fb96362a8},
affiliation={Faculty of Economics, Leibniz University Hannover, Hanover, Germany},
abstract={Ridepooling is a new mobility service mainly for people in cities and urban areas. By matching the routes of customers with similar start and end points while driving in an optimally pooled manner, meaningful reductions in road traffic and related emissions can be achieved. Such services must meet customers’ demands appropriately to achieve sustainable customer acceptance. Service providers face diverse customer expectations and prejudices that differ from those toward existing transportation modes. Today, most ridepooling trips are conducted with only one customer, confirming impressions of nonoptimal operation. Using a survey-based approach, possible relevant constructs for the acceptance of and intention to use ridepooling services are analyzed. Testing constructs from the Unified Theory of Acceptance and Use of Technology 2 and environmental awareness, partial least squares analysis was performed with the software SmartPLS to investigate a dataset of 224 respondents. Results suggest that attitude toward use, perceived usefulness, and performance expectancy have an influence on the behavioral intention to use ridepooling services. In contrast, environmental awareness, price value, and effort expectancy do not have such an influence. The study expands the literature about customer acceptance of ridepooling service as well as new mobility services in general. Further, the paper provides research implications and recommendations for the development and implementation of the ridepooling concept for service providers. © National Academy of Sciences: Transportation Research Board 2021.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Shrivastwa202193,
author={Shrivastwa, R.-R. and Guilley, S. and Danger, J.-L.},
title={Multi-source Fault Injection Detection Using Machine Learning and Sensor Fusion},
journal={Communications in Computer and Information Science},
year={2021},
volume={1497 CCIS},
pages={93-107},
doi={10.1007/978-3-030-90553-8_7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119873264&doi=10.1007%2f978-3-030-90553-8_7&partnerID=40&md5=a70d37a59b72c3a2a06ebf933c078cdd},
affiliation={Secure-IC S.A.S., Rennes, France; LTCI, Télécom Paris, Institut Polytechnique de Paris, Paris, France},
abstract={Fault attacks have raised serious concern with the growing amount of connected devices. Even a small vulnerability might compromise a complete network. It is therefore important to secure all the devices in the connected architecture. A solution to this problem is presented in this paper where we provide a hardware framework, called Smart Monitor, that utilizes a set of sensors (digital or physical) placed on the chip alongside the security target to be protected. The framework continuously monitors the status of the sensors and its Artificial Intelligence (AI) core produces two outputs viz. presence of a fault and type of detected perturbation. The types of attack sources can be electromagnetic, clock-glitch, laser, temperature, etc. In this work we utilize Electro-Magnetic (EM) and Clock-Glitch (CG) as sources of fault injections. Both attacks are performed in multiple settings to increase attack diversity. The framework is able to detect the presence of an attack with 92% accuracy for mixed or multiple attack sources, and further classify the type of perturbation with 78% accuracy keeping the false positive rate at 0%. Overall, this two-stage detection framework is a cost-effective countermeasure that can be deployed easily in any integrated circuit to safeguard against multiple fault attacks. The AI core is further evaluated for consistency in performance on hardware using High Level Synthesis (HLS), as a proof-of-concept, emulating real-world scenario. © 2021, Springer Nature Switzerland AG.},
author_keywords={Artificial Intelligence (AI);  Cyber-physical attacks;  Cyber-protection;  Decision making process;  Embedded security;  High-Level Synthesis (HLS);  Internet of Things (IoT);  Machine Learning (ML);  Naive Bayes Classifier;  Threat detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Goel202118,
author={Goel, R.K. and Yadav, C.S. and Vishnoi, S. and Singh, L. and Pachauri, P.},
title={Team Cognition Approach in Agile Software Development},
journal={Journal of Engineering Science and Technology Review},
year={2021},
volume={14},
number={4},
pages={18-25},
doi={10.25103/jestr.144.03},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119679307&doi=10.25103%2fjestr.144.03&partnerID=40&md5=037f2a443a7e8a4c293f7277ae0af3b5},
affiliation={Department of Computer Science & Engineering, Noida Institute of Engineering & Technology, Greater Noida, India; Department of Physics, Noida Institute of Engineering & Technology, Greater Noida, India; Department of Electronics and Communication, Noida Institute of Engineering & Technology, Greater Noida, India; Department of Mechanical Engineering, Noida Institute of Engineering & Technology, Greater Noida, India},
abstract={Crowdsourcing cognitive science (CCS) provides a cutting-edge development strategy for scientific advancement and innovation to leverage the positive features of the platform. Utilizing the positive qualities of the platform and mitigating risk is the prime decisive task. The crowdsourcing paradigm shortens software development time by increasing the parallelism of design, coding and testing through flexible implementation. Although software development tasks using traditional paradigms were more complex and interdependent, they can be made easy using the crowdsourcing. CCS offers a new way for scientific advancement in this field. The objective of this study is to build more adaptable quality software. As per WHO/World Bank, 15% people in the world have challenges due to disabilities. In the Mission 2030 for Sustainable Development, disability cannot be seen as a hurdle for the lack of digital access and for the achievement of their human rights. It is also important to reinforce the exogenous effect in order to sustain the internal development of human capital and technology for agile process convergence. Crowd-sourced design reduces development cost, time, and effort by involving intelligence, creativity, and critical thinking of people at different levels of society. In this study, internal consistency and reliability of team are also assessed with the help of Cronbach's alpha using IBM-SPSS. The given approach has been validated for its effectiveness and verified with the help of Smart India Hackathon (SIH-2019) datasets. © 2021. All Rights Reserved.},
author_keywords={Agile software development;  linguistic diversity;  Micro-task;  Team cognition;  WCAG guidelines},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2021805,
author={Liu, P. and Ji, S. and Zhang, X. and Dai, Q. and Lu, K. and Fu, L. and Chen, W. and Cheng, P. and Wang, W. and Beyah, R.},
title={IFIZZ: Deep-State and Efficient Fault-Scenario Generation to Test IoT Firmware},
journal={Proceedings - 2021 36th IEEE/ACM International Conference on Automated Software Engineering, ASE 2021},
year={2021},
pages={805-816},
doi={10.1109/ASE51524.2021.9678785},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119332298&doi=10.1109%2fASE51524.2021.9678785&partnerID=40&md5=379720e6e70d7e7dae737bfd1db68b9c},
affiliation={Zhejiang University, Hangzhou, China; Binjiang Institute of Zhejiang University, Hangzhou, China; Zhejiang University NGICS Platform, Hangzhou, China; University of Minnesota Twin Cities, Minneapolis, United States; Georgia Institute of Technology, Atlanta, United States},
abstract={IoT devices are abnormally prone to diverse errors due to harsh environments and limited computational capabilities. As a result, correct error handling is critical in IoT. Implementing correct error handling is non-trivial, thus requiring extensive testing such as fuzzing. However, existing fuzzing cannot effectively test IoT error-handling code. First, errors typically represent corner cases, thus are hard to trigger. Second, testing error-handling code would frequently crash the execution, which prevents fuzzing from testing following deep error paths.In this paper, we propose IFIZZ, a new bug detection system specifically designed for testing error-handling code in Linux-based IoT firmware. IFIZZ first employs an automated binary-based approach to identify realistic runtime errors by analyzing errors and error conditions in closed-source IoT firmware. Then, IFIZZ employs state-aware and bounded error generation to reach deep error paths effectively. We implement and evaluate IFIZZ on 10 popular IoT firmware. The results show that IFIZZ can find many bugs hidden in deep error paths. Specifically, IFIZZ finds 109 critical bugs, 63 of which are even in widely used IoT libraries. IFIZZ also features high code coverage and efficiency, and covers 67.3% more error paths than normal execution. Meanwhile, the depth of error handling covered by IFIZZ is 7.3 times deeper than that covered by the state-of-the-art method. Furthermore, IFIZZ has been practically adopted and deployed in a worldwide leading IoT company. We will open-source IFIZZ to facilitate further research in this area. © 2021 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2021,
author={Liu, Y. and Liu, X. and Xu, S.},
title={Design and Implementation of Data Processing Software for Internet of Things Based on Virtual Reality},
journal={Mobile Information Systems},
year={2021},
volume={2021},
doi={10.1155/2021/8557690},
art_number={8557690},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118555875&doi=10.1155%2f2021%2f8557690&partnerID=40&md5=d0d97c47684cf0aa19b871874029277d},
affiliation={Information Technology and Equipment Center, Guangzhou College of Technology and Business, Guangdong, Guangzhou, 510800, China; Office of Informatization Management, Sun Yat-Sen University, Guangdong, Guangzhou, 510275, China},
abstract={With the rapid development of the times, the application of the Internet of Things also develops rapidly, resulting in a linear increase in the data generated by the application. Nowadays, people's needs for the forms and types of data are becoming more and more diversified, and the demand for the analysis and processing of Internet of Things data is only increasing. In order to meet the demand for data analysis of the Internet of Things, this article mainly introduces the design and implementation of the data analysis and processing software of the Internet of Things based on virtual reality. Realistic data analysis algorithms are applied to the data analysis function modules of the Internet of Things data analysis and processing software, and then the design of the data collection, data capture, and data analysis and processing functions of the Internet of Things data analysis and processing system is improved. Finally. The realization of the data analysis and processing software of the Internet of Things is carried out through different environments for and various aspects of software testing. The test results show that the realization rate of the software function is as high as 90%, and the data loss rate is reduced to 5%. © 2021 Yuwang Liu et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kalabayev2021,
author={Kalabayev, R. and Sukhova, E. and Rovshenov, G. and Kontarev, R.},
title={Acidizing workflow for optimized well performance in zhdanov and lam oil fields offshore caspian sea},
journal={Society of Petroleum Engineers - SPE Europec featured at 82nd EAGE Conference and Exhibition, EURO 2021},
year={2021},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118465854&partnerID=40&md5=89a872be44c253b9d1d739ebf5948132},
affiliation={Schlumberger, United States; Dragon Oil, United Arab Emirates},
abstract={Successful sandstone matrix stimulation treatments require addressing complex mineralogy, correctly identifying formation damage, selecting the best stimulation fluids, and placing these fluids correctly. The objective of this paper is to demonstrate a workflow considering laboratory testing, advanced software modeling including acid and diverter fluid efficiency calibration using field experimental data, field execution, and relevant case studies in two oil fields located in the Cheleken block, offshore Caspian Sea. Implementation of the workflow has led to positive results. Matrix acidizing was selected as the primary method for restoring production of the oil wells drilled into sandstone reservoirs due to the reservoir characteristics. Deep Zhdanov wells and shallower Lam wells possess ∼15 and ∼250 md permeability and ∼90 and ∼50°C static reservoir temperature, respectively. The target rock mineralogy in both fields predominantly consists of quartz, chlorite, and carbonate minerals. Fluids selection, stimulation design and job execution followed the above mentioned workflow. Treatment modeling considered calibration factors derived from field testing and incorporated several acid and diverter systems. A mix of bullhead and coiled tubing placed treatments were employed. The first step of the workflow considered characterization of the rock mineralogy and selection of the best-fit treatment fluids. Rock dissolution and X-ray diffraction (XRD) tests were run to develop the optimum formulations for the treatment conditions. Further, the results of the laboratory testing were incorporated into the advanced matrix acidizing simulator to model and optimize the treatment schedules. The recently developed matrix stimulation software incorporates geochemical, thermal, and placement simulations calibrated with experimental data. Offset well stimulation treatment pressure match was done by calibrating the acid and diverter fluid efficiency, and those calibrated values were considered for design simulations for the following acid treatments. In this paper, the term "acid efficiency"is defined as a measure of the relative rate at which the acid can penetrate when it flows in the rock matrix as a function of matrix porosity and the overall acid reactivity. The term "diverter efficiency"is defined as a measure of the viscosity developed by a given diverter when it flows in the rock matrix. Such a calibration method accounts for the actual reservoir large-scale acid-rock reaction kinetics. Finally, diagnostic tests and main acid treatments were executed that enabled achieving the desired levels of skin reduction, reservoir placement, zone coverage, and hydrocarbon production rates. Several acid stimulation operations were conducted including three cases in which a low-temperature well with carbonate damage needed repeated acidizing and two additional cases that involved wells with deep, hot, and clay-rich pay zones. Several fluid schedules were applied including foam diversion technique. The above approach uses a unique method of acid efficiency calibration using field experimental data. It requires good knowledge of reservoir rock mineralogy, porosity, and permeability profiles in the zones of interest. Pretreatment skin is calibrated using production data prior to acid efficiency calibration based on matching the actual treatment pressures. The pressure behavior observed during the following treatments closely matched the design pressures confirming applicability of the approach. Copyright © 2021 Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Harel2021322,
author={Harel, D. and Marron, A.},
title={Introducing Dynamical Systems andChaos Early in Computer Science andSoftware Engineering Education Can Help Advance Theory and Practice ofSoftware Development and Computing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={13036 LNCS},
pages={322-334},
doi={10.1007/978-3-030-89159-6_20},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118116694&doi=10.1007%2f978-3-030-89159-6_20&partnerID=40&md5=0e490ee8e497ebe6bc2475b7617a502a},
affiliation={Weizmann Institute of Science, Rehovot, Israel},
abstract={Dynamical systems, i.e., systems that progress along time according to fixed rules, exhibit many special phenomena like the emergence of interesting patterns, bifurcation of behavior, the appearance of chaos despite determinism and boundedness, and sensitive dependence on initial conditions. Such phenomena are encountered in diverse fields, such as fluid dynamics, biological population analysis and economic and financial operations. The study of dynamical systems, their properties, and the mathematical and computerized tools for dealing with them, are often designated as part of advanced curricula in physics or mathematics. Consequently, many computer science students, perhaps the majority thereof, graduate without ever being exposed to such concepts. We argue that with the pervasiveness of dynamical systems and manifestation of their properties in the real world, these concepts should be introduced early on; in undergraduate studies in computer science and related fields, and perhaps even in high school. Available introductory courses demonstrate that only a minimal foundation of knowledge in mathematics is needed for the basic understanding of the key ideas. Such an introduction would deepen one’s understanding of the world and highlight important capabilities and limitations of mathematical and software tools for analysis, simulation, testing and verification of complex systems. In turn, this can lead to enhancement and enrichment of languages, tools and methodologies for dealing with dynamical systems, and of research in computer science and software engineering in general. © 2021, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mäkitalo2021,
author={Mäkitalo, N. and Linkola, S. and Laurinen, T. and Männistö, T.},
title={Towards novel and intentional cooperation of diverse autonomous robots: An architectural approach},
journal={CEUR Workshop Proceedings},
year={2021},
volume={2978},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117785596&partnerID=40&md5=60c4d83427c10cb0bb73b1c8b052a0da},
affiliation={Department of Computer Science, University of Helsinki, Finland},
abstract={In most autonomous robot approaches, the individual robot’s goals and cooperation behavior are fixed during the design. Moreover, the robot’s design may limit its ability to perform other than initially planned tasks. This leaves little room for novel dynamic cooperation where new (joint) actions could be formed or goals adjusted after deployment. In this paper, we address how situational context augmented with peer modeling can foster cooperation opportunity identification and cooperation planning. As a practical contribution, we introduce our new software architecture that enables developing, training, testing, and deploying dynamic cooperation solutions for diverse autonomous robots. The presented architecture operates in three different worlds: in the Real World with real robots, in the 3D Virtual World by emulating the real environments and robots, and in an abstract 2D Block World that fosters developing and studying large-scale cooperation scenarios. Feedback loops among these three worlds bring data from one world to another and provide valuable information to improve cooperation solutions. © 2021 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).},
author_keywords={Autonomous robots;  Ontology-based reasoning;  Peer modeling;  Robot cooperation;  Robot software architecture},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Smirnov2021847,
author={Smirnov, K.K. and Nazarov, A.V. and Blinov, V.V.},
title={Methods of designing electrical equipment for testing very large scale integrated circuit},
journal={International Journal of Nanotechnology},
year={2021},
volume={18},
number={9-10},
pages={847-868},
doi={10.1504/IJNT.2021.118161},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117311565&doi=10.1504%2fIJNT.2021.118161&partnerID=40&md5=2b6ce2ed2698557aaf381c42d6185a91},
affiliation={Scientific Research Institute of System Analysis (SRISA/NIISI RAS), Nakhimovsky Ave., 36, Building 1, Moscow, 117218, Russian Federation; Moscow Aviation Institute (National Research University), Volokolamskoe sh, 4, Moscow, 125993, Russian Federation},
abstract={Due to the constant increase in the level of integration of modern integrated circuits, more stringent requirements are imposed not only on the effectiveness of the methods of their functional control, but also on the timing of the development of electrical equipment. Equipment in the new conditions should work at lower supply voltages, at increased operating frequencies, with a number of chip pins is measured in thousands. The dimension and complexity of the task of designing electrical equipment also increase sharply, and the need to maintain the quality of very large scale integrated (VLSI) circuit development at the proper level leads to a sharp increase in the timing of their commissioning. The way out of this situation lies in the development of new VLSI testing methods and new methods for electrical equipment designing. The paper proposes one of such fundamental approaches, paving the way for full automation of the technological process of designing test solutions, ensuring effective production functional control of VLSI in an acceptable time frame. The method is based on establishing the exact correspondence of mathematical and topological models of VLSI chip. The problems of electrical equipment designing include: the need to take into account the peculiarities of wiring of differential pairs on printed board of electrical equipment, configuring diverse equipment, the avalanche-like growth in the volume and complexity of tests, the increase in the operating frequencies of modern VLSI, numerous technological and other limitations. To solve them, the paper proposes a number of constructive methods, including exact methods of local optimisation. Copyright © 2021 Inderscience Enterprises Ltd.},
author_keywords={Automatic design;  Mathematical models;  Microchips testing;  Software;  Software;  Testing of integrated circuits;  Ultra-large integrated circuits},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tang2021,
author={Tang, Y. and Jiang, H. and Zhou, Z. and Li, X. and Ren, Z. and Kong, W.},
title={Detecting Compiler Warning Defects Via Diversity-Guided Program Mutation},
journal={IEEE Transactions on Software Engineering},
year={2021},
doi={10.1109/TSE.2021.3119186},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117258535&doi=10.1109%2fTSE.2021.3119186&partnerID=40&md5=4025186df049466da0524fb0f2c5901a},
affiliation={School of Software, Dalian University of Technology, Dalian, Liaoning, China, (e-mail: tangyixuan@mail.dlut.edu.cn); School of Software, Dalian University of Technology, Dalian, Liaoning, China, 116621 (e-mail: jianghe@dlut.edu.cn); School of Software, Dalian University of Technology, 12399 Dalian, Liaoning, China, 116024 (e-mail: cszide@gmail.com); the SnT Centre for Security, Reliability and Trust, University of Luxembourg, 81872 Luxembourg, Luxembourg, Luxembourg, (e-mail: xiaochen.li@uni.lu); School of Software, Dalian university of technology, Dalian, Liaoning, China, (e-mail: zren@dlut.edu.cn); School of Software, Dalian University of Technology, Dalian, Liaoning, China, (e-mail: wqkong@dlut.edu.cn)},
abstract={Compiler diagnostic warnings help developers identify potential programming mistakes during program compilation. However, these warnings could be erroneous due to the defects of compiler warning diagnostics. Although many techniques have been proposed to automatically generate test programs for compiler warning defect detection, the effectiveness of these techniques on defect-nding is still limited, due to their ability at generating warning-sensitive test program structures. Therefore, in this paper, we propose a DIversity-guided PROgram Mutation approach, called DIPROM, to construct diverse warning-sensitive programs for effective compiler warning defect detection. Given a seed test program, DIPROM rst removes its dead code to reduce false positive warning defects. Then, the abstract syntax tree (AST) of the test program is constructed; DIPROM iteratively mutates the structures of the AST to generate warning-sensitive program variants. To improve the diversity of program variants, DIPROM applies a novel diversity function to guide the selection of the best program variants in each iteration. With the selected program variants, differential testing is conducted to effectively detect warning defects in different compilers. In the experiments, we evaluate DIPROM with two popular C compilers (i.e., GCC and Clang). Experimental results show that DIPROM can detect 75.36% and 218.42% more warning defects than two state-of-the-art approaches (i.e., Epiphron and Csmith), respectively. Meanwhile, DIPROM is efcient, which only spends 61.14% of the time of comparative approaches on average in nding the same number of warning defects. We at last applied DIPROM on the latest development versions of GCC and Clang. After two months running, DIPROM reported 12 new warning defects; nine of them have been conrmed/xed by developers. IEEE},
author_keywords={Codes;  Compiler Testing;  Differential Testing;  Feature extraction;  Program Mutation;  Program processors;  Programming;  Software;  Test Program Generation;  Testing;  Tools},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aleti20213,
author={Aleti, A.},
title={On the Effectiveness of SBSE Techniques: Through Instance Space Analysis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12914 LNCS},
pages={3-6},
doi={10.1007/978-3-030-88106-1_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117155656&doi=10.1007%2f978-3-030-88106-1_1&partnerID=40&md5=38e3ee1ffd7c075ee43de05f56e60087},
affiliation={Faculty of Information Technology, Monash University, Melbourne, Australia},
abstract={Search-Based Software Engineering is now a mature area with numerous techniques developed to tackle some of the most challenging software engineering problems, from requirements to design, testing, fault localisation, and automated program repair. SBSE techniques have shown promising results, giving us hope that one day it will be possible for the tedious and labour intensive parts of software development to be completely automated, or at least semi-automated. In this talk, I will focus on the problem of objective performance evaluation of SBSE techniques. To this end, I will introduce Instance Space Analysis (ISA), which is an approach to identify features of SBSE problems that explain why a particular instance is difficult for an SBSE technique. ISA can be used to examine the diversity and quality of the benchmark datasets used by most researchers, and analyse the strengths and weaknesses of existing SBSE techniques. The instance space is constructed to reveal areas of hard and easy problems, and enables the strengths and weaknesses of the different SBSE techniques to be identified. I will present on how ISA enabled us to identify the strengths and weaknesses of SBSE techniques in two areas: Search-Based Software Testing and Automated Program Repair. Finally, I will end my talk with potential future directions of the objective assessment of SBSE techniques. © 2021, Springer Nature Switzerland AG.},
author_keywords={Instance space analysis;  Search-based software engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Du20213845,
author={Du, C. and Jin, T. and Guo, Y. and Jia, B. and Li, B.},
title={FastAFLGo: Toward a directed greybox fuzzing},
journal={Computers, Materials and Continua},
year={2021},
volume={69},
number={3},
pages={3845-3855},
doi={10.32604/cmc.2021.017697},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113422274&doi=10.32604%2fcmc.2021.017697&partnerID=40&md5=7a356719a5d0981e2951fa4acd19e2d5},
affiliation={School of Information Science and Technology, North China University of Technology, Beijing, 100144, China; Department of Computer Science, University of Illinois Springfield, Springfield, IL  62703, United States; Civil Aviation Management Institute of China, Beijing, 100102, China},
abstract={While the size and complexity of software are rapidly increasing, not only is the number of vulnerabilities increasing, but their forms are diversifying. Vulnerability has become an important factor in network attack and defense. Therefore, automatic vulnerability discovery has become critical to ensure software security. Fuzzing is one of the most important methods of vulnerability discovery. It is based on the initial input, i.e., a seed, to generate mutated test cases as new inputs of a tested program in the next execution loop. By monitoring the path coverage, fuzzing can choose high-value test cases for inclusion in the new seed set and capture crashes used for triggering vulnerabilities. Although there have been remarkable achievements in terms of the number of discovered vulnerabilities, the reduction of time cost is still inadequate. This paper proposes a fast directed greybox fuzzing model, FastAFLGo. A fast convergence formula of temperature is designed, and the energy scheduling scheme can quickly determine the best seed to make the program execute toward the target basic blocks. Experimental results show that FastAFLGo can discover more vulnerabilities than the traditional fuzzing method in the same execution time. © 2021 Tech Science Press. All rights reserved.},
author_keywords={Directed;  Fuzzing;  Greybox;  Power schedule},
document_type={Article},
source={Scopus},
}

@ARTICLE{Menendez2021,
author={Menendez, H.D. and Clark, D.},
title={Hashing Fuzzing: Introducing Input Diversity to Improve Crash Detection},
journal={IEEE Transactions on Software Engineering},
year={2021},
doi={10.1109/TSE.2021.3100858},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112596038&doi=10.1109%2fTSE.2021.3100858&partnerID=40&md5=3a926444fb86e3266f72f1f365d2cbe2},
affiliation={Computer Science, University College London, 4919 London, London, United Kingdom of Great Britain and Northern Ireland, (e-mail: h.menendez@mdx.ac.uk); Computer Science, University College London, London, London, United Kingdom of Great Britain and Northern Ireland, (e-mail: david.clark@ucl.ac.uk)},
abstract={The utility of a test set of program inputs is strongly influenced by its diversity and its size. Syntax coverage has become a standard proxy for diversity. Although more sophisticated measures exist, such as proximity of a sample to a uniform distribution, methods to use them tend to be type dependent. We use r-wise hash functions to create a novel, semantics preserving, testability transformation for C programs that we call HashFuzz. Use of HashFuzz improves the diversity of test sets produced by instrumentation-based fuzzers. We evaluate the effect of the HashFuzz transformation on eight programs from the Google Fuzzer Test Suite using four state-of-the-art fuzzers that have been widely used in previous research. We demonstrate pronounced improvements in the performance of the test sets for the transformed programs across all the fuzzers that we used. These include strong improvements in diversity in every case, maintenance or small improvement in branch coverage -- up to 4.8% improvement in the best case, and significant improvement in unique crash detection numbers -- between 28% to 97% increases compared to test sets for untransformed programs. IEEE},
author_keywords={Fuzz Testing;  Fuzzing;  Generators;  Hash functions;  HashFuzz;  Instruments;  Software;  System Testing;  System testing;  Testing;  Universal Hashing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kothari2021,
author={Kothari, N.H. and Bhalja, B.R. and Pandya, V. and Tripathi, P.},
title={A rate-of-change-of-current based fault classification technique for thyristor-controlled series-compensated transmission lines},
journal={International Journal of Emerging Electric Power Systems},
year={2021},
doi={10.1515/ijeeps-2021-0031},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112277486&doi=10.1515%2fijeeps-2021-0031&partnerID=40&md5=5b637ccec434ca5e34837765ea9ec26c},
affiliation={Department of Electrical Engineering, RK University, Gujarat, Rajkot, India; Department of Electrical Engineering, Marwadi University, Gujarat, Rajkot, India; Department of Electrical Engineering, Indian Institute of Technology Roorkee, Uttarakhand, Roorkee, India; Department of Electrical Engineering, School of Technology, Pandit Deendayal Petroleum University, Gandhinagar, Raison, India; Department of Electrical Engineering, Institute of Engineering and Technology, Dr. A. P. J. Abdul Kalam Technical University, Uttar Pradesh, Lucknow, India},
abstract={This paper presents a new fault classification technique for Thyristor-Controlled Series-Compensated (TCSC) transmission lines using Support Vector Machine (SVM). The proposed technique is based on the utilization of post-fault magnitude of Rate-of-Change-of-Current (ROCC). Fault classification has been carried out by giving ROCC of three-phases and zero sequence current as inputs to SVM classifier. The performance of SVM as a binary-class, and multi-class classifier has been evaluated for the proposed feature. The validity of the suggested technique has been tested by modeling a TCSC based 400 kV, 300 km long transmission line using PSCAD/EMTDC software package. Based on the above model, a large number of diversified fault cases (41,220 cases) have been generated by varying fault and system parameters. The effect of window length, current transformer (CT) saturation, noise-signal, and sampling frequency have also been studied. It has been found that the proposed technique provides an accuracy of 99.98% for 37,620 test cases. Moreover, the performance of the suggested technique has also been found to be consistent upon evaluating in a 12-bus power system model consisting of a 365 kV, 60 Hz, 300 km long TCSC line. Comparative evaluation of the proposed SVM based technique with other recent techniques clearly indicates its superiority in terms of fault classification accuracy. © 2021 Walter de Gruyter GmbH, Berlin/Boston 2021.},
author_keywords={fault classification;  rate-of-change-of-current (ROCC);  support vector machine;  thyristor-controlled series-compensator},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Moli2021252,
author={Moli, S. and Faqiang, C.},
title={Evaluation on the Application of Online Games in English Education},
journal={Proceedings - 2021 2nd International Conference on Education, Knowledge and Information Management, ICEKIM 2021},
year={2021},
pages={252-256},
doi={10.1109/ICEKIM52309.2021.00062},
art_number={9479494},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111601977&doi=10.1109%2fICEKIM52309.2021.00062&partnerID=40&md5=c37c7fe96764024c2be2479681d1c295},
affiliation={Shandong Institute of Commerce and Technology, Jinan, China},
abstract={In today's digital age, based on the advancement of information technology, the integration of technological innovation into traditional classrooms has been achieved and the form has also shown interest and diversity. At the same time, the popularization of computers and development of multimedia technology allow teachers to easily design and optimize more interesting learning games. In order to effectively enhance Chinese college students' interest in English language learning and academic level, this article breaks the traditional classroom teaching method and innovatively integrates the kahoot! game teaching model. It is open to 100 Chinese college students and lasts for 365 days, that is, one academic year, two semesters. The study mainly focuses on college English teaching reform. Besides, the study uses SPSS 23.0 software to carry out sample t-test on the evaluation data of college English academic level in the early, middle and end of the experiment. In summary, it is found that the reform of the game teaching model can increase students' interest in English language learning within one semester, but the academic level is not significantly improved. However, when the game teaching mode lasts for 2 semesters, it can significantly improve students' grammar, vocabulary, listening and speaking skills, but it still cannot surpass the traditional teaching mode in terms of reading and writing. © 2021 IEEE.},
author_keywords={component;  English learning;  kahoot;  online games;  technology development},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pielecha202147,
author={Pielecha, I.},
title={Energy management system of the hybrid ultracapacitor-battery electric drive vehicles},
journal={Archives of Transport},
year={2021},
volume={58},
number={2},
pages={47-62},
doi={10.5604/01.3001.0014.8797},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107920481&doi=10.5604%2f01.3001.0014.8797&partnerID=40&md5=721dd45653b8590033568d509331ae91},
affiliation={Poznan University of Technology, Faculty of Civil and Transport Engineering, Poznan, Poland},
abstract={The search for new, alternative propulsion and energy sources in transport is one of the economic and technological priorities of the current decade. The modern development of hybrid drives and electric means of transport makes it possible to at least partially diversify conventional drive systems. The study discusses the use of a battery and ultracapacitor in electric vehicles. Simulation analyzes of energy flow were performed using the solutions of electric drive systems and various energy storage control algorithms. The research was carried out in relation to the use of braking energy, its conversion into electricity and its storage in a battery or ultracapacitor. The operating conditions of the battery and the ultracapacitor were assessed in terms of specific energy consumption while driving. The article proposed the use of a drive system connected in series, the last link of which was an ultracapacitor. Such a solution significantly reduced the use of the battery as well as its regular charging-discharging. At the same time, it required the use of a high-capacity ultracapacitor, which contributed to increasing its charging time. The analyzes were carried out using standardized research tests as well as tests in real traffic conditions. The research was carried out with the use of the AVL Cruise software for the analysis of energy flow in vehicles; a middle class passenger vehicle was selected for the tests, equipped with an electrochemical battery and - in the next stage of the research - an ultracapacitor. Three research models were used: I) typical electric drive system; II) a system with the use of ultracapacitors ran by a simple control algorithm; III) a system with the use of ultracapacitors with an advanced control algorithm (the algorithm took into account the change of driving conditions to the ultracapacitor charging conditions). The advantages of using ultracapacitors in the electric drive of a vehicle were demonstrated, especially for results obtained in real traffic conditions. Analyzing the simulation tests results allowed to determine the most advantageous options of utilizing these systems, in particular in the aspect of increased possibilities of algorithms controlling the flow of electricity in the drive system. © 2021 Warsaw University of Technology. All rights reserved.},
author_keywords={Battery;  Electric vehicle;  Energy flow modeling;  Ultracapacitor},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2021,
author={Zhang, W. and Jiao, C. and Zhou, Q. and Liu, Y. and Xu, T.},
title={Gender-Based Deep Learning Firefly Optimization Method for Test Data Generation},
journal={Computational Intelligence and Neuroscience},
year={2021},
volume={2021},
doi={10.1155/2021/8056225},
art_number={8056225},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107642234&doi=10.1155%2f2021%2f8056225&partnerID=40&md5=5d22d119401e17636cc6810d65ee5671},
affiliation={State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, Henan, 450000, China; Software College, Zhongyuan University of Technology, Zhengzhou, Henan, 450000, China; School of Information Engineering, Zhengzhou University, Zhengzhou, Henan, 450000, China; The Jackson Laboratory for Genomic Medicine, Farmington, CT  06032, United States},
abstract={Software testing is a widespread validation means of software quality assurance in industry. Intelligent optimization algorithms have been proved to be an effective way of automatic test data generation. Firefly algorithm has received extensive attention and been widely used to solve optimization problems because of less parameters and simple implement. To overcome slow convergence rate and low accuracy of the firefly algorithm, a novel firefly algorithm with deep learning is proposed to generate structural test data. Initially, the population is divided into male subgroup and female subgroup. Following the randomly attracted model, each male firefly will be attracted by another randomly selected female firefly to focus on global search in whole space. Each female firefly implements local search under the leadership of the general center firefly, constructed based on historical experience with deep learning. At the final period of searching, chaos search is conducted near the best firefly to improve search accuracy. Simulation results show that the proposed algorithm can achieve better performance in terms of success coverage rate, coverage time, and diversity of solutions. © 2021 Wenning Zhang et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Aycock2021,
author={Aycock, K. and Blinova, K. and Casciola, M. and Craven, B. and Di Prima, M. and D'Souza, G. and Duraiswamy, N. and Farahmand, M. and Hariharan, P. and Herbertson, L. and Jamiolkowski, M. and Lu, Q. and Malinauskas, R. and Patel, M. and Rinaldi, J. and Sivan, S. and Vesnovsky, O. and Weaver, J.},
title={Cardiovascular medical devices: Regulatory science research overview in the office of science and engineering laboratories (OSEL) at the food and drug administration (FDA)},
journal={Proceedings of the 2021 Design of Medical Devices Conference, DMD 2021},
year={2021},
doi={10.1115/DMD2021-1085},
art_number={V001T02A011},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107217878&doi=10.1115%2fDMD2021-1085&partnerID=40&md5=9003ec8b9610a7702a5f9b7f49bb7d72},
affiliation={Electrophysiology Research Group Lead, Cardiovascular Research Group, FDA, Center for Devices and Radiological Health (CDRH), OSEL, Silver Spring, MD, United States; Hemodynamics Research Group Lead, Cardiovascular Research Group, FDA, Center for Devices and Radiological Health (CDRH), OSEL, Silver Spring, MD, United States; Hemocompatibility Research Group Lead, Cardiovascular Research Group, FDA, Center for Devices and Radiological Health (CDRH), OSEL, Silver Spring, MD, United States; Device Durability Research Group Lead, Cardiovascular Research Group, FDA, Center for Devices and Radiological Health (CDRH), OSEL, Silver Spring, MD, United States},
abstract={The Cardiovascular Research Program in OSEL at the FDA consists of a diverse group of engineers and scientists who seek to drive innovation in cardiovascular device technology through development and standardization of pre-clinical test methods using in vitro, in vivo, and in silico models. The goal is to improve the pre- and post-market regulatory review processes and to accelerate patient access to safe and effective cardiovascular medical devices (e.g., heart valves, ventricular assist devices, cardiopulmonary bypass and cardiac mapping systems, ablation catheters, pacemakers, defibrillators, cardiac occluders, etc). © 2021 by ASME.},
author_keywords={Cardiovascular;  Device durability;  Electrophysiology;  FDA;  Hemocompatibility;  Hemodynamics;  OSEL},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2021,
author={Wang, J. and Yang, Y. and Wang, S. and Chen, C. and Wang, D. and Wang, Q.},
title={Context-aware Personalized Crowdtesting Task Recommendation},
journal={IEEE Transactions on Software Engineering},
year={2021},
doi={10.1109/TSE.2021.3081171},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106729716&doi=10.1109%2fTSE.2021.3081171&partnerID=40&md5=67f37fe295058a12ed25afe41c5f8b51},
affiliation={Institute of Software, Chinese Academy of Sciences, Institute of Software Chinese Academy of Sciences, 53036 Beijing, Beijing, China, 100190 (e-mail: junjie@iscas.ac.cn); School of Systems and Enterprises, Stevens Institute of Technology, 33694 Hoboken, New Jersey, United States, (e-mail: yyang4@stevens.edu); Department of Electrical Engineering and Computer Science, York University, 7991 Toronto, Ontario, Canada, (e-mail: wangsong@eecs.yorku.ca); Faculty of Information Technology, Monash University, Melbourne, Victoria, Australia, 3800 (e-mail: chunyang.chen@monash.edu); Institute of Software, Chinese Academy of Sciences, Institute of Software Chinese Academy of Sciences, 53036 Beijing, Beijing, China, (e-mail: dandan@iscas.ac.cn); Institute of Software, Chinese Academy of Sciences, Beijing, Beijing, China, 100190 (e-mail: wq@itechs.iscas.ac.cn)},
abstract={Crowdsourced software testing (short for crowdtesting) is a special type of crowdsourcing. It requires that crowdworkers master appropriate skill-sets and commit significant effort for completing a task. Abundant uncertainty may arise during a crowdtesting process due to imperfect information between the task requester and crowdworkers. For example, a worker frequently chooses tasks in an ad hoc manner in crowdtesting context, and an inappropriate task selection may lead to the worker's failing to detect any bugs, and significant testing effort unpaid and wasted. Recent studies have explored methods for supporting task requesters to make informed decisions on task pricing, worker recommendation, and so on. Unfortunately, very few study offers decision making support from the crowdworkers' perspectives. We motivate this study through a pilot study, revealing the large portion (74\%) of unpaid crowdworkers' effort due to the inappropriate task choice. Drawn from our previous work on context-aware crowdworker recommendations, we advocate a more effective alternative to manual task selection would be to provide contextualized and personalized task recommendation considering the diverse distribution of worker preference and expertise, with objectives to increase their winning chances and to potentially reduce the frequency of unpaid crowd work. This paper proposes a context-aware personalized task recommendation approach PTRec, consisting of a testing context model and a learning-based task recommendation model to aid dynamic worker decision in selecting crowdtesting tasks. The testing context model is constructed in two perspectives, i.e., process context and resource context, to capture the in-process progress-oriented information and crowdworkers' characteristics respectively. Built on top of this context model, the learning-based task recommendation model extracts 60 features automatically, and employs random forest learner to generate dynamic and personalized task recommendation which matches workers' expertise and interest. The evaluation is conducted on 636 crowdtesting tasks involving 2,404 crowdworkers from one of the largest crowdtesting platforms, and results show the potential in recommending proper tasks to workers so as to improve bug detection efficiency and increase their monetary earnings. IEEE},
author_keywords={Computer bugs;  Context modeling;  Crowdsourced testing;  Crowdsourcing;  Feature extraction;  Task analysis;  Task recommendation;  Testing;  Testing context model;  Videos},
document_type={Article},
source={Scopus},
}

@ARTICLE{Swathi2021144,
author={Swathi, B. and Tiwari, H.},
title={Integrated Pairwise Testing based Genetic Algorithm for Test Optimization},
journal={International Journal of Advanced Computer Science and Applications},
year={2021},
volume={12},
number={4},
pages={144-150},
doi={10.14569/IJACSA.2021.0120419},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105795180&doi=10.14569%2fIJACSA.2021.0120419&partnerID=40&md5=5f44feaebac0595a8cf9e8dc8f57a1be},
affiliation={Department of CSE, Jain University, Bengaluru, India; Centre for Incubation, Innovation, Research and Consultancy (CIIRC), Jyothy Institute of Technology, Bengaluru, Karnataka, India},
abstract={Generation of Test cases in software testing is an important and a complex activity as it deals with diversified range of inputs. Fundamentally, test case generation is considered to be a multi-objective problem as it aims to cover many targets. Deriving test cases for the Web Applications has become critical to the most of the enterprises. In this paper, a solution for generating test cases for web applications is proposed, the solution uses the System Graph (consisting of links and data dependencies) considering that test cases were based on a combination of input values and data dependencies. Pairwise testing is used to derive the test cases to be executing from entire test cases and then a genetic algorithm is proposed to generate test cases specific to functional testing. The proposed approach was tested through two distinct experiments by measuring the code coverage at every generation and results show that genetic algorithm used increased the fitness value and code coverage. Overall, the results of the paper validate the proposed approach and algorithm, having potential in further construct an automated integrated solution for generating test cases for the entire process. © 2021, International Journal of Advanced Computer Science and Applications. All Rights Reserved.},
author_keywords={fitness value;  genetic algorithm;  multi objective optimization;  pairwise testing;  Test case generation;  test optimization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gromova2021168,
author={Gromova, A. and Itkin, I. and Pavlov, S.},
title={Generation of Testing Metrics by Using Cluster Analysis of Bug Reports},
journal={Communications in Computer and Information Science},
year={2021},
volume={1288 CCIS},
pages={168-181},
doi={10.1007/978-3-030-71472-7_14},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104722835&doi=10.1007%2f978-3-030-71472-7_14&partnerID=40&md5=778707407a7d28869eb74e22609d60ae},
affiliation={Exactpro Systems, 2nd Yuzhnoportovy Projezd 20A Str. 4, Moscow, 115088, Russian Federation; Exactpro Systems, Suite 3.02, St Clements House, 27 Clements Lane, London, EC4N 7AE, United Kingdom},
abstract={One of the most significant challenges of defect reporting is how to compute and predict the testing metrics. Any software development project needs certain suitable testing metrics. Cluster analysis can be used to generate them. The interpretation of the received clusters helps to determine explicit and implicit characteristics of software testing and development. This paper describes several software solutions for clustering bug reports. We have extracted bug reports related to three open-source JBOSS projects and experimented using that data. Our experiments demonstrate that effective results can be achieved in the area of defect clustering. We provide the results achieved by using two clustering algorithms: k-means and EM. Our research shows that the usage of the EM algorithm generates more detailed information about the specifics of the project than the usage of the k-means algorithm. So, EM gives a possibility to create more diverse testing metrics suitable for project needs. © 2021, Springer Nature Switzerland AG.},
author_keywords={Bug report;  Cluster analysis;  Defect management;  Testing metrics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Banchero202158,
author={Banchero, M. and Robledo, F. and Romero, P. and Sartor, P. and Servetti, C.},
title={Max-Diversity Orthogonal Regrouping of MBA Students Using a GRASP/VND Heuristic},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2021},
volume={12559 LNCS},
pages={58-70},
doi={10.1007/978-3-030-69625-2_5},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103519660&doi=10.1007%2f978-3-030-69625-2_5&partnerID=40&md5=c8a4cc92c2e2ec72742a9192aa0e604d},
affiliation={Instituto de Computación, INCO, Facultad de Ingeniería, Universidad de la República, Montevideo, Uruguay; IEEM Business School, Universidad de Montevideo, Lord Ponsomby 2542, Montevideo, Uruguay},
abstract={Students from Master in Business Administration (MBA) programs are usually split into teams. Many schools rotate the teams at the beginning of every term, so that each student works with a different set of peers during every term. Diversity within every team is desirable regarding gender, major, age and other criteria. Achieving diverse teams while avoiding -or minimizing- the repetition of student pairs is a time-consuming complex task for MBA Directors. The Max-Diversity Orthogonal Regrouping (MDOR) problem is here introduced, where the goal is to maximize a global notion of diversity, considering multiple stages (i.e., terms) and intra-diversity within the teams. A hybrid GRASP/VND heuristic combined with Tabu Search is developed for its resolution. Its effectiveness has been tested in real-life groups from the MBA program offered at IEEM Business School, Universidad de Montevideo, Uruguay, with a notorious gain regarding team diversity and repetition level. © 2021, Springer Nature Switzerland AG.},
author_keywords={Diversity;  GRASP;  MBA teams;  Orthogonal regrouping;  VND},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bansal2021111,
author={Bansal, A. and Jain, A. and Anand, A. and Annk, S.},
title={Proposal of iterative genetic algorithm for test suite generation},
journal={International Journal of Information System Modeling and Design},
year={2021},
volume={12},
number={1},
pages={111-130},
doi={10.4018/IJISMD.2021010106},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101961679&doi=10.4018%2fIJISMD.2021010106&partnerID=40&md5=800a70ac312047afb597e35104d327b9},
affiliation={Netaji Subhas University of Technology, India; Delhi University, India},
abstract={Huge and reputed software industries are expected to deliver quality products. However, industry suffers from a loss of approximately $500 billion due to shoddy software quality. The quality of the product in terms of its accuracy, efficiency, and reliability can be revamped through testing by focusing attention on testing the product through effective test case generation and prioritization. The authors have proposed a test-case generation technique based on iterative listener genetic algorithm that generates test cases automatically. The proposed technique uses its adaptive nature and solves the issues like redundant test cases, inefficient test coverage percentage, high execution time, and increased computation complexity by maintaining the diversity of the population which will decrease the redundancy in test cases. The performance of the technique is compared with four existing testcase generation algorithms in terms of computational complexity, execution time, coverage, and it is observed that the proposed technique outperformed. © 2021 IGI Global. All rights reserved.},
author_keywords={Automated Testing;  Evolutionary Algorithm;  Genetic Algorithm;  Machine Learning;  Software Modelling;  Software Quality;  Software Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nina202136852,
author={Nina, H. and Pow-Sang, J.A. and Villavicencio, M.},
title={Systematic Mapping of the Literature on Secure Software Development},
journal={IEEE Access},
year={2021},
volume={9},
pages={36852-36867},
doi={10.1109/ACCESS.2021.3062388},
art_number={9363884},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101857461&doi=10.1109%2fACCESS.2021.3062388&partnerID=40&md5=00cdd44d73401157eaaae6f37ade489c},
affiliation={Maestría en Informática, Pontificia Universidad Católica Del Perú, Lima, Peru; Facultad de Ingeniería en Electricidad y Computación, Escuela Superior Politécnica Del Litoral, Guayaquil, Ecuador},
abstract={The accelerated growth in exploiting vulnerabilities due to errors or failures in the software development process is a latent concern in the Software Industry. In this sense, this study aims to provide an overview of the Secure Software Development trends to help identify topics that have been extensively studied and those that still need to be. Therefore, in this paper, a systematic mapping review with PICo search strategies was conducted. A total of 867 papers were identified, of which only 528 papers were selected for this review. The main findings correspond to the Software Requirements Security, where the Elicitation and Misuse Cases reported more frequently. In Software Design Security, recurring themes are security in component-based software development, threat model, and security patterns. In the Software Construction Security, the most frequent topics are static code analysis and vulnerability detection. Finally, in Software Testing Security, the most frequent topics are vulnerability scanning and penetration testing. In conclusion, there is a diversity of methodologies, models, and tools with specific objectives in each secure software development stage. © 2013 IEEE.},
author_keywords={construction;  design;  requirements;  security;  Software development;  systematic mapping review;  testing;  vulnerability},
document_type={Article},
source={Scopus},
}

@ARTICLE{LeFranc2021,
author={Le Franc, A. and Carpentier, P. and Chancelier, J.-P. and Le Lara, M.},
title={EMSx: a numerical benchmark for energy management systems},
journal={Energy Systems},
year={2021},
doi={10.1007/s12667-020-00417-5},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101067475&doi=10.1007%2fs12667-020-00417-5&partnerID=40&md5=51e7a6314edf5ce8c066000229fc9e51},
affiliation={CERMICS, École des Ponts ParisTech and Efficacity, Champs-sur-Marne, France; UMA ENSTA Paris, Palaiseau, France; CERMICS, École des Ponts ParisTech, Champs-sur-Marne, France},
abstract={Inserting renewable energy in the electric grid in a decentralized manner is a key challenge of the energy transition. However, at local scale, both production and demand display erratic behavior, which makes it challenging to match them. It is the goal of Energy Management Systems (EMS) to achieve such balance at least cost. We present EMSx, a numerical benchmark for testing control algorithms for the management of electric microgrids equipped with a photovoltaic unit and an energy storage system. EMSx is made of three key components: the EMSx dataset, provided by Schneider Electric, contains a diverse pool of realistic microgrids with a rich collection of historical observations and forecasts; the EMSx mathematical framework is an explicit description of the assessment of electric microgrid control techniques and algorithms; the EMSx software EMSx.jl is a package, implemented in the Julia language, which enables to easily implement a microgrid controller and to test it. All components of the benchmark are publicly available, so that other researchers willing to test controllers on EMSx may reproduce experiments easily. Eventually, we showcase the results of standard microgrid control methods, including Model Predictive Control, Open Loop Feedback Control and Stochastic Dynamic Programming. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.},
author_keywords={Electric microgrid;  Multistage stochastic optimization;  Numerical benchmark},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xiang2021,
author={Xiang, Y. and Huang, H. and Li, M. and Li, S. and Yang, X.},
title={Looking For Novelty in Search-based Software Product Line Testing},
journal={IEEE Transactions on Software Engineering},
year={2021},
doi={10.1109/TSE.2021.3057853},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100850109&doi=10.1109%2fTSE.2021.3057853&partnerID=40&md5=5f225d6d5b5f4f2fa4b1a0f1d5d89780},
affiliation={School of Software Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: gzhuxiang_yi@163.com); School of Software Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: hhan@scut.edu.cn); School of Computer Science, University of Birmingham, 1724 Birmingham, Birmingham, United Kingdom of Great Britain and Northern Ireland, (e-mail: M.Li.8@cs.bham.ac.uk); School of Software Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: chickl@qq.com); School of Software Engineering, South China University of Technology, 26467 Guangzhou, Guangdong, China, (e-mail: xwyang@scut.edu.cn)},
abstract={Testing software product lines (SPLs) is difficult due to a huge number of possible products to be tested. Recently, there has been a growing interest in similarity-based testing of SPLs, where similarity is used as a surrogate metric for the t-wise coverage. In this context, one of the primary goals is to sample, by optimizing similarity metrics using search-based algorithms, a small subset of test cases (i.e., products) as dissimilar as possible, thus potentially making more t-wise combinations covered. Prior work has shown, by means of empirical studies, the great potential of current similarity-based testing approaches. However, the rationale of this testing technique deserves a more rigorous exploration. To this end, we perform correlation analyses to investigate how similarity metrics are correlated with the t-wise coverage. We find that similarity metrics generally have significantly positive correlations with the t-wise coverage. This well explains why similarity-based testing works, as the improvement on similarity metrics will potentially increase the t-wise coverage. Moreover, we explore, for the first time, the use of the novelty search (NS) algorithm for similarity-based SPL testing. The algorithm rewards &#x201C;novel&#x201D; individuals, i.e., those being different from individuals discovered previously, and this well matches the goal of similarity-based SPL testing. We find that the novelty score used in NS has (much) stronger positive correlations with the t-wise overage than previous approaches relying on a genetic algorithm (GA) with a similarity-based fitness function. Experimental results on 31 software product lines validate the superiority of NS over GA, as well as other state-of-the-art approaches, concerning both t-wise coverage and fault detection capacity. Finally, we investigate whether it is useful to combine two satisfiability solvers when generating new individuals in NS, and how the performance of NS is affected by its key parameters. In summary, looking for novelty provides a promising way of sampling diverse test cases for SPLs. IEEE},
author_keywords={Correlation;  correlation analysis;  Fault detection;  Frequency modulation;  Measurement;  novelty search;  product sampling;  similarity-based testing;  Software;  Software product line testing;  Software product lines;  t-wise coverage;  Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma202170,
author={Ma, W.-L. and Yang, Q.-G. and Zhang, G.-Y.},
title={Blast testing and numerical simulation analysis of diversion tunnel upper section for Qianping reservoir},
journal={International Journal of Industrial and Systems Engineering},
year={2021},
volume={37},
number={1},
pages={70-83},
doi={10.1504/IJISE.2021.112465},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099666074&doi=10.1504%2fIJISE.2021.112465&partnerID=40&md5=95189167cda196d455fecc44f9758fa5},
affiliation={North China University of Water Resources and Electric Power, Zhengzhou, Henan, 450046, China; Henan Provincial Water Conservancy Research Institute, Zhengzhou, Henan, 450003, China; Henan University of Urban Construction, Pingdingshan, Henan, 467036, China},
abstract={This paper expounds the basic theory of rock mass blasting technology, the rock blasting excavation of Qianping reservoir project was taken as an example, by means of numerical simulation and experimental research, the finite element model for rock blasting excavation in Qianping reservoir is established. The rock blast on the upper section of the diversion tunnel for Qianping reservoir is simulated by using the dynamic analysis module LS-DYNA in the finite element method analysis software ANSYS, through the comparative analysis of blast testing and numerical simulation results. The results show that, the dynamic analysis results are in good agreement with the field test, the correctness and validity of the finite element model are proved, finite element dynamic analysis model is a general calculation model, rock mechanics parameters in the model can be adjusted in the calculation process, it can be used to simulate the blasting excavation process of different rock properties. © 2021 Inderscience Enterprises Ltd.},
author_keywords={Blast testing;  Data of explosive filled;  Diversion tunnel;  LS-DYNA;  Numerical simulation;  Parameters of hole arrangement},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shah2021381,
author={Shah, A.R. and Banday, M.T. and Sheikh, S.A.},
title={Design of a drag and touch multilingual universal captcha challenge},
journal={Advances in Intelligent Systems and Computing},
year={2021},
volume={1086},
pages={381-393},
doi={10.1007/978-981-15-1275-9_31},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087533111&doi=10.1007%2f978-981-15-1275-9_31&partnerID=40&md5=b3fc30af50548f4f0a6689a7ef7c2359},
affiliation={PG Department of Electronics and Instrumentation Technology, University of Kashmir, Srinagar, Jammu and Kashmir  190006, India},
abstract={In order to protect sensitive data from bots, service providers protect their Web sites through some of the CAPTCHA tests. This test ensures that protected resources are accessible to only legitimate human users and no computer program (bot) gets their access. With a growing number of Web sites and Web portals offering more and more sensitive data for access by its legitimate human users coupled with the advances in techniques used to break CAPTCHA challenges, more robust and complex CAPTCHA challenges were developed. Though some of these tests offer the desired level of security, however, they often are inaccessible to a large population of the Web sites and also suffer from other usability issues. This paper presents a new CAPTCHA challenge based on the mouse motion event and is also usable on touch screens. This test can be made highly secure, lightweight, multilingual, and universal and does not suffer from any usability issues. The test can be coded for English or any other regional language, therefore, overcoming the language barrier of the CAPTCHA challenges. Experimentation with the proposed model implementation of the CAPTCHA challenge has proved its robustness, diversity, lightweight, and usability. © Springer Nature Singapore Pte Ltd. 2021.},
author_keywords={CAPTCHA;  HIP;  Protected resources;  Regional CAPTCHA;  Website security},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kim2021,
author={Kim, Y. and Hong, S.},
title={DEMINER: test generation for high test coverage through mutant exploration},
journal={Software Testing Verification and Reliability},
year={2021},
volume={31},
number={1-2},
doi={10.1002/stvr.1715},
art_number={e1715},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074668003&doi=10.1002%2fstvr.1715&partnerID=40&md5=3621bfc85b2d5811b9de1f0a1c54165b},
affiliation={School of Computing, KAIST, Daejeon, South Korea; School of Computer Science and Electrical Engineering, Handong Global University, Pohang, South Korea},
abstract={Most software testing techniques test a target program as it is and fail to utilize valuable information of diverse test executions on many variants/mutants of the original program in test generation. This paper proposes a new test generation technique DEMINER, which utilizes mutant executions to guide test generation on the original program for high test coverage. DEMINER first generates various mutants of an original target program and then extracts runtime information of mutant executions, which covered unreached branches by the mutation effects. Using the obtained runtime information, DEMINER inserts guideposts, artificial branches to replay the observed mutation effects, to the original target programs. Finally, DEMINER runs automated test generation on the original program with guideposts and achieves higher test coverage. We implemented DEMINER for C programs through software mutation and guided test generation such as concolic testing and fuzzing. We have shown the effectiveness of DEMINER on six real-world target programs: Busybox-ls, Busybox-printf, Coreutils-sort, GNU-find, GNU-grep and GNU-sed. The experiment results show that DEMINER improved branch coverage by 63.4% and 19.6% compared with those of the conventional concolic testing techniques and the conventional fuzzing techniques on average, respectively. © 2019 John Wiley & Sons, Ltd.},
author_keywords={automated test generation;  concolic testing;  fuzzing;  mutation analysis;  test coverage},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dass20202065,
author={Dass, S. and Namin, A.S.},
title={A Sensitivity Analysis of Evolutionary Algorithms in Generating Secure Configurations},
journal={Proceedings - 2020 IEEE International Conference on Big Data, Big Data 2020},
year={2020},
pages={2065-2072},
doi={10.1109/BigData50022.2020.9378307},
art_number={9378307},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103850085&doi=10.1109%2fBigData50022.2020.9378307&partnerID=40&md5=88dac6d3384e2d90ab9711cabd68b05e},
affiliation={Texas Tech University, Computer Science Department, United States},
abstract={The growth of Cyber-physical Systems (CPS) has been increased in recent years. This has led to the coupling of highly complex cyber-physical components. With the integration of such complex components, new security challenges have emerged. Studies involving security issues in CPS have been quite difficult to be generalized due to the presence of heterogeneity and the diversity of the CPS components. These systems are subject to various vulnerabilities, threats and attacks, as a consequence of complex versions of CPS being introduced over time. This paper deals with vulnerabilities caused due to improper configurations in the software component of cyber-physical systems. Evolutionary algorithms such as Genetic Algorithms (GA) and Particle Swarm Optimization (PSO) can be employed to adequately test the underlying software for certain categories of vulnerabilities. This paper provides a detailed sensitivity analysis of these evolutionary algorithms in order to find out whether changing parameters involved in tuning these algorithms affect the overall performance. This analysis is based on the estimate of the number of generation of secure vulnerability pattern vectors under the variation of different parameters. The results indicate that while there is no evidence of influential parameters in Genetic Algorithms (i.e., mutation rate and population size), changes in the parameters involved in Particle Swarm Optimization algorithms (i.e., velocity rate and fitness range) have some positive impacts on the number of secure configurations generated. © 2020 IEEE.},
author_keywords={Cyber-Physical Systems;  Genetic Algorithm;  Particle Swarm Optimization;  security;  Sensitivity Analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DambiskiGomesdeCarvalho2020565,
author={Dambiski Gomes de Carvalho, G. and Martins de Resende, L.M. and Gomes de Carvalho, H. and Pontes, J. and Oliveira Correa, R.},
title={The local innovation agents program: a literature review on the largest Brazilian small business innovation support program},
journal={International Journal of Innovation Science},
year={2020},
volume={12},
number={5},
pages={565-588},
doi={10.1108/IJIS-03-2020-0022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096592386&doi=10.1108%2fIJIS-03-2020-0022&partnerID=40&md5=eb3933a5ae62a790e004d6a344fca9b7},
affiliation={Department of Industrial Engineering, Universidade Tecnologica Federal do Parana, Ponta Grossa, Brazil; Department of Science and Innovation, Centro Internacional de Tecnologia de Software – CITS, Ponta Grossa, Brazil; Department of Business Administration, Universidade Federal de Sergipe, Sao Cristovao, Brazil},
abstract={Purpose: This paper aims to analyze the characteristics of Brazilian micro and small businesses (MSBs) and the main lessons derived from the largest small business innovation support program in the country, the Local Innovation Agents – Agentes Locais de Inovação (ALI) Program. Design/methodology/approach: In total, 34 papers were selected from the Web of Science and Scopus databases (28), as well as from the Revista de Administração e Inovação – RAI (6), a seminal Brazilian innovation journal. The papers were analyzed in terms of the regional context, methodological approach and main findings. Regional complementary analyzes of some program figures were also performed by Spearman correlation and Wilcoxon signed-rank tests. Findings: The review revealed a low innovation level among Brazilian MSBs and that the platform (incremental product) and brand (marketing) innovation dimensions were the most developed across different regions and industries. Reviewed papers also showed that all MSBs were able to improve innovation over the program independent of previous management and innovation levels, besides positive relationships between management and innovation. The complementary analysis provided a regional panorama of the program figures and corroborated MSBs innovation improvement. Research limitations/implications: This review analyzed relevant papers and figures related to the program, summarized main lessons and provided future research venues. Practical implications: Different innovation strategies reviewed may be implemented by MSBs owners. Policymakers may also benefit from the program experience. Originality/value: Despite the high number of publications and the relevance of the largest Brazilian innovation support program, there were still no literature reviews comprehending the diverse lessons derived from the ALI Program, as well as a regional panorama of the program figures. © 2020, Emerald Publishing Limited.},
author_keywords={Innovation;  Micro enterprises;  Policy;  Program;  Regional;  Small businesses;  SMEs;  Support},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Devi2020489,
author={Devi, S.V.G. and Nalini, C.},
title={Performance analysis of k-means clustering based hyperbolic tangent instituted classification of automated coding contracts},
journal={Proceedings of the 3rd International Conference on Intelligent Sustainable Systems, ICISS 2020},
year={2020},
pages={489-497},
doi={10.1109/ICISS49785.2020.9315994},
art_number={9315994},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100770434&doi=10.1109%2fICISS49785.2020.9315994&partnerID=40&md5=22a1145b23b79e5b6be049c6ef44f68c},
affiliation={Bharath Institute of Higher Education and Research, Department of Computer Science and Engineering, Chennai, India},
abstract={The conceptual development of software coding specific contracts wherein programmers' liaison being very moderate is very definitive to substantially develop contracts that are optimized adequately alleviating the problems in concurring with software specifications, aiding thorough and extended testing of diverse software from the real world. The relevant framework put forward preliminarily infers dependency specifications indicative of behavior and semantic aspects organized as constraints or conditions in a Decision tree transpired as Automated Programming Contracts. The technicalities to refine the generated contracts are furthermore applied for enhanced verification practices via a Modified form of K-Means Clustering and the contracts are further classified using Activation function instituted TanH2-Entropy Classifier. The classification performance of the proposed algorithm is then compared with prevalent algorithms to show effective Memory and Computation influential performances and substantial soundness of errors identification ability. © 2020 IEEE.},
author_keywords={Activation function;  Contracts;  Decision Tree;  Entropy;  K Means clustering;  Tangent Hyperbolic},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Olsina2020,
author={Olsina, L. and Tebes, G. and Peppino, D. and Becker, P.},
title={Approaches used to Verify and Validate a Software Testing Ontology as an Artifact},
journal={2020 IEEE Congreso Bienal de Argentina, ARGENCON 2020 - 2020 IEEE Biennial Congress of Argentina, ARGENCON 2020},
year={2020},
doi={10.1109/ARGENCON49523.2020.9505430},
art_number={9505430},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114035872&doi=10.1109%2fARGENCON49523.2020.9505430&partnerID=40&md5=2bb340a1c50560557f2fda1e23c4a463},
affiliation={School of Engineering, UNLPam GIDIS_Web General Pico, La Pampa, Argentina},
abstract={Verification and Validation (V&V) are undoubtedly two key aspects of the artifact construction process. V&V entails activities, methods, tools, and ultimately strategies for ensuring quality when developing artifacts. Software engineering production lines, stakeholders and researchers produce diverse artifacts. For example, researchers may build ontologies, which can be both conceptualized and implemented artifacts. Ontologies are playing a central role in many application domains in particular, as well as in software and knowledge engineering disciplines in general. To show the applicability of V&V approaches, this work considers a software testing ontology as an artifact. This was developed following the Design Science Research methodology in the context of a layered ontological architecture. This paper illustrates the V&V approaches used, involving activities and methods for static and dynamic testing, evaluation and assessment. It addresses V&V approaches for the ontology conceptualization, as well as a functional dynamic testing method for its implementation. ©2020 IEEE},
author_keywords={Artifact;  Software Testing Ontology;  V&V Activities and Methods;  Verification and Validation (V&V)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2020385,
author={Wang, S. and Zhang, X.},
title={An empirical study of the impacts of assets and financial assets of families on their expenditure and consumption -based on CFPS database},
journal={Proceedings - 2020 Management Science Informatization and Economic Innovation Development Conference, MSIEID 2020},
year={2020},
pages={385-388},
doi={10.1109/MSIEID52046.2020.00082},
art_number={9382508},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103742587&doi=10.1109%2fMSIEID52046.2020.00082&partnerID=40&md5=882908b516f3ba88e3a2b15cb50b209d},
affiliation={Shanghai University, School of Economics, Shanghai, China},
abstract={With the continuous development of China's economy, the income and expenditure of Chinese households became more diversified. Many families had assets and investments. The wealth effects of these assets played an increasingly obvious role in driving consumption growth. This paper used the data of the National Bureau of Statistics and the China Family Panel Studies (CFPS) collected by Peking University from 2010 to 2019 to study the impacts of values of family's total assets and financial assets on the total expenditure and consumption expenditure of Chinese households. We constructed two Panel Models, and used Stata 12 software to run the Regression tests. The results of panel regressions reveal that the family's total assets and financial assets have positive impacts on both total expenditure and consumption expenditure of families. These mean that the wealth effects of the total assets and financial assets are obvious. © 2020 IEEE.},
author_keywords={CFPS database;  Consumption expenditure;  Financial assets;  Panel regression},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mao202046,
author={Mao, C. and Wen, L. and Chen, T.Y.},
title={Adaptive random test case generation based on multi-objective evolutionary search},
journal={Proceedings - 2020 IEEE 19th International Conference on Trust, Security and Privacy in Computing and Communications, TrustCom 2020},
year={2020},
pages={46-53},
doi={10.1109/TrustCom50675.2020.00020},
art_number={9343017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101276590&doi=10.1109%2fTrustCom50675.2020.00020&partnerID=40&md5=b462bf81f7af99a13d621adb51c5297b},
affiliation={School of Software and IoT Engineering, Jiangxi University of Finance and Economics, Nanchang, 330013, China; Swinburne University of Technology, Department of Computer Science and Software Engineering, Melbourne, VIC  3122, Australia},
abstract={Diversity is the key factor for test cases to detect program failures. Adaptive random testing (ART) is one of the effective methods to improve the diversity of test cases. Being an ART algorithm, the evolutionary adaptive random testing (eAR) only increases the distance between test cases to enhance its failure detection ability. This paper presents a new ART algorithm, MoesART, based on multi-objective evolutionary search. In this algorithm, in addition to the dispersion diversity, two other new diversities (or optimization objectives) are designed from the perspectives of the balance and proportionality of test cases. Then, the Pareto optimal solution returned by the NSGA-II framework is used as the next test case. In the experiments, the typical block failure pattern in the cases of two-dimensional and three-dimensional input domains is used to validate the effectiveness of the proposed MoesART algorithm. The experimental results show that MoesART exhibits better failure detection ability than both eAR and the fixed-sized-candidate-set ART (FSCS-ART), especially for the programs with three-dimensional input domain. © 2020 IEEE.},
author_keywords={Adaptive random testing;  Diversity;  Multiobjective evolutionary search;  Software testing;  Test case},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Velusamy20201,
author={Velusamy, G. and Lent, R.},
title={Smart site diversity for a high throughput satellite system with software-defined networking and a virtual network function},
journal={Future Internet},
year={2020},
volume={12},
number={12},
pages={1-17},
doi={10.3390/fi12120225},
art_number={225},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097525253&doi=10.3390%2ffi12120225&partnerID=40&md5=1eef8f6a778941e2b708cd40a307309b},
affiliation={College of Technology, University of Houston, Houston, TX  77204, United States},
abstract={High Throughput Satellite (HTS) systems aim to push data rates to the order of Terabit/s, making use of Extremely High Frequencies (EHF) or free-space optical (FSO) in the feeder links. However, one challenge that needs to be addressed is that the use of such high frequencies makes the feeder links vulnerable to atmospheric conditions, which can effectively disable channels at times or temporarily increases the bit error rates. One way to cope with the problem is to introduce site diversity and to forward the data through the gateways not affected, or at least less constrained, by adverse conditions. In this paper, a virtual network function (VNF) introduced through reinforcement learning defines a smart routing service for an HTS system. Experiments were conducted on an emulated ground-satellite system in CloudLab, testing a VNF implementation of the approach with software-defined networking virtual switches, which indicate the expected performance of the proposed method. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={High throughput satellites;  Learning automata;  Q-Learning;  Reinforcement learning;  Routing;  SDN;  Smart gateway diversity;  VNF},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bruel2020,
author={Bruel, R. and Sabatier, P.},
title={serac: A R package for ShortlivEd RAdionuclide chronology of recent sediment cores},
journal={Journal of Environmental Radioactivity},
year={2020},
volume={225},
doi={10.1016/j.jenvrad.2020.106449},
art_number={106449},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093922130&doi=10.1016%2fj.jenvrad.2020.106449&partnerID=40&md5=aaf140ab3d02084d583e8f5e85a5e1c4},
affiliation={CARRTEL, Université Savoie-Mont Blanc, INRA, Thonon-les-Bains, 74200, France; Rubenstein Ecosystem Science Laboratory, University of Vermont, Burlington, VT  05401, United States; EDYTEM, Université Savoie-Mont Blanc, CNRS, Le Bourget Du Lac, 73370, France},
abstract={Short-lived radionuclides are measured in surface sediment to provide a geochronology for the past century. Age-depth models are produced from 210Pbex activity-derived sedimentation rates and corroborated by known events, such as 137Cs and 241Am activities that are result of fallout from nuclear weapon tests and the Chernobyl accident. Different methods of age depth modelling using such data require expertise in lake sedimentation processes. Here, we present a package, serac, that allows the user to compute an age-depth model and generate a graph, the age-depth correspondence in a text file, and metadata, using the free open-source statistical software R. serac ensures the reproducibility of age-depth or age-mass depth models and allows testing of several 210Pbex models (CFCS, CIC, CRS, CRS piecewise) and sedimentation hypotheses (changes in sedimentation rates, presence of instantaneous deposits, varved sedimentation, etc.). Using several case studies, including lakes and lagoon in different environments, we demonstrate the use of the program in diverse situations that may be encountered. The rising number of sediment cores in recent palaeo studies and the need to correlate them require reproducible methods. serac is a user-friendly code that enables age model computation for the past century and encourages the standardisation of outputs. © 2020 Elsevier Ltd},
author_keywords={137Cs;  210Pb model;  Age model;  Metadata;  R package;  Shortlived radionuclide},
document_type={Article},
source={Scopus},
}

@ARTICLE{Islam2020,
author={Islam, M. and Kharma, N. and Grogono, P.},
title={Mutation operators for Genetic Programming using Monte Carlo Tree Search},
journal={Applied Soft Computing},
year={2020},
volume={97},
doi={10.1016/j.asoc.2020.106717},
art_number={106717},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091477789&doi=10.1016%2fj.asoc.2020.106717&partnerID=40&md5=28ba4b3267109ed2432ff0a7fd4ac967},
affiliation={Gina Cody School of Engineering and Computer Science, Concordia University, 1455 De Maisonneuve Blvd. W. MontrealQC, H3G 1M8, Canada},
abstract={Expansion is a novel mutation operator for Genetic Programming (GP). It uses Monte Carlo simulation to repeatedly expand and evaluate programs using unit instructions, which extends the search beyond the immediate – often misleading – horizon of offspring programs. To evaluate expansion, a standard Koza-style tree-based representation is used and a comparison is carried out between expansion and sub-tree crossover as well as point mutation. Using a diverse set of benchmark symbolic regression problems, we prove that expansion provides for better fitness performance than point mutation, when included with crossover. Expansion also provides a significant boost to fitness when compared to GP using crossover only, with similar or lower levels of program bloat. Despite expansion's success in improving evolutionary performance, it does not eliminate the problem of program bloat. In response, an analogous genetic operator, reduction, is proposed and tested for its ability to keep a check on program size. We conclude that the best fitness can be achieved by including these three operators in GP: crossover, point mutation and expansion. © 2020},
author_keywords={Computational intelligence;  Evolutionary computation;  Expansion;  Genetic Programming;  Monte Carlo Simulation;  Monte Carlo Tree Search;  Program synthesis;  Reduction;  Symbolic regression},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Martins2020691,
author={Martins, H. and Araujo, F. and da Cunha, P.R.},
title={Benchmarking Serverless Computing Platforms},
journal={Journal of Grid Computing},
year={2020},
volume={18},
number={4},
pages={691-709},
doi={10.1007/s10723-020-09523-1},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087642711&doi=10.1007%2fs10723-020-09523-1&partnerID=40&md5=339be767d673ed8dda08c5d791cc4c17},
affiliation={Centre for Informatics and Systems of the University of Coimbra, Department of Informatics Engineering, University of Coimbra, Coimbra, Portugal},
abstract={We propose a benchmarking test suite to evaluate performance of cloud serverless platforms and an open source software tool to automate the test process. Additionally, we used this setup to compare the commercial offers of Amazon, Google, Microsoft, and IBM. The work builds on ideas and experiments reported in the literature that, nevertheless, did not offer a “standard” set of coherent and comprehensive tests, capable of analyzing diverse serverless platforms, across the same set of features. To that end, we defined seven tests that cover scalability (latency and throughput), the effect of allocated memory, the performance for CPU-bound cases, the impact of payload size, the influence of the programming language, resource management (e.g., reuse of containers), and overall platform overhead. We created a software tool to deploy the test code and collect metrics in a manner agnostic to the serverless platforms under study. At a time when serverless computing popularity is rising, this benchmarking suite and software test tool enable developers to take informed decisions on the suitability and performance of each provider’s serverless offer. Additionally, they also help to identify attributes in need for improvement in the existing platforms. © 2020, Springer Nature B.V.},
author_keywords={Benchmarking;  Cloud computing;  Serverless computing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kim202010140,
author={Kim, S.J. and Cho, J. and Lee, C. and Shon, T.},
title={Smart seed selection-based effective black box fuzzing for IIoT protocol},
journal={Journal of Supercomputing},
year={2020},
volume={76},
number={12},
pages={10140-10154},
doi={10.1007/s11227-020-03245-7},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082751190&doi=10.1007%2fs11227-020-03245-7&partnerID=40&md5=fecc6d83d88d6d220c57675bb6be89fb},
affiliation={Department of Computer Engineering, Ajou University, Suwon, South Korea; Security Division, IBM MEA, Dubai, United Arab Emirates; Department of Computer Science and Engineering, Seoul National University of Science and Technology, Seoul, South Korea; Department of Cyber Security, Ajou University, Suwon, South Korea},
abstract={Connections of cyber-physical system (CPS) components are gradually increasing owing to the introduction of the Industrial Internet of Things (IIoT). IIoT vulnerability analysis has become a major issue because complex skillful cyber-attacks on CPS systems exploit their zero-day vulnerabilities. However, current white box techniques for vulnerability analysis are difficult to use in real heterogeneous environments, where devices supplied by various manufacturers and diverse firmware versions are used. Therefore, we herein propose a novel protocol fuzzing test technique that can be applied in a heterogeneous environment. As seed configuration can significantly influence the test result in a black box test, we update the seed pool using test cases that travel different program paths compared to the seed. The input, output, and Delta times are used to determine if a new program area has been searched in the black box environment. We experimentally verified the effectiveness of the proposed. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={CPS;  Fuzzing test;  IIoT;  Vulnerability analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Maziku2020148,
author={Maziku, H.},
title={Improved Data Accuracy Assessment Tool for Information Management Systems},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={148-152},
doi={10.1145/3442555.3442579},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105737739&doi=10.1145%2f3442555.3442579&partnerID=40&md5=44f1fa23d7cab33bf6ff493e018628dd},
affiliation={University of Dar Es Salaam(UDSM), Tanzania},
abstract={Developing countries are increasingly taking advantage of the rapid advancement in ICT to replace paper-based operations with Information Management Systems (IMS) such as District Health Information Software (DHIS). While the adoption of IMS presents significant benefits, challenges exist in the quality of IMS data. Inaccurate, incomplete or redundant data in IMS has misled organisations into making incorrect decisions leading to customer dissatisfaction and high cost implications. There is an urgent need for IMS stakeholders to have mechanisms of assessing the quality of data prior to data analysis, data sharing or decision making. Data Accuracy Assessment Tool (DAAT) assesses and identifies errors in a pair of context related datasets. DAAT provides ability for Data Managers to easily compare datasets by choosing attributes of their interest from a pool of diverse attributes that define the data. Through reports and visualization, the tool reveals the accuracy of data in real time using metrics such as validity, completeness and duplication of data. DAAT is scalable since it can be integrated with any IMS such as DHIS. The tool has been tested using four years Voluntary Medical Male Circumcision (VMMC) program data from JHPIEGO's AIDSFree project in Tanzania. © 2020 ACM.},
author_keywords={Data Quality Assessment, Accuracy;  Information Management Systems, Human Centered Design},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bressana2020218,
author={Bressana, P. and Zilberman, N. and Soulé, R.},
title={Finding hard-to-find data plane bugs with a PTA},
journal={CoNEXT 2020 - Proceedings of the 16th International Conference on Emerging Networking EXperiments and Technologies},
year={2020},
pages={218-231},
doi={10.1145/3386367.3431313},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097615997&doi=10.1145%2f3386367.3431313&partnerID=40&md5=5589b7a4fe84efc301df8f3e81529658},
affiliation={Università della Svizzera Italiana, Italy; University of Oxford, United Kingdom; Yale University, United States},
abstract={Bugs in network hardware can cause tremendous problems. However, programmable network devices have the potential to provide greater visibility into the internal behavior of devices, allowing us to more quickly find and identify problems. In this paper, we provide a taxonomy of data plane bugs, and use the taxonomy to derive a Portable Test Architecture (PTA) which offers essential abstractions for testing on a variety of network hardware devices. PTA is implemented with a novel data plane design that (i) separates target-specific from target-independent components, allowing for portability, and (ii) allows users to write a test program once at compile time, but dynamically alter the behavior via runtime configuration. We report 12 diverse bugs on different hardware targets, and their associated software, exposed using PTA. © 2020 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Livinskii2020,
author={Livinskii, V. and Babokin, D. and Regehr, J.},
title={Random testing for C and C++ compilers with YARPGen},
journal={Proceedings of the ACM on Programming Languages},
year={2020},
volume={4},
number={OOPSLA},
doi={10.1145/3428264},
art_number={196},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097586177&doi=10.1145%2f3428264&partnerID=40&md5=8e8b44c693df64f98a1854b631e48bbd},
affiliation={University of Utah, United States; Intel Corporation, United States},
abstract={Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel® C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20% for LLVM and 40% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours. © 2020 Owner/Author.},
author_keywords={automated testing;  compiler defect;  compiler testing;  random program generation;  random testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lv2020515,
author={Lv, S. and Yin, B.},
title={Testing DNN-Based Path Planning Algorithms by Metamorphic Testing},
journal={Proceedings - 2020 7th International Conference on Dependable Systems and Their Applications, DSA 2020},
year={2020},
pages={515-526},
doi={10.1109/DSA51864.2020.00088},
art_number={9331244},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100566000&doi=10.1109%2fDSA51864.2020.00088&partnerID=40&md5=c101e5231d2793aa133c257e5c6587dd},
affiliation={Beihang University, School of Automation Science and Electrical Engineering, Beijing, China},
abstract={Deep Neural Networks (DNNs) are increasingly applied to solve path planning problems in recent years. However, unexpected or incorrect behaviors of DNNs greatly threaten the reliability of DNN-based path planning algorithms. Therefore, the reliability should be evaluated through the software testing process. The quality of the training dataset is of great importance to the pre-trained DNN models. The pretrained model may still lack generality by using a randomly generated and insufficient training dataset. And DNN-based system testing is faced with Oracle problems. Because Metamorphic Testing (MT) has been shown considerable effectiveness in alleviating the absence of oracle problems. To increase the reliability of DNN-based path planning algorithms, in this paper, we present a test technique specialized for DNN-based path planning algorithms based on metamorphic testing. We present a framework for systematically designing sixteen metamorphic relations (MRs) by combining input transformations and output relations. And experiments are carried out on an actually released business software system, which demonstrates that our method is effective. The results show that our approach can effectively improve the diversity of test data, the accuracy of the DNN model, and the reliability of the software. © 2020 IEEE.},
author_keywords={deep learning testing;  DNN-based path planning algorithm;  metamorphic testing;  neuron coverage},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Clarke-Midura2020,
author={Clarke-Midura, J. and Sun, C. and Pantic, K.},
title={Making apps: An approach to recruiting youth to computer science},
journal={ACM Transactions on Computing Education},
year={2020},
volume={20},
number={4},
doi={10.1145/3425710},
art_number={30},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097248921&doi=10.1145%2f3425710&partnerID=40&md5=55a70078aa24e36e89a54167e9536a8b},
affiliation={Department of Instructional Technology and Learning Sciences, Utah State University, 2830 Old Main Hill, Logan, UT  84322, United States; Teacher Education Department, Weber State University, 1351 Edvalson St., Dept 1304, Room# 335, Ogden, UT  84408-1304, United States},
abstract={In response to the need to broaden participation in computer science, we designed a summer camp to teach middle-school-aged youth to code apps with MIT App Inventor. For the past four summers, we have observed significant gains in youth's interest and self-efficacy in computer science, after attending our camps. The majority of these youth, however, were youth from our local community. To provide equal access across the state and secure more diversity, we were interested in examining the effect of the camp on a broader population of youth. Thus, we partnered with an outreach program to reach and test our camps on youth from low-income high-poverty areas in the Intermountain West. During the summer of 2019, we conducted two sets of camps: locally advertised app camps that attracted youth from our local community and a second set of camps as part of a larger outreach program for youth from low-income high-poverty areas. The camps for both populations followed the same design of personnel, camp activities, structure, and curriculum. However, the background of the participants was slightly different. Using survey data, we found that the local sample experienced significant gains in both self-efficacy and interest, while the outreach group only reported significant gains in self-efficacy after attending the camp. However, the qualitative data collected from the outreach participants indicated that they had a positive experience both with the camp and their mentors. In this article, we discuss the camp design and findings in relation to strategies for broadening participation in Computer Science education. © 2020 ACM.},
author_keywords={access;  diversity;  high school youth;  interest;  Middle school youth;  near-peer mentor;  self-efficacy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhao2020,
author={Zhao, S. and Liu, R. and Wu, X. and Ye, C. and Zia, A.W.},
title={A programmable and self-adaptive dynamic pressure delivery and feedback system for efficient intermittent pneumatic compression therapy},
journal={Sensors and Actuators, A: Physical},
year={2020},
volume={315},
doi={10.1016/j.sna.2020.112285},
art_number={112285},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090582707&doi=10.1016%2fj.sna.2020.112285&partnerID=40&md5=23c7244fbfcbf3b44fb30a0f18fc25f6},
affiliation={Institute of Textiles and Clothing, The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong},
abstract={Controlled dynamic pressure delivery is essential for efficiently promoting hemodynamics of the lower extremities in pneumatic compression therapy. Pressure dosage control is challenging because of diverse influencing factors, such as irregular interface contact geometries, different stiffnesses of body tissues and pressure materials, and customized pressure recipes and individual patient demands. To address these challenges, a programmable and self-adaptive dynamic pressure delivery and feedback system for enhanced compression therapy were developed in this study. This system comprised a soft pneumatic actuator (SPA) unit in a stocking laminated with multi-bladder, a programmable pressure control unit for pneumatic and interface pressure control, matrix soft sensors for real-time interface pressure detection, and differential pressure sensors for pneumatic pressure monitoring as well as connective accessories (air tubes and valves). A fluid–structure interaction model was developed to analyse the interaction between the bladder and lower limb by considering essential factors (bladder and soft tissue stiffness, contact area, and bladder-lower limb interface characteristics). An improved proportional–integral–derivative controller was designed to regulate dynamic interface pressure and personalized pneumatic compression treatment. Dedicated software was developed to monitor, process, and display the tested pressure data. In vitro and in vivo experiments were performed to validate the developed system. This study integrated theoretical and experimental approaches to a novel solution for customized dynamic pressure therapy. The findings enhanced our understanding of the working mechanisms and characteristics of the SPA–lower limb system. Moreover, the results serve as a foundation for the design and innovation of advanced programmable units for efficient compression treatment in various applications in healthcare, medicine, and rehabilitation. © 2020 Elsevier B.V.},
author_keywords={Pneumatic compression therapy;  Pressure feedback;  Programmable pressure controller;  Self-adaptive dynamic pressure;  Soft pneumatic actuator},
document_type={Article},
source={Scopus},
}

@ARTICLE{Song20201650,
author={Song, Y. and Gong, Y.-Z.},
title={Web service composition on IoT reliability test based on cross entropy},
journal={Computational Intelligence},
year={2020},
volume={36},
number={4},
pages={1650-1662},
doi={10.1111/coin.12302},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087688798&doi=10.1111%2fcoin.12302&partnerID=40&md5=3e72d083ccd92c1a8efe9ff626a8ba7f},
affiliation={State Key Laboratory of Networking and Switching Technology, Beijing University of Posts and Telecommunications, Beijing, China},
abstract={Web service has developed the managed IoT application to let connected devices easily and securely interact with cloud applications and other devices. As an important factor for web service, the reliability of web services refers to the probability of web service running success. For modeling web service composition, we should abstract the process of web service composition. Due to the diversity and complexity of web service composition, it is unlikely to do exhaustive testing. In order to improve the quality of web service composition test cases and find out which path leads to the greatest probability of service combination failure, heuristic test case generation method is adopted to obtain the optimal test path. First, the web service composition test is abstracted into the MDP model. The QoS of the web service composition is taken as the software test optimization goal, and the cross-entropy strategy is used to optimize the test case. The experimental results show that the test profile given by the cross-strategy is better than the random test strategy. Detect and exclude the same number of software defects. Cross-entropy strategy can significantly reduce the number of test cases, reduce test costs, and improve defect detection efficiency. © 2020 Wiley Periodicals, Inc.},
author_keywords={cross entropy;  MDP model;  QoS;  test;  web service composite},
document_type={Article},
source={Scopus},
}

@ARTICLE{Beeler2020824,
author={Beeler, W.H. and Griffith, K.A. and Evans, S.B. and Golden, D.W. and Jagsi, R.},
title={Visiting Professorship in Academic Radiation Oncology},
journal={International Journal of Radiation Oncology Biology Physics},
year={2020},
volume={108},
number={3},
pages={824-829},
doi={10.1016/j.ijrobp.2020.05.004},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087404066&doi=10.1016%2fj.ijrobp.2020.05.004&partnerID=40&md5=3d5ac0991aa85a3c6d57d8fe8b1d262b},
affiliation={Department of Radiation Oncology, University of Michigan, Ann Arbor, MI, United States; Department of Biostatistics, University of Michigan, Ann Arbor, MI, United States; Department of Therapeutic Radiology, Yale University, New Haven, CT, United States; Department of Radiation and Cellular Oncology, University of Chicago, Chicago, IL, United States; Center for Bioethics and Social Sciences in Medicine, University of Michigan, Ann Arbor, MI, United States},
abstract={Purpose: Visiting professorship is an enjoyable activity that is also influential in academic promotional processes as evidence of the invitee's national reputation. Little is known, however, about the factors considered when selecting visiting professors (VPs) or whether this practice reflects objective criteria. We sought to characterize the process and diversity of participants in visiting professorships within academic radiation oncology (RO) to determine whether opportunities are equitably distributed. Methods and Materials: Surveys were distributed to program directors (PDs) of every 2018 RO residency program accredited by the Accreditation Council for Graduate Medical Education. PDs were asked to identify all VPs over the past 2 years and to describe their departments’ decision-making processes. Publicly available demographic and academic characteristics were obtained for each VP, and results were compared by VP gender and hosting program (HP) 2019 Doximity rank using the χ2 test for categorical data and t test for continuous data. Results: The PD response rate was 60 of 93 (65%); 6 surveys were ≥50% incomplete and were excluded. Over a 2-year timeframe, 51 of 54 departments hosted 233 VPs, of whom 29% were women. The mean number of hosted VPs (5; range, 1-19) and gender distribution (35% women; range, 0-100%) did not significantly differ by HP rank (P =.17 and 0.65, respectively), nor did the selection criteria by which VPs were primarily chosen (subject matter expertise, teaching reputation, and resident interest). Women received significantly lower honoraria amounts than men (P =.035) despite no significant differences by gender in academic rank (P =.71), VP department rank (0.19), or M-index (0.83). Conclusion: Although sample size is limited, this study suggests that academic RO programs have a relatively equitable approach to selecting VPs that emphasizes trainee education and reflects the gender diversity of RO faculty more generally. Care should be taken to ensure that these similarly qualified women are offered the same monetary amount of honoraria as their male colleagues. © 2020 Elsevier Inc.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Scatalon2020,
author={Scatalon, L.P. and Garcia, R.E. and Barbosa, E.F.},
title={Teaching Practices of Software Testing in Programming Education},
journal={Proceedings - Frontiers in Education Conference, FIE},
year={2020},
volume={2020-October},
doi={10.1109/FIE44824.2020.9274256},
art_number={9274256},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098595438&doi=10.1109%2fFIE44824.2020.9274256&partnerID=40&md5=f690e407f60649f6b94052fec5e8b2c5},
affiliation={University of São Paulo (ICMC-USP), São Carlos-SP, Brazil},
abstract={This Research Full Paper presents an overview of the practices that have been used to integrate software testing into programming education. Introductory programming courses compose the core of several undergraduate programs, since programming is a crucial technical skill for professionals in many areas. Given the subject importance, researchers have been conducting several studies to investigate teaching approaches that can help overcoming students' learning difficulties. In particular, studies on introducing software testing into this context present evidence that testing practices can improve students' programming performance and habits. There are many teaching approaches in programming education, which involve different choices of programming paradigm and language, support tools and development practices, such as version control. Likewise, the integration of software testing into such diverse context can also happen in many different ways. Therefore, investigating the ways to teach programming and testing at the same time can help instructors with informed choices. In this sense, we identified teaching practices that have been adopted to integrate software testing into programming education. To do so, we further analyzed a subgroup of 195 papers that returned in our systematic mapping on this research domain. We selected papers describing empirical studies (e.g. survey, qualitative studies, experiments, case studies and experience reports), since this kind of study involves applying a given teaching practice in order to collect evidence or report the observed experience. Overall, our results shed light on how the integration of software testing has been done in different classroom contexts of programming education. We discuss the practices in terms of their application context (i.e. the course), how testing was introduced in theory and practice, and the adopted support tools. We also discuss an important gap regarding the lack of instruction in testing concepts, even when students are responsible to write their own tests. © 2020 IEEE.},
author_keywords={Computer Science Education;  Programming Fundamentals;  Software Testing;  Teaching Practices},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fu2020121,
author={Fu, C.-M. and Wang, Y.-C.},
title={X-on-interposer ecosystem to migrate IoT1.0 to value-added IoT2.0},
journal={Proceedings of Technical Papers - International Microsystems, Packaging, Assembly, and Circuits Technology Conference, IMPACT},
year={2020},
volume={2020-October},
pages={121-124},
doi={10.1109/IMPACT50485.2020.9268589},
art_number={9268589},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098126932&doi=10.1109%2fIMPACT50485.2020.9268589&partnerID=40&md5=677b7b86460842a78b784e590f9e4aee},
affiliation={Yuan Consulting Ltd., Biomedical Incubation Center D110, No.2, Shengyi 2nd Rd, Zhubei City, Hsinchu County, 302, Taiwan; National Cheng Kung University, NCKU, Department of Civil Engineering, Tainan City, 701, Taiwan},
abstract={IoT applications normally involve heterogeneous integration of MCU, RF, sensor chips and discrete components, with the diversity but smaller quantity. PCB prototype is the first practical and cheap proof-of-concept (from 0 to 1) assembly using available package-ICs, components for initial functions test, software development and market trials, although with lower entry-barrier without much differentiation. SiP (system-in-package) is the next rationale revision option (from 1 to 2) for not only decent structural scaling, but also for system-level PPA (performance, power, formfactor and total cost) co-optimization, especially for the value-added sensor fusion, edge IoT2.0 applications or so called AIoT transformation.Such attempt of PCB migration to SiP options is not trivial to-be LEGO-like flows or simple ROI justification, if without deeper domain knowledges and chiplet design flow insights. Traditional SOC mindsets may be not sufficient, due to lacking system-level PPA(performance, power, area) considerations and more, such as thermal, stress management, decent what-if analysis and meaningful design-technology co-optimization among various structural stacking options. Some heterogenous cases are discussed in this article about IP/chip/package strategy, and potentially are developed for decent application-driven interposer platform technology proposals. © 2020 IEEE.},
author_keywords={AIoT transformation;  chiplet design flow;  design-technology co-optimization;  SiP;  structural scaling;  X-on-interposer},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Villanes202031,
author={Villanes, I.K. and Endo, A.T. and Dias-Neto, A.C.},
title={Using App Attributes to Improve Mobile Device Selection for Compatibility Testing},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={31-39},
doi={10.1145/3425174.3425215},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095866521&doi=10.1145%2f3425174.3425215&partnerID=40&md5=c3078fbc31fcd33a868d999dd5a95691},
affiliation={Federal University of Amazonas (UFAM), Manaus AM, Brazil; Universidade Tecnológica Federal Do Parana (UTFPR), Cornélio Procópio PR, Brazil},
abstract={Mobile apps have been developed with the aim of attracting a large and diverse number of users. An impediment factor, especially for the Android platform, is a large number of hardware and software configurations available in the market, so app developers face the challenge of producing a highly compatible app. For compatibility testing, an app can be tested in a subset of devices that sufficiently covers the characteristics of devices adopted by users. Moreover, this subset needs to be extracted from up-to-date sources since new devices and APIs are frequently introduced in the market, while others are deprecated. This paper presents an always-updated, app-specific approach, called DeSeCT, for mobile Device Selection for Compatibility Testing. Our approach is divided into the following steps: First, it crawls the Web to obtain an updated list of devices; second, attributes of the app under test are used to filter unwanted mobile devices; and a multiobjective genetic algorithm is employed to tailor a feature-coverage optimized set of mobile devices. The results provide evidence that DeSeCT can produce a better selection of mobile devices for compatibility testing in comparison with the state-of-the-art. DeSeCT avoids the selection of approximately 15% to 18% of unwanted devices. © 2020 ACM.},
author_keywords={Android;  app attributes;  compatibility testing;  fragmentation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Semeráth2020187,
author={Semeráth, O. and Babikian, A.A. and Li, A. and Marussy, K. and Varró, D.},
title={Automated generation of consistent models with structural and attribute constraints},
journal={Proceedings - 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems, MODELS 2020},
year={2020},
pages={187-199},
doi={10.1145/3365438.3410962},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097001666&doi=10.1145%2f3365438.3410962&partnerID=40&md5=1d3900e2c0feb02e28418b1554f66256},
affiliation={Budapest University of Technology, Budapest, Hungary; McGill University, Montreal, Canada},
abstract={Automatically synthesizing consistent models is a key prerequisite for many testing scenarios in autonomous driving or software tool validation where model-based systems engineering techniques are frequently used to ensure a designated coverage of critical cornercases. From a practical perspective, an inconsistent model is irrelevant as a test case (e.g. false positive), thus each synthetic model needs to simultaneously satisfy various structural and attribute well-formedness constraints. While different logic solvers or dedicated graph solvers have recently been developed, they fail to handle either structural or attribute constraints in a scalable way. In the current paper, we combine a structural graph solver that uses partial models with an SMT-solver to automatically derive models which simultaneously fulfill structural and attribute constraints while key theoretical properties of model generation like completeness or diversity are still ensured. This necessitates a sophisticated bidirectional interaction between different solvers which carry out consistency checks, decision, unit propagation, concretization steps. We evaluate the scalability and diversity of our approach in the context of three complex case studies. © 2020 ACM.},
author_keywords={model generation;  partial modeling;  SMT-solvers},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Behera20202721,
author={Behera, R.K. and Bala, P.K. and Jain, R.},
title={A rule-based automated machine learning approach in the evaluation of recommender engine},
journal={Benchmarking},
year={2020},
volume={27},
number={10},
pages={2721-2757},
doi={10.1108/BIJ-01-2020-0051},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089400122&doi=10.1108%2fBIJ-01-2020-0051&partnerID=40&md5=3a66c822742a21454bbf77c8909a184b},
affiliation={Indian Institute of Management RanchiRanchi, India; School of Computer Engineering, Kalinga Institute of Industrial Technology, Bhubaneswar, India; Area of Information Systems and Business Analytics, Indian Institute of Management Ranchi, Ranchi, India; Montclair State University, Montclair, NJ, United States},
abstract={Purpose: Any business that opts to adopt a recommender engine (RE) for various potential benefits must choose from the candidate solutions, by matching to the task of interest and domain. The purpose of this paper is to choose RE that fits best from a set of candidate solutions using rule-based automated machine learning (ML) approach. The objective is to draw trustworthy conclusion, which results in brand building, and establishing a reliable relation with customers and undeniably to grow the business. Design/methodology/approach: An experimental quantitative research method was conducted in which the ML model was evaluated with diversified performance metrics and five RE algorithms by combining offline evaluation on historical and simulated movie data set, and the online evaluation on business-alike near-real-time data set to uncover the best-fitting RE. Findings: The rule-based automated evaluation of RE has changed the testing landscape, with the removal of longer duration of manual testing and not being comprehensive. It leads to minimal manual effort with high-quality results and can possibly bring a new revolution in the testing practice to start a service line “Machine Learning Testing as a service” (MLTaaS) and the possibility of integrating with DevOps that can specifically help agile team to ship a fail-safe RE evaluation product targeting SaaS (software as a service) or cloud deployment. Research limitations/implications: A small data set was considered for A/B phase study and was captured for ten movies from three theaters operating in a single location in India, and simulation phase study was captured for two movies from three theaters operating from the same location in India. The research was limited to Bollywood and Ollywood movies for A/B phase, and Ollywood movies for simulation phase. Practical implications: The best-fitting RE facilitates the business to make personalized recommendations, long-term customer loyalty forecasting, predicting the company's future performance, introducing customers to new products/services and shaping customer's future preferences and behaviors. Originality/value: The proposed rule-based ML approach named “2-stage locking evaluation” is self-learned, automated by design and largely produces time-bound conclusive result and improved decision-making process. It is the first of a kind to examine the business domain and task of interest. In each stage of the evaluation, low-performer REs are excluded which leads to time-optimized and cost-optimized solution. Additionally, the combination of offline and online evaluation methods offer benefits, such as improved quality with self-learning algorithm, faster time to decision-making by significantly reducing manual efforts with end-to-end test coverage, cognitive aiding for early feedback and unattended evaluation and traceability by identifying the missing test metrics coverage. © 2020, Emerald Publishing Limited.},
author_keywords={Automated recommender engine evaluation;  Machine learning in software testing;  Personalization;  Recommendation system;  Recommender engine;  Recommender engine evaluation metrics},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Coviello2020,
author={Coviello, C. and Romano, S. and Scanniello, G. and Antoniol, G.},
title={GASSER: Genetic algorithm for teSt suite reduction},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2020},
doi={10.1145/3382494.3422157},
art_number={3422157},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095806208&doi=10.1145%2f3382494.3422157&partnerID=40&md5=5943bf8c1aff4698792158d816a599bb},
affiliation={University of Basilicata, Potenza, Italy; University of Bari, Bari, Italy; Polytechnique de Montreal, Montreal, Canada},
abstract={Background. Regression testing is a practice that ensures a System Under Test (SUT) still works as expected after changes. The simplest regression testing approach is Retest-all, which consists of re-executing the entire Test Suite (TS) on the new version of the SUT. When SUT and its TS grow in size, applying Retest-all could be expensive. Test Suite Reduction (TSR) approaches would allow overcoming the above-mentioned issues by reducing TSs while preserving their fault-detection capability. Aim. In this paper, we introduce GASSER (Genetic Algorithm for teSt SuitE Reduction), a new approach for TSR based on a multiobjective evolutionary algorithm, namely NSGA-II. Method. GASSER reduces TSs by maximizing statement coverage and diversity of test cases, and by minimizing the size of the reduced TSs. Results. The preliminary study shows that GASSER reduces more the TS size with a small effect on fault-detection capability when compared with traditional approaches. Conclusions. These outcomes highlight the potential benefits of the use of multi-objective evolutionary algorithm in TSR field and pose the basis for future work. © 2020 IEEE Computer Society. All rights reserved.},
author_keywords={Genetic Algorithm;  Regression Testing;  Test Suite Reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Santos2020232,
author={Santos, I. and Filho, J.C.C. and Souza, S.R.S.},
title={A survey on the practices of mobile application testing},
journal={Proceedings - 2020 46th Latin American Computing Conference, CLEI 2020},
year={2020},
pages={232-241},
doi={10.1109/CLEI52000.2020.00034},
art_number={9458347},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111737372&doi=10.1109%2fCLEI52000.2020.00034&partnerID=40&md5=2014d98004c0188d9ca1296ad86385e6},
affiliation={Institute of Mathematics and Computer Science (ICMC), University of São Paulo (USP), São Carlos, Brazil},
abstract={[Context:] Mobile devices have become increasingly popular, and mobile applications should guarantee a very high level of reliability and quality. Mobile application testing needs to consider several unique requirements that distinguish it from conventional software testing. [Objective:] Our study aims to establish an overview of the testing practices conducted in mobile companies, to identify weaknesses that can be improved to make the testing activity more effective. [Method:] The survey questions were carefully designed using the Goal/Question/Metric method to provide relevant information to the questions raised in our study. [Results and Conclusions:] Our study outlines that native applications are more common. The testing level more performed is the system test and the positions that perform testing levels and objectives are described. Practices related to testing technique selection in the context of mobile applications are highlighted. In the context of this study, Cucumber and Selenium are the testing tools most used to automate testing activity. Some mobile testing characteristics were outlined to understand how the testing in mobile applications run on different devices, how testers deal with the diversity of operating systems that are constantly updated and whether tests are unified to testing a mobile app that runs in different platforms. Furthermore, we report the main challenges faced by testers during the validation of the mobile app. © 2020 IEEE.},
author_keywords={Mobile testing;  Software quality;  Software testing practices;  Survey},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Byun202097,
author={Byun, T. and Rayadurgam, S.},
title={Manifold for Machine Learning Assurance},
journal={Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER 2020},
year={2020},
pages={97-100},
doi={10.1145/3377816.3381734},
art_number={9397537},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104680473&doi=10.1145%2f3377816.3381734&partnerID=40&md5=3c02c94d1235b00e1506089694626fc7},
affiliation={University of Minnesota, Minneapolis, MN, United States},
abstract={The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure-A manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-Time monitoring provides an independent means to assess trustability of the target system's output. Ccs Concepts • Software and its engineering ? Software testing and debugging; • Computing methodologies ? Machine learning. © 2020 ACM.},
author_keywords={machine learning testing;  neural networks;  variational autoencoder},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gerasimou2020322,
author={Gerasimou, S. and Eniser, H.F. and Sen, A. and Cakan, A.},
title={Importance-Driven Deep Learning System Testing},
journal={Proceedings - 2020 ACM/IEEE 42nd International Conference on Software Engineering: Companion, ICSE-Companion 2020},
year={2020},
pages={322-323},
doi={10.1145/3377812.3390793},
art_number={9270311},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098600029&doi=10.1145%2f3377812.3390793&partnerID=40&md5=0e67f2ea82418dfe4a7c6d6a55afc848},
affiliation={University of York, York, United Kingdom; MPI-SWS, Kaiserslautern, Germany; Bogazici University, Istanbul, Turkey},
abstract={Deep Learning (DL) systems are key enablers for engineering intelligent applications. Nevertheless, using DL systems in safety- A nd security-critical applications requires to provide testing evidence for their dependable operation. We introduce DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems and across multiple DL datasets demonstrates the usefulness and effectiveness of DeepImportance. © 2020 ACM.},
author_keywords={Deep Neural Networks;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Campos2020115,
author={Campos, J.R. and Costa, E.},
title={Fault injection to generate failure data for failure prediction: A case study},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2020},
volume={2020-October},
pages={115-126},
doi={10.1109/ISSRE5003.2020.00020},
art_number={09251077},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097350528&doi=10.1109%2fISSRE5003.2020.00020&partnerID=40&md5=f23ca52c87d68f7f014022862f21caaa},
affiliation={Cisuc, Department of Informatics Engineering, University of Coimbra, Portugal},
abstract={Due to the complexity of modern software, identifying every fault before deployment is extremely difficult or even not possible. Such residual faults can ultimately lead to failures, often incurring considerable risks or costs. Online Failure Prediction (OFP) is a fault-tolerance technique that attempts to predict the occurrence of failures in the near future and thus prevent/mitigate their consequences. Combined with recent technological developments, Machine Learning (ML) has been successfully used to create predictive models for OFP. However, as failures are rare events, failure data are often not available for building accurate models. Although fault injection has been accepted as a viable solution to generate realistic failure data, fault injectors are difficult to implement/update and thus research on Operating System (OS)-level OFP has become stale, with most works using data from outdated OSs. In this paper, we conduct a comprehensive fault injection campaign on an up-to-date Linux kernel and thoroughly study its behavior in the presence of faults. We then transform the data to explore and assess the predictive performance of various ML techniques for OFP. Finally, we study the influence of different OFP parameters (i.e., lead-time, prediction-window) and compare the results with existing related work. Results suggest that the various failures observed can be grouped into categories that can then be accurately predicted and distinguished by diverse ML models. ©2020 IEEE.},
author_keywords={Dependability;  Failure Prediction;  Fault Injection;  Machine Learning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Araujo2020133,
author={Araujo, F. and Medeiros, I. and Neves, N.},
title={Generating Tests for the Discovery of Security Flaws in Product Variants},
journal={Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2020},
year={2020},
pages={133-142},
doi={10.1109/ICSTW50294.2020.00033},
art_number={9155944},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091780013&doi=10.1109%2fICSTW50294.2020.00033&partnerID=40&md5=c9f5c85f23b2db39229286fe08b8fba3},
affiliation={Universidade de Lisboa, Lasige, Faculdade de Ciências, Portugal},
abstract={Industrial products, like vehicles and trains, integrate embedded systems implementing diverse and complicated functionalities. Such functionalities are programmable by software and contain a multitude of parameters necessary for their configuration, which have been increasing due to the market diversification and customer demand. In addition, industrial products are often built by aggregating different software parts (components), constituting thus product variants. Product variants with such variability need to be tested adequately, in particular if one is concerned with security vulnerabilities. While efficient automated testing approaches already exist, such as fuzzing, no tool is able to use results from previous testing campaigns to increase the efficiency of security testing the next product variant that shares certain functionalities. This paper presents an approach that can ignore already covered functionalities by previous tests and give more importance to blocks of code that have yet to be checked. The benefit is to avoid repeating unnecessary work, hence increasing the speed and the coverage in the new variant. The approach was implemented in a tool based on the AFL fuzzer and was validated with a set of programs of different versions. The experimental results show that the tool can perform better than AFL in our testing scenario. © 2020 IEEE.},
author_keywords={Coverage testing;  Fuzzing;  Software security;  Software Variant testing;  Vulnerability detection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Neto2020254,
author={Neto, F.G.D.O. and Dobslaw, F. and Feldt, R.},
title={Using mutation testing to measure behavioural test diversity},
journal={Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2020},
year={2020},
pages={254-263},
doi={10.1109/ICSTW50294.2020.00051},
art_number={9155915},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091771592&doi=10.1109%2fICSTW50294.2020.00051&partnerID=40&md5=b094ce3393248175eb84af9f7d92a962},
affiliation={Chalmers and the University of Gothenburg, Dept. of Computer Science and Engineering, Gothenburg, Sweden},
abstract={Diversity has been proposed as a key criterion to improve testing effectiveness and efficiency. It can be used to optimise large test repositories but also to visualise test maintenance issues and raise practitioners' awareness about waste in test artefacts and processes. Even though these diversitybased testing techniques aim to exercise diverse behavior in the system under test (SUT), the diversity has mainly been measured on and between artefacts (e.g., inputs, outputs or test scripts). Here, we introduce a family of measures to capture behavioural diversity (b-div) of test cases by comparing their executions and failure outcomes. Using failure information to capture the SUT behaviour has been shown to improve effectiveness of history-based test prioritisation approaches. However, historybased techniques require reliable test execution logs which are often not available or can be difficult to obtain due to flaky tests, scarcity of test executions, etc. To be generally applicable we instead propose to use mutation testing to measure behavioral diversity by running the set of test cases on various mutated versions of the SUT. Concretely, we propose two specific b-div measures (based on accuracy and Matthew's correlation coefficient, respectively) and compare them with artefact-based diversity (a-div) for prioritising the test suites of 6 different open-source projects. Our results show that our b-div measures outperform a-div and random selection in all of the studied projects. The improvement is substantial with an average increase in average percentage of faults detected (APFD) of between 19% to 31% depending on the size of the subset of prioritised tests. © 2020 IEEE.},
author_keywords={diversity-based testing;  empirical study;  test prioritisation;  test selection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dobslaw2020346,
author={Dobslaw, F. and De Oliveira Neto, F.G. and Feldt, R.},
title={Boundary Value Exploration for Software Analysis},
journal={Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2020},
year={2020},
pages={346-353},
doi={10.1109/ICSTW50294.2020.00062},
art_number={9155629},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091750702&doi=10.1109%2fICSTW50294.2020.00062&partnerID=40&md5=a03976487ab972efa77e3ab8eb762294},
affiliation={Chalmers and the University of Gothenburg, Dept. of Computer Science and Engineering, Gothenburg, Sweden},
abstract={For software to be reliable and resilient, it is widely accepted that tests must be created and maintained alongside the software itself. One safeguard from vulnerabilities and failures in code is to ensure correct behavior on the boundaries between subdomains of the input space. So-called boundary value analysis (BVA) and boundary value testing (BVT) techniques aim to exercise those boundaries and increase test effectiveness. However, the concepts of BVA and BVT themselves are not generally well defined, and it is not clear how to identify relevant sub-domains, and thus the boundaries delineating them, given a specification. This has limited adoption and hindered automation. We clarify BVA and BVT and introduce Boundary Value Exploration (BVE) to describe techniques that support them by helping to detect and identify boundary inputs. Additionally, we propose two concrete BVE techniques based on information-theoretic distance functions: (i) an algorithm for boundary detection and (ii) the usage of software visualization to explore the behavior of the software under test and identify its boundary behavior. As an initial evaluation, we apply these techniques on a much used and well-tested date handling library. Our results reveal questionable behavior at boundaries highlighted by our techniques. In conclusion, we argue that the boundary value exploration that our techniques enable is a step towards automated boundary value analysis and testing, which can foster their wider use and improve test effectiveness and efficiency. © 2020 IEEE.},
author_keywords={boundary value analysis;  boundary value testing;  test diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Leveau2020164,
author={Leveau, J. and Blanc, X. and Reveillere, L. and Falleri, J.-R. and Rouvoy, R.},
title={Fostering the Diversity of Exploratory Testing in Web Applications},
journal={Proceedings - 2020 IEEE 13th International Conference on Software Testing, Verification and Validation, ICST 2020},
year={2020},
pages={164-174},
doi={10.1109/ICST46399.2020.00026},
art_number={9159101},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091592915&doi=10.1109%2fICST46399.2020.00026&partnerID=40&md5=6099e2ba8863f8287db90adaa1c0fac0},
affiliation={Univ. Bordeaux, Bordeaux Inp, Cnrs, LaBRI, UMR5800, F-33400, Talence, France; Univ. Lille, Inria, France},
abstract={Exploratory testing (ET) is a software testing approach that complements automated testing by leveraging business expertise. It has gained momentum over the last decades as it appeals testers to exploit their business knowledge to stress the system under test (SUT). Exploratory tests, unlike automated tests, are defined and executed on-the-fly by testers. Testers who perform exploratory tests may be biased by their past experience and therefore may miss anomalies or unusual interactions proposed by the SUT. This is even more complex in the context of web applications, which typically expose a huge number of interaction paths to their users. As testers of these applications cannot remember all the sequences of interactions they performed, they may fail to deeply explore the application scope. This paper therefore introduces a new approach to assist testers in widely exploring any web application. In particular, our approach monitors the online interactions performed by the testers to suggest in real-time the probabilities of performing next interactions. Looking at these probabilities, we claim that the testers who favour interactions that have a low probability (because they were rarely performed), will increase the diversity of their explorations. Our approach defines a prediction model, based on n-grams, that encodes the history of past interactions and that supports the estimation of the probabilities. Integrated within a web browser extension, it automatically and transparently injects feedback within the application itself. We conduct a controlled experiment and a qualitative study to assess our approach. Results show that it prevents testers to be trapped in already tested loops, and succeeds to assist them in performing deeper explorations of the SUT. © 2020 IEEE.},
author_keywords={Exploratory test;  n-gram;  test;  web applications},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Trindade2020,
author={Trindade, B.C. and Gold, D.F. and Reed, P.M. and Zeff, H.B. and Characklis, G.W.},
title={Water pathways: An open source stochastic simulation system for integrated water supply portfolio management and infrastructure investment planning},
journal={Environmental Modelling and Software},
year={2020},
volume={132},
doi={10.1016/j.envsoft.2020.104772},
art_number={104772},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089746684&doi=10.1016%2fj.envsoft.2020.104772&partnerID=40&md5=867722d93193075b7f836de1a28adbd7},
affiliation={Department of Civil and Environmental Engineering, Cornell University, Ithaca, NY, United States; Center on Financial Risk in Environmental Systems, Gillings School of Global Public Health and UNC Institute for the Environment, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States},
abstract={Financial risk, access to capital, regulatory processes, and regional competition for limited water sources represent dominant concerns in the United States as well as the broader global water supply sector. This work introduces the WaterPaths simulation software: a generalizable, cloud-compatible, open-source exploratory modeling system designed to inform long-term regional investments in water infrastructure while simultaneously aiding regions to improve their short-term weekly management decisions, often made in response to droughts. Uniquely, WaterPaths has the capability to identify coordinated planning and management for groups of water utilities sharing water resources. WaterPaths’ exploits dynamic and adaptive risk-of-failure (ROF) rules to trigger management and planning actions in temporally consistent pathways. The compact and efficient ROF-based representation of decision pathways allows WaterPaths to scale efficiently with the number of regional actors and their candidate actions. Lastly, as a platform for supporting decision making under deep uncertainty, WaterPaths accounts for a broad range of uncertainties including hydrological or climate extremes, permitting time, demand growth, effectiveness of water-use restrictions, construction costs, and financing uncertainties. To demonstrate the capabilities of WaterPaths, we introduce a new hypothetical water resources test case, the Sedento Valley. The Sedento Valley test case contains three resource-sharing water utilities that seek to regionally coordinate their policies for drought mitigation and infrastructure investment. The three utilities are challenged by a diverse set of deep uncertainties that encompass natural and human systems stressors. The Sedento Valley test case contributes a new opportunity for benchmarking decision-support tools and water-resources systems simulation software. © 2020 Elsevier Ltd},
author_keywords={Decision making under deep uncertainty;  Infrastructure pathways;  Robustness;  Simulation;  Water supply},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2020,
author={Liu, Y. and Xu, H. and Tang, D. and Xu, F. and Mathews, J.P. and Hou, W. and Yan, X. and Ding, F.},
title={Coalbed methane production of a heterogeneous reservoir in the Ordos Basin, China},
journal={Journal of Natural Gas Science and Engineering},
year={2020},
volume={82},
doi={10.1016/j.jngse.2020.103502},
art_number={103502},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089225247&doi=10.1016%2fj.jngse.2020.103502&partnerID=40&md5=d8eb301aa29ab32c7756d81ac9c51bba},
affiliation={Key Laboratory of Groundwater Resources and Environment, Ministry of Education, Jilin University, Changchun, 130021, China; Jilin Provincial Key Laboratory of Water Resources and Environment, Jilin University, China; School of Energy Resources, China University of Geosciences (Beijing), Beijing, 100083, China; PetroChina Coalbed Methane Company Limited, Beijing, 100083, China; Leone Family Department of Energy and Mineral Engineering, The EMS Energy Institute, Pennsylvania State University, University ParkPA  16802, United States},
abstract={Macrolithotypes control the pore-fracture distribution heterogeneity in coal impacting gas adsorption, diffusion coefficient, and the gas flow. However, the behaviors that are impacted by coal macrolithotype are traditionally ignored and the drainage radius that linked to the macrolithotype contributions are lack of systematic research all the way. Thus, to evaluate the gas production relationship for macrolithotype, four blocks (bright, semi-bright, semi-dull, and dull) were obtained from the same seam. The methane absorption, diffusion, and permeability data were obtained and used in mathematical modeling to identify well production, drainage radius, and well-interference locations. Production data for 104 wells and 11 test wells were also obtained and with COMET3 simulation software to validate the mathematical model. From the bright to dull coal: there was a lower micropore contribution, a lower gas capacity, and lower gas diffusion coefficient. The cleat frequency and aperture differences impacted the reservoir permeability with bright coal having 195% higher than the dull coal. As these macrolithotype differences impact the coalbed methane (CBM) development and cause heterogeneous gas flow systems within the resource, a coupled mathematical model for comprehensive consideration of the adsorbent-diffusion-seepage was established capturing the macrolithotype contribution to the pressure propagation, gas flow, and gas drainage radius estimation. The bright coal had the higher Langmuir volume, CH4 diffusion coefficient, and permeability thus, producing a higher gas production volume, and a larger gas drainage area than the dull macrolithotype. Thus, the macrolithotype diversity and its stratification cause partitioning of the gas flow with multi-stage pressure drop effecting exploration and development. In addition, the well interference was also observed and show that the wells of bright and semi-bright coal have a continuous pressure drop, which achieving the purpose of overall pressure reduction. © 2020 Elsevier B.V.},
author_keywords={CBM;  Coal macrolithotype;  Gas drainage radius;  Gas flow;  Productivity characteristics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Temraz2020,
author={Temraz, A. and Alobaid, F. and Lanz, T. and Elweteedy, A. and Epple, B.},
title={Operational Flexibility of Two-Phase Flow Test Rig for Investigating the Dynamic Instabilities in Tube Boiling Systems},
journal={Frontiers in Energy Research},
year={2020},
volume={8},
doi={10.3389/fenrg.2020.517740},
art_number={517740},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091902364&doi=10.3389%2ffenrg.2020.517740&partnerID=40&md5=d0243b83e3f30da4706f8dd7003aee63},
affiliation={Mechanical Engineering Department, Institute for Energy Systems and Technology, Technical University of Darmstadt, Darmstadt, Germany; Mechanical Power and Energy Department, Military Technical College, Cairo, Egypt},
abstract={The safe operation of a two-phase heat exchanger can be performed by determining the instability threshold values of power plant parameters. Thus, the power plant parameters must be designed outside these thresholds to avoid undesirable instability. The fluctuations in mass flow and system pressure are undesirable processes, resulting in system failure. In diverse heat transfer distribution, that can lead to burn-out of heat exchanger tubes. Therefore, the maintaining of flow stability in a power plant is of particular relevance. The researchers and engineers can predict the threshold of flow instability with dynamic models validated with experimental records. The phenomenon of dynamic flow instabilities in two-phase flows is an important issue that has high relevance for many industries. The awareness about the triggers and the effects of fluid dynamic instabilities is of great importance in the design and operation of steam generators, refrigeration systems, thermosiphons or boiling water reactors. These instabilities manifest themselves in variations of mass flow, pressure, and fluid properties. In this work, the dynamic instabilities involved in evaporation processes were briefly discussed with simple models to understand the mechanism of its different types. Additionally, it is presented the design and construction of a two-phase flow test rig using the similarity and the scaling criteria. A heat recovery steam generator (HRSG) designed by Doosan Heavy Industries and Construction is scaled down to 4 × 4 × 2 m3. The combination of the flow diagram and the thermohydraulic design is represented in three-dimension model of the test rig using the Siemens NX 10.0 software. The flexibility of operation was taken into account in the design of this test rig. Finally, we provided preliminary results to showcase some functionalities and the capability of the test rig to characterize the existing flow pattern through the evaporator and investigate the internal characteristic curve of the evaporator. © Copyright © 2020 Temraz, Alobaid, Lanz, Elweteedy and Epple.},
author_keywords={density-wave oscillation;  dynamic instabilities;  natural circulation;  pressure-drop oscillation;  tube boiling;  two-phase flow;  vertical HRSG},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Derakhshanfar2020211,
author={Derakhshanfar, P. and Devroey, X. and Zaidman, A. and Van Deursen, A. and Panichella, A.},
title={Good Things Come in Threes: Improving Search-based Crash Reproduction with Helper Objectives},
journal={Proceedings - 2020 35th IEEE/ACM International Conference on Automated Software Engineering, ASE 2020},
year={2020},
pages={211-223},
doi={10.1145/3324884.3416643},
art_number={9285999},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099265606&doi=10.1145%2f3324884.3416643&partnerID=40&md5=e349033eb5d67d34857d7fbe5df0e67c},
affiliation={Delft University of Technology, Delft, Netherlands},
abstract={Writing a test case reproducing a reported software crash is a common practice to identify the root cause of an anomaly in the software under test. However, this task is usually labor-intensive and time-taking. Hence, evolutionary intelligence approaches have been successfully applied to assist developers during debugging by generating a test case reproducing reported crashes. These approaches use a single fitness function called Crash Distance to guide the search process toward reproducing a target crash. Despite the reported achievements, these approaches do not always successfully reproduce some crashes due to a lack of test diversity (premature convergence). In this study, we introduce a new approach, called MOHO, that addresses this issue via multi-objectivization. In particular, we introduce two new Helper-Objectives for crash reproduction, namely test length (to minimize) and method sequence diversity (to maximize), in addition to Crash Distance. We assessed MO-HO using five multi-objective evolutionary algorithms (NSGA-II, SPEA2, PESA-II, MOEA/D, FEMO) on 124 non-trivial crashes stemming from open-source projects. Our results indicate that SPEA2 is the best-performing multi-objective algorithm for MO-HO. We evaluated this best-performing algorithm for MO-HO against the state-of-the-art: single-objective approach (Single-Objective Search) and decomposition-based multi-objectivization approach (De-MO). Our results show that MO-HO reproduces five crashes that cannot be reproduced by the current state-of-the-art. Besides, MO-HO improves the effectiveness (+10% and +8% in reproduction ratio) and the efficiency in 34.6% and 36% of crashes (i.e., significantly lower running time) compared to Single-Objective Search and De-MO, respectively. For some crashes, the improvements are very large, being up to +93.3% for reproduction ratio and -92% for the required running time. © 2020 ACM.},
author_keywords={crash reproduction;  multi-objective evolutionary algorithms;  search-based software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Matolak2020,
author={Matolak, D.W. and Guvenc, I. and Mehrpouyan, H. and Carr, G.},
title={Hyper-Spectral Communications and Networking for ATM: Results and Prospective Future},
journal={Integrated Communications, Navigation and Surveillance Conference, ICNS},
year={2020},
volume={2020-September},
doi={10.1109/ICNS50378.2020.9222856},
art_number={9222856},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094899800&doi=10.1109%2fICNS50378.2020.9222856&partnerID=40&md5=95cac249e25556e222677f4d71dd7df9},
affiliation={University of South Carolina, Columbia, SC, United States; North Carolina State University, Raleigh, NC, United States; Boise State University, Boise, ID, United States; Architecture Technology Corporation, Campbell, CA, United States},
abstract={Over the past two years we have worked on a project for NASA's Aeronautics Research Mission Directorate (ARMD) University Leadership Initiative (ULI) program. Our project is entitled Hyper-Spectral Communications, Networking and ATM as Foundation for Safe and Efficient Future Flight: Transcending Aviation Operational Limitations with Diverse and Secure Multi-Band, Multi-Mode, and mmWave Wireless links. For brevity we abbreviate this title HSCNA. The four-institution HSCNA project is the only ULI program to address communications and networking, and thus far has been extremely productive: we have published 10 journal papers, 54 conference papers, 2 book chapters, and multiple technical reports, with another 10-20 papers in review, and 2 patent applications. In addition to publications we are developing a dual-band radio system for flight testing in the 2020 Boeing Eco-Demonstrator program, have developed systems for assessing wideband short-range millimeter wave (mmWave) airport radio links, and systems for detection of unauthorized unmanned aircraft systems (UAS). We have also developed a future Concept of Operations (ConOps) document and are developing a simulation tool to assess gains of our HSCNA technologies when used in the National Airspace System (NAS). In this paper we summarize our project and provide example results and findings. We first provide a short overview of the ULI program and its goals within the ARMD Strategic Implementation Plan. We then describe our project's six primary tasks, which are specifically, (i) the ConOps development; (ii) a comprehensive categorization and evaluation of current and planned communications technologies that can be used for aviation, across frequency spectrum spanning five orders of magnitude (e.g., 3 MHz HF through 100 GHz), including evaluation of performance gaps; (iii) design, development, and proof-of-concept testing of a multi-band aviation communication system; (iv) evaluation of mmWave frequency bands and technologies for use in advanced airport communication applications; (v) evaluation of RF detection of unauthorized UAS via several techniques; and, (vi) development of a simulation system to enable exploration of potential gains of these HSCNA technologies in ATM. The example results we provide include analyses, computer simulations, laboratory experiments, and field testing. We also describe plans for the final phase of our project, and discuss impacts and future work. © 2020 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{KhajehArzani2020,
author={Khajeh Arzani, H. and Kabiri Ataabadi, A. and Chaparian, Y.},
title={Experimental and numerical assessment of impact resistance of FMLs including one rubber layer},
journal={Journal of the Brazilian Society of Mechanical Sciences and Engineering},
year={2020},
volume={42},
number={9},
doi={10.1007/s40430-020-02520-1},
art_number={455},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089231559&doi=10.1007%2fs40430-020-02520-1&partnerID=40&md5=6cc63ced5e268abab1aabe8dc663d701},
affiliation={Department of Mechanical and Aerospace Engineering, Malek Ashtar University of Technology, Isfahan, Iran; Department of Mechanical Engineering, Malek Ashtar University of Technology, Isfahan, 83145/115, Iran},
abstract={Fiber metal laminates have lately attracted considerable interest due to their diverse use in industrial sectors. Indeed, FMLs are virtually applied by combining both the metal and composite layers in order to achieve better outcomes. The preference of FML over metal or composite has been verified in many studies. The preliminary objective of the present study, however, is to make attempts to evaluate the influence of rubber layer on FML impact resistance once subjected to high-velocity impact. To this end, a numerical analysis is administered following an experiment. In this regard, the applied materials consist of 2024-T3 aluminum alloy, woven glass/epoxy prepreg and nitrile butadiene rubber. The steel projectile for impact tests was geometrically cylinder and flat-ended. It is noteworthy that all tests were run by a high-speed gas gun, and the numerical analysis was carried out in LS-DYNA software. Ultimately, the obtained results indicated that by inserting a rubber layer among the laminates, aluminum layer can bend further, and more kinetic energy can be dissipated from the projectile. Moreover, the special perforation energy and ballistic limit velocity, V50, increased dramatically. © 2020, The Brazilian Society of Mechanical Sciences and Engineering.},
author_keywords={Ballistic limit velocity;  Fiber metal laminates;  LS-DYNA;  Special perforation energy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Truong2020,
author={Truong, A.N. and Pham, C.H. and Hancock, G.},
title={Power-Actuated Fasteners Connecting High-Strength Steel Plate to Mild Steel Plate under Monotonic Shear Loading},
journal={Journal of Structural Engineering (United States)},
year={2020},
volume={146},
number={9},
doi={10.1061/(ASCE)ST.1943-541X.0002785},
art_number={04020193},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087648996&doi=10.1061%2f%28ASCE%29ST.1943-541X.0002785&partnerID=40&md5=1a27f99b8826ac9cef3928445ac8ffb3},
affiliation={School of Civil Engineering, Univ. of Sydney, Sydney, NSW  2006, Australia},
abstract={This research proposes an innovative method utilizing power-actuated fasteners (PAFs) for the shear connection joining cold-formed steel (CFS) plates to hot-rolled steel (HRS) plates. An experimental program comprising 66 single-lap-joint shear strength tests was conducted to investigate the behavior of the connectors and connections under quasi-static monotonic loading. The parameters studied were the thickness of the CFS sheet, the strength of the base material (i.e., HRS), the diameter of the fastener, and the manufacturers of the fasteners. Fasteners from two different companies, Hilti and Ramset, were used in the program to ensure diversity. Three failure modes were captured including (1) bearing of CFS sheet, (2) shear fracture of the fastener, and (3) pullout by shear from the HRS sheet. The test results are compared with the predictions of the equations the Australian/New Zealand Standard for cold-formed steel structures or the North American Specification for cold-formed steel structural members. It is revealed that the equation for the bearing capacity results in unconservative predictions for the specimens tested. Meanwhile, the predictions of shear fracture and pullout by shear underestimate the actual strength of the connections. © 2020 American Society of Civil Engineers.},
author_keywords={Cold-formed steel;  Powder-actuated fastener;  Shear connections;  Shear strength},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rau2020,
author={Rau, N.M. and Hasan, K. and Ahamed, S.I. and Asan, O. and Flynn, K.E. and Basir, M.A.},
title={Designing a tablet-based prematurity education app for parents hospitalized for preterm birth},
journal={International Journal of Medical Informatics},
year={2020},
volume={141},
doi={10.1016/j.ijmedinf.2020.104200},
art_number={104200},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086509997&doi=10.1016%2fj.ijmedinf.2020.104200&partnerID=40&md5=b221985fd63adc9f7326a4d733bd6965},
affiliation={Medical College of Wisconsin, Department of Pediatrics, 999 N 92ndSt, Suite C410, PO Box 1997, Wauwatosa, WI  53226, United States; Marquette University, Department of Computer Science, Katharine R. Cudahy Hall, Room 201 1313 W. Wisconsin Avenue, Milwaukee, WI  53233, United States; Medical College of Wisconsin, Department of Medicine, 9200 W. Wisconsin Ave, Suite C5500, Milwaukee, WI  53226, United States},
abstract={Background: As technology has advanced over the last decade, handheld Mobile Health (mHealth) applications have increased in popularity. Pregnancy is one area of mHealth that has rapidly expanded, however very few pregnancy apps are developed in collaboration with health professionals. This creates an environment where the pregnancy information women are accessing may be inaccurate or even dangerous. Additionally, there are relatively few medical apps devoted to prematurity or targeted to women at risk for premature birth. To address the gap in premature birth education, we assembled a multidisciplinary team, including health care professionals, and developed the Preemie Prep for Parents (P3) app. Methods: Our team previously conducted 5 focus group meetings to assess the information needs of our target audience. Based on this information we developed a low fidelity P3 prototype. Our software development team transferred the low fidelity prototype into a high fidelity prototype which was hosted on Test Flight (a beta testing platform). We performed heuristic evaluation as well as user testing to improve the P3 app. Results: User testing of the high fidelity P3 prototype was performed with 13 diverse participants. 6 participants were parents of currently admitted Neonatal Intensive Care Unit (NICU) babies and 7 participants were women who had been or were currently pregnant. The native language of participants included English, Spanish, and Hmong and their educational level varied between completing high school and graduate degree. Participants provided feedback on the content of the P3 app, as well as its organization and aesthetics. The feedback led to 83 iterations of the P3 app prior to its deployment. Overall, participants noted that the information was “informative” and “reliable”. They also noted that the P3 app provided control over the information they could view and when they viewed it, stating “I could see info on my time”. Overall, participants felt that the P3 app was a valuable tool for mothers in preterm labor and it would help them ask questions. Conclusions: Development of a mHealth app provides unique challenges regarding content, reliability of information, organization, and aesthetics. Creation of the P3 app to address the educational needs of women at risk for premature birth required assembling a multidisciplinary team, which included target users, and implementing an iterative design process. The efficacy of this app in improving user knowledge and decreasing anxiety is currently being tested in a randomized controlled trial. © 2020 Elsevier B.V.},
author_keywords={Mobile applications;  Mobile health;  Patient education;  Pregnancy;  Premature birth;  Telemedicine},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pannu2020,
author={Pannu, P. and Sharma, D.K.},
title={A low-profile quad-port UWB MIMO antenna using defected ground structure with dual notch-band behavior},
journal={International Journal of RF and Microwave Computer-Aided Engineering},
year={2020},
volume={30},
number={9},
doi={10.1002/mmce.22288},
art_number={e22288},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085553010&doi=10.1002%2fmmce.22288&partnerID=40&md5=17c4ab95e7aa5c26d60fb698b2252339},
affiliation={Department of Electronics and Communication Engineering, SRM Institute of Science and Technology, Delhi-NCR campus, Ghaziabad, India},
abstract={A compact (45 × 45 × 1.6 mm3) ultrawide-band (UWB), multiple-input multiple-output (MIMO) design using microstrip line feeding is presented. The proposed design comprises four elliptical monopoles placed orthogonally on a cost-effective FR-4 substrate. In order to improve the impedance bandwidth and lessen the return loss of the MIMO antenna, defects in ground plane are created by etching symmetrical square slots and half-rings. Moreover, a different method (of unsymmetrical H-shaped slot with C-shaped slot) was proposed into the patch to introduce dual-band rejection performance from UWB at center frequency 5.5 GHz (covering lower WLAN as well as upper WLAN) and 7.5 GHz (X band). In addition, a stub is introduced at the edge of each defected ground structure to obtain isolation &gt;–22 dB covering entire performing band from 2 to 16.8 GHz (where, S11 &lt; –10 dB). The proposed design has miniaturized size, very low envelop correlation coefficient less than 0.1, stable gain (2-4 dBi except for notch bands). Furthermore, various MIMO performance parameters are within their specifications, such as diversity gain (= 10 dB), total active reflection coefficient (&lt;–5 dB, and channel capacity loss (&lt;0.35 bits/s/Hz). The presented design is optimized using the HFSS software, and fabricated design is tested using vector network analyzer. The experimental results are in good agreement with the simulation results. © 2020 Wiley Periodicals, Inc.},
author_keywords={DGS;  elliptical;  MIMO;  notched band;  UWB},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fernandes2020,
author={Fernandes, E.L. and Rojas, E. and Alvarez-Horcajo, J. and Kis, Z.L. and Sanvito, D. and Bonelli, N. and Cascone, C. and Rothenberg, C.E.},
title={The road to BOFUSS: The basic OpenFlow userspace software switch},
journal={Journal of Network and Computer Applications},
year={2020},
volume={165},
doi={10.1016/j.jnca.2020.102685},
art_number={102685},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084668323&doi=10.1016%2fj.jnca.2020.102685&partnerID=40&md5=7a3da1980ea602aab498f9f2b0fae75b},
affiliation={Queen Mary University of London, United Kingdom; University of Alcala, Spain; Ericsson, Hungary; Politecnico di Milano, Italy; University of Pisa, Italy; Open Networking Foundation, United States; INTRIG, University of Campinas (UNICAMP), Brazil},
abstract={Software switches are pivotal in the Software-Defined Networking (SDN) paradigm, particularly in the early phases of development, deployment and testing. Currently, the most popular one is Open vSwitch (OVS), leveraged in many production-based environments. However, due to its kernel-based nature, OVS is typically complex to modify when additional features or adaptation is required. To this regard, a simpler user-space is key to perform these modifications. In this article, we present a rich overview of BOFUSS, the basic OpenFlow user-space software switch. BOFUSS has been widely used in the research community for diverse reasons, but it lacked a proper reference document. For this purpose, we describe the switch, its history, architecture, uses cases and evaluation, together with a survey of works that leverage this switch. The main goal is to provide a comprehensive overview of the switch and its characteristics. Although the original BOFUSS is not expected to surpass the high performance of OVS, it is a useful complementary artefact that provides some OpenFlow features missing in OVS and it can be easily modified for extended functionality. Moreover, enhancements provided by the BEBA project brought the performance from BOFUSS close to OVS. In any case, this paper sheds light to researchers looking for the trade-offs between performance and customization of BOFUSS. © 2020 Elsevier Ltd},
author_keywords={Data plane programmability;  Open source;  OpenFlow;  Software switches;  Software-defined networking},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Kalysch2020,
author={Kalysch, A. and Deutel, M. and Müller, T.},
title={Template-based Android inter process communication fuzzing},
journal={ACM International Conference Proceeding Series},
year={2020},
doi={10.1145/3407023.3407052},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117541061&doi=10.1145%2f3407023.3407052&partnerID=40&md5=5b1f2be548aeb5577ccfcbd2db6babd7},
affiliation={Friedrich-Alexander University, Erlangen-Nürnberg (FAU), Germany},
abstract={Fuzzing is a test method in vulnerability assessments that calls the interfaces of a program in order to find bugs in its input processing. Automatically generated inputs, based on a set of templates and randomness, are sent to a program at a high rate, collecting crashes for later investigation. We apply fuzz testing to the inter process communication (IPC) on Android in order to find bugs in the mechanisms how Android apps communicate with each other. The sandboxing principle on Android usually ensures that apps can only communicate to other apps via programmatic interfaces. Unlike traditional operating systems, two Android apps running in the same user context are not able to access the data of each other (security) or quit the other app (safety). Our IPC fuzzer for Android detects the structure of data sent within Intents between apps by disassembling and analyzing an app's bytecode. It relies on multiple mutation engines for input generation and supports post-mortem analysis for a detailed insight into crashes. We tested 1488 popular apps from the Google Play-Store, enabling us to crash 450 apps with intents that could be sent from any unprivileged app on the same device, thus undermining the safety guarantees given by Android. We show that any installed app on a device could easily crash a series of other apps, effectively rendering them useless. Even worse, we discovered flaws in popular frameworks like Unity, the Google Services API, and the Adjust SDK. Comparing our implementation to previous research shows improvements in the depth and diversity of our detected crashes. © 2020 ACM.},
author_keywords={Android security;  Fuzzing;  Inter-process communication},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jahan2020170,
author={Jahan, S. and Riley, I. and Walter, C. and Gamble, R.F.},
title={Extending Context Awareness by Anticipating Uncertainty with Enki and Darjeeling},
journal={Proceedings - 2020 IEEE International Conference on Autonomic Computing and Self-Organizing Systems Companion, ACSOS-C 2020},
year={2020},
pages={170-175},
doi={10.1109/ACSOS-C51401.2020.00051},
art_number={9196337},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092725808&doi=10.1109%2fACSOS-C51401.2020.00051&partnerID=40&md5=62dc7ca85ee3287b3986af2ceb770f8e},
affiliation={University of Tulsa, Tandy School of Computer Science, Tulsa, OK, United States; University of Mississippi, Department of Computer and Information Science, University, MS, United States},
abstract={A self-adaptive system (SAS) requires automated planning that alters its behavior to properly operate in dynamic environments. To select a successful adaptation, the SAS must be context aware, which includes knowledge about a system's internal and environmental conditions, strategies to monitor conditions, and the capability to reason over an adaptation's relevance to its current conditions. Operational and environmental conditions are subject to foreseeable sources of uncertainty. Processes should be embedded in the SAS that generate data across a diverse set of conditions to investigate such sources and anticipate their conditions. Enki is a technology that applies a genetic algorithm to generate scenarios with diverse conditions. These scenarios should be further investigated to configure adaptations that address unexpected system behavior and failures. Darjeeling, an automated program repair tool can accept generated scenarios as input and apply genetic programming to generate patches from failed tests. Our prior work created a framework to evaluate patches by assessing their risk of requirements violation and their degree of security compliance confidence. In this paper, we incorporate these third-party tools, Enki and Darjeeling, into our framework that employs a MAPE-K loop of a previous assessed example system to extend its context awareness and increases automated capabilities. © 2020 IEEE.},
author_keywords={context awareness;  self-adaptive systems;  uncertainty},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Augusto202081,
author={Augusto, C. and Olivero, M.A. and Moran, J. and Morales, L. and De La Riva, C. and Aroba, J. and Tuya, J.},
title={Test-Driven Anonymization in Health Data: A Case Study on Assistive Reproduction},
journal={Proceedings - 2020 IEEE International Conference on Artificial Intelligence Testing, AITest 2020},
year={2020},
pages={81-82},
doi={10.1109/AITEST49225.2020.00019},
art_number={9176766},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092265739&doi=10.1109%2fAITEST49225.2020.00019&partnerID=40&md5=30a605f9656c768fda53ac25b517380f},
affiliation={University of Oviedo, Department of Computing, Gijón, Spain; ISTI-CNR-Pisa, Italy IWT2-University of Seville, Sevilla, Spain; Web Engineering and Early Testing Research Group, University of Seville, Sevilla, Spain; University of Huelva, Department of Information Technologies, Huelva, Spain},
abstract={Artificial intelligence (AI) is a broad field whose prevalence in the health sector has increased during recent years. Clinical data are the basic staple that feeds intelligent healthcare applications, but due to its sensitive character, its sharing and usage by third parties require compliance with both confidentiality agreements and security measures. Data Anonymization emerges as a solution to both increasing the data privacy and reducing the risk against unintentional disclosure of sensitive information through data modifications. Although the anonymization improves privacy, the diverse modifications also harm the data functional suitability. These data modifications can affect applications that employ the anonymized data, especially those that are data-centric such as the AI tools. To obtain a trade-off between both qualities (privacy and functional suitability), we use the Test-Driven Anonymization (TDA) approach, which anonymizes incrementally the data to train the AI tools and validates with the real data until maximizing its quality. The approach is evaluated on a real-world dataset from the Spanish Institute for the Study of the Biology of Human Reproduction (INEBIR). The anonymized datasets are used to train AI tools and select the dataset that gets the best trade-off between privacy and functional quality requirements. The results show that TDA can be successfully applied to anonymize the clinical data of the INEBIR, allowing third parties to transfer without transgressing user privacy and develop useful AI Tools with the anonymized data. © 2020 IEEE.},
author_keywords={Anonymization;  Artificial intelligence;  Health-Care Data;  k-Anonymity;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2020545,
author={Guo, C. and He, T. and Yuan, W. and Guo, Y. and Hao, R.},
title={Crowdsourced requirements generation for automatic testing via knowledge graph},
journal={ISSTA 2020 - Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2020},
pages={545-548},
doi={10.1145/3395363.3404363},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088925039&doi=10.1145%2f3395363.3404363&partnerID=40&md5=21c71e0a84a6a42439c56fc59939b594},
affiliation={Nanjing University, China},
abstract={Crowdsourced testing provides an effective way to deal with the problem of Android system fragmentation, as well as the application scenario diversity faced by Android testing. The generation of test requirements is a significant part of crowdsourced testing. However, manually generating crowdsourced testing requirements is tedious, which requires the issuers to have the domain knowledge of the Android application under test. To solve these problems, we have developed a tool named KARA, short for Knowledge Graph Aided Crowdsourced Requirements Generation for Android Testing. KARA first analyzes the result of automatic testing on the Android application, through which the operation sequences can be obtained. Then, the knowledge graph of the target application is constructed in a manner of pay-as-you-go. Finally, KARA utilizes knowledge graph and the automatic testing result to generate crowdsourced testing requirements with domain knowledge. Experiments prove that the test requirements generated by KARA are well understandable, and KARA can improve the quality of crowdsourced testing. The demo video can be found at https://youtu.be/kE-dOiekWWM. © 2020 ACM.},
author_keywords={Android GUI Testing;  Crowdsourced Requirements;  Knowledge Graph},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cowgill2020679,
author={Cowgill, B. and Dell'acqua, F. and Deng, S. and Hsu, D. and Verma, N. and Chaintreau, A.},
title={Biased Programmers? or Biased Data? A Field Experiment in Operationalizing AI Ethics},
journal={EC 2020 - Proceedings of the 21st ACM Conference on Economics and Computation},
year={2020},
pages={679-681},
doi={10.1145/3391403.3399545},
art_number={3399545},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089283815&doi=10.1145%2f3391403.3399545&partnerID=40&md5=3d498e25540c24f42946ea9fc17594fc},
affiliation={Columbia University, New York, United States},
abstract={Why do biased algorithmic predictions arise, and what interventions can prevent them? We examine this topic with a field experiment about using machine learning to predict human capital. We randomly assign approximately 400 AI engineers to develop software under different experimental conditions to predict standardized test scores of OECD residents. We then assess the resulting predictive algorithms using the realized test performances, and through randomized audit-like manipulations of algorithmic inputs. We also used the diversity of our subject population to measure whether demographically non-traditional engineers were more likely to notice and reduce algorithmic bias, and whether algorithmic prediction errors are correlated within programmer demographic groups. This document describes our experimental design and motivation; the full results of our experiment are available at https://ssrn.com/abstract=3615404. © 2020 Owner/Author.},
author_keywords={algorithmic fairness;  field experiment},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Derakhshanfar2020309,
author={Derakhshanfar, P. and Devroey, X. and Zaidman, A. and Van Deursen, A. and Panichella, A.},
title={Crash reproduction using helper objectives},
journal={GECCO 2020 Companion - Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
year={2020},
pages={309-310},
doi={10.1145/3377929.3390077},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089754350&doi=10.1145%2f3377929.3390077&partnerID=40&md5=4fa0e4195fcbef7f432ee8b872cabe26},
affiliation={Delft University of Technology, Delft, Netherlands},
abstract={Evolutionary-based crash reproduction techniques aid developers in their debugging practices by generating a test case that reproduces a crash given its stack trace. In these techniques, the search process is typically guided by a single search objective called Crash Distance. Previous studies have shown that current approaches could only reproduce a limited number of crashes due to a lack of diversity in the population during the search. In this study, we address this issue by applying Multi-Objectivization using Helper-Objectives (MO-HO) on crash reproduction. In particular, we add two helper-objectives to the Crash Distance to improve the diversity of the generated test cases and consequently enhance the guidance of the search process. We assessed MO-HO against the single-objective crash reproduction. Our results show that MO-HO can reproduce two additional crashes that were not previously reproducible by the single-objective approach. © 2020 Owner/Author.},
author_keywords={Crash reproduction;  MOEA;  Search-based software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2020,
author={Li, C. and Sun, J. and Palade, V.},
title={Diversity-guided Lamarckian random drift particle swarm optimization for flexible ligand docking},
journal={BMC Bioinformatics},
year={2020},
volume={21},
number={1},
doi={10.1186/s12859-020-03630-2},
art_number={286},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087663189&doi=10.1186%2fs12859-020-03630-2&partnerID=40&md5=21e4fc485aa48eebb8ef62e1365ee63f},
affiliation={Key Laboratory of Advanced Process Control for Light Industry (Ministry of Education), No. 1800, Lihu Avenue, Wuxi, Jiangsu, 214122, China; Faculty of Engineering and Computing, Coventry University, Priory Street, Coventry, CV1 5FB, United Kingdom},
abstract={Background: Protein-ligand docking has emerged as a particularly important tool in drug design and development, and flexible ligand docking is a widely used method for docking simulations. Many docking software packages can simulate flexible ligand docking, and among them, Autodock is widely used. Focusing on the search algorithm used in Autodock, many new optimization approaches have been proposed over the last few decades. However, despite the large number of alternatives, we are still lacking a search method with high robustness and high performance. Results: In this paper, in conjunction with the popular Autodock software, a novel hybrid version of the random drift particle swarm optimization (RDPSO) algorithm, called diversity-guided Lamarckian RDPSO (DGLRDPSO), is proposed to further enhance the performance and robustness of flexible ligand docking. In this algorithm, a novel two-phase diversity control (2PDC) strategy and an efficient local search strategy are used to improve the search ability and robustness of the RDPSO algorithm. By using the PDBbind coreset v.2016 and 24 complexes with apo-structures, the DGLRDPSO algorithm is compared with the Lamarckian genetic algorithm (LGA), Lamarckian particle swarm optimization (LPSO) and Lamarckian random drift particle swarm optimization (LRDPSO). The experimental results show that the 2PDC strategy is able to enhance the robustness and search performance of the proposed algorithm; for test cases with different numbers of torsions, the DGLRDPSO outperforms the LGA and LPSO in finding both low-energy and small-RMSD docking conformations with high robustness in most cases. Conclusion: The DGLRDPSO algorithm has good search performance and a high possibility of finding a conformation with both a low binding free energy and a small RMSD. Among all the tested algorithms, DGLRDPSO has the best robustness in solving both holo- and apo-structure docking problems with different numbers of torsions, which indicates that the proposed algorithm is a reliable choice for the flexible ligand docking in Autodock software. © 2020 The Author(s).},
author_keywords={Autodock software;  Diversity control strategy;  Flexible ligand docking;  Random drift particle swarm optimization;  Search algorithms;  Solis and Wets local search},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xu2020,
author={Xu, H. and Wang, Y. and Fan, S. and Xie, P. and Liu, A.},
title={DSmith: Compiler Fuzzing through Generative Deep Learning Model with Attention},
journal={Proceedings of the International Joint Conference on Neural Networks},
year={2020},
doi={10.1109/IJCNN48605.2020.9206911},
art_number={9206911},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093834400&doi=10.1109%2fIJCNN48605.2020.9206911&partnerID=40&md5=42b69bf257fa35fbbd9100dfe800758a},
affiliation={National University of Defense Technology, College of Computer, China; Academy of Military Sciences, Institute of War, China},
abstract={Compiler fuzzing is a technique to test the functionalities of compiler. It requires well-formed test cases (i.e., programs) that have correct lexicons and syntax to pass the parsing stage of a compiler. Recently, advanced compiler fuzzing methods generate effective test cases by deep neural networks, which learn the language model of regular programs to guarantee test case quality. However, most of these methods fail to capture long-distance dependencies of syntax (e.g., paired curly braces) in a program. As a result, they may generate test cases with syntax errors, which cannot pass the parsing stage to test the compiler functionality. In this paper, we propose a framework, namely DSmith, to capture long-distance dependencies of syntax for a robust test case generation. Specifically, DSmith memorizes the hidden state of each token in a program and leverages the interactions of these hidden states to embed the long-distance dependencies between tokens. It then adopts an encoder-decoder architecture with the embedding of these long-distance dependencies to build a language model of regular programs. Finally, DSmith uses the built language model to generate test cases according to four novel generation strategies, which significantly increase the diversity of test cases. Extensive experiments show that DSmith increases the parsing pass rate of the generated programs by an average of 19% and significantly improves the code coverage of the compiler, compared with state-of-the-art methods. Benefiting from the high pass rate and broad code coverage, DSmith has found eleven brand new bugs in currently supported GCC compiler versions. © 2020 IEEE.},
author_keywords={attention;  compiler;  fuzzing;  neural network;  syntax},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Malhotra2020332,
author={Malhotra, R. and Gupta, S., Shreyagupta Bt2k16@dtu.ac.in and Singh, T.},
title={A Systematic Review on Application of Deep Learning Techniques for Software Quality Predictive Modeling},
journal={2020 International Conference on Computational Performance Evaluation, ComPE 2020},
year={2020},
pages={332-337},
doi={10.1109/ComPE49325.2020.9200103},
art_number={9200103},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092737018&doi=10.1109%2fComPE49325.2020.9200103&partnerID=40&md5=67241530031411ee62a02fc0a9b08ab0},
affiliation={Delhi Technological University, Dept. of Computer Science and Engineering, Delhi, India},
abstract={Software quality prediction is the process of evaluating the software developed for various metrics like defect prediction, bug localisation, effort estimation etc. To evaluate these metrics a myriad of techniques have been developed in the literature, from manual assessment to application of machine learning and statistical testing. These methodologies, however, had lower accuracy in determining SQPMs due to their inability to model the complex relationships in the training data. With the wide emergence of deep learning, not only has the accuracy of the pre-existing models enhanced, but it has also opened doors for new metrics that could be evaluated and automated. This study performs a systematic literature review of research papers published from January 1990 to January 2019 that used deep learning to evaluate software quality prediction metrics (SQPM). The paper identifies 20 primary studies and 7 categories of application of deep learning in SQPM. Models using deep learning techniques significantly outperform other traditional methodologies in almost all studies. The concept and external threats to the models are limited, however the time taken to train these models is large. The techniques, currently predominantly applied for defect prediction, have shown promising results in other diverse software engineering fields like code search and effort estimation by modeling the source code efficiently. There is, hence, scope for incorporating deep learning further with pragmatic use and diverse application. The need to find scalable solutions, however, still persists. © 2020 IEEE.},
author_keywords={deep learning;  defect prediction;  effort estimation;  fault localisation;  software quality prediction metrics;  systematic literature review},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kazemi2020,
author={Kazemi, Z. and Fazeli, M. and Hely, D. and Beroulle, V.},
title={Hardware Security Vulnerability Assessment to Identify the Potential Risks in A Critical Embedded Application},
journal={Proceedings - 2020 26th IEEE International Symposium on On-Line Testing and Robust System Design, IOLTS 2020},
year={2020},
doi={10.1109/IOLTS50870.2020.9159739},
art_number={9159739},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091554891&doi=10.1109%2fIOLTS50870.2020.9159739&partnerID=40&md5=ff8af1947dc21faa66c36dc4ffce8cfd},
affiliation={Univ. Grenoble Alpes, Grenoble Inp Lcis, Valence, 26000, France; Bogazici University, Dep. of Computer Eng., Bebek Istanbul, Turkey},
abstract={Internet of Things (IoT) is experiencing significant growth in the safety-critical applications which have caused new security challenges. These devices are becoming targets for different types of physical attacks, which are exacerbated by their diversity and accessibility. Therefore, there is a strict necessity to support embedded software developers to identify and remediate the vulnerabilities and create resilient applications against such attacks. In this paper, we propose a hardware security vulnerability assessment based on fault injection of an embedded application. In our security assessment, we apply a fault injection attack by using our clock glitch generator on a critical medical IoT device. Furthermore, we analyze the potential risks of ignoring these attacks in this embedded application. The results will inform the embedded software developers of various security risks and the required steps to improve the security of similar MCU-based applications. Our hardware security assessment approach is easy to apply and can lead to secure embedded IoT applications against fault attacks. © 2020 IEEE.},
author_keywords={Detection and Mitigation;  Fault Injection Attacks;  Hardware Security;  Internet of Things (IoT)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kasatkin2020172,
author={Kasatkin, N.V. and Petrov, V.V. and Konikh, G.S.},
title={Zaramag HPP-1 on the River Ardon in the Republic of North Ossetia — Alania: Preoperational Program of Hydraulic Structure Safety Tests and its Implementation},
journal={Power Technology and Engineering},
year={2020},
volume={54},
number={2},
pages={172-177},
doi={10.1007/s10749-020-01186-y},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089955616&doi=10.1007%2fs10749-020-01186-y&partnerID=40&md5=44ba8fb1eda0be29381b6829d42fe58a},
affiliation={JSC “Lenhydroproject”, St. Petersburg, Russian Federation},
abstract={Zaramag HPP-1 is an extremely challenging project, unique in its design and in head. Prior to start-up, the hydraulic structures are subjected to a multistage testing program at different heads. The first test results are presented in this paper. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={daily regulation pond;  diversion tunnel;  penstocks;  test program},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhuge20203044,
author={Zhuge, Y. and Ning, H. and Mathen, P. and Cheng, J.Y. and Krauze, A.V. and Camphausen, K. and Miller, R.W.},
title={Automated glioma grading on conventional MRI images using deep convolutional neural networks},
journal={Medical Physics},
year={2020},
volume={47},
number={7},
pages={3044-3053},
doi={10.1002/mp.14168},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084489541&doi=10.1002%2fmp.14168&partnerID=40&md5=d5c207840daeb0221f2a7e061d30e773},
affiliation={Radiation Oncology Branch, National Cancer Institute, National Institutes of Health, Bethesda, MD  20892, United States; Division of Radiation Oncology and Developmental Radiotherapeutics, BC Cancer, Vancouver, BC, Canada},
abstract={Purpose: Gliomas are the most common primary tumor of the brain and are classified into grades I-IV of the World Health Organization (WHO), based on their invasively histological appearance. Gliomas grading plays an important role to determine the treatment plan and prognosis prediction. In this study we propose two novel methods for automatic, non-invasively distinguishing low-grade (Grades II and III) glioma (LGG) and high-grade (grade IV) glioma (HGG) on conventional MRI images by using deep convolutional neural networks (CNNs). Methods: All MRI images have been preprocessed first by rigid image registration and intensity inhomogeneity correction. Both proposed methods consist of two steps: (a) three-dimensional (3D) brain tumor segmentation based on a modification of the popular U-Net model; (b) tumor classification on segmented brain tumor. In the first method, the slice with largest area of tumor is determined and the state-of-the-art mask R-CNN model is employed for tumor grading. To improve the performance of the grading model, a two-dimensional (2D) data augmentation has been implemented to increase both the amount and the diversity of the training images. In the second method, denoted as 3DConvNet, a 3D volumetric CNNs is applied directly on bounding image regions of segmented tumor for classification, which can fully leverage the 3D spatial contextual information of volumetric image data. Results: The proposed schemes were evaluated on The Cancer Imaging Archive (TCIA) low grade glioma (LGG) data, and the Multimodal Brain Tumor Image Segmentation (BraTS) Benchmark 2018 training datasets with fivefold cross validation. All data are divided into training, validation, and test sets. Based on biopsy-proven ground truth, the performance metrics of sensitivity, specificity, and accuracy are measured on the test sets. The results are 0.935 (sensitivity), 0.972 (specificity), and 0.963 (accuracy) for the 2D Mask R-CNN based method, and 0.947 (sensitivity), 0.968 (specificity), and 0.971 (accuracy) for the 3DConvNet method, respectively. In regard to efficiency, for 3D brain tumor segmentation, the program takes around ten and a half hours for training with 300 epochs on BraTS 2018 dataset and takes only around 50 s for testing of a typical image with a size of 160 × 216 × 176. For 2D Mask R-CNN based tumor grading, the program takes around 4 h for training with around 60 000 iterations, and around 1 s for testing of a 2D slice image with size of 128 × 128. For 3DConvNet based tumor grading, the program takes around 2 h for training with 10 000 iterations, and 0.25 s for testing of a 3D cropped image with size of 64 × 64 × 64, using a DELL PRECISION Tower T7910, with two NVIDIA Titan Xp GPUs. Conclusions: Two effective glioma grading methods on conventional MRI images using deep convolutional neural networks have been developed. Our methods are fully automated without manual specification of region-of-interests and selection of slices for model training, which are common in traditional machine learning based brain tumor grading methods. This methodology may play a crucial role in selecting effective treatment options and survival predictions without the need for surgical biopsy. © 2020 American Association of Physicists in Medicine},
author_keywords={brain tumor;  convolutional neural networks;  deep learning;  Glioma grading;  MRI image},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abrath2020841,
author={Abrath, B. and Coppens, B. and Mishra, M. and Van Den Broeck, J. and De Sutter, B.},
title={Breakpad: Diversified binary crash reporting},
journal={IEEE Transactions on Dependable and Secure Computing},
year={2020},
volume={17},
number={4},
pages={841-856},
doi={10.1109/TDSC.2018.2823751},
art_number={8332984},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045223891&doi=10.1109%2fTDSC.2018.2823751&partnerID=40&md5=92715fe31e6a576c86dd23993b5c6da7},
affiliation={Computer Systems Lab, Ghent University, Ghent, Oost-Vlaanderen, Belgium; Electronics and Information Systems, Ghent University, Gent, Oost-Vlaanderen, Belgium},
abstract={This paper introduces \DeltaΔBreakpad. It extends the Breakpad crash reporting system to handle software diversity effectively and efficiently by replicating and patching the debug information of diversified software versions. Simple adaptations to existing open source compiler tools are presented that on the one hand introduce significant amounts of diversification in the code and stack layout of ARMv7 binaries to mitigate the widespread deployment of code injection and code reuse attacks, while on the other hand still supporting accurate crash reporting. An evaluation on SPEC2006 benchmarks demonstrates that the corresponding computational, storage, and communication overheads are small. © 2004-2012 IEEE.},
author_keywords={crash reporting;  software diversity;  Software security},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2020443,
author={Liu, Y. and Lin, J. and Cleland-Huang, J.},
title={Traceability Support for Multi-Lingual Software Projects},
journal={Proceedings - 2020 IEEE/ACM 17th International Conference on Mining Software Repositories, MSR 2020},
year={2020},
pages={443-454},
doi={10.1145/3379597.3387440},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093673754&doi=10.1145%2f3379597.3387440&partnerID=40&md5=5650523c11ffd3e68443cfefcaa28f78},
affiliation={University of Notre Dame, Notre Dame, United States},
abstract={Software traceability establishes associations between diverse software artifacts such as requirements, design, code, and test cases. Due to the non-trivial costs of manually creating and maintaining links, many researchers have proposed automated approaches based on information retrieval techniques. However, many globally distributed software projects produce software artifacts written in two or more languages. The use of intermingled languages reduces the efficacy of automated tracing solutions. In this paper, we first analyze and discuss patterns of intermingled language use across multiple projects, and then evaluate several different tracing algorithms including the Vector Space Model (VSM), Latent Semantic Indexing (LSI), Latent Dirichlet Allocation (LDA), and various models that combine mono-and cross-lingual word embeddings with the Generative Vector Space Model (GVSM). Based on an analysis of 14 Chinese-English projects, our results show that best performance is achieved using mono-lingual word embeddings integrated into GVSM with machine translation as a preprocessing step. © 2020 ACM.},
author_keywords={Cross-lingual information retrieval;  Generalized Vector Space Model;  Traceability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Haensel202045,
author={Haensel, J. and Adriano, C.M. and Dyck, J. and Giese, H.},
title={Collective risk minimization via a bayesian model for statistical software testing},
journal={Proceedings - 2020 IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems, SEAMS 2020},
year={2020},
pages={45-56},
doi={10.1145/3387939.3388616},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093107940&doi=10.1145%2f3387939.3388616&partnerID=40&md5=4a01e72760b4e56dd396f62802f93686},
abstract={In the last four years, the number of distinct autonomous vehicles platforms deployed in the streets of California increased 6-fold, while the reported accidents increased 12-fold. This can become a trend with no signs of subsiding as it is fueled by a constant stream of innovations in hardware sensors and machine learning software. Meanwhile, if we expect the public and regulators to trust the autonomous vehicle platforms, we need to find better ways to solve the problem of adding technological complexity without increasing the risk of accidents. We studied this problem from the perspective of reliability engineering in which a given risk of an accident has severity and probability of occurring. Timely information on accidents is important for engineers to anticipate and reuse previous failures to approximate the risk of accidents in a new city. However, this is challenging in the context of autonomous vehicles because of the sparse nature of data on the operational scenarios (driving trajectories in a new city). Our approach was to mitigate data sparsity by reducing the state space through monitoring of multiple-vehicles operations. We then minimized the risk of accidents by determining proper allocation of tests for each equivalence class. Our contributions comprise (1) a set of strategies to monitor the operational data of multiple autonomous vehicles, (2) a Bayesian model that estimates changes in the risk of accidents, and (3) a feedback control-loop that minimizes these risks by real-locating test effort. Our results are promising in the sense that we were able to measure and control risk for a diversity of changes in the operational scenarios. We evaluated our models with data from two real cities with distinct traffic patterns and made the data available for the community. © 2020 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reddy20201410,
author={Reddy, S. and Lemieux, C. and Padhye, R. and Sen, K.},
title={Quickly generating diverse valid test inputs with reinforcement learning},
journal={Proceedings - International Conference on Software Engineering},
year={2020},
pages={1410-1421},
doi={10.1145/3377811.3380399},
art_number={3380399},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094319403&doi=10.1145%2f3377811.3380399&partnerID=40&md5=f7b326704b29a4d2d39dfaf2650d66d4},
affiliation={University of California, Berkeley, United States},
abstract={Property-based testing is a popular approach for validating the logic of a program. An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver. However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs. Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver. However, collecting such information reduces the speed at which tests can be executed. In this paper, we propose and study a blackbox approach for generating valid test inputs.We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs. This formalization highlights the role of a guide which governs the space of choices within a random input generator. We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator. We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks. We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines. © 2020 Association for Computing Machinery.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{He2020961,
author={He, P. and Meister, C. and Su, Z.},
title={Structure-invariant testing for machine translation},
journal={Proceedings - International Conference on Software Engineering},
year={2020},
pages={961-973},
doi={10.1145/3377811.3380339},
art_number={3380339},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094138593&doi=10.1145%2f3377811.3380339&partnerID=40&md5=47779157e2a039c6ce6f5836ccf31bd9},
affiliation={Department of Computer Science, Eth Zurich, Switzerland},
abstract={In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored. To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of similar source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic. © 2020 Association for Computing Machinery.},
author_keywords={Machine translation;  Metamorphic testing;  Structural invariance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2020739,
author={Zhang, X. and Xie, X. and Ma, L. and Du, X. and Hu, Q. and Liu, Y. and Zhao, J. and Sun, M.},
title={Towards characterizing adversarial defects of deep learning software from the lens of uncertainty},
journal={Proceedings - International Conference on Software Engineering},
year={2020},
pages={739-751},
doi={10.1145/3377811.3380368},
art_number={3380368},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091918936&doi=10.1145%2f3377811.3380368&partnerID=40&md5=53b0054266a1e0b5957b3ef89234c823},
affiliation={Peking University, China, China; Nanyang Technological University, Singapore, Singapore; Kyushu University, Japan},
abstract={Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by our method is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software. © 2020 Association for Computing Machinery.},
author_keywords={Adversarial attack;  Deep learning;  Software testing;  Uncertainty},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Frunzaru20201987,
author={Frunzaru, V. and Corbu, N.},
title={Students’ attitudes towards knowledge and the future of work},
journal={Kybernetes},
year={2020},
volume={49},
number={7},
pages={1987-2002},
doi={10.1108/K-07-2019-0512},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079588061&doi=10.1108%2fK-07-2019-0512&partnerID=40&md5=e6f3a0cea2e3e0d86955c77e9f534082},
affiliation={College of Communication and Public Relations, National University of Political Studies and Public Administration, Bucharest, Romania},
abstract={Purpose: The purpose of this paper is to investigate to what extent secondary school students’ interest in intellectual development influences key abilities necessary to cope with the future of work. In the ever-changing world of work, deeply influenced by new technologies and cultural diversity in the workforce, young people must develop three essential traits to increase their capacity to quickly adapt to the situation in the labour market: openness to lifelong learning, critical thinking skills related to online information (of which online fact-checking is a key component) and openness to a multicultural society. In this paper, it is argued that these traits are directly related to young people’s interest in intellectual development but that additional interdependencies between these three traits complicate this equation. Design/methodology/approach: The authors conducted a survey of secondary school students in the 12th grade (N = 1221). A hypothesized conceptual model was tested with AMOS software for structural equation modelling. Findings: The findings show that students who are more interested in intellectual development are more open to lifelong learning. The relationship between intellectualism and lifelong learning is also mediated by online fact-checking. Moreover, the higher the interest in lifelong learning, the higher the openness to multiculturality. There is, however, no direct relationship between interest in intellectual development and multiculturality. Practical implications: The results of this study will help making recommendations to three key stakeholders: young people, teachers and policymakers. They could have a practical impact on the labour market in the future. Originality/value: This paper examines a topic that has not been systematically studied, namely, the possible influence of intellectualism on the future of work. The findings highlight the possible negative effects of a lack of interest in intellectual development on lifelong learning, living and working in a multicultural environment and processing online information. © 2020, Emerald Publishing Limited.},
author_keywords={Disinformation;  Fact-checking;  Intellectualism;  Knowledge economy;  Lifelong learning;  Multiculturality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Naith2020209,
author={Naith, Q. and Ciravegna, F.},
title={Definitive guidelines toward effective mobile devices crowdtesting methodology},
journal={International Journal of Crowd Science},
year={2020},
volume={4},
number={2},
pages={209-228},
doi={10.1108/IJCS-01-2020-0002},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123112790&doi=10.1108%2fIJCS-01-2020-0002&partnerID=40&md5=7b199fc83c1be99cd059ebaa9451bb2f},
affiliation={Department of Computer Science, University of Sheffield, Sheffield, United Kingdom},
abstract={Purpose: This paper aims to gauge developers’ perspectives regarding the participation of the public and anonymous crowd testers worldwide, with a range of varied experiences. It also aims to gather their needs that could reduce their concerns of dealing with the public crowd testers and increase the opportunity of using the crowdtesting platforms. Design/methodology/approach: An online exploratory survey was conducted to gather information from the participants, which included 50 mobile application developers from various countries with diverse experiences across Android and iOS mobile platforms. Findings: The findings revealed that a significant proportion (90%) of developers is potentially willing to perform testing via the public crowd testers worldwide. This on condition that several fundamental features were available, which enable them to achieve more realistic tests without artificial environments on large numbers of devices. The results also demonstrated that a group of developers does not consider testing as a serious job that they have to pay for, which can affect the gig-economy and global market. Originality/value: This paper provides new insights for future research in the study of how acceptable it is to work with public and anonymous crowd workers, with varying levels of experience, to perform tasks in different domains and not only in software testing. In addition, it will assist individual or small development teams who have limited resources or who do not have thousands of testers in their private testing community, to perform large-scale testing of their products. © 2020, Qamar Naith and Fabio Ciravegna.},
author_keywords={Compatibility testing;  Gig-economy;  Large-scale crowdtesting;  Mobile app testing;  Public and anonymous crowd testers},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Corteggiani2020294,
author={Corteggiani, N. and Francillon, A.},
title={HardSnap: Leveraging Hardware Snapshotting for Embedded Systems Security Testing},
journal={Proceedings - 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2020},
year={2020},
pages={294-305},
doi={10.1109/DSN48063.2020.00046},
art_number={9153377},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090414625&doi=10.1109%2fDSN48063.2020.00046&partnerID=40&md5=69a5280a3d46011011011429d02ccf3c},
affiliation={Eurecom, France},
abstract={Advanced dynamic analysis techniques such as fuzzing and Dynamic Symbolic Execution (DSE) are a cornerstone of software security testing and are becoming popular with embedded systems testing. Testing software in a virtual machine provides more visibility and control. VM snapshots also save testing time by facilitating crash reproduction, performing root cause analysis and avoiding re-executing programs from the start. However, because embedded systems are very diverse virtual machines that perfectly emulate them are often unavailable. Previous work therefore either attempt to model hardware or perform partial emulation (forwarding interaction to the real hardware), which leads to inaccurate or slow emulation. However, such limitations are unnecessary when the whole design is available, e.g., to the device manufacturer or on open hardware. In this paper, we therefore propose a novel approach, called HardSnap, for co-testing hardware and software with a high level of introspection. HardSnap aims at improving security testing of hardware/software co-designed systems, where embedded systems designers have access to the whole HW/SW stack. HardSnap is a virtual-machine-based solution that extends visibility and controllability to the hardware peripherals with a negligible overhead. HardSnap introduces the concept of a hardware snapshot that collects the hardware state (together with software state). In our prototype, Verilog hardware blocks are either simulated in software or synthesized to an FPGA. In both cases HardSnap is able to generate HW/SW snapshot on demand. HardSnap is designed to support new peripherals automatically, to have high performance, and full controllability and visibility on software and hardware. We evaluated HardSnap on open-source peripherals and synthetic firmware to demonstrate improved ability to find and diagnose security issues. © 2020 IEEE.},
author_keywords={Embedded Systems;  Hardware Snapshotting;  Security Analysis;  Symbolic Execution},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Aldababsa20201188,
author={Aldababsa, M. and Goztepe, C. and Kurt, G.K. and Kucur, O.},
title={Bit error rate for NOMA network},
journal={IEEE Communications Letters},
year={2020},
volume={24},
number={6},
pages={1188-1191},
doi={10.1109/LCOMM.2020.2981024},
art_number={9037237},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086479629&doi=10.1109%2fLCOMM.2020.2981024&partnerID=40&md5=cc4e02fbb3d32b21f053f55caf027023},
affiliation={Department of Electronics Engineering, Gebze Technical University, Gebze, 41400, Turkey; Department of Electronics and Communication Engineering, Istanbul Technical University, Istanbul, 34469, Turkey},
abstract={This letter examines the bit error rate (BER) performance of downlink non-orthogonal multiple access networks for binary phase-shift keying modulation. Exact BER expression is derived for each user in closed-form under additive white Gaussian noise and Rayleigh fading channels in perfect and imperfect successive interference cancellation (SIC) cases. Next, in perfect SIC case, the asymptotic BER expression in a high signal-to-noise ratio (SNR) region is obtained to express the behavior of the network with diversity and array gains. On the other hand, in imperfect SIC case, the upper bound for BER is attained, and at high SNR values, the BER reveals an error floor, and hence a zero diversity gain is achieved. Then, a feasible range of power allocation coefficients is found such that a good BER performance can be provided for each user. Finally, through simulations and software-defined radio-based real-time tests, analytical results are validated. © 1997-2012 IEEE.},
author_keywords={binary phase shift keying;  Bit error rate;  non-orthogonal multiple access;  power allocation;  successive interference cancellation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ramírez2020113,
author={Ramírez, A. and Delgado-Pérez, P. and Ferrer, J. and Romero, J.R. and Medina-Bulo, I. and Chicano, F.},
title={A systematic literature review of the SBSE research community in Spain},
journal={Progress in Artificial Intelligence},
year={2020},
volume={9},
number={2},
pages={113-128},
doi={10.1007/s13748-020-00205-3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084053136&doi=10.1007%2fs13748-020-00205-3&partnerID=40&md5=c2cd94da28fa817ea84077344bd35124},
affiliation={Dpto. Informática y Análisis Numérico, University of Córdoba, Córdoba, Spain; Dpto. Ingeniería Informática, University of Cádiz, Cádiz, Spain; Dpto. Lenguajes y Sistemas Informáticos, University of Málaga, Málaga, Spain},
abstract={Since its appearance in 2001, search-based software engineering has allowed software engineers to use optimisation techniques to automate distinctive human problems related to software management and development. The scientific community in Spain has not been alien to these advances. Their contributions cover both the optimisation of software engineering tasks and the proposal of new search algorithms. This review compiles the research efforts of this community in the area. With this aim, we propose a protocol to describe the review process, including the search sources, inclusion and exclusion criteria of candidate papers, the data extraction procedure and the categorisation of primary studies. After retrieving more than 3700 papers, 232 primary studies have been selected, whose analysis gives a precise picture of the current research state of the community, trends and future challenges. With 145 authors from 19 distinct institutions, results show that a diversity of tasks, including software planning, requirements, design and testing, and a large variety of techniques has been used, from exact search to evolutionary computation and swarm intelligence. Further, since 2015, specific scientific events have helped to bring together the community, improving collaborations, financial funding and internationalisation. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Research trends;  Search-based software engineering;  Spanish community;  Systematic review},
document_type={Review},
source={Scopus},
}

@ARTICLE{Xie2020,
author={Xie, X. and Zhang, H. and Khan, S.A. and Gao, M. and Lin, Y.},
title={A Movable Electrode Triboelectric Nanogenerator Fabricated Using a Pencil Lead for Self-Powered Locating Collision},
journal={Advanced Engineering Materials},
year={2020},
volume={22},
number={6},
doi={10.1002/adem.202000109},
art_number={2000109},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081714732&doi=10.1002%2fadem.202000109&partnerID=40&md5=63eedc01f97d52bc48c8dbb82f84ee16},
affiliation={State Key Laboratory of Electronic Thin Films and Integrated Devices, University of Electronic Science and Technology of China, Chengdu, 610054, China; College of Information and Computer, Taiyuan University of Technology, Taiyuan, 030024, China; Department of Electrical Engineering, Sukkur IBA University, Sukkur, 65200, Pakistan},
abstract={The configurations of triboelectric nanogenerators (TENGs) are usually determined by their specific applications. The immobility of triboelectrodes limits the TENGs’ possible application in dynamic sensing. A subtle novel TENG composed of a movable pencil lead electrode is reported herein. The electricity can be extracted from the pencil lead, by touching an arch-shaped unit, irrespective of whether the touched surface is the polymer layer or metal foil. It is found that the TENG device can deliver the maximal voltage of 2.37 V at the vibration frequency of 8 Hz. Moreover, due to the high sensitivity to collision/pressure, a sensor array system made of four TENG units is constructed to demonstrate the potential locating application. With the assistance of a self-made testing program, the TENG array illustrates the capacity of self-powered locating collisions that is independent of external power sources. Herein, not only TENG-based devices’ diversity is promoted but also a new active locating route that greatly boosts the development of intelligent interaction is presented. © 2020 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim},
author_keywords={locating;  movable electrodes;  pencil lead;  self-powered;  triboelectric nanogenerators},
document_type={Article},
source={Scopus},
}

@ARTICLE{Majd2020,
author={Majd, A. and Vahidi-Asl, M. and Khalilian, A. and Poorsarvi-Tehrani, P. and Haghighi, H.},
title={SLDeep: Statement-level software defect prediction using deep-learning model on static code features},
journal={Expert Systems with Applications},
year={2020},
volume={147},
doi={10.1016/j.eswa.2019.113156},
art_number={113156},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077495784&doi=10.1016%2fj.eswa.2019.113156&partnerID=40&md5=4f7426cf00845674d73b0d9d3be3e13d},
affiliation={Faculty of Computer Science and Engineering, Shahid Beheshti University G. C.Tehran, Iran; Department of Software Engineering, Faculty of Computer Engineering, University of Isfahan, Isfahan, Iran},
abstract={Software defect prediction (SDP) seeks to estimate fault-prone areas of the code to focus testing activities on more suspicious portions. Consequently, high-quality software is released with less time and effort. The current SDP techniques however work at coarse-grained units, such as a module or a class, putting some burden on the developers to locate the fault. To address this issue, we propose a new technique called as Statement-Level software defect prediction using Deep-learning model (SLDeep). The significance of SLDeep for intelligent and expert systems is that it demonstrates a novel use of deep-learning models to the solution of a practical problem faced by software developers. To reify our proposal, we defined a suite of 32 statement-level metrics, such as the number of binary and unary operators used in a statement. Then, we applied as learning model, long short-term memory (LSTM). We conducted experiments using 119,989 C/C++ programs within Code4Bench. The programs comprise 2,356,458 lines of code of which 292,064 lines are faulty. The benchmark comprises a diverse set of programs and versions, written by thousands of developers. Therefore, it tends to give a model that can be used for cross-project SDP. In the experiments, our trained model could successfully classify the unseen data (that is, fault-proneness of new statements) with average performance measures 0.979, 0.570, and 0.702 in terms of recall, precision, and accuracy, respectively. These experimental results suggest that SLDeep is effective for statement-level SDP. The impact of this work is twofold. Working at statement-level further alleviates developer's burden in pinpointing the fault locations. Second, cross-project feature of SLDeep helps defect prediction research become more industrially-viable. © 2019},
author_keywords={Defect;  Fault prediction model;  Machine learning;  Software fault proneness;  Software metric},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Xu2020,
author={Xu, T. and Darwazeh, I.},
title={Deep Learning for Over-the-Air Non-Orthogonal Signal Classification},
journal={IEEE Vehicular Technology Conference},
year={2020},
volume={2020-May},
doi={10.1109/VTC2020-Spring48590.2020.9128869},
art_number={9128869},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088317338&doi=10.1109%2fVTC2020-Spring48590.2020.9128869&partnerID=40&md5=1f912ca5ef890b45427048575ce35501},
affiliation={University College London, Department of Electronic and Electrical Engineering, London, United Kingdom},
abstract={Non-cooperative communications, where a receiver can automatically distinguish and classify transmitted signal formats prior to detection, are desirable for low-cost and low-latency systems. This work focuses on the deep learning enabled blind classification of multi-carrier signals covering their orthogonal and non-orthogonal varieties. We define Type-I signals with large feature diversity and Type-II signals with strong feature similarity. We evaluate time-domain and frequency-domain convolutional neural network (CNN) models with wireless channel/hardware impairments. Experimental systems are designed and tested, using software defined radio (SDR) devices, operated for different signal formats in line-of-sight and non-line-of-sight communication link scenarios. Testing, using four different time-domain CNN models, showed the pre-trained CNN models to have limited efficiency and utility due to the mismatch between the analytical/simulation and practical/real-world environments. Transfer learning, which is an approach to fine-tune learnt signal features, is applied based on measured over-the-air time-domain signal samples. Experimental results indicate that transfer learning based CNN can efficiently distinguish different signal formats for Type-I in both line-of-sight and non-line-of-sight scenarios relative to the non-transfer-learning approaches. Type-II signals are not identified correctly in the experiment even with the transfer learning assistance leading to potential applications in secure communications. © 2020 IEEE.},
author_keywords={conventional neural network (CNN);  deep learning;  Non-cooperative;  non-orthogonal;  secure communication;  SEFDM;  signal classification;  software defined radio;  transfer learning;  waveform},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jagtap2020,
author={Jagtap, V. and Agarwal, S. and Wagh, A. and Gennert, M.},
title={Transportable open-source application program interface and user interface for generic humanoids: TOUGH},
journal={International Journal of Advanced Robotic Systems},
year={2020},
volume={17},
number={3},
doi={10.1177/1729881420921607},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084641643&doi=10.1177%2f1729881420921607&partnerID=40&md5=60caf766336cdfc4f0eda0a1ac62dba3},
affiliation={Robotics Engineering, Worcester Polytechnic Institute, Worcester, MA, United States; Ghost Robotics Corporation, Worcester, MA, United States},
abstract={Humanoid robotics is a complex and highly diverse field. Humanoid robots may have dozens of sensors and actuators that together realize complicated behaviors. Adding to the complexity is that each type of humanoid has unique application program interfaces, thus software written for one humanoid does not easily transport to others. This article introduces the transportable open-source application program interface and user interface for generic humanoids, a set of application program interfaces that simplifies the programming and operation of diverse humanoid robots. These application program interfaces allow for quick implementation of complex tasks and high-level controllers. Transportable open-source application program interface and user interface for generic humanoids has been developed for, and tested on, Boston Dynamics’ Atlas V5 and NASA’s Valkyrie R5 robots. It has proved successful for experiments on both robots in simulation and hardware, demonstrating the seamless integration of manipulation, perception, and task planning. To encourage the rapid adoption of transportable open-source application program interface and user interface for generic humanoids for education and research, the software is available as Docker images, which enable quick setup of multiuser simulation environments. © The Author(s) 2020.},
author_keywords={Humanoid robots;  open-source projects and applications for robotics;  ROS},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liu2020,
author={Liu, F. and Wang, L. and Li, X. and Huang, C.},
title={ComDA: A common software for nonlinear and Non-Gaussian Land Data Assimilation},
journal={Environmental Modelling and Software},
year={2020},
volume={127},
doi={10.1016/j.envsoft.2020.104638},
art_number={104638},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080898024&doi=10.1016%2fj.envsoft.2020.104638&partnerID=40&md5=7b15560cf5b1dd7736e502674888de11},
affiliation={Cold and Arid Regions Environmental and Engineering Research Institute, Chinese Academy of Sciences, Lanzhou, 730000, China; School of Environmental and Geographical Sciences, Shanghai Normal University, Shanghai, 200234, China; National Tibetan Plateau Data Center, Institute of Tibetan Plateau Research, Chinese Academy of Sciences, Beijing, 100101, China; CAS Center for Excellence in Tibetan Plateau Earth Sciences, Chinese Academy of Sciences, Beijing, 100101, China; University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Common software for land data assimilation is urgently needed to implement a wide variety of assimilation applications; however, a fast, easy-to-use, and multidisciplinary application-oriented assimilation platform has not been achieved. Therefore, we developed Common software for Nonlinear and non-Gaussian Land Data Assimilation (ComDA). ComDA integrates multiple algorithms (including diverse Kalman and particle filters), models and observation operators (e.g., common land model (CoLM), Advanced Integral Equation Model (AIEM)), and provides general interfaces for additional operators. Using mixed-language programming and parallel computing technologies (Open Multi-Processing (OpenMP), Message Passing Interface (MPI) and Compute Unified Device Architecture (CUDA)), ComDA can assimilate various land surface variables and remote sensing observations. High-performance computing and synthetic tests and real-world tests indicate that ComDA achieves the standard of common land data assimilation software with parallel computation, multiple operators, and assimilation algorithms and is compatible with many models. ComDA can be applied for multidisciplinary data assimilation. © 2020 Elsevier Ltd},
author_keywords={Common software;  Land data assimilation;  Multidisciplinary applications;  Parallel computing;  Remote sensing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hu2020789,
author={Hu, Z. and Deng, Y. and Liu, P. and Peng, H.},
title={The semiotic mechanism of cultural landscape genes of traditional settlements [传统聚落文化景观基因的符号机制]},
journal={Dili Xuebao/Acta Geographica Sinica},
year={2020},
volume={75},
number={4},
pages={789-803},
doi={10.11821/dlxb202004009},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086407338&doi=10.11821%2fdlxb202004009&partnerID=40&md5=9ca26cd9e3cea54f3572e5209c4e340c},
affiliation={College of City & Tourism, Hengyang Normal University, Hengyang, Hunan  421002, China; Institute of Geographic Sciences and Natural Resources Research, CAS, Beijing, 100101, China; Key Laboratory of Intelligent Information Processing of Hunan Province, Hengyang, Hunan  421002, China; School of Economics and Management and Rural Vitalization Institution, Changsha University, Changsha, 410022, China},
abstract={The concept of cultural landscape genes of traditional settlements (CLGTS) was proposed by Chinese scholars in 2003. Since then, CLGTS has been playing a key role in capturing the deep-level geographic features of traditional settlements. However, there is a lack of work on covering CLGTS from the perspective of semiotics. Now, people are often involved in difficulties when they are trying to explore the nature of cultural landscapes of traditional settlements through using CLGTS. Obviously, it is of great significance to explore the concepts and methods of semiotic mechanism of CLGTS under the support of semiology. To lock this issue, we outline the dialectical features of CLGTS through the following five aspects. (1) For a given traditional settlement, its whole image at the macro-scale is in accordance with its cultural landscape genes at the micro-scale. (2) For the cultural landscape gene of a given traditional settlement, its core characterizations are in accordance with its appearance features. (3) For a given traditional settlement, its self-updating mechanism at local scale is in accordance with its global characterizations. (4) CLGTS can be treated as the scientific analysis method merged with the quantitative and qualitative approaches for dissecting the cultural features of traditional settlements. (5) For a given traditional settlement, its outstanding features of cultural landscape are in accordance with its rich cultural connotation. Then, this work proves the diversity of forms and complexity of spatial structures of CLGTS through ample examples. To some extent, this reveals the nonlinearity, self-organization, as well as self-iteration features of CLGTS. Based on the above, this research presents a conceptual framework of semiotic mechanism of CLGTS. Within the framework, we further summarize the symbols' main features, classifications, and expression ways of CLGTS. Through this work, we make clear the requisite theoretical conditions of making symbols of CLGTS by employing GIS. Ultimately, based on the aforementioned conceptual framework, this paper develops a prototype program for making symbols of CLGTS. The test results of the prototype program with a case of ancient village of Hunan Province show that it can run well in serving to establish a symbol database of CLGTS for a given region. Hence, this research proves that semiotic mechanism of CLGTS will make sense of perfecting the theory of CLGTS and forwarding its digital protection. © 2020, Science Press. All right reserved.},
author_keywords={Cultural landscape genes of traditional settlements;  Digitalization;  Features;  Symbol database;  Symbolic mechanism},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Baughan2020,
author={Baughan, A. and August, T. and Yamashita, N. and Reinecke, K.},
title={Keep it Simple: How Visual Complexity and Preferences Impact Search Efficiency on Websites},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2020},
doi={10.1145/3313831.3376849},
art_number={3376849},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091279798&doi=10.1145%2f3313831.3376849&partnerID=40&md5=9c2bdfb51546062b65fbd9a439a5c712},
affiliation={University of Washington, Seattle, MA, United States; Ntt Communication Science Laboratories, Keihanna, Japan},
abstract={We conducted an online study with 165 participants in which we tested their search efficiency and information recall. We confirm that the visual complexity of a website has a significant negative effect on search efficiency and information recall. However, the search efficiency of those who preferred simple websites was more negatively affected by highly complex websites than those who preferred high visual complexity. Our results suggest that diverse visual preferences need to be accounted for when assessing search response time and information recall in HCI experiments, testing software, or A/B tests. © 2020 ACM.},
author_keywords={design;  information recall;  search efficiency;  usability;  user interface;  visual appeal;  visual complexity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Florea2020299,
author={Florea, R. and Stray, V.},
title={A Qualitative Study of the Background, Skill Acquisition, and Learning Preferences of Software Testers},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={299-305},
doi={10.1145/3383219.3383252},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090843455&doi=10.1145%2f3383219.3383252&partnerID=40&md5=cebd2222fdf551d9316e98be9c4ff0af},
affiliation={University of Oslo, Oslo, Norway},
abstract={Context: There is an indisputable industrial need for highly skilled individuals in the role of software testers. However, little is known about the educational background of these professionals, their first contact with the role, their preferences in acquiring skills, the impediments they face, and their perception of the software testing role. Objective: In the current paper, we report on the background, skills, learning preferences, and role profiles as described by professionals in software testing, spanning over a significant number of industries, countries, and software development models. Method: We conducted 19 in-depth, semi-structured interviews with software testing practitioners, across eight industries. We performed a content and thematic analysis of the collected data. Results: The practitioners in software testing had diverse educational backgrounds, and their first contact with the testing role was accidental. Exploratory testing was the preferred testing technique, while curiosity was identified as the most important feature in their skill set. Our respondents collaborated extensively with the developers, whom they perceived as a learning source and symbiotic work partner. Conclusion: The professionals in software testing described their skills as a rather undefined heap of knowledge, increasing with each work task. They used mainly informal and hands-on learning approaches. They found it necessary for education providers to present information on software testing. Generally, companies assisted them well in their skill development but need to allocate sufficient time for the learning. We identified five specialties of the role: product owner in testing, UX tester, DevOps tester, test-script automator, and test-process coordinator. © 2020 ACM.},
author_keywords={Hiring Software Testers;  Skill Acquisition;  Software Tester;  Software Testing;  Software Testing Profiles},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2020100,
author={Zhang, X.-Y. and Zheng, Z.},
title={Exploring the Characteristics of Spectra Distribution and Their Impacts on Fault Localization},
journal={ACM International Conference Proceeding Series},
year={2020},
pages={100-109},
doi={10.1145/3383219.3383230},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090833715&doi=10.1145%2f3383219.3383230&partnerID=40&md5=cd50d1c98b10b69b9dae881d1bb73ad1},
affiliation={National Institute of Informatics, Tokyo, Japan; School of Automation Science and Electronic Engineering, Beihang University, Buaa Beijing, China},
abstract={Spectrum-Based Fault Localization (SBFL) follows the basic intuitions that the faulty parts are more likely to be covered by failure-revealing test cases and less likely to be covered by passed test cases. However, due to the diversity of programs and faults, many other characteristics (related to program structure, test suites, and type of faulty components) will influence the practical application of SBFL. For example, a statement can be covered by numerous failure-revealing test cases, and also covered by numerous passed test cases. To get more indicators about the faulty components towards a better application of SBFL, we extend the scope of spectrum-based knowledge from the basic intuitions to the Characteristics of Spectra Distribution (CSDs for short). That is, we explore the relationships between different types of statements and their spectra. Firstly, we introduce the concepts of Failure-Independent, Failure-Related, and Failure-Exclusionary to describe the relationships between different types of statements and their executions. Then, we propose two probabilistic models, with and without the noise of fault interference, respectively, to identify various CSDs for each type of statements. As the analysis results, we introduce a visualization technique to generalize the identified CSDs and provide an overall picture of spectra distribution and its dynamics. Finally, based on our analysis and also the observation of the program spectra of current benchmarks, we design a technique to filter the potential non-faulty statements to improve the accuracy of SBFL. © 2020 ACM.},
author_keywords={probabilistic model;  software fault localization;  spectrum-based characteristics;  spectrum-based fault localization;  visualization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Thomas2020,
author={Thomas, J. and Kirtane, K. and Cherian, T. and Sanghani, R.R. and Loganathan, S. and Suhalka, K.},
title={Study of Tire Contribution to Vehicle Noise, Ride and Handling Performance using DOE (Design of Experiments) Techniques},
journal={SAE Technical Papers},
year={2020},
volume={2020-April},
number={April},
doi={10.4271/2020-01-1238},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083835666&doi=10.4271%2f2020-01-1238&partnerID=40&md5=6757e6d8cbfae26b7ef919af9c0b96be},
affiliation={Ceat Ltd.},
abstract={Automotive Industry has always kept the customer in focus and strived for their delight using innovative technologies and meticulous engineering. From a vehicle dynamics point of view, the customer's performance expectations have diversified over the last decade. To meet these expectations, there has been significant developments in the system and subsystem engineering of chassis elements. One such contributing element is the tire as it can synergize with vehicle noise, ride and handling performance irrespective of propulsion energies. These critical customer touch points, they need to be optimized and tuned to meet stringent OEM targets. The objective of this paper is to understand the aspects of tire design, structure and tread compound on vehicle noise, ride and handling performance using a full factorial DOE (Design of Experiment) approach. The traditional iterative approach of tire tuning is time-intensive and may not quantify the interdependence within the tire. Since tire development is a conscious trade-off process, the use of DOE approach AIDS in understanding individual contribution and interaction effects. This study was carried out using 16 different tire sets, high dynamic instrumentation, advanced analysis software and logics on a popular Japanese Hatchback under controlled proving ground test conditions. Some of the significant findings of this study are presented in this paper. © 2020 SAE International. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kauffman2020156,
author={Kauffman, K. and Marietta, D. and Raquet, J. and Carson, D. and Leishman, R.C. and Canciani, A. and Schofield, A. and Caporellie, M.},
title={Scorpion: A Modular Sensor Fusion Approach for Complementary Navigation Sensors},
journal={2020 IEEE/ION Position, Location and Navigation Symposium, PLANS 2020},
year={2020},
pages={156-167},
doi={10.1109/PLANS46316.2020.9110165},
art_number={9110165},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087074694&doi=10.1109%2fPLANS46316.2020.9110165&partnerID=40&md5=aeab4f69846047b932b9fe3e761a8967},
affiliation={IS4S, Beavercreek, OH, United States; Air Force Institute of Technology, WPAFB, OH, United States; CCDC/C5ISR, APG, MD, United States},
abstract={There is a great need to decrease our reliance on GPS by utilizing novel complementary navigation sensors. While a number of complementary navigation sensors have been studied, each one has trade-offs in availability, reliability, accuracy and applicability in various environments. The development of a robust estimator therefore requires the integration of many diverse sensors into a sensor fusion platform. Unfortunately, as the number of sensors added to the system grows larger, so does the difficulty of developing a sensor fusion solution that optimally integrates them all into a single navigation estimate. In addition, a sensor fusion solution with many sensors is susceptible to sensor failures, modeling errors, and other phenomena which can cause degradation of the fusion solution. In this paper, we propose an open architecture for sensor fusion that allows for the development of modular navigation filters, sensor integration strategies, and integrity algorithms. The primary goal of this architecture is to allow for the rapid development of a novel complementary PNT sensor, fusion strategy, or integrity algorithm without modification of any other part of the system. In the future, this architecture will enable the community to develop a repository of well-tested software modules for sensor fusion which will in turn allow for the iterative development of robust estimators, where users may pick and choose the components that they wish to use from the repository and build an estimator that fits their application. In addition, domain experts in the community on a particular sensor phenomenology may contribute modules to the repository without needing to be experts in all aspects of sensor fusion. To facilitate this community engagement, we have developed an open source implementation of the architecture, which will be made available as a reference implementation of the architecture and approach. This paper details the design and overall approach to the open architecture, as well as shows some experimental results that were obtained by running flight data through the reference implementation. © 2020 IEEE.},
author_keywords={Bayesian Filter;  Kalman Filtering;  Modular;  Navigation;  Open Architecture;  Sensor Fusion},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kyffin2020,
author={Kyffin, W. and Gandy, D. and Burdett, B.},
title={A study of the reproducibility and diversity of the supply chain in manufacturing hot isostatically pressed type 316L stainless steel components for the civil nuclear sector},
journal={Journal of Nuclear Engineering and Radiation Science},
year={2020},
volume={6},
number={2},
doi={10.1115/1.4044752},
art_number={21116},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085986546&doi=10.1115%2f1.4044752&partnerID=40&md5=e330d4dc53765c971768db7aa5c23a39},
affiliation={Nuclear AMRC, Rotherham, South Yorkshire, S60 5WG, United Kingdom; EPRI, Charlotte, NC  28262-850, United States; W B Burdett Associates, Truro, Cornwall, TR1 2HX, United Kingdom},
abstract={Hot isostatic pressing (HIP) of type 316 L stainless steel powder has been an established manufacturing practice for more than 25 years in the oil and gas sector and more recently in the naval defense sector. To demonstrate the capability of the powder metallurgy HIP (PM/HIP) for nuclear power applications, a systematic study of 316 L commercial powder production, encapsulation/consolidation providers, and selected HIP parameters was undertaken by the Nuclear AMRC in collaboration with the Electric Power Research Institute (EPRI). In this study, the 316 L powder specification limited the oxygen content of the powder to under 130 parts per million (ppm), which reflects the improvements that commercial powder suppliers have been making over the past decade to ensure greater powder cleanliness. The test program assessed powder supply, HIP service provider, and HIP sustain time. Excellent test results were achieved across the full range of variables studied with all billets meeting the specification requirements of ASTM A988 and additional requirements imposed based on nuclear manufacturing standards. Significantly, the study demonstrated the robustness of the PM/HIP supply chain, as material produced via differing HIP service providers resulted in very consistent material properties across the destructive test program. Furthermore, no significant difference in material properties was noted for material HIP between 2 and 8 h hold time, suggesting that the HIP process window is large. Both these results are significant from an end-user standpoint as they highlight the uniformity of the process through the full manufacturing cycle from powder procurement to destructive testing. Despite all material passing specification requirements, some property variation was noted for differing powder suppliers. Considering the systematic approach, this was attributed to powder composition, with both low oxygen and high nitrogen contents contributing to improvements in Charpy impact strength and tensile strength, respectively. Copyright © 2020 by ASME.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2020,
author={Lee, S. and Kim, J. and Woo, S. and Yoon, C. and Scott-Hayward, S. and Yegneswaran, V. and Porras, P. and Shin, S.},
title={A comprehensive security assessment framework for software-defined networks},
journal={Computers and Security},
year={2020},
volume={91},
doi={10.1016/j.cose.2020.101720},
art_number={101720},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078829933&doi=10.1016%2fj.cose.2020.101720&partnerID=40&md5=c6e5de13010de2664994b56ceb3016d9},
affiliation={Graduate School of Information Security, School of Computing, KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, 34141, South Korea; School of Electrical Engineering, KAIST, 291 Daehak-ro, Yuseong-gu, Daejeon, 34141, South Korea; ETRI, 218 Gajeong-ro, Yuseong-gu, Daejeon, 34129, South Korea; S2W Lab, 240 Pangyoyeok-ro, Bundang-gu, Seongnam-si, South Korea; Computer Science Laboratory, SRI International, Menlo Park, CA, United States; Centre for Secure Information Technologies, Queen's University Belfast, Belfast, United Kingdom},
abstract={As Software-Defined Networking (SDN) is getting popular, its security issue is being magnified as a new controversy, and this trend can be found from recent studies of presenting possible security vulnerabilities in SDN. Understanding the attack surface of SDN is necessary, and it is the starting point to make it more secure. However, most existing studies depend on empirical methods in different environments, and thus they have stopped short of converging on a systematic methodology or developing automated systems to rigorously test for security flaws in SDNs. Therefore, we need to disclose any possible attack scenarios in diverse SDN environments and examine how these attacks operate in those environments. Inspired by the necessity for disclosing the vulnerabilities in diverse SDN operating scenarios, we suggest an SDN penetration tool, DELTA, to regenerate known attack scenarios in diverse test cases. Furthermore, DELTA can even provide a chance of discovering unknown security problems in SDN by employing a fuzzing module. In our evaluation, DELTA successfully reproduced 26 known attack scenarios, across diverse SDN controller environments, and also discovered 9 novel SDN application mislead attacks. © 2020},
author_keywords={Network security;  Penetration testing;  Security;  Software-Defined Networking},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Khuri20201082,
author={Khuri, N.},
title={Mining environmental chemicals with boosted trees},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2020},
pages={1082-1089},
doi={10.1145/3341105.3373897},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083037493&doi=10.1145%2f3341105.3373897&partnerID=40&md5=223e463109c7922377d3fa562064e215},
affiliation={Department of Computer Science, Wake Forest University, Winston-Salem, NC, United States},
abstract={In response to scientific reports and legislative actions, the United States Environmental Protection Agency (EPA) launched an Endocrine Disruptor Screening Program (EDSP). The overarching aim of this program is to develop novel analytical and computational tools and methods for the detection of environmental chemicals causing aberrations in estrogen, androgen, or thyroid hormone systems. As a result of this program, over 700 diverse environmental chemicals have been tested in in vitro and in vivo assays and deposited into public databases. In this work, machine learning classifiers were developed to predict putative disruptors of the human sodium/iodide symporter that plays a crucial role in the biosynthesis of thyroid hormones. Two powerful ensemble algorithms, Random forest and eXtreme Gradient Boosting Tree, were trained with EDSP experimental data and evaluated by repeated cross-validation as well as by retrospective validation. Within its applicability domain, Boosted Tree classifier achieved high performance and discriminated between inhibitors and noninhibitors with an accuracy of 87%, precision of 85% and recall of 63%. Additionally, 98 inhibitors were also predicted among 1,741 human endogenous and exogenous metabolites, including approved oral drugs. Further experimental studies will be needed to validate these predictions and elucidate the mechanism of interactions. © 2020 ACM.},
author_keywords={Binary classification;  Boosted trees;  Environmental chemicals},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jamil20201566,
author={Jamil, A.M. and Othmane, L.B. and Valani, A. and Abdelkhalek, M. and Tek, A.},
title={The current practices of changing secure software: An empirical study},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2020},
pages={1566-1575},
doi={10.1145/3341105.3373922},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083037068&doi=10.1145%2f3341105.3373922&partnerID=40&md5=bc2ca5370d64662cd22eecd0b080ac0c},
affiliation={Iowa State University, United States; Security Compass; Cyber Electra},
abstract={Developers change the code of their software to add new features, fix bugs, or enhance its structure. Such frequent changes impact occasionally the security of the software. This paper reports a qualitative study of the practices of changing secure-software in the industry. The study involves interviews with eleven developers and security experts working on banking software, software for control systems, and software consultation companies. Through these interviews, we identified that the main security aspects are: dependency vulnerabilities, authentication and authorization, and OWASP 10 vulnerabilities. The common techniques used to assess software after code change are: code review, code analysis, testing, and keywords search. The main challenges that practitioners face are the diversity of the security issues and the lack of effectiveness of the security assurance tools in detecting vulnerabilities. The study suggests that developers of secure software need techniques that support effective security assurance of modified software. © 2020 ACM.},
author_keywords={Code change;  Secure code;  Secure code change},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brodley2020,
author={Brodley, C. and Cuny, J.},
title={The MSCS New Pathways Consortium-a National Invitation},
journal={2020 Research on Equity and Sustained Participation in Engineering, Computing, and Technology, RESPECT 2020 - Proceedings},
year={2020},
doi={10.1109/RESPECT49803.2020.9272440},
art_number={9272440},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098764865&doi=10.1109%2fRESPECT49803.2020.9272440&partnerID=40&md5=551c4f232be7dd4c9296ec07c73aed70},
affiliation={Khoury College of Computer Sciences, Northeastern University, Boston, MA, United States},
abstract={High tech is the U.S. economy's fastest growing sector. Yet, the current tech talent pipeline falls far short of meeting demand. What's more, the demographics of the tech workforce remain stubbornly out of sync with the overall population. This is in large part because our education system still struggles to attract diverse people into computational disciplines. Women represent more than 50% of Bachelor's degree recipients, but only 19% of computer science (CS) graduates. Similarly, underrepresented minorities represent 25% of Bachelor's degree recipients, but just 10% of CS graduates. The diversity of thought, race, background, and gender in CS is essential to building a robust, high quality, and ethical tech sector. One place where innovation might bridge the gap-a place historically overlooked by higher education-is the Master's degree. Since 2013, Northeastern University's Khoury College of Computer Sciences has been testing, refining, and growing the Align program, a Master's of Science in computer science (MSCS) for people who studied something other than CS as undergraduates. The goal is to create a new pathway or onramp to CS for all students, paying particular attention to the recruitment and success of women and underrepresented minorities. In mid2019, we launched the MSCS New Pathways Consortium-an effort to collaborate with colleges and universities across the country to scale this approach. Here, we invite others to join the Consortium and, together, make the MSCS the new MBA, a professional degree that people can access regardless of prior experience and knowledge of computing. © 2020 IEEE.},
author_keywords={Computer science education;  diversity;  non-majors},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jhaa20201155,
author={Jhaa, M. and Jha, R.},
title={Optimal Release Time for Software Systems},
journal={2020 6th International Conference on Advanced Computing and Communication Systems, ICACCS 2020},
year={2020},
pages={1155-1160},
doi={10.1109/ICACCS48705.2020.9074453},
art_number={9074453},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084649464&doi=10.1109%2fICACCS48705.2020.9074453&partnerID=40&md5=3064a5bf3fac0a97801be6b54eecc483},
affiliation={Citicorp Services India Limited, Pune, India; FIS Solutions (India) Pvt. Ltd., Pune, India},
abstract={Testing the lifecycle is a challenge when it comes to maintaining a high layer of software accuracy obtaining the software's optimal release time. The enterprise urgency to understand when to update and break trial to improve the software's reliability, maintaining the software market growth and decrease the price of research. Companies usually put their product on the market sooner, in order to reach the market. Software testing is a mechanism by which corporations update, Troubleshoot or upgrade their software when used as a debugging method, it guarantees optimum product release, increasing software stability while compressing the economic expense of testing. Today, its journey on market is dynamic due to distributed nature and the diversity of the software making patch an intrinsic testing element. A Sew is a chunk of software to repair or support a device program for fixing or improving it. An important issue within the software advanced preparation to complete when to prevent testing and deliver the software to the users. In this paper, the cost optimal release action, which decreases entire normal cost of program, will be addressed. Scientists have been working in the field to reduce entire testing cost, but so far, accuracy has not been studied in the system for optimal time scheduling. In this paper, we discuss accuracy, which is considerable aspect of quality of program. We therefore suggest reliability development, Design software testing to make the software system stable and cost-effective to fix testing cost issues, product delivery time, and acceptable reliability levels. Using real-life failed software dataset, the numeric illustration was implemented. © 2020 IEEE.},
author_keywords={Release failure data set;  Reliability;  software cost;  SRGM (Software reliability growth models);  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{OrvizFernández202081,
author={Orviz Fernández, P. and David, M. and Duma, D.C. and Ronchieri, E. and Gomes, J. and Salomoni, D.},
title={Software Quality Assurance in INDIGO-DataCloud Project: a Converging Evolution of Software Engineering Practices to Support European Research e-Infrastructures},
journal={Journal of Grid Computing},
year={2020},
volume={18},
number={1},
pages={81-98},
doi={10.1007/s10723-020-09509-z},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081256218&doi=10.1007%2fs10723-020-09509-z&partnerID=40&md5=fe8b15744c9cf937784944e4e88d251f},
affiliation={IFCA (CSIC-UC), Santander, Spain; LIP, Lisbon, Portugal; INFN - CNAF, Bologna, Italy},
abstract={From the advent of Grid technology – as the new paradigm of distributed computing – to the current days of Cloud computing models, the continuous need of new tools and services to match the scientific community requirements has been addressed in Europe through dedicated software development projects for e–Infrastructure creation, operation and management. This work presents the most significant software quality breakthroughs obtained in one of such projects, INDIGO–DataCloud, the main challenges and barriers confronted throughout the lifespan of the project, and how they were partially or totally overcome. The knowledge base established throughout the last 15 years of diverse software development initiatives in Europe for sustaining distributed research e-Infrastructures, supported by the advances in the area of software engineering, definitely contributed to improve the quality and reliability of the software delivered, and consequently, the operational stability of the European e–Infrastructures. INDIGO–DataCloud project is a good evidence of such insights, where, unlike the preceding trend found in past projects, the enforcement of Software Quality Assurance practices has been present since the very early stages of the software lifecycle. © 2020, Springer Nature B.V.},
author_keywords={DevOps;  Quality assurance;  Software metrics;  Software reliability;  Software testing techniques},
document_type={Article},
source={Scopus},
}

@ARTICLE{Belleville2020343,
author={Belleville, B. and Iru, R. and Tsiritsi, C. and Ozarska, B.},
title={Planing characteristics of Papua New Guinea timber species from plantations and regrowth forests},
journal={European Journal of Wood and Wood Products},
year={2020},
volume={78},
number={2},
pages={343-349},
doi={10.1007/s00107-020-01495-z},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078727447&doi=10.1007%2fs00107-020-01495-z&partnerID=40&md5=60f1c172eaecf107b3d1dacec66ebf64},
affiliation={Faculty of Science, School of Ecosystem and Forest Sciences, The University of Melbourne, Burnley campus, 500 Yarra Boulevard, Richmond, VC  3121, Australia; Timber and Forestry Training College, PO Box 2132, Buimo Road, Lae, Morobe Province, Papua New Guinea},
abstract={Although Papua New Guinea (PNG) has a rich and diverse forest cover, there is limited information on processing characteristics for plantations and regrowth forests available. Consequently, the PNG timber processing industry is restricted to a few species, producing low-quality products, which limits opportunities for the processors. Sound knowledge of machining characteristics based on some systematic methods has been identified as essential for assessing the ability in processing raw material into appearance products. Therefore, a testing program was conducted to assess the planing characteristics and most common causes for planing degrade of 25 species sourced from the Morobe and West New Britain provinces, PNG. A total of 18 wood species proved to machine very well with more than 90% of assessed boards being graded either “excellent and requiring very light sanding” or “good and requiring light sanding”. Eight species from this group obtained a perfect score, i.e. requiring very light sanding. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ferreira2020,
author={Ferreira, F. and Vale, G. and Diniz, J.P. and Figueiredo, E.},
title={On the proposal and evaluation of a test-enriched dataset for configurable systems},
journal={ACM International Conference Proceeding Series},
year={2020},
doi={10.1145/3377024.3377045},
art_number={a16},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079907741&doi=10.1145%2f3377024.3377045&partnerID=40&md5=f5e67d8615b6fb3713c2bb31e25bf1b3},
affiliation={Federal University of Minas Gerais, Belo Horizonte, Minas Gerais, Brazil; University of Passau, Germany},
abstract={Configurable systems offer advantages compared to single systems since developers should maintain a unique platform to address a diversity of deployment contexts and usages. To ensure that all configurations correctly execute, developers spend considerable effort testing different system configurations. This testing process is essential because configurations that fail may potentially hurt user experience and degrade the reputation of a project. Previous studies have reported and created repositories of open-source configurable systems, although they neglected their test suites. Considering the importance of testing configurable systems, we reviewed the literature to find test suites of open-source configurable systems. As we found only 10 configurable systems with test suite available and considering that a test suite for configurable systems may be useful for different research topics, we created test suites for 20 additional configurable systems and evaluated the test suites coverage of all 30 configurable systems. Surprisingly, our test suites were able to find several failures in existing systems, mainly because of feature interactions, which enforces the need of test suites available for open source configurable systems. Aiming at finding common characteristics for fault-prone components (e.g., classes) on configurable systems, we group them based on software quality metrics (e.g., coupling between objects and lines of code). As result, we found that 44% of the configurable systems of our dataset have failures and these failures are concentrated in few classes. © 2020 Association for Computing Machinery.},
author_keywords={Dataset of Open-source Configurable Systems;  Feature Interactions;  Software Failures;  Testing Configurable Systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Grabner20201542,
author={Grabner, M.J. and Li, X. and Fu, S.},
title={An Adaptive BLAST Successive Interference Cancellation Method for High Data Rate Perfect Space-Time Coded MIMO Systems},
journal={IEEE Transactions on Vehicular Technology},
year={2020},
volume={69},
number={2},
pages={1542-1553},
doi={10.1109/TVT.2019.2954207},
art_number={8903551},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079820845&doi=10.1109%2fTVT.2019.2954207&partnerID=40&md5=a812d5bf9195fb5083c606f626c134c3},
affiliation={Department of Electrical Engineering, University of North Texas, Denton, TX  76207, United States; US Army Research Laboratory (ARL), Aberdeen Proving Ground, MD  21005, United States},
abstract={Linear dispersion (LD) based perfect space-time codes (STBCs) are an efficient means of increasing a multiple-input multiple-output (MIMO) system's overall diversity gain while maintaining the same spectral efficiency as a traditional spatial multiplexed (SM) MIMO system. Because the decoding procedure of LD codes traditionally requires the entire code to be received and decoded simultaneously, complexity increases proportional to the square of the MIMO array size. In this paper, we leverage the increased number of spatial and temporal layers at the decoder to dynamically reduce the complexity of a BLAST optimum ordering and successive interference cancellation (SIC) detector based on the instantaneous system capacity and data rate. The novel approach proposed in this paper is channel code and modulation agnostic, meaning the underlying constellation can be HEX or QAM and there is no feedback from a forward error correction (FEC) decoder, which makes the design useful in a wide range of MIMO systems employing LD codes with linear detectors. We investigate the method's bit error rate (BER) using MIMO dimensions up to $8 \times 8$ and bits per channel use (BPCU) up to 32. We analyze the system's run-time complexity and BER performance in software and implement perfect coding along with the novel method presented here in a custom MIMO orthogonal frequency division multiplexing (OFDM) system and test it over-the-air using an Ettus Research X310 software-defined radio (SDR) testbed. © 1967-2012 IEEE.},
author_keywords={Interference cancellation;  MIMO;  OFDM;  SDR;  space-time codes},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chu2020249,
author={Chu, A.H.-P. and Chauhan, S.V.S. and Gao, G.X.},
title={GPS Multireceiver Direct Position Estimation for Aerial Applications},
journal={IEEE Transactions on Aerospace and Electronic Systems},
year={2020},
volume={56},
number={1},
pages={249-262},
doi={10.1109/TAES.2019.2915393},
art_number={8709757},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079649849&doi=10.1109%2fTAES.2019.2915393&partnerID=40&md5=9d175a9ac5b7b472e8337d1a62caaa54},
affiliation={Department of Electrical and Computer Engineering, University of Illinois at Urbana-Champaign, Urbana, IL  61801, United States; Department of Aerospace Engineering, University of Illinois at Urbana-Champaign, Urbana, IL, United States},
abstract={Modern aviation safety increasingly depends on reliable GPS services, while signal degrading effects such as multipath and masking often occur during critical flight phases, such as takeoff and landing. In this regard, we propose multireceiver direct position estimation (MR-DPE), which operates a network of DPE receivers to enhance GPS accuracy under degraded signal conditions. A DPE receiver directly estimates navigation solutions in the position-velocity-time domain with a maximum-likelihood approach, bypassing the intermediate range measurements. Whereas prior works have shown the enhanced accuracy of DPE with weak signals, MR-DPE provides further improvement by leveraging the information redundancy and the geometric diversity provided by the network of receivers and antennas. We implemented MR-DPE using software-defined radio and tested it with simulated GPS signals to show improved GPS accuracy under degraded environments. We conducted comprehensive, full-scale flight experiments, a first for DPE-related works. A wide range of flight profiles was explored and analyzed, especially those prone to signal multipath and masking, thus validating the claimed benefits of MR-DPE in GPS accuracy. © 1965-2011 IEEE.},
author_keywords={Direct positioning;  global navigation satellite system (GNSS);  global positioning system (GPS);  GNSS receiver;  maximum-likelihood estimation;  multi-receiver},
document_type={Article},
source={Scopus},
}

@ARTICLE{Luo2020,
author={Luo, C. and Goncalves, J. and Velloso, E. and Kostakos, V.},
title={A survey of context simulation for testing mobile context-aware applications},
journal={ACM Computing Surveys},
year={2020},
volume={53},
number={1},
doi={10.1145/3372788},
art_number={21},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079569830&doi=10.1145%2f3372788&partnerID=40&md5=32fce11ca2048046920774b3c05778e3},
affiliation={School of Computing and Information Systems, University of Melbourne, Parkville, VIC  3010, Australia},
abstract={Equipped with an abundance of small-scale microelectromechanical sensors, modern mobile devices such as smartphones and smartwatches can now offer context-aware services to users in mobile environments. Although advances in mobile context-aware applications have made our everyday environments increasingly intelligent, these applications are prone to bugs that are highly difficult to reproduce and repair. Compared to conventional computer software, mobile context-aware applications often have more complex structures to process a wide variety of dynamic context data in specific scenarios. Accordingly, researchers have proposed diverse context simulation techniques to enable low-cost and effective tests instead of conducting costly and time-consuming real-world experiments. This article aims to give a comprehensive overview of the state-ofthe-art context simulation methods for testing mobile context-aware applications. In particular, this article highlights the technical distinctions and commonalities in previous research conducted across multiple disciplines, particularly at the intersection of software testing, ubiquitous computing, and mobile computing. This article also discusses how each method can be implemented and deployed by testing tool developers and mobile application testers. Finally, this article identifies several unexplored issues and directions for further advancements in this field. © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Mobile devices;  Multimedia;  Sensors;  Software testing},
document_type={Review},
source={Scopus},
}

@ARTICLE{Semeráth202057,
author={Semeráth, O. and Farkas, R. and Bergmann, G. and Varró, D.},
title={Diversity of graph models and graph generators in mutation testing},
journal={International Journal on Software Tools for Technology Transfer},
year={2020},
volume={22},
number={1},
pages={57-78},
doi={10.1007/s10009-019-00530-6},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073960121&doi=10.1007%2fs10009-019-00530-6&partnerID=40&md5=eccbbce045e0fe7305a489553ce1eeb6},
affiliation={MTA-BME Lendület Cyber-Physical Systems Research Group, Budapest, Hungary; Department of Measurement and Information Systems, Budapest University of Technology and Economics, Budapest, Hungary; Department of Electrical and Computer Engineering, McGill University, Montreal, Canada},
abstract={When custom modeling tools are used for designing complex safety-critical systems (e.g., critical cyber-physical systems), the tools themselves need to be validated by systematic testing to prevent tool-specific bugs reaching the system. Testing of such modeling tools relies upon an automatically generated set of models as a test suite. While many software testing practices recommend that this test suite should be diverse, model diversity has not been studied systematically for graph models. In the paper, we propose different diversity metrics for models by generalizing and exploiting neighborhood and predicate shapes as abstraction. We evaluate such shape-based diversity metrics using various distance functions in the context of mutation testing of graph constraints and access policies for two separate industrial DSLs. Furthermore, we evaluate the quality (i.e., bug detection capability) of different (random and consistent) model generation techniques for mutation testing purposes. © 2019, The Author(s).},
author_keywords={Graph diversity metrics;  Model diversity;  Model generators;  Mutation testing;  Shape analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Thome2020163,
author={Thome, J. and Shar, L.K. and Bianculli, D. and Briand, L.},
title={An Integrated Approach for Effective Injection Vulnerability Analysis of Web Applications through Security Slicing and Hybrid Constraint Solving},
journal={IEEE Transactions on Software Engineering},
year={2020},
volume={46},
number={2},
pages={163-195},
doi={10.1109/TSE.2018.2844343},
art_number={8373739},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048163216&doi=10.1109%2fTSE.2018.2844343&partnerID=40&md5=f193e8df582396d0911d2ad873411eb6},
affiliation={Interdisciplinary Centre for Security Reliability and Trust (SnT), University of Luxembourg, 29 avenue JF KennedyL-1855, Luxembourg; School of Computer Science and Engineering, Nanyang Technological University, Singapore, 639798, Singapore},
abstract={Malicious users can attack Web applications by exploiting injection vulnerabilities in the source code. This work addresses the challenge of detecting injection vulnerabilities in the server-side code of Java Web applications in a scalable and effective way. We propose an integrated approach that seamlessly combines security slicing with hybrid constraint solving; the latter orchestrates automata-based solving with meta-heuristic search. We use static analysis to extract minimal program slices relevant to security from Web programs and to generate attack conditions. We then apply hybrid constraint solving to determine the satisfiability of attack conditions and thus detect vulnerabilities. The experimental results, using a benchmark comprising a set of diverse and representative Web applications/services as well as security benchmark applications, show that our approach (implemented in the JOACO tool) is significantly more effective at detecting injection vulnerabilities than state-of-the-art approaches, achieving 98 percent recall, without producing any false alarm. We also compared the constraint solving module of our approach with state-of-the-art constraint solvers, using six different benchmark suites; our approach correctly solved the highest number of constraints (665 out of 672), without producing any incorrect result, and was the one with the least number of time-out/failing cases. In both scenarios, the execution time was practically acceptable, given the offline nature of vulnerability detection. © 1976-2012 IEEE.},
author_keywords={constraint solving;  search-based software engineering;  static analysis;  Vulnerability detection},
document_type={Article},
source={Scopus},
}

@CONFERENCE{deLaet2020297,
author={de Laet, T.},
title={Does a mandatory but non-binding test for aspiring students impact the diversity in an engineering bachelor?},
journal={SEFI 48th Annual Conference Engaging Engineering Education, Proceedings},
year={2020},
pages={297-306},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107175511&partnerID=40&md5=aee674b786751fedb30cea3ed3988d76},
affiliation={KU Leuven, Leuven Engineering and Science Education Center (LESEC), Leuven, Belgium},
abstract={Flanders, the Dutch-speaking part of Belgium, has a very open access to higher education. As a result universities have to accept any student with a secondary education diploma into the engineering bachelor, even if the secondary education program does not provide the required knowledge and competencies. To ensure that aspiring students are aware of the required level of mathematical problem solving skills, the universities are since 2013 organizing a non-mandatory and non-binding positioning test in the summer prior to entering higher education. In 2017, the Flemish government decided to make participation to a positioning test mandatory for aspiring engineering students from 2018 on. This mandatory participation could form an additional hurdle for aspiring students, which might impact students of minority groups. This is a concern as female students, pioneering students, and socio-economically challenged students are underrepresented in Flanders' engineering education, while industry still requires more engineers. This paper is the first to study the impact on the diversity within the engineering program of the mandatory participation to the positioning test mandatory. This paper takes a quantitative yet descriptive approach and looks at the student population regarding gender, socio-economic status, pioneering status, and disability. The first results indicate that the extra hurdle of mandatory participation attracts more students and does not threaten the diversity in engineering bachelors. Furthermore, the mandatory participation to the positioning test did not affect the already better performance of female compared to male students, and reduced the performance gap for pioneering and learning-disabled students. © 2020 SEFI 48th Annual Conference Engaging Engineering Education, Proceedings. All rights reserved.},
author_keywords={Admission;  Diversity;  Engineering;  Entrance test;  Gender},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sun2020205374,
author={Sun, Z. and Hu, C. and Li, C. and Wu, L.},
title={Domain ontology construction and evaluation for the entire process of software testing},
journal={IEEE Access},
year={2020},
volume={8},
pages={205374-205385},
doi={10.1109/ACCESS.2020.3037188},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102804842&doi=10.1109%2fACCESS.2020.3037188&partnerID=40&md5=0a0cb9ed0d86e09b6e0d49d52de50b7c},
affiliation={Institute of Computer Application, China Academy of Engineering Physics, Mianyang, 621900, China},
abstract={As an important part of software engineering, software testing is a knowledge-intensive work. In the process of software testing, inconsistent knowledge expression, diverse knowledge carriers, and a few experienced people have mastered most of the knowledge, which hinders the transfer and sharing of domain knowledge. Ontology is widely used in various stages of software engineering to define the semantic relationship between relevant information and knowledge. To solve the problem of knowledge silo in the process of software testing, this article forms an Entire Process Ontology on Software Testing (EPOST). EPOST covers the concepts and relationships of software testing process information, software test object information, and software defect information. The concepts and terms in the ontology are extracted from ISTQB, SWEBOK, IEEE std.829-2008 standard, and IEEE std.610.12-1990 standard. We adopt a comprehensive ontology construction method based on Dev. 101 method and Methontology method. The developed ontology is successfully evaluated by using validation and verification tests. Ontology verification uses an improved FOCA evaluation method by adding a cohesion metric. The evaluation result infers that EPOST has a high quality of ontology and good domain coverage, and achieves the purpose of ontology construction. Finally, we make a case study on the role of EPOST in software testing process. The results show that ontology-based application in the software testing process can promote the sharing and transmission of domain knowledge, and improve the testing process and testing quality. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.},
author_keywords={Domain ontology;  Knowledge management;  Ontology construction;  Ontology evaluation;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mandalawi202029,
author={Mandalawi, M.A. and Sabry, M. and You, G. and Sabry, M.},
title={Characterising and Structural Review of the Rock Mass and Its Geological Structures at Open Pit Mine in Queensland-Australia},
journal={Sustainable Civil Infrastructures},
year={2020},
pages={29-51},
doi={10.1007/978-3-030-34178-7_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102399663&doi=10.1007%2f978-3-030-34178-7_4&partnerID=40&md5=d4b61f4021e5e92bfef72018c595bdf6},
affiliation={Faculty of Science and Technology, Federation University, Mt. Helen, VIC  3350, Australia; School of Computing, Engineering and Mathematics, Western Sydney University, Sydney, Australia},
abstract={Rock characterisation is important to the feasibility of the Handlebar Hill open cut mine at Mt Isa, Queensland-Australia, because of the complex structural geology and the diversity of slope formations. The different rocks are affected by complete and moderate oxidisations coupled with mining works, giving rise to potential slope instability. Through the characterisation of these rocks, there is more confidence in the prediction of their behaviors in terms of failure mechanisms and slope stability. The objective of this research was to evaluate the properties of the pit rock masses. The geotechnical engineering practice approach was based on defining the parameters of the Hoek-Brown, Barton-Bandis and Mohr-Coulomb failure criteria. A program involving investigation that included field measurements, laboratory tests, hydrogeological settings, empirical indices and findings using the RocLab program was applied. The inputs help to analysis of pit slope stability and to understand the effects of different pit configurations on slope performance to allow safe and economic mining operations. © Springer Nature Switzerland AG 2020.},
author_keywords={Geological structures;  Intact rock;  Lithological domains;  Orebody deposits},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schwarz2020283,
author={Schwarz, J.S. and Elshinawy, R. and Ramírez Acosta, R.P. and Lehnhoff, S.},
title={Ontological Integration of Semantics and Domain Knowledge in Hardware and Software Co-simulation of the Smart Grid},
journal={Communications in Computer and Information Science},
year={2020},
volume={1297},
pages={283-301},
doi={10.1007/978-3-030-66196-0_13},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101574746&doi=10.1007%2f978-3-030-66196-0_13&partnerID=40&md5=d9b04e09c677127781146932f8c0015a},
affiliation={Department of Computing Science, University of Oldenburg, Oldenburg, Germany; OFFIS – Institute for Information Technology, Oldenburg, Germany},
abstract={The transition of the power system to more decentralized power plants and intelligent devices in a smart grid leads to a significant rise in complexity. For holistic and integrated designing and testing of new technologies before their implementation in the field co-simulation is an important approach. It allows to couple diverse software simulation components from different domains for prototyping, but also to couple hardware devices in other testing approaches such as Hardware-in-the-Loop (HIL) and remote laboratory coupling, which enable more complete, realistic, and reproducible validation testing. In the planning and evaluation of co-simulation scenarios, experts from different domains have to collaborate. To assist the stakeholder in this process, we propose to integrate on the one hand semantics of simulation components, their exchanged data and on the other hand domain knowledge in the planning, execution, and evaluation of interdisciplinary co-simulation based on ontologies. This approach aims to allow the high-level planning of simulation and the seamless integration of its information to simulation scenario specification, execution and evaluation. Thus, our approach intents to improve the usability of large-scale interdisciplinary hardware and software co-simulation scenarios. © 2020, Springer Nature Switzerland AG.},
author_keywords={Co-simulation;  Energy scenarios;  Hardware in the Loop (HIL);  Information model;  Ontology;  Simulation;  Simulation planning;  Smart grid},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Milord2020,
author={Milord, L.E. and Waterman, A.},
title={DreamCoder - Responding to a stem crisis with an inquiry-based space education curriculum},
journal={Proceedings of the International Astronautical Congress, IAC},
year={2020},
volume={2020-October},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100916299&partnerID=40&md5=b69b72e6595a1ba267a2a286e24056d8},
affiliation={DreamUp, PBC, Washington, DC, United States},
abstract={While the need to motivate students to pursue and excel in science, technology, engineering, and mathematics (STEM) coursework is ever more apparent, the ability for educators to do so remains a challenge. A 2016 U.S. Department of Education report stated that a strong STEM education is one that “is culturally responsive, employs problem- and inquiry-based approaches, and engages students in hands-on activities that offer opportunities to interact with STEM professionals” [1]. DreamUp's DreamCoder, an inquiry-based program geared primarily to middle and high school students (ages 11-18), provides educators with a relevant, hands-on, and engaging curriculum that allows students to conduct an engineering investigation in space. Specifically, students conceptualize, design, and code a research idea that is transmitted to hardware on the International Space Station (ISS). Prior to this transmission, students test and iterate their code using replica hardware in their classrooms. Once the students' code is executed on the ISS hardware's sensors in microgravity, the students receive their data and present their results to their community. This presentation can be done in a way that reflects the students' particular interests, for example through a podcast, blog post, infographic, or a traditional scientific report. As such, students' personal values can be incorporated beginning with the inception of their project idea, and concluding with the final presentation, thereby increasing their engagement in the program and in STEM subjects more broadly. Preliminary data from beta and pilot tests of DreamCoder in the United States, coupled with results from surveys of students who participated in the program under its Australian moniker, Cuberider, have demonstrated the program's strength. A relevant example is found at Sydney Secondary College Leichhardt in Australia, where approximately 230 students took part in the program's pilot year in 2018. After participating in the program, 75 percent of students reported either “liking” or “loving” the course, and more than 60 percent of students identified that the program helped develop their skills in collaboration. Data gleaned from the US pilot and Australian implementation of the program is being used to adapt DreamCoder to the context of a classroom in the United States while maintaining students' freedom to explore the topics in space that they find most exciting and interesting. These adaptations and improvements will allow for DreamCoder to be used by educators in traditional and non-traditional settings to inspire a diverse range of students in Australia, the United States, and beyond. © 2020 by DLR-SART.},
author_keywords={Coding;  Computer engineering;  Education;  Microgravity research;  STEM;  Student research},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2020374,
author={Wang, J.},
title={A hybrid grid-based many-objective optimisation algorithm for software defect prediction},
journal={International Journal of Computing Science and Mathematics},
year={2020},
volume={12},
number={4},
pages={374-384},
doi={10.1504/IJCSM.2020.112675},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100017277&doi=10.1504%2fIJCSM.2020.112675&partnerID=40&md5=13545295d799fa039565d68e18dbfb12},
affiliation={School of Computer Science and Technology, Taiyuan University of Science and Technology, Taiyuan, 030024, China},
abstract={How to apply limited test resources to detect error module is one of the challenges of software defect prediction problem. To solve the problem, a many-objective software defect prediction model is proposed by considering the probability of detection and false alarm rate, the Balance value and F-measure as defect prediction objectives. At the same time, a hybrid grid-based many-objective optimisation algorithm is designed to solve the model. In the designed algorithm, the adaptive dominant region operator is introduced into the grid-based many-objective optimisation algorithm to improve the performance of algorithm in balancing dynamically the convergence and diversity of population. The simulation results show that the proposed algorithm has better performance in solving many-objective the software defect prediction problem. © 2020 Inderscience Enterprises Ltd.},
author_keywords={Adaptive dominant region operator;  Convergence;  Diversity;  False alarm rate;  Many-objective optimisation;  Software defect prediction problem;  The probability of detection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jin2020302,
author={Jin, H. and Kitamura, T. and Choi, E.-H. and Tsuchiya, T.},
title={A Comparative Study on Combinatorial and Random Testing for Highly Configurable Systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12543 LNCS},
pages={302-309},
doi={10.1007/978-3-030-64881-7_20},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097810499&doi=10.1007%2f978-3-030-64881-7_20&partnerID=40&md5=5467a913a97877eca1068584bee2a95d},
affiliation={Osaka University, Suita, Japan; AIST, Ikeda, Japan},
abstract={Highly configurable systems (HCSs), such as software product lines, have complex configuration spaces. Combinatorial Testing and Random Testing are the main approaches to testing of HCSs. In this paper, we empirically compare their strengths with respect to scalability and diversity of sampled configurations (i.e., tests). We choose Icpl and QuickSampler to respectively represent Combinatorial Testing and Random Testing. Experiments are conducted to evaluate the t-way coverage criterion of generated test suites for HCS benchmarks. © 2020, IFIP International Federation for Information Processing.},
author_keywords={Combinatorial testing;  Random testing;  Software product line},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bokulich20204048,
author={Bokulich, N.A. and Ziemski, M. and Robeson, M.S., II and Kaehler, B.D.},
title={Measuring the microbiome: Best practices for developing and benchmarking microbiomics methods},
journal={Computational and Structural Biotechnology Journal},
year={2020},
volume={18},
pages={4048-4062},
doi={10.1016/j.csbj.2020.11.049},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097572158&doi=10.1016%2fj.csbj.2020.11.049&partnerID=40&md5=173704d11b8efa5a0995ab15ce06cfa3},
affiliation={Laboratory of Food Systems Biotechnology, Institute of Food, Nutrition, and Health, ETH Zürich, Switzerland; University of Arkansas for Biomedical Sciences, Department of Biomedical Informatics, Little Rock, AR, United States; School of Science, University of New South Wales, Canberra, Australia},
abstract={Microbiomes are integral components of diverse ecosystems, and increasingly recognized for their roles in the health of humans, animals, plants, and other hosts. Given their complexity (both in composition and function), the effective study of microbiomes (microbiomics) relies on the development, optimization, and validation of computational methods for analyzing microbial datasets, such as from marker-gene (e.g., 16S rRNA gene) and metagenome data. This review describes best practices for benchmarking and implementing computational methods (and software) for studying microbiomes, with particular focus on unique characteristics of microbiomes and microbiomics data that should be taken into account when designing and testing microbiomics methods. © 2020 The Author(s)},
author_keywords={Amplicon sequencing;  Benchmarking;  Best practices;  Marker-gene sequencing;  Metagenomics;  Microbiome;  Software development},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Sun2020735,
author={Sun, X. and Cheng, R. and Chen, J. and Ang, E. and Legunsen, O. and Xu, T.},
title={Testing configuration changes in context to prevent production failures},
journal={Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020},
year={2020},
pages={735-751},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096807667&partnerID=40&md5=06dfae7f180161081b104660ad989c76},
affiliation={University of Illinois at Urbana-Champaign, United States; Cornell University, United States},
abstract={Large-scale cloud services deploy hundreds of configuration changes to production systems daily. At such velocity, configuration changes have inevitably become prevalent causes of production failures. Existing misconfiguration detection and configuration validation techniques only check configuration values. These techniques cannot detect common types of failure-inducing configuration changes, such as those that cause code to fail or those that violate hidden constraints. We present ctests, a new type of tests for detecting failure-inducing configuration changes to prevent production failures. The idea behind ctests is simple-connecting production system configurations to software tests so that configuration changes can be tested in the context of code affected by the changes. So, ctests can detect configuration changes that expose dormant software bugs and diverse misconfigurations. We show how to generate ctests by transforming the many existing tests in mature systems. The key challenge that we address is the automated identification of test logic and oracles that can be reused in ctests. We generated thousands of ctests from the existing tests in five cloud systems. Our results show that ctests are effective in detecting failure-inducing configuration changes before deployment. We evaluate ctests on real-world failure-inducing configuration changes, injected misconfigurations, and deployed configuration files from public Docker images. Ctests effectively detect real-world failure-inducing configuration changes and misconfigurations in the deployed files. © 2020 Proceedings of the 14th USENIX Symposium on Operating Systems Design and Implementation, OSDI 2020. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tarasov2020,
author={Tarasov, V.N. and Logvinov, I.M.},
title={Using the TAR3D program for 3D data visualization in geoelectric studies.},
journal={Geoinformatics 2020 - XIXth International Conference "Geoinformatics: Theoretical and Applied Aspects"},
year={2020},
art_number={18273},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094209706&partnerID=40&md5=663778971235770849771c8969748fb6},
affiliation={Institute of Geophysics NAS of Ukrane, Ukraine},
abstract={In recent years, actively developing programs for three-dimensional data visualization. There are no such areas of life where 3D images are not needed. As a rule, available programs do not always provide the opportunity to get the image in the desired form. Therefore, to build three-dimensional models, in addition to widespread and well-known GIS products, the authors of the report offer their alternative solution. In the Matlab environment, which is a high-level language was written TAR3D program for three-dimensional data visualization. The program was tested on geoelectric data obtained within Ukraine and abroad. The operational experience of the program shows that it is suitable not only for deep geological and geophysical data. The process of building complex 3D models is simple. The block design of diverse objects mentioned in the text is used. The final model is saved in vector form with the ability to continue interactive graphic editing. The rotation of the model in any direction, the use of a coordinate marker on the body of objects gives an undeniable advantage for researchers who are engaged in the analysis of three-dimensional objects. © Geoinformatics 2020 - XIXth International Conference "Geoinformatics: Theoretical and Applied Aspects". All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jovanovikj202021,
author={Jovanovikj, I. and Weidmann, N. and Yigitbas, E. and Anjorin, A. and Sauer, S. and Engels, G.},
title={A Model-Driven Mutation Framework for Validation of Test Case Migration},
journal={Communications in Computer and Information Science},
year={2020},
volume={1262 CCIS},
pages={21-29},
doi={10.1007/978-3-030-58167-1_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094159110&doi=10.1007%2f978-3-030-58167-1_2&partnerID=40&md5=b4c0ce8730a92ba8ef4365015cc657ca},
affiliation={Department of Computer Science, Paderborn University, Paderborn, Germany},
abstract={Software testing is important in software migration as it is used to validate the migration and ensure functional equivalence which is a key requirement. Developing new test cases for the migrated system is costly and, therefore, the migration of existing test cases is an attractive alternative. As the migrated test cases validate the whole migration, their migration should be clearly validated as well. Applying mutation analysis to validate test case migration is a promising solution candidate. However, due to the diversity of the migration context, applying mutation analysis is quite challenging for conceptual and especially for practical reasons. The different types of test cases combined with the different technologies used, make the application of existing, mostly code-based and mutation score-oriented, mutation tools and frameworks barely possible. In this paper, we present a flexible and extensible model-driven mutation framework applicable in different migration scenarios. We also present a case study, where our mutation framework was applied in industrial context. © 2020, Springer Nature Switzerland AG.},
author_keywords={Mutation analysis;  Software migration;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2020,
title={12th International Symposium on Search-Based Software Engineering, SSBSE 2020},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12420 LNCS},
page_count={262},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092905426&partnerID=40&md5=7282a787d92501fd812d00fa25b9f337},
abstract={The proceedings contain 19 papers. The special focus in this conference is on Search-Based Software Engineering. The topics include: Commonality-Driven Unit Test Generation; preface; search-Based Software Testing for Formal Software Verification – and Vice Versa; using a Genetic Algorithm to Optimize Configurations in a Data-Driven Application; measuring and Maintaining Population Diversity in Search-Based Unit Test Generation; search@Home: A Commercial Off-the-Shelf Environment for Investigating Optimization Problems; exploring the Use of Genetic Algorithm Clustering for Mobile App Categorisation; impact of Test Suite Coverage on Overfitting in Genetic Improvement of Software; bet and Run for Test Case Generation; bytecode-Based Multiple Condition Coverage: An Initial Investigation; an Application of Model Seeding to Search-Based Unit Test Generation for Gson; generating Diverse Test Suites for Gson Through Adaptive Fitness Function Selection; defects4J as a Challenge Case for the Search-Based Software Engineering Community; automated Unit Test Generation for Python; do Quality Indicators Prefer Particular Multi-objective Search Algorithms in Search-Based Software Engineering?; it Is Not Only About Control Dependent Nodes: Basic Block Coverage for Search-Based Crash Reproduction; search-Based Testing for Scratch Programs.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Tebes2020,
author={Tebes, G. and Rivera, B. and Becker, P. and Papa, M.F. and Peppino, D. and Olsina, L.},
title={Specifying the design science research process: An applied case of building a software testing ontology},
journal={23rd Iberoamerican Conference on Software Engineering, CIbSE 2020},
year={2020},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092278958&partnerID=40&md5=d2c878d5513c7775cb42c15aec41a0c2},
affiliation={GIDIS_Web, Facultad de Ingeniería, UNLPam, General Pico, LP, Argentina},
abstract={Design Science Research (DSR) is a rigorous investigation approach that promotes the development of artifacts to meet a useful solution to a relevant domain problem. The artifact should be an innovative solution for a non-trivial problem. After determining the relevance of a given problem/solution, the development of the artifact involves a cycle of design-development-evaluation activities, which iterates as many times as needed before the artifact is finally verified, validated and communicated for its use. The cutting-edge literature about the DSR approach permitted us to observe that, at the moment of this work, the perspectives of its process model are weakly specified. Aimed at contributing to this aspect, this work represents the DSR process specification using the SPEM language. Moreover, we illustrate its main activities by applying them in the construction process of an artifact, i.e., the software testing ontology named TestTDO. This top-domain ontology was recently developed, verified, validated and evaluated in order to afterward building strategies that help achieve diverse test goals' purposes when carrying out test projects. © CIbSE 2020.},
author_keywords={Artifact;  DSR;  DSR process;  SPEM;  Top-domain Testing Ontology},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ispoglou20202271,
author={Ispoglou, K.K. and Austin, D. and Mohan, V. and Payer, M.},
title={FuzzGen: Automatic fuzzer generation},
journal={Proceedings of the 29th USENIX Security Symposium},
year={2020},
pages={2271-2287},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091942677&partnerID=40&md5=629597b541da5bbe5b847763d7a29285},
affiliation={Google Inc; EPFL},
abstract={Fuzzing is a testing technique to discover unknown vulnerabilities in software. When applying fuzzing to libraries, the core idea of supplying random input remains unchanged, yet it is non-trivial to achieve good code coverage. Libraries cannot run as standalone programs, but instead are invoked through another application. Triggering code deep in a library remains challenging as specific sequences of API calls are required to build up the necessary state. Libraries are diverse and have unique interfaces that require unique fuzzers, so far written by a human analyst. To address this issue, we present FuzzGen, a tool for automatically synthesizing fuzzers for complex libraries in a given environment. FuzzGen leverages a whole system analysis to infer the library's interface and synthesizes fuzzers specifically for that library. FuzzGen requires no human interaction and can be applied to a wide range of libraries. Furthermore, the generated fuzzers leverage LibFuzzer to achieve better code coverage and expose bugs that reside deep in the library. FuzzGen was evaluated on Debian and the Android Open Source Project (AOSP) selecting 7 libraries to generate fuzzers. So far, we have found 17 previously unpatched vulnerabilities with 6 assigned CVEs. The generated fuzzers achieve an average of 54.94% code coverage; an improvement of 6.94% when compared to manually written fuzzers, demonstrating the effectiveness and generality of FuzzGen. © 2020 by The USENIX Association. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Allen2020,
author={Allen, J.L. and Hager, B.},
title={Simulation and closed-loop testing of camera, radar, and lidar sensors for highly automated verification and validation of data fusion systems},
journal={AIAA Scitech 2020 Forum},
year={2020},
volume={1 PartF},
doi={10.2514/6.2020-0895},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091940188&doi=10.2514%2f6.2020-0895&partnerID=40&md5=9302258c7347e44b075e5c76188d5f1a},
affiliation={dSPACE, Inc., Wixom, MI  48393, United States},
abstract={Aerospace environments consist of both static and dynamic objects both in the air and on the ground. Dynamic objects include aircraft, UAVs, ground vehicles, birds, and people. Static objects involve runways, runway markers, buildings, and any other scenery/objects. Detecting, classifying, identifying and tracking these objects is needed for collision avoidance algorithms deployed in current aircraft systems. Multiple sensors, such as cameras, radar, LIDAR, etc., need to be used for capturing the object’s characteristics and location. Data fusion is used to combine data from these sensors. Advanced algorithm techniques are then employed for object identification. Detect-and-Avoid (DAA) systems are highly dependent on the performance of communications, navigation and surveillance (CNS) systems for detection, tracking and avoiding (DTA) tasks and maneuvers. To effectively detect objects, multiple high-performance, reliable and accurate avionics sensors and systems are used. This includes: non-cooperative sensors (e.g., visual and thermal cameras, Laser radar (LIDAR) and acoustic sensors) and cooperative systems [e.g., Automatic Dependent Surveillance-Broadcast (ADS-B) and Traffic Collision Avoidance System (TCAS). It is important to utilize this sensor technology for object detection and identification to improve the integrity of these systems in all sorts of potential scenario situations and environmental conditions. This paper will explore how to enhance autonomous sensor testing capabilities and system quality assurance using completely automated Hardware-in-the-loop (HIL) and/or Software in the Loop (SIL) simulation environments that integrate autonomous sensor technology, such as cameras, RADAR, LIDAR, and other key technologies such as GNSS/maps and bus communications, such as ARINC or Ethernet. The key to performing such real-time testing is the ability to stimulate the various sensor Electronic Control Units (ECUs) through closed-loop simulation of the vehicle, its environment, traffic, surroundings, along with synchronization with vehicle bus and application data. These techniques are necessary for developing and implementing a complete System Verification and Validation (V&V) solution that can handle diverse types of perception systems in the loop. In order to handle the testing necessary for these types of systems, new technologies and effective techniques for optimization of test processes and certification relative to DO178C will be shown, along with examples of specific use-cases for development of aero-applications utilizing simulated scenarios. A cornerstone for testing quality and process certification is traceability, which is necessary in order to ensure the compliance of the development and testing processes. A system integration environment is introduced that can tie together the large amounts of common data, models, and test facets that a company faces when implementing these types of test systems. In order to effectively use the MBD tools in the development process, it is necessary to be able to manage this data and metadata in an efficient manner that relates directly to the actual tools and methods used. This test management capability also is critical when it comes to managing the scale of test environments – such as cloud-based testing, which can encompass hundreds to thousands of simulations/tests. The tools and system proposed in this paper is used for test process management and advanced test methodologies, as well as handling the traceability process for certification needs. © 2020, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Blazytko2020235,
author={Blazytko, T. and Schlögel, M. and Aschermann, C. and Abbasi, A. and Frank, J. and Wörner, S. and Holz, T.},
title={AURORA: Statistical crash analysis for automated root cause explanation},
journal={Proceedings of the 29th USENIX Security Symposium},
year={2020},
pages={235-252},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091924830&partnerID=40&md5=9c51005e0f1f57d78bb6c9fda8c6d3e6},
affiliation={Ruhr-Universität Bochum, Germany},
abstract={Given the huge success of automated software testing techniques, a large amount of crashes is found in practice. Identifying the root cause of a crash is a time-intensive endeavor, causing a disproportion between finding a crash and fixing the underlying software fault. To address this problem, various approaches have been proposed that rely on techniques such as reverse execution and backward taint analysis. Still, these techniques are either limited to certain fault types or provide an analyst with assembly instructions, but no context information or explanation of the underlying fault. In this paper, we propose an automated analysis approach that does not only identify the root cause of a given crashing input for a binary executable, but also provides the analyst with context information on the erroneous behavior that characterizes crashing inputs. Starting with a single crashing input, we generate a diverse set of similar inputs that either also crash the program or induce benign behavior. We then trace the program's states while executing each found input and generate predicates, i. e., simple Boolean expressions that capture behavioral differences between crashing and non-crashing inputs. A statistical analysis of all predicates allows us to identify the predicate pinpointing the root cause, thereby not only revealing the location of the root cause, but also providing an analyst with an explanation of the misbehavior a crash exhibits at this location. We implement our approach in a tool called AURORA and evaluate it on 25 diverse software faults. Our evaluation shows that AURORA is able to uncover root causes even for complex bugs. For example, it succeeded in cases where many millions of instructions were executed between developer fix and crashing location. In contrast to existing approaches, AURORA is also able to handle bugs with no data dependency between root cause and crash, such as type confusion bugs. © 2020 by The USENIX Association. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang202027,
author={Wang, Y. and Mäntylä, M.V. and Demeyer, S. and Wiklund, K. and Eldh, S. and Kairi, T.},
title={Software test automation maturity: A survey of the state of the practice},
journal={ICSOFT 2020 - Proceedings of the 15th International Conference on Software Technologies},
year={2020},
pages={27-38},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091748062&partnerID=40&md5=671c7ffe725f4a1b7229a9c500188c51},
affiliation={M3S Research Unit, University of Oulu, Oulu, Finland; Universiteit Antwerpen and Flanders Make, Antwerp, Belgium; Ericsson AB, Stockholm, Sweden; Eficode, Helsinki, Finland},
abstract={The software industry has seen an increasing interest in test automation. In this paper, we present a test automation maturity survey serving as a self-assessment for practitioners. Based on responses of 151 practitioners coming from above 101 organizations in 25 countries, we make observations regarding the state of the practice of test automation maturity: a) The level of test automation maturity in different organizations is differentiated by the practices they adopt; b) Practitioner reported the quite diverse situation with respect to different practices, e.g., 85% practitioners agreed that their test teams have enough test automation expertise and skills, while 47% of practitioners admitted that there is lack of guidelines on designing and executing automated tests; c) Some practices are strongly correlated and/or closely clustered; d) The percentage of automated test cases and the use of Agile and/or DevOps development models are good indicators for a higher test automation maturity level; (e) The roles of practitioners may affect response variation, e.g., QA engineers give the most optimistic answers, consultants give the most pessimistic answers. Our results give an insight into present test automation processes and practices and indicate chances for further improvement in the present industry. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Assessment;  Best practice;  Improvement;  Maturity;  Software;  Test automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Borges2020306,
author={Borges, O.T. and Couto, J.C. and Ruiz, D.D.A. and Prikladnicki, R.},
title={How machine learning has been applied in software engineering?},
journal={ICEIS 2020 - Proceedings of the 22nd International Conference on Enterprise Information Systems},
year={2020},
volume={2},
pages={306-313},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091402351&partnerID=40&md5=2befddb9858cdc38dbe11f8846d96678},
affiliation={School of Technology, PUCRS, Porto Alegre, Brazil},
abstract={Machine Learning (ML) environments are composed of a set of techniques and tools, which can help in solving problems in a diversity of areas, including Software Engineering (SE). However, due to a large number of possible configurations, it is a challenge to select the ML environment to be used for a specific SE domain issue. Helping software engineers choose the most suitable ML environment according to their needs would be very helpful. For instance, it is possible to automate software tests using ML models, where the model learns software behavior and predicts possible problems in the code. In this paper, we present a mapping study that categorizes the ML techniques and tools reported as useful to solve SE domain issues. We found that the most used algorithm is Naïve Bayes and that WEKA is the tool most SE researchers use to perform ML experiments related to SE. We also identified that most papers use ML to solve problems related to SE quality. We propose a categorization of the ML techniques and tools that are applied in SE problem solving, linking with the Software Engineering Body of Knowledge (SWEBOK) knowledge areas. Copyright © 2020 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved},
author_keywords={Machine Learning;  Mapping Study;  Software Engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Afzal2020160231,
author={Afzal, U. and Mahmood, T. and Khan, A.H. and Jan, S. and Rasool, R.U. and Qamar, A.M. and Khan, R.U.},
title={Feature Selection Optimization in Software Product Lines},
journal={IEEE Access},
year={2020},
volume={8},
pages={160231-160250},
doi={10.1109/ACCESS.2020.3020795},
art_number={9183963},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091323467&doi=10.1109%2fACCESS.2020.3020795&partnerID=40&md5=47adc629502b0e1927f128d5afb77d1d},
affiliation={Computer Science Department, Federal Urdu University of Arts Science and Technology, Karachi, Pakistan; Computer Science Department, Institute of Business Administration, Karachi, Pakistan; Computer Science Department, Dhanani School of Science and Engineering, Habib University, Karachi, Pakistan; Department of Computer Science and IT, University of Engineering and Technology, Peshawar, Pakistan; Centre for Applied Informatics (CAI), Institute for Sustainable Industries Liveable Cities Engineering and Science, Victoria University, Melbourne, VIC, Australia; Department of Computer Science, College of Computer, Qassim University, Buraydah, Saudi Arabia; Department of Computing, School of Electrical Engineering and Computer Science, National University of Sciences and Technology, Islamabad, 44000, Pakistan; BIND Research Group, College of Computer, Qassim University, Buraydah, Saudi Arabia; Department of Information Technology, College of Computer, Qassim University+, Buraydah, Saudi Arabia; Intelligent Analytics Group, College of Computer, Qassim University, Buraydah, Saudi Arabia},
abstract={Feature modeling is a common approach for configuring and capturing commonalities and variations among different Software Product Lines (SPL) products. This process is carried out by a set of SPL design teams, each working on a different configuration of the desired product. The integration of these configurations leads to inconsistencies in the final product design. The typical solution involves extensive deliberation and unnecessary resource usage, which makes SPL inconsistency resolution an expensive and unoptimized process. We present the first comprehensive evaluation of swarm intelligence (using Particle Swarm Optimization) to the problem of resolving inconsistencies in a configured integrated SPL product. We call it ${o}$ -SPLIT ( ${o}$ ptimization-based Software Product LIne Tool) and validate ${o}$ -SPLIT with standard ERP, SPLOT (Software Product Lines Online Tools), and BeTTy (BEnchmarking and TesTing on the analYsis) product configurations along with diverse feature set sizes. The results show that Particle Swarm Optimization can successfully optimize SPL product configurations. Finally, we implement ${o}$ -SPLIT as a decision-support tool in a real, local SPL setting and acquire subjective feedback from SPL designers which shows that the teams are convinced of the usability and high-level decision support provided by ${o}$ -SPLIT. © 2013 IEEE.},
author_keywords={feature models;  inconsistencies;  optimization;  particle swarm optimization;  Software product line},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Phadke2020,
author={Phadke, M.S. and Phadke, K.M.},
title={Utilizing design of experiments to reduce IT system testing cost},
journal={Proceedings - Annual Reliability and Maintainability Symposium},
year={2020},
volume={2020-January},
doi={10.1109/RAMS48030.2020.9153609},
art_number={9153609},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090431707&doi=10.1109%2fRAMS48030.2020.9153609&partnerID=40&md5=3e989b8228d0506a82785fe1d77ba847},
affiliation={Pliadkc Associates, 1 Shawnee Conrt, Colls Neck, NJ  07722, United States},
abstract={Orthogonal arrays are a powerful tool in quality and statistics. This paper shows new applications for improving testing effectiveness and efficiency in a multi-parameter environment, which is commonly encountered in today's software and systems. Unique aspects of this paper are applications in diverse areas-telecommunications, defense, automotive, information technology, and financial systems. © 2020 IEEE.},
author_keywords={DOE;  Orthogonal Arrays;  Software and System Testing;  Test and Evaluation;  Validation;  Verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feng20201237,
author={Feng, B. and Mera, A. and Lu, L.},
title={P2IM: Scalable and hardware-independent firmware testing via automatic peripheral interface modeling},
journal={Proceedings of the 29th USENIX Security Symposium},
year={2020},
pages={1237-1254},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088922240&partnerID=40&md5=96b4d868581725d851b923ff11e0bb6e},
affiliation={Northeastern University, United States},
abstract={Dynamic testing or fuzzing of embedded firmware is severely limited by hardware-dependence and poor scalability, partly contributing to the widespread vulnerable IoT devices. We propose a software framework that continuously executes a given firmware binary while channeling inputs from an off-the-shelf fuzzer, enabling hardware-independent and scalable firmware testing. Our framework, using a novel technique called P2IM, abstracts diverse peripherals and handles firmware I/O on the fly based on automatically generated models. P2IM is oblivious to peripheral designs and generic to firmware implementations, and therefore, applicable to a wide range of embedded devices. We evaluated our framework using 70 sample firmware and 10 firmware from real devices, including a drone, a robot, and a PLC. It successfully executed 79% of the sample firmware without any manual assistance. We also performed a limited fuzzing test on the real firmware, which unveiled 7 unique unknown bugs. © 2020 by The USENIX Association. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhao202039,
author={Zhao, Q.},
title={Presents the technology, protocols, and new innovations in industrial internet of things (IIoT)},
journal={EAI/Springer Innovations in Communication and Computing},
year={2020},
pages={39-56},
doi={10.1007/978-3-030-32530-5_3},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087795188&doi=10.1007%2f978-3-030-32530-5_3&partnerID=40&md5=dcd062fb8ebfe9fa2abba695fe9634d8},
affiliation={Sichuan Netop Telecom Co., Mianyang, Sichuan, China},
abstract={Smart devices are changing people’s daily life in the world, in which significant trend has already been extended to the industry sector. In the upcoming Industry 4.0, the connected smart devices all around the world via the Internet provide secure, real-time, and reliable services of sensing, communicating, and computing, making smart factories into realization. In Industry 4.0, there are some primary research issues including technology, protocol, innovation and application, and standards, making effects on the following aspects, such as chips, terminals, base stations, networks, software tools, testing devices, operating systems, and APPs. For the IIoT, there exist few works which combine above research issues to give the reader a systematical view of IIoT, which is the target of this chapter. The relation of above issues lies in the following: First, the innovation and application drive the development of industry. Second, the technology is the basis of an industry device. Third, the protocol is the link between industry devices. Fourth, standards drive multiple companies with diverse produced devices competing in the industry. The research issues are illustrated below. Reference architecture is the guidance for designing and producing industrial products. RAMI 4.0, the reference architecture model for Industry 4.0, can be analyzed from the aspects of layers and hierarchy levels. On the one hand, the layers include business, function, information, communication, integration, and asset. On the other hand, the hierarchy levels include connected world, enterprise, work center, station, control device, field device, and product. The protocol is important for connecting the products. The IIoT communication protocols can be classified into some kinds, which are as follows: First, the wireless sensor network includes WirelessHART, IEC 62591, ISA 100.11a, and Zigbee. Second, the M2M communication includes CoAP, OPC-UA, DDS, and Modbus. Third, the messaging includes MQTT, AMQP, and XMPP. Fourth, the low-power wide-area network (LPWAN) includes NB-IoT, Sigfox, LoRa, and LoRaWAN. Fifth, the cellular network includes 5G, 4G, 3G, LTE, WiMax, GPRS, and GMS. Sixth, the wireless local area network (WLAN) includes IEEE 802.11. Seventh, the wireless personal area network (WPAN) includes IEEE 802.15.4. Application field and industrial product are fundamental for IIoT. The primary IIoT application fields include cyber-physical systems, Industry 4.0, machine-to-machine communications, multi-agent systems, and wireless sensor networks. Moreover, the primary industrial products in the applications of IIoT include car and truck, car parts, electronic power, electronics, foodstuff, heating, and oil and gas. Standard and stack can guide the company for developing products. The IIoT communication standard and technology stack can be classified into layers and groups. On the one hand, layers are consisting of a framework layer, a transport layer, a network layer, a link layer, and a physical layer. On the other hand, main groups include WPAN, WLAN, cellular network, LPWAN, satellite network, and traditional industrial computer network (fieldbus). © Springer Nature Switzerland AG 2020.},
author_keywords={Industrial Internet of Things (IIoT);  Innovation;  Protocol;  Standard;  Technology},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Guo2020655,
author={Guo, L. and Zeng, G. and Li, Y.},
title={Online Fault Protection Method and Its Implementation Based on Control Bus for Electromechanical Composite Transmission Bench Test System},
journal={Lecture Notes in Electrical Engineering},
year={2020},
volume={638},
pages={655-669},
doi={10.1007/978-981-15-2862-0_63},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085216387&doi=10.1007%2f978-981-15-2862-0_63&partnerID=40&md5=1c7cafbf1e8510516ec0d681bffd64e0},
affiliation={China North Vehicle Research Institute, Beijing, 100072, China},
abstract={At present, the faults of the electromechanical composite transmission bench test system are presented in a complex and diverse manner, and the fault handling is mostly manual; its safety performance is reduced, and it is easy to cause delay and secondary failure. This paper aims at the safety of the system, guarantees that the fault can be handled in the shortest time after the fault occurs, proposes an online fault protection method for the electromechanical composite drive system based on control bus, and applies the method to the test bench. The method is oriented to the fault diagnosis of the electromechanical composite transmission bench test system and can provide a dynamic real-time protection mechanism for the system, and when a system-level alarm or fault occurs during the system operation, the system performs the protection program according to the preset protection logic. In order to ensure system safety and fault coordination, it can provide an information-based means of “reliability prediction” for fault diagnosis and health management of an electromechanical composite transmission. © Springer Nature Singapore Pte Ltd. 2020.},
author_keywords={Electrical composite transmission;  Electromechanical compound transmission control;  Fault diagnosis;  Reliability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Erazo2020438,
author={Erazo, A.B. and Medina, J.L.P.},
title={Algorithmic efficiency of stroke gesture recognizers: A comparative analysis},
journal={International Journal on Advanced Science, Engineering and Information Technology},
year={2020},
volume={10},
number={2},
pages={438-446},
doi={10.18517/ijaseit.10.2.10807},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085202226&doi=10.18517%2fijaseit.10.2.10807&partnerID=40&md5=325f189c0361abf5adcd214fd05c697f},
affiliation={Intelligent and Interactive Systems Lab (SI2-Lab), Universidad de Las Américas (UDLA), Quito, 170504, Ecuador},
abstract={Gesture interaction is today recognized as a natural, intuitive way to execute commands of an interactive system. For this purpose, several stroke gesture recognizers become more efficient in recognizing end-user gestures from a training set. Although the rate algorithms propose their rates of return there is a deficiency in knowing which is the most recommended algorithm for its use. In the same way, the experiments known by the most successful algorithms have been carried out under different conditions, resulting in non-comparable results. To better understand their respective algorithmic efficiency, this paper compares the recognition rate, the error rate, and the recognition time of five reference stroke gesture recognition algorithms, i.e., $1, $P, $Q, !FTL, and Penny Pincher, on three diverse gesture sets, i.e., NicIcon, HHReco, and Utopiano Alphabet, in a user-independent scenario. Similar conditions were applied to all algorithms, to be executed under the same characteristics. For the algorithms studied, the method agreed to evaluate the error rate and performance rate, as well as the execution time of each of these algorithms. A software testing environment was developed in JavaScript to perform the comparative analysis. The results of this analysis help recommending a recognizer where it turns out to be the most efficient. !FTL (NLSD) is the best recognition rate and the most efficient algorithm for the HHreco and NicIcon datasets. However, Penny Pincher was the faster algorithm for HHreco datasets. Finally, $1 obtained the best recognition rate for the Utopiano Alphabet dataset. © 2020, Insight Society.},
author_keywords={Algorithmic efficiency;  Gesture interaction;  Gesture recognition;  Stroke analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Krings2020139,
author={Krings, S. and Schmidt, J. and Skowronek, P. and Dunkelau, J. and Ehmke, D.},
title={Towards Constraint Logic Programming over Strings for Test Data Generation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12057 LNAI},
pages={139-159},
doi={10.1007/978-3-030-46714-2_10},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084698921&doi=10.1007%2f978-3-030-46714-2_10&partnerID=40&md5=a9b621d72687de804a643dd87b69a202},
affiliation={Niederrhein University of Applied Sciences, Mönchengladbach, Germany; Institut für Informatik, Heinrich-Heine-Universität, Düsseldorf, Germany; periplus instruments GmbH & Co. KG, Darmstadt, Germany},
abstract={In order to properly test software, test data of a certain quality is needed. However, useful test data is often unavailable because existing or hand-crafted data might not be diverse enough to enable desired test cases. Furthermore, using production data might be prohibited due to security or privacy concerns or other regulations. At the same time, existing tools for test data generation are often limited. In this paper, we evaluate to what extent constraint logic programming can be used to generate test data, focusing on strings in particular. To do so, we introduce a prototypical CLP solver over string constraints. As case studies, we use it to generate valid IBAN numbers, calendar dates and specific data in JSON. © 2020, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Justo20201242,
author={Justo, J. and Castro, J.},
title={Application of the TCD for the fracture prediction of rocks with U-shaped notches at different temperatures},
journal={Rock Mechanics for Natural Resources and Infrastructure Development- Proceedings of the 14th International Congress on Rock Mechanics and Rock Engineering, ISRM 2019},
year={2020},
pages={1242-1249},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084644588&partnerID=40&md5=4b692fa3d3df79f1b0429fd717aae40f},
affiliation={Group of Geotechnical Engineering, Universidad de Cantabria, E.T.S. de Ingenieros de Caminos, Canales y Puertos, Santander, Cantabria, Spain},
abstract={This work aims to analyse the fracture behaviour of rocks with U-shaped notches subjected to Mode I loading and to different temperature conditions. To this end, the so called Theory of Critical Distances (TCD) is applied to assess the fracture of two different isotropic rocks with diverse characteristics: A Moleano limestone and a Macael marble. The research is based on the results obtained from an experimental program comprising, for each rock and temperature, 6 tensile splitting (Brazilian) tests and 48 four-point bending tests using single-edge notched bending (SENB) specimens with notch radii varying from 0.15 mm to 15 mm. Likewise, different temperatures have been considered from room temperature up to 250ºC, which is a common range in geo-thermal applications for example. The TCD has proven to offer satisfactory load fracture prediction results for the two studied rocks, considering the notch effect and the temperature as a variable. © 2020 ISRM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2020545,
author={Liu, D. and Ernst, G. and Murray, T. and Rubinstein, B.I.P.},
title={Legion: Best-first concolic testing (competition contribution)},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2020},
volume={12076 LNCS},
pages={545-549},
doi={10.1007/978-3-030-45234-6_31},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084261776&doi=10.1007%2f978-3-030-45234-6_31&partnerID=40&md5=024fd433dd1e6be9a779f42f0270d42e},
affiliation={University of Melbourne, Melbourne, Australia; LMU Munich, Munich, Germany},
abstract={Legion is a grey-box coverage-based concolic tool that aims to balance the complementary nature of fuzzing and symbolic execution to achieve the best of both worlds. It proposes a variation of Monte Carlo tree search (MCTS) that formulates program exploration as sequential decision-making under uncertainty guided by the best-first search strategy. It relies on approximate path-preserving fuzzing, a novel instance of constrained random testing, which quickly generates many diverse inputs that likely target program parts of interest. In Test-Comp 2020 [1], the prototype performed within 90% of the best score in 9 of 22 categories. © The Author(s) 2020.},
author_keywords={Fuzzing;  Monte Carlo Search;  Symbolic Execution},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{ShirleyHelenJudith20201550,
author={Shirley Helen Judith, S. and Ameelia Roseline, A. and Hemajothi, S.},
title={Design of Multiple Input and Multiple Output Antenna for Wi-Max and WLAN Application},
journal={Lecture Notes on Data Engineering and Communications Technologies},
year={2020},
volume={35},
pages={1550-1562},
doi={10.1007/978-3-030-32150-5_156},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083680834&doi=10.1007%2f978-3-030-32150-5_156&partnerID=40&md5=996f03efb3febed3ff334631d0cb42e3},
affiliation={Department of Electronics and Communication Engineering, Panimalar Engineering College, Chennai, India; Department of Electronics and Communication Engineering, Prathyusha Engineering College, Chennai, India},
abstract={In this design, four port MIMO antenna of annular slot is proposed to get better gain and diversity performance of the frequency range from 2 to 6 GHZ for Wi-Max (3.5 GHZ) and WLAN (5 GHZ) application. To get the pattern diversity, the four micro strip feed lines are used and it is isolated by four shorts to maintain isolation. The Micro strip patch antenna are used because of the advantages like low structure in profile, cost of the fabrication is low and support both the circular and the linear polarizations. The antenna performance are analysed by the simulation results which produces the gain, directivity, return loss and radiated power. The antenna proposed used in WLAN and Wi-Max application. The antenna dimensions are thickness 0.8 mm, length 30 mm, width 38 mm. The Flame Retardant (FR4) substrate is used which has the relative permittivity value 4.3. The antenna design proposed is simulated using the Advanced Design System (ADS) software and output is tested using the network analyzer. © Springer Nature Switzerland AG 2020.},
author_keywords={Fabrication;  FR-4;  Gain;  Microstrip antenna;  Return loss;  Wi-Fi;  WLAN},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Hasnain202053649,
author={Hasnain, M. and Pasha, M.F. and Ghani, I. and Mehboob, B. and Imran, M. and Ali, A.},
title={Benchmark Dataset Selection of Web Services Technologies: A Factor Analysis},
journal={IEEE Access},
year={2020},
volume={8},
pages={53649-53665},
doi={10.1109/ACCESS.2020.2979253},
art_number={9027827},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082621801&doi=10.1109%2fACCESS.2020.2979253&partnerID=40&md5=6a57260dd4fbfe4d86b9b9cdb09e2d1d},
affiliation={School of IT, Monash University, Bandar Sunway, 47500, Malaysia; Department of Mathematics and Computer Sciences, Indiana University of Pennsylvania, Indiana, PA  15705, United States; Next Bridge (Pvt.) Ltd., Lahore, Pakistan},
abstract={Web services have emerged as an accessible technology with the standard 'Extensible Mark Up' (XML) language, which is known as 'Web Services Description Language' WSDL. Web services have become a promising technology to promote the interrelationship between service providers and users. Web services users' trust is measured by quality metrics. Web service quality metrics vary in many benchmark datasets used in the existing studies. The selection of a benchmark dataset is problematic to classify and retest web services. This paper proposes a method to rank web services quality metrics for the selection of benchmark web services datasets. To measure the diversity in quality metrics, factor analysis with Varimax rotation and scree plot is a well-established method. We use factor analysis to determine percentage variance among principal factors of four benchmark datasets. Our results showed that the two-factor solution explained 94.501, 76.524, and 45.009% variances in datasets A, B, and D, respectively. A three-factor solution explained 85.085% variance in dataset C. Reliability, and response time quality metrics were predicted as the most dominating quality metrics that contributed to explain the percentage variance in four datasets. Our proposed web metric ranking (WMR) method resulted in reliability as the top-most web metric with (57.62%) score and latency web metric at the bottom-most with (3.60%) score. The proposed WMR method showed a high (96.17%) ranking precision. Obtained results verified that factor solutions after reducing the dimensions could be generalized and used in the quality improvement of web services. In future works, the authors plan to focus on a dataset with dominating quality metrics to perform regression testing of web services. © 2013 IEEE.},
author_keywords={Factor analysis;  quality metrics;  regression testing;  reliability;  response time;  rotated loading;  web services},
document_type={Article},
source={Scopus},
}

@ARTICLE{Turlisova20201233,
author={Turlisova, J. and Jansone, A.},
title={The initial stage of development of a new computer program for the processing of psychophysiological tests},
journal={Advances in Intelligent Systems and Computing},
year={2020},
volume={1131 AISC},
pages={1233-1237},
doi={10.1007/978-3-030-39512-4_188},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081920011&doi=10.1007%2f978-3-030-39512-4_188&partnerID=40&md5=0f94a9ea3477092f232bb18f51b8cca4},
affiliation={Faculty of Science and Engineering, Liepaja University, Liela Street 14, Liepaja, Latvia},
abstract={Nowadays technologies tend to facilitate not only the learning process itself but also analyzing the results of this process, in order to create a detailed correction plan for every learner. Educational and study processes must be appraising in the terms of the perspectives in diverse electronic technology usage and development tendencies to interest contemporary youth in the learning process. The aim of this study is to depict the meaning of the computer programs in the processing of psychophysiological tests. © Springer Nature Switzerland AG 2020.},
author_keywords={Computer assessment;  Computer program;  Visual perception;  Visual – motor integration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2020145,
author={Chen, L. and Fu, J. and Lu, J. and Yang, L. and Ding, C.},
title={Composition and Changing Trends of Birds in Summer in the Qagan Nur Wetland of Inner Mongolia [内蒙古查干淖尔湿地夏季鸟类组成和变化趋势]},
journal={Linye Kexue/Scientia Silvae Sinicae},
year={2020},
volume={56},
number={1},
pages={145-153},
doi={10.11707/j.1001-7488.20200114},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081666413&doi=10.11707%2fj.1001-7488.20200114&partnerID=40&md5=5e804d4a413bcc3954a93d314ac1f0e4},
affiliation={School of Ecology and Nature Conservation, Beijing Forestry University, Beijing, 100083, China; Key Laboratory of Forest Protection of National Forestry and Grassland Administration National Bird Banding Center of China Research Institute of Forest Ecology, Environment and Protection, CAF, Beijing, 100091, China; Beijing Bird Watching Society, Beijing, 100029, China},
abstract={Objective: This study aimed to understand bird composition and quantity change trend in summer, and the effect of habitat changes on bird community composition in order to provide scientific basis for the protection and management of the Qagan Nur wetland of Inner Mongolia in China. Method: By using line- and point-sampling methods, bird surveys at the Qagan Nur wetland were conducted for six consecutive years in August of each year from 2009 to 2014. The dominance index, the diversity index and the evenness index of the bird communities were analysed with the collected data. The IBM SPSS Statistics, Excel software and R software were used to test the bird community differences for different years with the Chi-square test, and analyse the correlation between meteorological factors and bird composition in each year with the linear regression analysis. Result: A total of 171 species in 37 families within 18 orders were recorded. Among them, there were 99 species of summer birds, 53 species of migrants, 16 species of residents, 2 species of vagrants and 1 species of winter bird. According to the habitat chosen by these birds, there were 58 species of grallatores, 50 species of songbirds, 41 species of natatores, 13 species of raptors, 4 species of terrestores, and 5 species of scansores. According to their endangered status, there were 2 species ranked as key protected first class species in China, and 23 species ranked as key protected second class species in China. Baer's Pochard (Aythya baeri) and Siberian Crane (Grus leucogeranus) were listed as critically endangered; the Oriental White Stork (Ciconia boyciana), Saker Falcon (Falco cherrug) and Far Eastern Curlew (Numenius madagascariensis) were listed as endangered; There were 7 species listed as vulnerable and 10 species listed as near threatened by IUCN. There were 73, 44, 156, 126, and 92 species listed on the List of Bird Species in accordance with the migratory bird protection agreement between China and America, Australia, Russia, South Korea and Japan, respectively. In the six years, the similarity index for summer birds was relatively high, and the compositions of the dominant species, the typical species and the rare species changed in different years. The result of the Chi-square test showed that there were significant differences in the number of species, the number of birds and the diversity index across years. The number of orders and families increased significantly from year to year. The number of summer species, the percentage of resident species and the percentage of the relative number of resident birds also increased from year to year. The number of grallatores and natatores decreased, whereas the percentage of the relative number of songbirds increased significantly. The results of the correlation analysis between meteorological factors and bird composition revealed that bird families were significantly positively correlated with the min temperature. The diversity index and the evenness index were extremely significantly positively correlated with the min temperature. The dominance index was extremely significantly negatively correlated with the min temperature. The number of summer species was significantly negatively correlated with the max temperature and was extremely significantly positively correlated with the relative humidity. The percentage of the relative number of natatores was significantly positively correlated with the relative humidity. The number of natatores species was significantly negatively correlated with the average temperature. Conclusion: With the shrinking of the Qagan Nur wetland and lake, the increasing of saline alkali land, grassland and forest land, the species and quantity of grallatores and natatores have a downward trend, while the number of songbirds has increased. It is suggested that long-term avian surveillance systems should be established and that effective management methods should be taken to protect the bird communities and their habitats in the Qagan Nur wetland. © 2020, Editorial Department of Scientia Silvae Sinicae. All right reserved.},
author_keywords={Birds in summer;  Composition;  The Qagan Nur wetland;  Trends},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ata2020122,
author={Ata, N. and Ameur, A.A. and Yilmaz, O.},
title={Bottleneck analysis of Turkish and algerian sheep breeds using microsatellite markers},
journal={IFMBE Proceedings},
year={2020},
volume={78},
pages={122-131},
doi={10.1007/978-3-030-40049-1_16},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080959808&doi=10.1007%2f978-3-030-40049-1_16&partnerID=40&md5=d36e38451f47c64746cb2472d69a01d4},
affiliation={Department of Animal Science, Faculty of Agriculture, Aydin Adnan Menderes University, Aydin, 090100, Turkey; Laboratory of Physiopathology and Biochemistry of Nutrition, Department of Biology, University of Tlemcen, Tlemcen, Algeria},
abstract={The present study was performed to reveal genetic diversity and bottleneck of Turkish and Algerian autochthonous sheep breeds using fifteen microsatellites marker recommended by FAO (2011). Animal material for the study was consisted of 180 head sheep raised in different location in Turkey and Algeria. A total of 349 alleles were detected from fifteen markers studied. The mean number of alleles (23.26), observed heterozygosity (0.76) and polymorphic information content (0.89) findings indicated that noticeable genetic variability in sheep population studied. Fourteen out of the sixteen microsatellite markers studied had a positive FIS value. The mean value of FIS was 0.061.The infinite allele model (IAM), two-phase mutation model (TPM) and stepwise mutation model (SMM) in the Bottleneck software were used to check genetic bottleneck. The Lshaped curve obtained from analyze indicates absence of bottleneck in studied sheep population raised in Turkey and Algeria. Consequently, it can be said that these results will help to develop conservation and breeding strategies for the sheep population. © Springer Nature Switzerland AG 2020.},
author_keywords={Genetic diversity;  Genetic resource;  Microsatellite},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nasser2020352,
author={Nasser, A.B. and Hujainah, F. and Al-Sewari, A.A. and Zamli, K.Z.},
title={An improved jaya algorithm-based strategy for t-way test suite generation},
journal={Advances in Intelligent Systems and Computing},
year={2020},
volume={1073},
pages={352-361},
doi={10.1007/978-3-030-33582-3_34},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077773801&doi=10.1007%2f978-3-030-33582-3_34&partnerID=40&md5=f47e9dd7f8b31033bd7660301d8ff589},
affiliation={Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, Kuantan, Pahang  26300, Malaysia},
abstract={. In the field of software testing, several meta-heuristics algorithms have been successfully used for finding an optimized t-way test suite (where t refers to covering level). T-way testing strategies adopt the meta-heuristic algorithms to generate the smallest/optimal test suite. However, the existing t-way strategies’ results show that no single strategy appears to be superior in all problems. The aim of this paper to propose a new variant of Jaya algorithm for generating t-way test suite called Improved Jaya Algorithm (IJA). In fact, the performance of meta-heuristic algorithms highly depends on the intensification and diversification capabilities. IJA enhances the intensification and diversification capabilities by introducing new operators search such lévy flight and mutation operator in Jaya Algorithm. Experimental results show that the IJA variant improves the results of original Jaya algorithm, also overcomes the problems of slow convergence of Jaya algorithm. © Springer Nature Switzerland AG 2020.},
author_keywords={Improved Jaya algorithm;  Jaya Algorithm;  Meta-heuristics;  T-way testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NezhadShokouhi2020602,
author={NezhadShokouhi, M.M. and Majidi, M.A. and Rasoolzadegan, A.},
title={Software defect prediction using over-sampling and feature extraction based on Mahalanobis distance},
journal={Journal of Supercomputing},
year={2020},
volume={76},
number={1},
pages={602-635},
doi={10.1007/s11227-019-03051-w},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074686452&doi=10.1007%2fs11227-019-03051-w&partnerID=40&md5=26fb6a9c2964e22bf47652ff32ce3dd6},
affiliation={Department of Computer Engineering, Software Quality Lab, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Computer Engineering, Center of Excellence on Soft Computing and Intelligent Information Processing, Ferdowsi University of Mashhad, Mashhad, Iran; Department of Computer Engineering, Ferdowsi University of Mashhad, Mashhad, Iran},
abstract={As the size of software projects becomes larger, software defect prediction (SDP) will play a key role in allocating testing resources reasonably, reducing testing costs, and speeding up the development process. Most SDP methods have used machine learning techniques based on common software metrics such as Halstead and McCabe’s cyclomatic. Datasets produced by these metrics usually do not follow Gaussian distribution, and also, they have overlaps in defect and non-defect classes. In addition, in many of software defect datasets, the number of defective modules (minority class) is considerably less than non-defective modules (majority class). In this situation, the performance of machine learning methods is reduced dramatically. Therefore, we first need to create a balance between minority and majority classes and then transfer the samples into a new space in which pair samples with same class (must-link set) are near to each other as close as possible and pair samples with different classes (cannot-link) stay as far as possible. To achieve the mentioned objectives, in this paper, Mahalanobis distance in two manners will be used. First, the minority class is oversampled based on the Mahalanobis distance such that generated synthetic data are more diverse from other minority data, and minority class distribution is not changed significantly. Second, a feature extraction method based on Mahalanobis distance metric learning is used which try to minimize distances of sample pairs in must-links and maximize the distance of sample pairs in cannot-links. To demonstrate the effectiveness of the proposed method, we performed some experiments on 12 publicly available datasets which are collected NASA repositories and compared its result by some powerful previous methods. The performance is evaluated in F-measure, G-Mean, and Matthews correlation coefficient. Generally, the proposed method has better performance as compared to the mentioned methods. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Feature extraction;  Mahalanobis distance;  Over-sampling;  Software defect prediction;  Software metrics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao2020443,
author={Gao, W. and Song, Y. and Ma, B.},
title={Evolutionary Generation of Test Data Based on Reduction of Initial Population Data},
journal={Lecture Notes in Electrical Engineering},
year={2020},
volume={594},
pages={443-451},
doi={10.1007/978-981-32-9698-5_50},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072991473&doi=10.1007%2f978-981-32-9698-5_50&partnerID=40&md5=9fd079afeebc5a7686a62e95a045ca64},
affiliation={School of Computer and Information Technology, Mudanjiang Normal University, Mudanjiang, Heilongjiang  157011, China; College of Health Management, Mudanjiang Medical University, Mudanjiang, Heilongjiang  157011, China},
abstract={When Genetic Algorithm is used to evolve test data for path coverage, if the similarity of some test data of initial population is high, it will cause the lack of diversity of individuals, which directly affects the rate of optimal solution at subsequent evolution generation. A method for evolutionary generation of test data based on reduction of initial population data is proposed. A program will be expressed as a binary tree according to the branch number of the tested program. And all the executable paths of a program will be represented as binary encoding, different path of that program are obtained, test data reduction is conducted according to the similarity. The reduced data are evolved as the initial population of the Genetic Algorithm to generate test data to meet the requirements. The proposed method is used to generate test data of three benchmark programs, and compared with existing method, the experimental results show that the proposed method can effectively generate test data after reduction of initial population data, and have better performance in the number of generations and running time. © 2020, Springer Nature Singapore Pte Ltd.},
author_keywords={Data reduction;  Equivalence class;  Executable path;  Genetic Algorithm;  Initial population data;  Path coverage},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Laitupa2019,
author={Laitupa, I.W.},
title={Effectiveness of CCDP-IFAD Program to Fishermen's Incomes of Fish Smoked in Ternate City and the Right Strategy (Case Study fish Smoked IndustriGroup at Village Faudu, HiriIsland)},
journal={Journal of Physics: Conference Series},
year={2019},
volume={1364},
number={1},
doi={10.1088/1742-6596/1364/1/012079},
art_number={012079},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078320341&doi=10.1088%2f1742-6596%2f1364%2f1%2f012079&partnerID=40&md5=80fa7bf66a3f659fda4b4d8baa7ee7ba},
affiliation={Muhammadiyah University of North Maluku, Maluku, Indonesia},
abstract={Coastal Community Development Project-International Fund for Agricultural Development (CCDP-IFAD) program presents to effort economic empowerment, one of the industrial development effort of smoked fish. The city of Ternate is located in a geographical area and wide in North Maluku, two great potentials that can be optimized for the development of fishermen's economy. So that, this research was conducted as evaluative research and expected useful to its industrial development. The purposes of this research are fathom of; (1) the program effectiveness of CCDP-IFAD through fishermen's incomes of fish smoked in Faudu Village of Ternate City and (2) to formulate the right strategy in fish-smoked industrial development in Kelurahan Faudu. The research method was qualitative and quantitative research. Inference analyses sampling in pairs to compare revenue value before and after doing the program, and SWOT analysis to formulate right strategy. The result of the research showed that (1) inference analyses sampling test concluded that the program was not significantly effect to fishermen's revenue. (2) the result of SWOT analyses had the best strategy was optimization the power of resources both natural or human resources and supporting institutions and the chance Such as high market absorption, tourism and still open diversity to prevent the weakness and threatens. The strategy was evaluated to stimulate quality of life through developing business aspects. © Published under licence by IOP Publishing Ltd.},
author_keywords={development strategy;  fish smoked industry;  The effectiveness of CCDP-IFAD},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ardiyaningrum2019,
author={Ardiyaningrum, M. and Retnowati, T.H. and Jailani and Trisniawati},
title={Online Measurement to Assess A Problem Solving Skills Based on Multimedia Instrument},
journal={Journal of Physics: Conference Series},
year={2019},
volume={1339},
number={1},
doi={10.1088/1742-6596/1339/1/012065},
art_number={012065},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077813071&doi=10.1088%2f1742-6596%2f1339%2f1%2f012065&partnerID=40&md5=ede85dd9edaa050dcd3c13233282b493},
affiliation={Post Graduate Program, Universitas Negeri Yogyakarta, Indonesia; Department of Madrasah ibtida'Iyah Teacher Education, Universitas Alma Ata, Indonesia; Department of Evaluation and Research Education, Universitas Negeri Yogyakarta, Indonesia; Department of Mathematic Education, Universitas Negeri Yogyakarta, Indonesia; Department of Primary Teacher Education, Universitas Sarjanawiyata Tamansiswa, Indonesia},
abstract={Problem solving ability is a goal that must be achieved in the learning process of all subjects at the elementary school level. Thus, there is a need for identification of problem solving abilities from prospective primary school teachers. The effort to identify this capability was carried out using Google Form as a test tool that facilitated researchers to carry out problem solving ability tests which is multimedia based. This research is a development research, which uses the development of the 4-D approach. The development is carried out on instruments for assessing mathematical problem solving abilities. The test participants consisted of 230 students from the elementary school teacher education program and the Madrasah Ibtida'iyah (Islamic elementary school) Teacher Education Study Program in the Special Region of Yogyakarta. The results indicate that the condition of students' abilities that are quite diverse, with abilities below 0 are still quite high. This indicates that students' problem solving abilities, prospective elementary school/Madrasah Ibtida'iyah teachers, still need to be improved. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wei201996,
author={Wei, F. and Meng, L. and Qiang, L. and Leilei, L.},
title={The Synchronized Monitoring System for Operation Capability of Diver},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={96-98},
doi={10.1145/3388218.3388221},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086182361&doi=10.1145%2f3388218.3388221&partnerID=40&md5=8b5953d77c2f193c487837a5790150b1},
affiliation={Sanda University, Shanghai, China; Naval Specialty Medical Center, Shanghai, China},
abstract={Objective:Divers underwater operation capacity will gradually weakened as underwater operation time, until completely lost his ability to do homework, but the degree of weakened at different time after operation can't test.This study aimed at laboratory trained divers in the complete test ability of homework problems at work, a kind of design can be in real-time test platform diving underwater operation in testing the ability, to monitor the divers underwater operation ability during the test.Methods:The underwater operation ability test was divided into two aspects: physical test and reaction ability test.[1]. In this paper, we can carry out the physical test without decompression by using the method of the transition cabin of the pressurized cabin, and carry out the real-time test of the underwater reaction capability using the self-developed underwater operation capability real-time test device. Results: Using the laboratory pressurized chamber combined with real-time power bicycle do physical test, using the mature ability to respond to test software with waterproof, resistance to high pressure processing technology for underwater real-time response ability test, achieve real-time monitoring of underwater diving personnel when testing the effect of the operation ability[2].Conclusion: the test method can solve the problem that the operation ability cannot track and monitor in real time, which provides a convenient basis for the study of the ergonomic study of the divers. © 2019 ACM.},
author_keywords={compression chambers;  real-time testing;  Work ability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mingxi2019,
author={Mingxi, Z. and Qian, W.},
title={Research on an L-band polarization diversity monopole antenna},
journal={ICSIDP 2019 - IEEE International Conference on Signal, Information and Data Processing 2019},
year={2019},
doi={10.1109/ICSIDP47821.2019.9173084},
art_number={9173084},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091941317&doi=10.1109%2fICSIDP47821.2019.9173084&partnerID=40&md5=904e66e125efa29d9dc9d79872f20b5f},
affiliation={School of Electronic Information Engineering, Nanjing University of Aeronautics and Astronautics, Nanjing, Jiangsu, 210016, China; Aviation Key Laboratory of Science and Technology on High Performance Electromagnetic Windows, AVIC Research Institute for Special Structures of Aeronautical Composites, Jinan, Jiangsu, 250023, China},
abstract={An L band polarization diversity antenna based on printed dipole was proposed for the application of fully polarized MIMO electronic system. The antenna used two printed circular monopoles placed orthogonal to each other as radiators to form a dual-polarization receiving channel. The co-planar waveguide was used to excite the radiator, and the transmission line bended method was used to reduce the coupling between the ports. Reflective floor was used to achieve unidirectional radiation pattern performance. The antenna designed and optimized by full-wave electromagnetic simulation software was processed and tested according to the design result. The test results show that the return loss is less than -10dB in the bandwidth of 1.26GHz to 1.34GHz. The polarization port isolation of the antenna is greater than -20dB. The gain is greater than 6.6dB, and the cross-polarization level is about -18dB. The results show the feasibility of the antenna design proposed in this paper. © 2019 IEEE.},
author_keywords={Cross polarization level;  Dual polarized antenna;  Monopole antenna;  polarity diversity;  Polarization isolation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fang2019727,
author={Fang, W. and Stones, R.J. and Marbach, T.G. and Wang, G. and Liu, X.},
title={Towards a latin-square search engine},
journal={Proceedings - 2019 IEEE Intl Conf on Parallel and Distributed Processing with Applications, Big Data and Cloud Computing, Sustainable Computing and Communications, Social Computing and Networking, ISPA/BDCloud/SustainCom/SocialCom 2019},
year={2019},
pages={727-735},
doi={10.1109/ISPA-BDCloud-SustainCom-SocialCom48970.2019.00110},
art_number={9047384},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085485695&doi=10.1109%2fISPA-BDCloud-SustainCom-SocialCom48970.2019.00110&partnerID=40&md5=7020b01668d7ed6ac79de7d136e67c59},
affiliation={College of Computer Science, Nankai University, Tianjin, China},
abstract={Latin squares are combinatorial matrices that are widely used in diverse areas of research such as codes and cryptography, software testing, mathematical research, and experimental designs. All of these fields would benefit from a search engine for Latin squares. One major obstacle to developing a Latin-square search engine is that any Latin square has a large number of equivalent Latin squares, which are contained in multiple equivalence classes, and thus we need an efficient online method for canonical labelling Latin squares. Canonical labelling usually proceeds via the Nauty graph isomorphism software, but this incurs conversion costs. Moreover, the canonical labels are practically random members of their equivalence classes. A second obstacle is how large amounts of searchable Latin-square data may be stored efficiently. In this paper, we design data structures and algorithms suitable for a Latin-square search engine. We use a tree-based data structure for storing large numbers of Latin squares that also enables efficient search capabilities. We design an efficient canonical labelling algorithm (via partial Latin squares, PLSs) which does not require graph conversion, facilitates compression, and the labels are more humanly meaningful. We implement and experiment with a skeletal prototype of the Latin-square search engine. Experimental results confirm that the PLS method is faster than Nauty, and has reduced space requirements. © 2019 IEEE.},
author_keywords={Information retrieval;  Latin square;  Mathematical knowledge management;  Partial latin square;  Search engine;  Storage},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Garmatyuk2019,
author={Garmatyuk, D. and Simms, M. and Mudaliar, S.},
title={UWB Multicarrier Radar Target Scene Identification with 2-D Diversity Utilization and GLRT Refinement},
journal={IEEE Sensors Letters},
year={2019},
volume={3},
number={12},
doi={10.1109/LSENS.2019.2958013},
art_number={8926366},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082633872&doi=10.1109%2fLSENS.2019.2958013&partnerID=40&md5=12a71bade200419e273752d7fca0bdcc},
affiliation={Department of Electrical and Computer Engineering, Miami University, Oxford, OH  45056, United States; Northrop Grumman, Linthicum Heights, Anne Arundel County, MD  21090, United States; Sensors, Air Force Research Laboratory, Wright-Patterson Air Force Base, OH  45433, United States},
abstract={In this letter, we present a novel approach to target scene identification using ultrawideband software-defined radar system, which utilizes frequency-angle diversity and refines the decision via the numerically implemented generalized likelihood ratio test. The two stages of the method are discussed, as well as the Monte Carlo statistical simulation and its results. The experimental results are presented for two very similar target scenes, each of which had been successfully identified using training data available for five different scenarios. The proposed approach is tailored for use in multicarrier radar sensor systems; specifically, an orthogonal frequency division multiplexing system is considered in this letter. © 2017 IEEE.},
author_keywords={frequency diversity;  generalized likelihood ratio test (GLRT);  multiple hypotheses testing (MHT);  orthogonal frequency division multiplexing (OFDM) radar;  Sensor signal processing;  target identification;  ultrawideband (UWB) radar},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Waheed2019,
author={Waheed, T. and Aqeel-Ur-Rehman and Shaikh, F.K. and Khan, I.U.},
title={WBAN Performance Evaluation at PHY/MAC/Network Layer using Castalia Simulator},
journal={MACS 2019 - 13th International Conference on Mathematics, Actuarial Science, Computer Science and Statistics, Proceedings},
year={2019},
doi={10.1109/MACS48846.2019.9024824},
art_number={9024824},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082487391&doi=10.1109%2fMACS48846.2019.9024824&partnerID=40&md5=ca97434ebc2e01a314a740f8f5f863bd},
affiliation={Usman Institute of Technology, Electrical Department, Karachi, Pakistan; Sir Syed University of Engineering and Technology, Faculty of Basic and Applied Sciences, Karachi, Pakistan; Mehran University of Engineering and Technology, Telecommunication Department, Jamshoro, Pakistan; FEST, Hamdard University, Department of Computing, Karachi, Pakistan},
abstract={WBANs are gaining acceptance in academia and industry due to their support for diverse applications, the most prominent of which is remote patient healthcare. With the tremendous effort directed for Smart healthcare solutions, WBANs are evolving side by side with cloud computing and IoT. With the changing technological scenario new communication standards, protocols and mechanisms are introduced to make WBANs acceptable. WBANs comprises of wearable/implanted sensors that monitor physiological parameters such as heartbeat, blood pressure, oxygen without restricting the patient in hospital environment. Because of WBANs, tele-medicine will experience a tremendous growth and efficient design and implementation will remain a focus of ongoing research. New Applications, energy conservation, MAC techniques, Routing protocols, security considerations are of major interest for the industry. As WBANs carries critical data so it is necessary that data should be delivered without loss with a smaller number of retransmissions. In addition to this the standard introduced for WBANs will gain attention and new healthcare products based on it will flood the market. WBANs has been implemented and tested through hardware using FPGAs and through software with the help of Simulators. Various Simulators has been used to evaluate the performance of WBANs including NS2, NS3, OPNET and Qualnet. In this paper we have implementation of WBANs on Castalia Simulator and evaluated its performance for 1 -hop topology. This paper presents the various results for Packet loss when transmission power, transmission rate and RAP periods are varied. © 2019 IEEE.},
author_keywords={Castalia;  healthcare;  MAC;  OMNET++;  Routing;  Simulation;  WBAN},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lanui2019158,
author={Lanui, A. and Chiew, T.K.},
title={A Cloud-Based Solution for Testing Applications' Compatibility and Portability on Fragmented Android Platform},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2019},
volume={2019-December},
pages={158-164},
doi={10.1109/APSEC48747.2019.00030},
art_number={8945645},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078158536&doi=10.1109%2fAPSEC48747.2019.00030&partnerID=40&md5=c5da47ed2b502270ddf524d19578a556},
affiliation={Android Development Department, Red Ant Technology Sdn. Bhd., Subang Jaya, Malaysia; Department of Software Engineering, University of Malaya, Kuala Lumpur, Malaysia},
abstract={Testing is a vital activity in software development. The ISO/IEC has defined a standard for system and software quality models called ISO/IEC 25010:2011 to be a guideline and scope for testing any applications. Testing of mobile applications according to this standard, however, is more challenging than other types of software. The diversity of Android devices and various versions of Android operating system, for example, has created a large fragmentation of the Android platform. This fragmentation hinders testing of Android applications especially in relation to portability and compatibility. Existing solutions are either neglecting portability and compatibility issues or lack flexibility in fulfilling needs of the different organizations. We propose a cloud testing model to address the fragmentation of Android platform and provide automated application testing services on the actual devices. The model can be configured in the public, private or hybrid setups to suit individual organizations' needs and budget. A prototype was built based on the model. 10 Android testers used the prototype and the Android Emulator to perform mobile application testing. Results show that the model has the potential to manage the challenging portability and compatibility testing on the Android platform in a flexible and scalable manner. © 2019 IEEE.},
author_keywords={Android;  cloud;  compatibility;  fragmentation, testing;  portability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kovalenko20191197,
author={Kovalenko, K.E. and Reavie, E.D. and Bramburger, A.J. and Cotter, A. and Sierszen, M.E.},
title={Nearshore-offshore trends in Lake Superior phytoplankton},
journal={Journal of Great Lakes Research},
year={2019},
volume={45},
number={6},
pages={1197-1204},
doi={10.1016/j.jglr.2019.09.016},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075437214&doi=10.1016%2fj.jglr.2019.09.016&partnerID=40&md5=5f0155be34053c644ec5df8ab0b69195},
affiliation={Natural Resources Research Institute, University of Minnesota, Duluth, United States; U.S. Environmental Protection Agency, Office of Research and Development, National Health and Ecological Effects Research Laboratory, Mid-Continent Ecology Division, Duluth, MN, United States},
abstract={Changes in phytoplankton community composition and structure can have broad-scale ecosystem effects; however, drivers of species diversity in planktonic systems are not well understood. In lakes, a common but not thoroughly tested assumption is that shallow, nearshore waters are much more diverse and productive, and contribute considerably more material and energy to pelagic food webs than deeper waters farther offshore. Lake Superior is a large, cold, oligotrophic freshwater system which can provide insight into community organization under oligotrophic conditions. We used epilimnion and deep chlorophyll layer phytoplankton data from a lake-wide sampling program conducted in 2011 and 2016 to test whether assemblage composition, total algal biovolume, cell concentrations, diversity, and richness vary with depth. Although lake depth was an important factor in structuring assemblage composition, there were no clear nearshore-offshore gradients in cell density or biovolume despite the exposure of nearshore areas to higher concentrations of watershed-derived nutrients. Shannon diversity increased slightly with increasing depth, whereas richness was uncorrelated. Understanding of the nearshore-offshore patterns in phytoplankton community characteristics in the Great Lakes has implications for designing monitoring strategies and for considering how further changes in climate and nutrient deposition would affect the base of the food web. © 2019 International Association for Great Lakes Research},
author_keywords={Algal community;  Deep chlorophyll maximum;  Littoral-pelagic gradient;  Shannon diversity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Atsbha2019,
author={Atsbha, T. and Wayu, S. and Gebretsadkan, N. and Gebremariam, T. and Giday, T.},
title={Rehabilitation of indigenous browse plant species following exclosure established on communal grazing lands in South Tigray, Ethiopia, and implication for conservation},
journal={Ecological Processes},
year={2019},
volume={8},
number={1},
doi={10.1186/s13717-019-0197-1},
art_number={43},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075435016&doi=10.1186%2fs13717-019-0197-1&partnerID=40&md5=306fcb411e7b2c8431fe69866131e30e},
affiliation={Tigray Agricultural Research Institute, Alamata Agriculture Research Center, P.O. Box 56, Alamata, Ethiopia},
abstract={Background: Despite the wide use of indigenous browse plant species, there is almost no information on the rehabilitation of indigenous browse species following area exclosure (AE) established on communal grazing lands (CGL) in Southern Tigray. The objectives of this study were to assess the rehabilitation of browse plant species following AE establishment on CGL. A total of 61 and 59 plots of 10 × 10 m2 size were laid down at 50-m intervals along parallel line transects at AE and GCL, respectively. Data collected on vegetation attributes were subjected to analysis of t test (unequal variances) using R-software. Results: The Shannon diversity index of the browse plant species was 1.25 and 0.81 in AE and CGL, respectively (P &lt; 0.001). The overall population structure of browse plant species in the AE shows a reverse J-shaped population curve and “good” regeneration status, which reveals that the future communities may be sustained. Leaf biomass and basal area of browse plant species were significantly higher in the AE than in CGL (P &lt; 0.001). After exclusion of grazing, AE was found to have positive effects on diversity and aboveground biomass of browse plant species. Conclusions: The study gives an understanding of the diversity, the pattern of population and regeneration of the browse plant species, which may help in the management and conservation of the species. Our results indicate that grazing exclusion is an effective management strategy to restore browse plant species. We concluded that the establishment of AE had a positive effect on the rehabilitation of browse plant species diversity and improved population structure and regeneration potentials of degraded grazing lands. Long-term monitoring and evaluation systems will be required to gain an informed understanding of the roles played by area exclosures in the rehabilitation and conservation of browse palnt species diversity. © 2019, The Author(s).},
author_keywords={Browse;  Rehabilitation;  Restoration},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kumar2019,
author={Kumar, S. and Kaltenberger, F. and Ramirez, A. and Kloiber, B.},
title={An SDR implementation of WiFi receiver for mitigating multiple co-channel ZigBee interferers},
journal={Eurasip Journal on Wireless Communications and Networking},
year={2019},
volume={2019},
number={1},
doi={10.1186/s13638-019-1512-3},
art_number={224},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071928623&doi=10.1186%2fs13638-019-1512-3&partnerID=40&md5=5ce3d7793add92cc2449747c532277dd},
affiliation={Communication Systems, Eurecom, Sophia Antipolis, Biot, France; Siemens AG Corporate Technology, Munich, Germany},
abstract={Machine-to-machine (M2M) communication is one of the vertical sectors that will benefit from 5G communication systems, but today, these systems are still dominated by technologies such as ZigBee and WiFi. An M2M scenario will experience dense deployment of ZigBee and WiFi nodes in order to route the data from one end to the other. In the 2.4 GHz industrial, scientific, and medical (ISM) band, both of the technologies perform co-channel overlapped operation and hence face severe cross technology co-channel interference (CCI). In contrast to cellular systems, which solve the CCI by centralized coordination through the base station, addressing CCI in the ISM band is non-trivial due to heterogeneous wireless technologies and the lack of centralized coordination. In this work, we first present interference mitigating receiver architectures for OFDM-based WiFi using single and multiple antennas. Our single antenna work is based on the localized estimation of excess noise caused by single and multiple co-channel narrowband interferers and scaling the log-likelihood ratios (LLRs) of the affected WiFi subcarriers. The simulation shows our method achieves a significant gain in SNR compared to the conventional method for a given packet error rate (PER) criterion. Next, we discuss maximal ratio combiner with LLR scaling (MLSC), which is a multi-antenna extension to our previous work. The simulation shows MLSC achieves diversity gain apart from the gain in SNR. Further, we propose soft-bit maximal ratio combiner with LLR scaling (SB-MLSC). SB-MLSC is an easy to implement version of MLSC. However, diversity combining in SB-MLSC is performed by combining the LLRs. Nonetheless, simulations show equivalence in performance by SB-MLSC and MLSC. Finally, as a significant part of this work, we implemented all our methods using a software-defined radio (SDR) and performed over-the-air (OTA) testing in the 2.4-GHz ISM band using standard WiFi and ZigBee frames. Results of OTA tests fall in complete agreement with our simulations indicating the practical applicability of our methods. Our methods apply to all the standards and related radio transmission techniques which are based on OFDM and face narrowband co-channel interference. Additionally, since our work focuses only on receiver side modifications, they can be integrated with the existing infrastructure with minimal modifications. © 2019, The Author(s).},
author_keywords={Co-channel interference;  Interference mitigation;  Software-defined radio;  WiFi-ZigBee},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gao201942,
author={Gao, X. and Xu, Y. and He, X. and Zhang, D. and Yang, L. and Cui, T.},
title={Design and Experiment of Diversion Turbine of Air-assisted High Speed Maize Precision Seed Metering Device [气送式高速玉米精量排种器导流涡轮设计与试验]},
journal={Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery},
year={2019},
volume={50},
number={11},
pages={42-52},
doi={10.6041/j.issn.1000-1298.2019.11.005},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076426569&doi=10.6041%2fj.issn.1000-1298.2019.11.005&partnerID=40&md5=ef014bc3d4cc168dc561badddb9b66d5},
affiliation={College of Engineering, China Agricultural University, Beijing, 100083, China; Key Laboratory of Soil-Machine-Plant System Technology, Ministry of Agriculture and Rural Affairs, Beijing, 100083, China; College of Mechanical and Electrical Engineering, Tarim University, Alar, 843300, China},
abstract={In order to make the flow field distributed orderly in the air-assisted high speed maize precision seed metering device, thereby improving the working performance of the seed metering device, three kinds of structure type diversion turbine were designed. Through the simulation and theoretical analysis of CFD method, the diversion turbine can effectively improve the air flow inside the seeding device and increase air speed where type hole in outer ring. At the air flow rate, the pressing force was increased, and the C diversion turbine with a large windward angle and a curved structure had better disturbance and guiding, and the effect was optimal. In order to obtain the best performance parameters of the C diversion turbine installed, the operation velocity, quantity of feeding and wind pressure were used as test factors, and the qualified index, the missing index and the multiple index were used as test indicators. A three-factor quadratic rotation orthogonal combination test was performed and the multiple regression analysis and response surface analysis of the test data were carried out by Design-Expert 8.0.6 software, and the influence of various factors on the indicators was obtained. The multi-objective optimization method was used to determine the optimal combination of parameters: the operation velocity was 9.8 km/h, the quantity of feeding was 1.8 kg/min, and wind pressure was 8 kPa and the qualified index of seed metering was the highest. At this time, the qualified index was 91.32%, the missing index was 2.83%, and the multiple index was 5.85%. The comparison verification test was carried out on the optimization results, and the comparison was made under the same conditions with the uninstalled diversion turbine seed metering device, and it was found that the installation of the diversion turbine can effectively improve the working performance of the seed metering device. © 2019, Chinese Society of Agricultural Machinery. All right reserved.},
author_keywords={Computational fluid dynamic;  Diversion turbine;  High speed seed metering device;  Maize},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang201951,
author={Zhang, Z. and Dai, J. and Zhao, L. and Qin, S.},
title={A web services testing approach based on difference measurement and adaptive random testing},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={51-57},
doi={10.1145/3371676.3371703},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078349783&doi=10.1145%2f3371676.3371703&partnerID=40&md5=fc33021ee67f89c564581e4a85a27035},
affiliation={Jiangsu University, School of Computer Science and Communication Engineering, Zhenjiang, 212013, China},
abstract={Nowadays, people's demand for Web services is increasing, but in the process of obtaining these services, there are some problems in the service, which have not been detected, resulting in a poor experience. Therefore, this paper proposes a difference measurement method based on FSCS (Fixed Sized Candidate Set) algorithm, which improves the traditional ART (Adaptive Random Testing) algorithm. By comparing the differences of each method in Web Services, the farthest method is selected for testing, which improves the testing efficiency and improves the service experience. The method first selects one of the multiple services that may have a potential error service for testing, each time picks the farthest service in the combined service, and then selects the farthest method from the service as a test case, and then measures the differences between the methods in the service, compare the test results with the expected results, so that the problems in the service can be effectively detected. The experimental results show that the proposed method based on difference metric and adaptive random test can detect the existing methods in the service and improve the detection efficiency. © 2019 Association for Computing Machinery.},
author_keywords={Adaptive random testing;  Diversity;  Software testing;  Testing system;  Web Services},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khan2019641,
author={Khan, W.A. and Bi, T. and Jia, K.},
title={Fault Nature Identification Based on Local End Data for Single Phase Adaptive Autoreclosing Scheme},
journal={iSPEC 2019 - 2019 IEEE Sustainable Power and Energy Conference: Grid Modernization for Energy Revolution, Proceedings},
year={2019},
pages={641-646},
doi={10.1109/iSPEC48194.2019.8975058},
art_number={8975058},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079506791&doi=10.1109%2fiSPEC48194.2019.8975058&partnerID=40&md5=500de6e23e140bb77e6f037f5194a112},
affiliation={North China Electric Power University, State Key Laboratory for Alternate Electrical Power System, Renewable Energy Sources, Beijing, China; Faculty of Engineering, Lahore Leads University, Lahore, Pakistan},
abstract={Fault nature identification is deemed indispensable for protection of transmission lines. As the overhead transmission lines are prone to atmospheric conditions. Therefore most of the faults, appears on high voltage (HV) or extra high voltage (EHV) transmission lines are temporary in nature. Temporary faults are followed by a secondary arc. Therefore the identification of secondary arc extinction instant is also important to perform a safest reclosing. Hence, adaptive auto-reclosure is one of the promising solutions for the same. In this paper a Continuous wavelet transform with Fast Fourier transform (CWTFT) based algorithm is presented to first identify the fault nature i.e. temporary or permanent, as well as to detect the secondary arc extinction instant in minimum time. Fault nature identification is done by computing an index. The index will retain a high value during secondary arc period and will attain a zero value after the arc is extinguished. This fact is used to detect arc extinction instant. The technique is also compatible with both shunt compensated and uncompensated transmission lines and is tested under various compensation levels. The efficiency of the presented algorithm is tested using MATLAB software for laborious simulations and for a diversity of fault conditions i.e. at different fault locations, load angles and fault resistance, where in all cases, maximum consistency has been achieved. © 2019 IEEE.},
author_keywords={Autoreclosing;  Continuous wavelet transform;  EHV transmission line},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tokumoto20191206,
author={Tokumoto, S. and Takayama, K.},
title={PHANTA: Diversified test code quality measurement for modern software development},
journal={Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
year={2019},
pages={1206-1207},
doi={10.1109/ASE.2019.00138},
art_number={8952538},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078956247&doi=10.1109%2fASE.2019.00138&partnerID=40&md5=e6cf453a09c6ca499ef7fb62be5138a7},
affiliation={Fujitsu Laboratories Ltd., Japan},
abstract={Test code is becoming more essential to the modern software development process. However, practitioners often pay inadequate attention to key aspects of test code quality, such as bug detectability, maintainability and speed. Existing tools also typically report a single test code quality measure, such as code coverage, rather than a diversified set of metrics. To measure and visualize quality of test code in a comprehensive fashion, we developed an integrated test code analysis tool called Phanta. In this show case, we posit that the enhancement of test code quality is key to modernizing software development, and show how Phanta's techniques measure the quality using mutation analysis, test code clone detection, and so on. Further, we present an industrial case study where Phanta was applied to analyze test code in a real Fujitsu project, and share lessons learned from the case study. © 2019 IEEE.},
author_keywords={Mutation Testing;  Software Testing;  Test Code},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2019305,
author={Chen, J. and Wang, G. and Hao, D. and Xiong, Y. and Zhang, H. and Zhang, L.},
title={History-guided configuration diversification for compiler test-program generation},
journal={Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
year={2019},
pages={305-316},
doi={10.1109/ASE.2019.00037},
art_number={8952321},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078940407&doi=10.1109%2fASE.2019.00037&partnerID=40&md5=490917d63582c3f07a85ff12ee191051},
affiliation={College of Intelligence and Computing, Tianjin University, Tianjin, China; Key Laboratory of High Confidence Software Technologies, Peking University, MoE, China; Department of Computer Science and Technology, EECS, Peking University, Beijing, China; University of NewcastleNSW, Australia},
abstract={Compilers, like other software systems, contain bugs, and compiler testing is the most widely-used way to assure compiler quality. A critical task of compiler testing is to generate test programs that could effectively and efficiently discover bugs. Though we can configure test generators such as Csmith to control the features of the generated programs, it is not clear what test configuration is effective. In particular, an effective test configuration needs to generate test programs that are bug-revealing, i.e., likely to trigger bugs, and diverse, i.e., able to discover different types of bugs. It is not easy to satisfy both properties. In this paper, we propose a novel test-program generation approach, called HiCOND, which utilizes historical data for configuration diversification to solve this challenge. HiCOND first infers the range for each option in a test configuration where bug-revealing test programs are more likely to be generated based on historical data. Then, it identifies a set of test configurations that can lead to diverse test programs through a search method (particle swarm optimization). Finally, based on the set of test configurations for compiler testing, HiCOND generates test programs, which are likely to be bug-revealing and diverse. We have conducted experiments on two popular compilers GCC and LLVM, and the results confirm the effectiveness of our approach. For example, HiCOND detects 75.00%, 133.33%, and 145.00% more bugs than the three existing approaches, respectively. Moreover, HiCOND has been successfully applied to actual compiler testing in a global IT company and detected 11 bugs during the practical evaluation. © 2019 IEEE.},
author_keywords={Compiler Testing;  Configuration;  History;  Search},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soto20191230,
author={Soto, M.},
title={Improving patch quality by enhancing key components of automatic program repair},
journal={Proceedings - 2019 34th IEEE/ACM International Conference on Automated Software Engineering, ASE 2019},
year={2019},
pages={1230-1233},
doi={10.1109/ASE.2019.00147},
art_number={8952342},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078916585&doi=10.1109%2fASE.2019.00147&partnerID=40&md5=8a24d3c8de02f5d45dd736b45f77e198},
affiliation={Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes. © 2019 IEEE.},
author_keywords={Automatic Program Repair;  Patch Quality},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lin201914959,
author={Lin, W. and Wang, D. and Cui, H. and Li, N.},
title={Application effect analysis of the test system of track and field web course based on software programming method},
journal={Cluster Computing},
year={2019},
volume={22},
pages={14959-14971},
doi={10.1007/s10586-018-2463-x},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044036351&doi=10.1007%2fs10586-018-2463-x&partnerID=40&md5=5636a18c31c7e68e1cfe8849c466e79e},
affiliation={College of Physical Education and Sports Science, Guangzhou University, Guangzhou, China; College of Physical Education and Sports Science, Shaanxi Normal University, Xi’an, China},
abstract={This paper tries to improve systematicness of the test module’s construction of the track and field (T&F) web course in physical education schools. Methods: literature study, expert interview, system design method, software programming method, questionnaire survey. Results: first, the test interface has the controllable multi-functional keys, the object and navigation path is easy to use and the links are smooth. Second, the question quality module of the question library is featured in its abundant test questions, various question types, reasonable evaluation standards and proper question quantity. Third, the diversity of the question combination helps inspire students and their creative thinking. Fourth, the effect of test administration is excellent, which can form a test paper quickly and easily, and possess functions of marking prompts and statistics. Conclusions: this research is applicable to the web-based test pattern of track and field teaching for physical education majors (selective course, required course, minor course), the operation guidance form for the test shall be strengthened; the editing and update of test papers shall be adjusted and arranged timely according to the teaching practice problems. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Application effect;  Physical education schools;  Test system;  Track and field;  Web course},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Naith201950,
author={Naith, Q. and Ciravegna, F.},
title={The key considerations in building a crowd-testing platform for software developers},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={50-57},
doi={10.1145/3371238.3371247},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076832242&doi=10.1145%2f3371238.3371247&partnerID=40&md5=293d104a3845c61ef9936402b5f00c7c},
affiliation={University of Sheffield, University of Jeddah, Sheffield, United Kingdom; University of Sheffield, Sheffield, United Kingdom},
abstract={External testing of mobile software on a larger number of mobile devices by several users is often needed to ensure quality. Currently, the evidence as to what extent developers accept large-scale crowd-testing is limited. This paper aims to (1) gauge developers’ perspectives with respect to the participation of the public and anonymous crowd testers, with varied experiences; (2) gather the developers’ needs that could reduce their concerns of dealing with the public crowd testers and increase the opportunity of using the crowd-testing platforms. An online exploratory survey, conducted to included 50 Android and iOS developers from different countries with diverse experiences. This paper revealed several findings including the information that must be provided by developers and crowd testers for achieving effective crowd-testing process; the factors that can ensure the reliability and accuracy of the results provided by the public crowd testers. The findings conclude that (90%) of developers are potentially willing to perform testing via the public crowd testers worldwide. This on condition that several fundamental features were available which enable them to perform more realistic tests without artificial environments on large numbers of devices. The results also demonstrated that a group of developers does not consider testing as a serious job that they have to pay for, which can affect the gig-economy and global market. We aim at helping the individual or small development teams who have limited resources to perform large-scale testing of their products. © 2019 Association for Computing Machinery.},
author_keywords={Gig-economy;  Large-scale crowd-testing;  Mobile App testing;  Public and Anonymous Crowd Testers},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Song2019,
author={Song, D. and Lee, M. and Oh, H.},
title={Automatic and scalable detection of logical errors in functional programming assignments},
journal={Proceedings of the ACM on Programming Languages},
year={2019},
volume={3},
number={OOPSLA},
doi={10.1145/3360614},
art_number={A188},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086821545&doi=10.1145%2f3360614&partnerID=40&md5=b0ac88897db91b0f2c1038672dcc7929},
affiliation={Department of Computer Science and Engineering, Korea University, 145 Anam-ro, Sungbuk-gu, Seoul, 02841, South Korea},
abstract={We present a new technique for automatically detecting logical errors in functional programming assignments. Compared to syntax or type errors, detecting logical errors remains largely a manual process that requires hand-made test cases. However, designing proper test cases is nontrivial and involves a lot of human effort. Furthermore, manual test cases are unlikely to catch diverse errors because instructors cannot predict all corner cases of diverse student submissions. We aim to reduce this burden by automatically generating test cases for functional programs. Given a reference program and a student's submission, our technique generates a counter-example that captures the semantic difference of the two programs without any manual effort. The key novelty behind our approach is the counter-example generation algorithm that combines enumerative search and symbolic verification techniques in a synergistic way. The experimental results show that our technique is able to detect 88 more errors not found by mature test cases that have been improved over the past few years, and performs better than the existing property-based testing techniques. We also demonstrate the usefulness of our technique in the context of automated program repair, where it effectively helps to eliminate test-suite-overfitted patches. © 2019 Association for Computing Machinery. All rights reserved.},
author_keywords={Automated Test Case Generation;  Program Synthesis;  Symbolic Execution},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gangopadhyay20191961,
author={Gangopadhyay, B. and Khastgir, S. and Dey, S. and Dasgupta, P. and Montana, G. and Jennings, P.},
title={Identification of Test Cases for Automated Driving Systems Using Bayesian Optimization},
journal={2019 IEEE Intelligent Transportation Systems Conference, ITSC 2019},
year={2019},
pages={1961-1967},
doi={10.1109/ITSC.2019.8917103},
art_number={8917103},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076820070&doi=10.1109%2fITSC.2019.8917103&partnerID=40&md5=f06f1bb7d984b80e15525a8ba5744a2d},
affiliation={Indian Institute of Technology, Kharagpur, India; University of Warwick, WMG, United Kingdom},
abstract={With advancements in technology, the automotive industry is experiencing a paradigm shift from assisted driving to highly automated driving. However, autonomous driving systems are highly safety critical in nature and need to be thoroughly tested for a diverse set of conditions before being commercially deployed. Due to the huge complexities involved with Advanced Driver Assistance Systems (ADAS) and Automated Driving Systems (ADS), traditional software testing methods have well-known limitations. They also fail to cover the infinite number of adverse conditions that can occur due to a slight change in the interactions between the environment and the system. Hence, it is important to identify test conditions that push the vehicle under test to breach its safe boundaries. Hazard Based Testing (HBT) methods, inspired by Systems-Theoretic Process Analysis (STPA), identify such parameterized test conditions that can lead to system failure. However, these techniques fall short of discovering the exact parameter values that lead to the failure condition. The presented paper proposes a test case identification technique using Bayesian Optimization. For a given test scenario, the proposed method learns parameter values by observing the system's output. The identified values create test cases that drive the system to violate its safe boundaries. STPA inspired outputs (parameters and pass/fail criteria) are used as inputs to the Bayesian Optimization model. The proposed method was applied to an SAE Level-4 Low Speed Automated Driving (LSAD) system which was modelled in a driving simulator. © 2019 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeAlmeida2019,
author={De Almeida, L.C. and De Sousa, R.T. and Nery, A.S. and Da Silva Filho, D.A. and Canedo, E.D. and Nunes, R.R.},
title={Design and evaluation of an SNMP-based energy consumption monitoring system for electrical grids},
journal={WCNPS 2019 - Workshop on Communication Networks and Power Systems},
year={2019},
doi={10.1109/WCNPS.2019.8896232},
art_number={8896232},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075599345&doi=10.1109%2fWCNPS.2019.8896232&partnerID=40&md5=616b127ca62954089db4e1ab6cca6489},
affiliation={University of Brasilia (UnB), Department of Electrical Engineering (ENE), Brazil},
abstract={Communications and data infrastructures are evolving in the most diverse views, ranging from the processing capacity, passing through wide geographic coverage, and finally to the resiliency and redundancy required in today's applications. However, physical problems such as excessive heat generation and increasing energy consumption continue to be increasingly present. Inspired by the evolution of the concept of Smart grids, we will approach the implementation of an integrated system for monitoring and measuring electricity consumption variables in data centers using open hardware and software libraries based on industry-wide standards such as Ethernet and the Simple Network Management Protocol (SNMP). Such system is conceived as a basic element for the complete management of a physical Information and Communication Technology infrastructure, since data centers are highly complex elements connected to and representative of Smart grids. For validation, a field-tested hardware and software prototype was developed, one that was capable of performing non-invasive measurements of electric current in data centers and make the data available via network, either in a local infrastructure or in a distributed architecture. Therefore, it could be implemented from small local data centers to huge distributed projects like Smart Grids, securely, inexpensively and highly integrable with other tools and systems. © 2019 IEEE.},
author_keywords={Consumption;  Data Center;  Electricity;  Monitoring;  Smart Grid;  SNMP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li20199619,
author={Li, X. and Wang, Y. and Yan, L. and Wang, K. and Deng, F. and Wang, F.-Y.},
title={ParallelEye-CS: A New Dataset of Synthetic Images for Testing the Visual Intelligence of Intelligent Vehicles},
journal={IEEE Transactions on Vehicular Technology},
year={2019},
volume={68},
number={10},
pages={9619-9631},
doi={10.1109/TVT.2019.2936227},
art_number={8807212},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073870634&doi=10.1109%2fTVT.2019.2936227&partnerID=40&md5=febf075322dc0a64ae80bf556d29d099},
affiliation={School of Automation, Beijing Institute of Technology, Beijing, 100081, China; State Key Laboratory for Management and Control of Complex Systems, Institute of Automation, Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China; College of Information Science and Technology, Beijing University of Chemical Technology, Beijing, 100029, China},
abstract={Virtual simulation testing is becoming indispensable for the intelligence testing of intelligent vehicles. However, even the most advanced simulation software provides rather limited test conditions. In the long run, intelligent vehicles are expected to work at SAE (Society of Automotive Engineers) level 4 or level 5. Researchers should make full use of virtual simulation scenarios to test the visual intelligence algorithms of intelligent vehicles under various imaging conditions. In this paper, we create realistic artificial scenes to simulate the self-driving scenarios, and collect a dataset of synthetic images from the virtual driving scenes, named 'ParallelEye-CS'. In the artificial scenes, we can flexibly change environmental conditions and automatically acquire accurate and diverse ground-truth labels. As a result, ParallelEye-CS has six ground-truth labels and includes twenty types of tests, which are divided into normal, environmental, and difficult tasks. Furthermore, we utilize ParallelEye-CS in combination with other publicly available datasets to conduct experiments for visual object detection. The experimental results indicate that: 1) object detection algorithms of intelligent vehicles can be tested under various scenario challenges; 2) mixed dataset can improve the accuracy of object detection algorithms, but domain shift is a serious issue worthy of attention. © 1967-2012 IEEE.},
author_keywords={intelligence testing;  Intelligent vehicles;  object detection;  synthetic images;  virtual simulation;  visual intelligence},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wilmanski20191217,
author={Wilmanski, T. and Rappaport, N. and Earls, J.C. and Magis, A.T. and Manor, O. and Lovejoy, J. and Omenn, G.S. and Hood, L. and Gibbons, S.M. and Price, N.D.},
title={Blood metabolome predicts gut microbiome α-diversity in humans},
journal={Nature Biotechnology},
year={2019},
volume={37},
number={10},
pages={1217-1228},
doi={10.1038/s41587-019-0233-9},
note={cited By 91},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071699515&doi=10.1038%2fs41587-019-0233-9&partnerID=40&md5=6cdcc53f351e9991d9a1c3ca267901a1},
affiliation={Institute for Systems Biology, Seattle, WA, United States; Arivale, Seattle, WA, United States; Center for Computational Medicine and Bioinformatics, University of Michigan, Ann Arbor, MI, United States; eScience Institute, University of Washington, Seattle, WA, United States; Institute for Systems Biology, Seattle, WA, United States},
abstract={Depleted gut microbiome α-diversity is associated with several human diseases, but the extent to which this is reflected in the host molecular phenotype is poorly understood. We attempted to predict gut microbiome α-diversity from ~1,000 blood analytes (laboratory tests, proteomics and metabolomics) in a cohort enrolled in a consumer wellness program (N = 399). Although 77 standard clinical laboratory tests and 263 plasma proteins could not accurately predict gut α-diversity, we found that 45% of the variance in α-diversity was explained by a subset of 40 plasma metabolites (13 of the 40 of microbial origin). The prediction capacity of these 40 metabolites was confirmed in a separate validation cohort (N = 540) and across disease states, showing that our findings are robust. Several of the metabolite biomarkers that are reported here are linked with cardiovascular disease, diabetes and kidney function. Associations between host metabolites and gut microbiome α-diversity were modified in those with extreme obesity (body mass index ≥ 35), suggesting metabolic perturbation. The ability of the blood metabolome to predict gut microbiome α-diversity could pave the way to the development of clinical tests for monitoring gut microbial health. © 2019, The Author(s), under exclusive licence to Springer Nature America, Inc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Eid2019,
author={Eid, H.T. and Al-Nohmi, N.M. and Wijewickreme, D. and Amarasinghe, R.S.},
title={Drained Peak and Residual Interface Shear Strengths of Fine-Grained Soils for Pipeline Geotechnics},
journal={Journal of Geotechnical and Geoenvironmental Engineering},
year={2019},
volume={145},
number={10},
doi={10.1061/(ASCE)GT.1943-5606.0002131},
art_number={06019010},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069529414&doi=10.1061%2f%28ASCE%29GT.1943-5606.0002131&partnerID=40&md5=66d0de3a68a533f1c2479c1895bf0fd2},
affiliation={Dept. of Civil Engineering, Qatar Univ., P.O. Box 2713, Doha, Qatar; Dept. of Civil Engineering, Univ. of British Columbia, Vancouver, BC  V6T 1Z4, Canada},
abstract={This study presents the results of interface torsional ring shear tests conducted at intermediate normal stress levels that are usually prevalent at soil-pipeline interfaces under near-shore conditions. Four normally consolidated soils of different plasticity and five solid surfaces with diverse roughness were utilized in this single-stage shear testing program. The drained peak and residual soil and interface shear strengths were measured and compared with the relevant data available in literature. The analyses and interpretation of the study results revealed that the drained peak and residual interface efficiencies (i.e., the ratio between soil and interface friction angles) are almost equal. These efficiencies can be estimated using one simple correlation that is valid regardless of the effective normal stress range. The correlation helps in the preliminary estimation of pipe-fine-grained soil shearing resistance when the beta (or effective stress) approach is adopted. © 2019 American Society of Civil Engineers.},
author_keywords={Energy pipelines;  Fine-grained soils;  Interface strength;  Offshore engineering;  Peak strength;  Residual strength;  Shear strength;  Surface roughness},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Souza201942,
author={Souza, M. and Dias-Neto, A.C. and Villanes, I.K. and Endo, A.T.},
title={On the exploratory testing of mobile apps},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={42-51},
doi={10.1145/3356317.3356322},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076688785&doi=10.1145%2f3356317.3356322&partnerID=40&md5=b0c447defab26150dbe87a343ed7dc43},
affiliation={Federal University of Technology - Parana (UTFPR), Brazil; Federal University of Amazonas (UFAM), Brazil},
abstract={While the literature acknowledges that mobile apps present different testing challenges and automated solutions have been pursued, it lacks a better understanding of how pervasive practices of manual testing (namely Exploratory Testing - ET) can be more effectively applied. This paper aims to investigate the use of ET in mobile apps. With this study, we intend to have a better understanding of how exploratory testing is employed, its effectiveness, and its usage in an ample and diverse range of apps. To do so, we conducted two studies. The first study was conducted for the purpose of applying ET to apps with diverse contexts and available on Google Play in order to analyze whether testers actually explore all possible scenarios that apps may display. The second study, also applied the ET, however in two apps that were developed by a software development company; this study has the objective of applying the ET in order to identify bugs of different levels, that often cannot be revealed using other techniques. As expected the first study revealed that there are several test scenarios that are not exploited by the testers, yet the 40 participants revealed on average 5 bugs in 1.5h of test sessions. The second study revealed 64 bugs and 21 issues in two apps. Such revealed bugs are of different criticality and category. ET has shown to be a promising technique to uncover bugs, though test professionals can be better guided to explore their apps and search for bugs in scenarios related to mobile specific events. © 2019 Association for Computing Machinery.},
author_keywords={Android;  Exploratory Testing;  Manual Tests;  Mobile Applications},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Welzer2019,
author={Welzer, T. and Družovec, M. and Kompara, M. and Hölbl, M.},
title={Cultural diversity in database teaching},
journal={29th Annual Conference of the European Association for Education in Electrical and Information Engineering, EAEEIE 2019 - Proceedings},
year={2019},
doi={10.1109/EAEEIE46886.2019.9000471},
art_number={9000471},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081231208&doi=10.1109%2fEAEEIE46886.2019.9000471&partnerID=40&md5=2945e15ed97cea18ed8ad4d6990bb0ed},
affiliation={University of Maribor, Faculty of Electrical Engineering and Computer Science, Maribor, Slovenia},
abstract={Despite a different understanding of the world, we are living in a global world in which we must cope not only with the communication in our own language and culture, but also with global communication in a common language, the so-called lingua franca. Very often this is the English language, that simplifies the communication, but does not dismiss the cultural differences between different national cultures and inside other cultural groups. In our contribution, we will concentrate on the cultural influence on teaching databases. Open questions that we want to discuss are: Which cultural viewpoints have to be considered (national, different users' groups); how culture influences expert work; and finally, yet importantly, do we have general solutions for those open questions? Besides the presentation of basic cultural concepts like culture, cultural differences, cultural awareness and others, we will also present the experiences of a student test group and students from the Erasmus+ program, who answered the question about cultural viewpoint inclusion into conceptual modelling (local students), or take part in the interview (Erasmus students). © 2019 IEEE.},
author_keywords={Conceptual modelling;  Cultural awareness;  Cultural issues;  Teaching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kaprocki2019257,
author={Kaprocki, N. and Velikic, G. and Teslic, N. and Krunic, M.},
title={Multiunit automotive perception framework: Synergy between AI and deterministic processing},
journal={IEEE International Conference on Consumer Electronics - Berlin, ICCE-Berlin},
year={2019},
volume={2019-September},
pages={257-260},
doi={10.1109/ICCE-Berlin47944.2019.8966168},
art_number={8966168},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078949846&doi=10.1109%2fICCE-Berlin47944.2019.8966168&partnerID=40&md5=1e2367a9020b037b6ea230667b719b26},
affiliation={Unviersity of Novi Sad, Faculty of Technical Sciences, Novi Sad, Serbia; RT-RK Institute for Computer Based Systems, Novi Sad, Serbia},
abstract={Since neural networks were first introduced into automotive systems, safety has been a major concern. The prevailing safety standard in the automotive industry, ISO26262, does not fully define testing and verification methods for software based on deep learning. In this paper, we propose a multiunit perception framework that increases the determinism of automotive systems incorporating deep learning. Our approach relies on ASIL decomposition and algorithm diversification, which are enabled through the utilization of multiple low ASIL perception units and one high ASIL monitor unit. In addition to the framework concept, we specify how each component can be mapped to appropriate hardware and software platforms. The practical feasibility of the perception framework is demonstrated with a proof of concept prototype. © 2019 IEEE.},
author_keywords={AI;  ASIL;  Automotive framework;  Deep learning;  Determinism;  Perception;  Safety},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ulger2019105,
author={Ulger, F. and Yuksel, S.E.},
title={A Standalone Open-Source System for Optical Inspection of Printed Circuit Boards},
journal={Signal Processing - Algorithms, Architectures, Arrangements, and Applications Conference Proceedings, SPA},
year={2019},
volume={2019-September},
pages={105-110},
doi={10.23919/SPA.2019.8936659},
art_number={8936659},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077755228&doi=10.23919%2fSPA.2019.8936659&partnerID=40&md5=c8d62e4460a055c2de82e05047afd06a},
affiliation={Hacettepe University, Department of Electrical and Electronics Engineering, Ankara, Turkey},
abstract={Automated inspection of Printed Circuit Boards (PCB) is substantial in decreasing scrap rates and the amount of revision for reliable production. This paper presents a standalone system with a benchmark software for defect detection and classification on bare, assembled boards and solder joints. Bare and assembled board defects are grouped into 4 groups each and solder joint into 2 groups. Focuses were made on gathering the findings in the literature under a compact system along with a comprehensible interface. Additionally, Optical Character Recognition (OCR) engine is integrated to the system to detect written text on integrated circuits (IC) for correct type defect detection. Also, polarity markers are detected via Binary Large Object (BLOB) detection to obtain polarity errors. The hardware used for the inspection is highly cost-effective such that the solely closed environment equipped with machine vision camera and proper illumination is sufficient. In addition, owing to the lightness of the image capturing box, inspection can be held in a diversity of locations. The software of the system and several image pairs to test are available in the repository. It is our hope that the software be used as a benchmark system for the optical inspection of printed circuit boards. © 2019 Division of Signal Processing and Electronic Systems, Poznan University of Technology (DSPES PUT).},
author_keywords={Assembled board;  Bare board;  Image subtraction;  Machine vision;  OCR;  PCB defects;  PCB inspection;  Solder joint},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhao2019449,
author={Zhao, W. and Ding, Z. and Xia, M. and Qi, Z.},
title={Systematically Testing and Diagnosing Responsiveness for Android Apps},
journal={Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019},
year={2019},
pages={449-453},
doi={10.1109/ICSME.2019.00077},
art_number={8918940},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077212925&doi=10.1109%2fICSME.2019.00077&partnerID=40&md5=d6b56896fbed21ade6e9029b98e820b2},
affiliation={Shanghai Jiao Tong University, Shanghai, China; AppetizerIO},
abstract={App responsiveness is the most intuitive interpretation of app performance from user's perspective. Traditional performance profilers only focus on one kind of program activities (e.g., CPU profiling), while the cause for slow responsiveness is diverse or even due to the joint effect of multiple kinds. Also, various test configurations, such as device hardware and wireless connectivity can have dramatic impact on particular program activities and indirectly affect app responsiveness. Conventional mobile testing lacks mechanisms to reveal configuration-sensitive bugs. In this paper, we propose AppSPIN, a tool to automatically diagnose app responsiveness bugs and systematically explore configuration-sensitive bugs. AppSPIN instruments the app to collect program events and UI responsiveness. The instrumented app is exercised with automated monkey testers and AppSPIN correlates excessive and lengthy program events with bad responsiveness detected at runtime. The diagnosis process also synthesizes the major resource bottleneck for the app. After one test run, AppSPIN automatically alters the test configuration to with most bottlenecked resource to further explore responsiveness bugs happened only with particular test configurations. Our preliminary experiments with 30 real-world apps show that AppSPIN can detect 123 responsiveness bugs and successfully diagnose the cause for 87% cases, within an average of 15-minute test time. Also with altered test configurations, AppSPIN uncovers a notable number of new bugs within four extra test runs. © 2019 IEEE.},
author_keywords={Android;  configuration;  Responsiveness},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{BenBraiek2019454,
author={Ben Braiek, H. and Khomh, F.},
title={DeepEvolution: A Search-Based Testing Approach for Deep Neural Networks},
journal={Proceedings - 2019 IEEE International Conference on Software Maintenance and Evolution, ICSME 2019},
year={2019},
pages={454-458},
doi={10.1109/ICSME.2019.00078},
art_number={8919189},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077212418&doi=10.1109%2fICSME.2019.00078&partnerID=40&md5=cf2c0140b311f132b1e4c3ba0f544d79},
affiliation={SWAT Lab. Polytechnique Montreal, Montreal, Canada},
abstract={The increasing inclusion of Deep Learning (DL) models in safety-critical systems such as autonomous vehicles have led to the development of multiple model-based DL testing techniques. One common denominator of these testing techniques is the automated generation of test cases, e.g., new inputs transformed from the original training data with the aim to optimize some test adequacy criteria. So far, the effectiveness of these approaches has been hindered by their reliance on random fuzzing or transformations that do not always produce test cases with a good diversity. To overcome these limitations, we propose, DeepEvolution, a novel search-based approach for testing DL models that relies on metaheuristics to ensure a maximum diversity in generated test cases. We assess the effectiveness of DeepEvolution in testing computer-vision DL models and found that it significantly increases the neuronal coverage of generated test cases. Moreover, using DeepEvolution, we could successfully find several corner-case behaviors. Finally, DeepEvolution outperformed Tensorfuzz (a coverage-guided fuzzing tool developed at Google Brain) in detecting latent defects introduced during the quantization of the models. These results suggest that search-based approaches can help build effective testing tools for DL systems. © 2019 IEEE.},
author_keywords={Computer Vision;  Deep Learning;  Metamorphic Testing;  Search Based Testing;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hayek2019,
author={Hayek, M. and Farhat, P. and Yamout, Y. and Ghorra, C. and Haraty, R.A.},
title={Web 2.0 Testing Tools: A Compendium},
journal={2019 International Conference on Innovation and Intelligence for Informatics, Computing, and Technologies, 3ICT 2019},
year={2019},
doi={10.1109/3ICT.2019.8910274},
art_number={8910274},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076429510&doi=10.1109%2f3ICT.2019.8910274&partnerID=40&md5=de1c5e2d6ef8b34cd44e56d13dd863f9},
affiliation={Lebanese American University, Dept. of Computer Science and Mathematics, Beirut, Lebanon},
abstract={Providing clients with high quality web applications has long been a major concern for web developers especially with the increased diversity of web frameworks and functionalities. In the past years, several tools and methodologies have been used for evaluating and measuring the quality of web services. One of the ways of guaranteeing high quality applications is done through testing. Testing is an important aspect of every software development process which companies rely on to elevate all their products to a standardized set of reliable software applications while ensuring that all client specifications are met. Therefore, a battery of testing tools, techniques and frameworks were invented to ensure the quality of web applications developed in order to serve clients around the clock. In this paper, we s urvey some of the widely known tools and models as far as web testing is concerned. © 2019 IEEE.},
author_keywords={Testing tools;  Web applications;  Web services},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Georgiadis2019,
author={Georgiadis, L. and Giannis, K. and Italiano, G.F. and Karanasiou, A. and Laura, L.},
title={Dynamic dominators and low-high orders in DAGs},
journal={Leibniz International Proceedings in Informatics, LIPIcs},
year={2019},
volume={144},
doi={10.4230/LIPIcs.ESA.2019.50},
art_number={50},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074859256&doi=10.4230%2fLIPIcs.ESA.2019.50&partnerID=40&md5=a65eb02fa1e73ff3c7c25618b1fdcf5d},
affiliation={Department of Computer Science and Engineering, University of Ioannina, Greece; LUISS University, Rome, Italy; Università di Roma “Tor Vergata”, Italy},
abstract={We consider practical algorithms for maintaining the dominator tree and a low-high order in directed acyclic graphs (DAGs) subject to dynamic operations. Let G be a directed graph with a distinguished start vertex s. The dominator tree D of G is a tree rooted at s, such that a vertex v is an ancestor of a vertex w if and only if all paths from s to w in G include v. The dominator tree is a central tool in program optimization and code generation, and has many applications in other diverse areas including constraint programming, circuit testing, biology, and in algorithms for graph connectivity problems. A low-high order of G is a preorder of D that certifies the correctness of D, and has further applications in connectivity and path-determination problems. We first provide a practical and carefully engineered version of a recent algorithm [ICALP 2017] for maintaining the dominator tree of a DAG through a sequence of edge deletions. The algorithm runs in O(mn) total time and O(m) space, where n is the number of vertices and m is the number of edges before any deletion. In addition, we present a new algorithm that maintains a low-high order of a DAG under edge deletions within the same bounds. Both results extend to the case of reducible graphs (a class that includes DAGs). Furthermore, we present a fully dynamic algorithm for maintaining the dominator tree of a DAG under an intermixed sequence of edge insertions and deletions. Although it does not maintain the O(mn) worst-case bound of the decremental algorithm, our experiments highlight that the fully dynamic algorithm performs very well in practice. Finally, we study the practical efficiency of all our algorithms by conducting an extensive experimental study on real-world and synthetic graphs. © Loukas Georgiadis, Konstantinos Giannis, Giuseppe F. Italiano, Aikaterini Karanasiou, and Luigi Laura.},
author_keywords={Connectivity;  Dominators;  Low-high orders},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hegstad20192541,
author={Hegstad, J.M. and Nelson, R.L. and Renny-Byfield, S. and Feng, L. and Chaky, J.M.},
title={Introgression of novel genetic diversity to improve soybean yield},
journal={Theoretical and Applied Genetics},
year={2019},
volume={132},
number={9},
pages={2541-2552},
doi={10.1007/s00122-019-03369-2},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067932240&doi=10.1007%2fs00122-019-03369-2&partnerID=40&md5=18190c56e2b92f35bd8621697b0434d9},
affiliation={Corteva Agriscience, 8305 NW 62nd Ave., Johnston, IA  50131, United States; Urbana, IL, United States},
abstract={Key message: Exotic soybean germplasm can be used to increase novel genetic diversity and yield potential of cultivars. Abstract: Modern North American soybean (Glycine max [L.] Merr.) cultivars have been derived from only a few ancestors. The objectives of this research were to develop breeding lines with novel genetic diversity that were equivalent to the yield of a commercial cultivar parent and within those lines identify regions of novel genetic diversity that were not present in the Corteva Agriscience elite soybean germplasm pool. Nine lines created from diverse germplasm (USDA-ARS breeding program at the University of Illinois) were crossed to a RM34Elite parent to develop populations and sublines for yield testing. Across yield tests at 30 locations conducted between 2014 and 2016, eleven breeding lines were identified that were equivalent to or significantly higher in yield when compared to the RM34Elite parent. Among the eleven final lines, the introgressed novel haplotypes that were not present in current Corteva Agriscience soybean germplasm occupied an estimated 0.8–10.0% of the genome. JH-2665, the highest yielding line across 3 years of testing, yielded 280 kg/ha more than the RM34Elite parent and had an estimated 8.6% of the genome containing novel diversity haplotypes. JH-2665 had 96 regions of novel diversity introgression ranging from 1 to 12 cM in size, with six regions over 6 cM in length. The methods reported demonstrate how high-yielding lines with novel genetic diversity can be developed. This material will be useful for expanding the genetic diversity needed to improve genetic gain in future soybean cultivar development. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Breeding;  Exotic germplasm;  Native diversity;  Plant introductions;  Soybean;  Yield},
document_type={Article},
source={Scopus},
}

@ARTICLE{Matinnejad2019919,
author={Matinnejad, R. and Nejati, S. and Briand, L.C. and Bruckmann, T.},
title={Test Generation and Test Prioritization for Simulink Models with Dynamic Behavior},
journal={IEEE Transactions on Software Engineering},
year={2019},
volume={45},
number={9},
pages={919-944},
doi={10.1109/TSE.2018.2811489},
art_number={8305644},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042870776&doi=10.1109%2fTSE.2018.2811489&partnerID=40&md5=fbebe6cd8ad4ad895f5b56b8285c0ad6},
affiliation={SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg, Luxembourg; Delphi Automotive Systems, Luxembourg, Luxembourg},
abstract={All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Among the different disciplines, the engineering of Cyber Physical Systems (CPSs) particularly relies on models with dynamic behaviors (i.e., models that exhibit time-varying changes). The Simulink modeling platform greatly appeals to CPS engineers since it captures dynamic behavior models. It further provides seamless support for two indispensable engineering activities: (1) automated verification of abstract system models via model simulation, and (2) automated generation of system implementation via code generation. We identify three main challenges in the verification and testing of Simulink models with dynamic behavior, namely incompatibility, oracle and scalability challenges. We propose a Simulink testing approach that attempts to address these challenges. Specifically, we propose a black-box test generation approach, implemented based on meta-heuristic search, that aims to maximize diversity in test output signals generated by Simulink models. We argue that in the CPS domain test oracles are likely to be manual and therefore the main cost driver of testing. In order to lower the cost of manual test oracles, we propose a test prioritization algorithm to automatically rank test cases generated by our test generation algorithm according to their likelihood to reveal a fault. Engineers can then select, according to their test budget, a subset of the most highly ranked test cases. To demonstrate scalability, we evaluate our testing approach using industrial Simulink models. Our evaluation shows that our test generation and test prioritization approaches outperform baseline techniques that rely on random testing and structural coverage. © 1976-2012 IEEE.},
author_keywords={output diversity;  search-based software testing;  signal features;  Simulink models;  structural coverage;  test generation;  test oracle;  test prioritization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kessel201935,
author={Kessel, M. and Atkinson, C.},
title={A platform for diversity-driven test amplification},
journal={A-TEST 2019 - Proceedings of the 10th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, co-located with ESEC/FSE 2019},
year={2019},
pages={35-41},
doi={10.1145/3340433.3342825},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076428162&doi=10.1145%2f3340433.3342825&partnerID=40&md5=27ab55373d795457a3d5aeb0177fb358},
affiliation={University of Mannheim, Mannheim, Germany},
abstract={Test amplification approaches take a manually written set of tests (input/output mappings) and enhance their effectiveness for some clearly defined engineering goal such as detecting faults. Conceptually, they can either achieve this in a "black box" way using only the initial "seed" tests or in a "white box" way utilizing additional inputs such as the source code or specification of the software under test. However, no fully black box approach to test amplification is currently available even though they can be used to enhance white box approaches. In this paper we introduce a new approach that uses the seed tests to search for existing redundant implementations of the software under test and leverages them as oracles in the generation and evaluation of new tests. The approach can therefore be used as a stand alone black box test amplification method or in tandem with other methods. In this paper we explain the approach, describe its synergies with other approaches and provide some evidence for its practical feasibility. Copyright © A-TEST 2019 - ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation, co-located with ESEC/FSE 2019.All right reserved.},
author_keywords={Automated testing;  Behavior;  Mining software repositories;  Observations;  Oracle problem;  Test amplification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Morán-López2019944,
author={Morán-López, A. and Ruiz-Cruz, J.A. and Córcoles, J. and Montejo-Garai, J.R. and Rebollar, J.M.},
title={Analytical expressions of the Q-factor for the complete resonant mode spectrum of the equilateral triangular waveguide cavity},
journal={Electronics Letters},
year={2019},
volume={55},
number={17},
pages={944-947},
doi={10.1049/el.2019.1420},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070995669&doi=10.1049%2fel.2019.1420&partnerID=40&md5=19090b75e53bc347286c215ec3e93256},
affiliation={Department of Electronic and Communications Technology, Escuela Politécnica Superior, Universidad Autónoma de Madrid, C/Francisco Tomás y Valiente, 11, Madrid, 28049, Spain; Department of Signals, Systems and Radiocommunications, Information Processing and Telecommunications Center, Universidad Politécnica de Madrid, Madrid, 28040, Spain},
abstract={This Letter presents the closed-form expressions of the unloaded Q-factor in metallic cavities with equilateral triangular cross section, computed from its analytical resonant mode solutions. The obtained analytical formulas for the Q-factor extend the very restricted range of problems with closed-form solutions, complementing the classic cases of the rectangular and circular cavities found in the literature. The presented derivation provides extremely rapid reference solutions for diverse applications such as filter design by test cases for microwave characterisation systems or numerical full-wave solvers. A universal Q-chart, valid for any aspect ratio, is given and the achieved analytical results for the first resonant modes are tested with numerical commercial software, showing excellent agreement. © The Institution of Engineering and Technology 2019.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Durieux2019302,
author={Durieux, T. and Madeiral, F. and Martinez, M. and Abreu, R.},
title={Empirical review of Java program repair tools: A large-scale experiment on 2,141 bugs and 23,551 repair attempts},
journal={ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2019},
pages={302-313},
doi={10.1145/3338906.3338911},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071934264&doi=10.1145%2f3338906.3338911&partnerID=40&md5=fde99a687b988d0311a2cb8fb557fb35},
affiliation={INESC-ID, IST, University of Lisbon, Portugal; Federal University of Uberlndia, Brazil; Polytechnic University of Hauts-de-France, France},
abstract={In the past decade, research on test-suite-based automatic program repair has grown significantly. Each year, new approaches and implementations are featured in major software engineering venues. However, most of those approaches are evaluated on a single benchmark of bugs, which are also rarely reproduced by other researchers. In this paper, we present a large-scale experiment using 11 Java test-suite-based repair tools and 2,141 bugs from 5 benchmarks. Our goal is to have a better understanding of the current state of automatic program repair tools on a large diversity of benchmarks. Our investigation is guided by the hypothesis that the repairability of repair tools might not be generalized across different benchmarks. We found that the 11 tools 1) are able to generate patches for 21% of the bugs from the 5 benchmarks, and 2) have better performance on Defects4J compared to other benchmarks, by generating patches for 47% of the bugs from Defects4J compared to 10-30% of bugs from the other benchmarks. Our experiment comprises 23,551 repair attempts, which we used to find causes of non-patch generation. These causes are reported in this paper, which can help repair tool designers to improve their approaches and tools. © 2019 ACM.},
author_keywords={Automatic program repair;  Benchmark overfitting;  Patch generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2019763,
author={Zhang, C. and Su, T. and Yan, Y. and Zhang, F. and Pu, G. and Su, Z.},
title={Finding and understanding bugs in software model checkers},
journal={ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2019},
pages={763-773},
doi={10.1145/3338906.3338932},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071922367&doi=10.1145%2f3338906.3338932&partnerID=40&md5=829395c3d72551f225748c36ab7b0177},
affiliation={East China Normal University, China; ETH Zurich, Switzerland; MPI-SWS, Germany},
abstract={Software Model Checking (SMC) is a well-known automatic program verification technique and frequently adopted for checking safety-critical software. Thus, the reliability of SMC tools themselves (i.e., software model checkers) is critical. However, little work exists on validating software model checkers, an important problem that this paper tackles by introducing a practical, automated fuzzing technique. For its simplicity and generality, we focus on control-flow reachability (e.g., whether or how many times a branch is reached) and address two specific challenges for effective fuzzing: oracle and scalability. Given a deterministic program, we (1) leverage its concrete executions to synthesize valid branch reachability properties (thus solving the oracle problem) and (2) fuse such individual properties into a single safety property (thus improving the scalability of fuzzing and reducing manual inspection). We have realized our approach as the MCFuzz tool and applied it to extensively test three state-of-the-art C software model checkers, CPAchecker, CBMC, and SeaHorn. MCFuzz has found 62 unique bugs in all three model checkers - 58 have been confirmed, and 20 have been fixed. We have further analyzed and categorized these bugs (which are diverse), and summarized several lessons for building reliable and robust model checkers. Our testing effort has been well-appreciated by the model checker developers, and also led to improved tool usability and documentation. © 2019 ACM.},
author_keywords={Fuzz Testing;  Software Model Checking;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ahmed2019201,
author={Ahmed, W. and Hassan, I.M. and Nayel, M. and Gaber, H.},
title={Simulated Testing Algorithm for μPMU Full Observation of Balanced Radial Distribution Grid},
journal={Proceedings of 2019 the 7th International Conference on Smart Energy Grid Engineering, SEGE 2019},
year={2019},
pages={201-207},
doi={10.1109/SEGE.2019.8859778},
art_number={8859778},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074109151&doi=10.1109%2fSEGE.2019.8859778&partnerID=40&md5=a7e6c74adefc2a54a5ace630378e234a},
affiliation={Faculty of Engineering, Assiut University, Assiut, Egypt; Faculty of Energy Systems and Nuclear Science, University of Ontario, Institute of Technology, UOITON, Canada},
abstract={Today's electric power distribution systems with development of distributed energy resources introduce variability, uncertainty, and opportunities to recruit diverse resources for grid services. Multiple resources on each feeder have more complex impacts on the circuit behavior that can be observed with voltage and current phase angle variations. Micro Phasor Measurement Units (μPMUs) take time-synchronized measurements of voltage, current and frequency that can tell grid operators what is happening, where, and when. This paper presents a new μPMUs power flow algorithm for complete observation of balanced radial distribution grid. This algorithm calculates all voltages in both high and low voltage buses, currents in all branches, line active and reactive power flow in all branches and total active and reactive power losses in the grid. This algorithm provides high quality data for distribution planners and operators, which will translate into better model accuracy and thus better results from distribution analysis tools. To test the validity of proposed algorithm, backward / forward sweep power flow program is developed and tested by ETAP software. © 2019 IEEE.},
author_keywords={backward / forward sweep;  ETAP software and full observation;  power flow;  μPMUs},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2019417,
author={Liu, M. and Li, K. and Chen, T.},
title={Security testing of web applications: A search-based approach for detecting SQL injection vulnerabilities},
journal={GECCO 2019 Companion - Proceedings of the 2019 Genetic and Evolutionary Computation Conference Companion},
year={2019},
pages={417-418},
doi={10.1145/3319619.3322026},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070610551&doi=10.1145%2f3319619.3322026&partnerID=40&md5=36ae3ba30e5df6f2dba2a8d18cb6d749},
affiliation={University of Electronic Science and Technology of China, Chengdu, China; University of Exeter, Exeter, United Kingdom; Nottingham Trent University, Nottingham, United Kingdom},
abstract={Web applications have become increasingly essential in many domains that operate on confidential data related to business. SQL injection attack is one of the most significant web application security risks. Detecting SQL injection vulnerabilities is essential for protecting the underlying web application. However, manually enumerating test cases is extremely challenging, if not impossible, given the potentially infinite number of user inputs and the likely nonexistence of one-to-one mapping between user inputs and malicious SQL statements. This paper proposes an automatic security test case generation approach to detect SQL injection vulnerabilities for web applications, following a search-based software engineering (SBSE) paradigm. Particularly, we propose a novel fitness function that evaluates the similarity between the SQL statements produced by feeding user inputs in the system under test and a known malicious SQL statement. For the search algorithm, we exploit differential evolution, which is robust in continuous optimization but it is under-investigated in SBSE. Based on three real-world web applications, we conduct experiments on 19 configurations that are of diverse forms of SQL statements and types of attacks. Results demonstrate that our approach is more effective, with statistical significance and high effect sizes, than the state-of-the-art. © 2019 Association for Computing Machinery.},
author_keywords={Differential evolution;  Search-based software engineering;  Security;  SQL injection;  Test case generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo201990,
author={Guo, J. and Li, S. and Lou, J.-G. and Yang, Z. and Liu, T.},
title={SarA: Self-replay augmented record and replay for android in industrial cases∗},
journal={ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2019},
pages={90-100},
doi={10.1145/3293882.3330557},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070640668&doi=10.1145%2f3293882.3330557&partnerID=40&md5=16da5aeec03782061d5f01ddaa29627e},
affiliation={Xi’an Jiaotong University Xi’an, China; Microsoft Research Asia, Beijing, China; Western Michigan University, Kalamazoo, MI, United States},
abstract={Record-and-replay tools are indispensable for quality assurance of mobile applications. Due to its importance, an increasing number of tools are being developed to record and replay user interactions for Android. However, by conducting an empirical study of various existing tools in industrial settings, researchers have revealed a gap between the characteristics requested from industry and the performance of publicly available record-and-replay tools. The study concludes that no existing tools under evaluation are sufficient for industrial applications. In this paper, we present a record-and-replay tool called SARA towards bridging the gap and targeting a wide adoption. Specifically, a dynamic instrumentation technique is used to accommodate rich sources of inputs in the application layer satisfying various constraints requested from industry. A self-replay mechanism is proposed to record more information of user inputs for accurate replaying without degrading user experience. In addition, an adaptive replay method is designed to enable replaying events on different devices with diverse screen sizes and OS versions. Through an evaluation on 53 highly popular industrial Android applications and 265 common usage scenarios, we demonstrate the effectiveness of SARA in recording and replaying rich sources of inputs on the same or different devices. © 2019 Association for Computing Machinery.},
author_keywords={Android;  Record-and-Replay;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2019386,
author={Li, C. and Zhou, M. and Gu, Z. and Chen, G. and Wang, Y. and Wu, J. and Gu, M.},
title={VBSAC: A value-based static analyzer for C},
journal={ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2019},
pages={386-389},
doi={10.1145/3293882.3338998},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070638053&doi=10.1145%2f3293882.3338998&partnerID=40&md5=63e7b96462533a78b56722a720de03c1},
affiliation={School of Software, Tsinghua University, Beijing, China},
abstract={Static analysis has long prevailed as a promising approach to detect program bugs at an early development process to increase software quality. However, such tools face great challenges to balance the false-positive rate and the false-negative rate in practical use. In this paper, we present VBSAC, a value-based static analyzer for C aiming to improve the precision and recall. In our tool, we employ a pluggable value-based analysis strategy. A memory skeleton recorder is designed to maintain the memory objects as a baseline. While traversing the control flow graph, diverse value-based plug-ins analyze the specific abstract domains and share program information to strengthen the computation. Simultaneously, checkers consume the corresponding analysis results to detect bugs. We also provide a user-friendly web interface to help users audit the bug detection results. Evaluation on two widely-used benchmarks shows that we perform better to state-of-the-art bug detection tools by finding 221∼339 more bugs and improving F-Score 9.88%∼40.32%. © 2019 Association for Computing Machinery.},
author_keywords={Bug detection;  Static analysis;  Value-based analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu201943,
author={Liu, K. and Koyuncu, A. and Kim, D. and Bissyandé, T.F.},
title={TBAR: Revisiting template-based automated program repair},
journal={ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2019},
pages={43-54},
doi={10.1145/3293882.3330577},
note={cited By 51},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070617174&doi=10.1145%2f3293882.3330577&partnerID=40&md5=51f82cef989c466121519fbd3b89cd88},
affiliation={University of Luxembourg, Luxembourg},
abstract={We revisit the performance of template-based APR to build comprehensive knowledge about the effectiveness of fix patterns, and to highlight the importance of complementary steps such as fault localization or donor code retrieval. To that end, we first investigate the literature to collect, summarize and label recurrently-used fix patterns. Based on the investigation, we build TBar, a straightforward APR tool that systematically attempts to apply these fix patterns to program bugs. We thoroughly evaluate TBar on the De-fects4J benchmark. In particular, we assess the actual qualitative and quantitative diversity of fix patterns, as well as their effectiveness in yielding plausible or correct patches. Eventually, we find that, assuming a perfect fault localization, TBar correctly/plausibly fixes 74/101 bugs. Replicating a standard and practical pipeline of APR assessment, we demonstrate that TBar correctly fixes 43 bugs from Defects4J, an unprecedented performance in the literature (including all approaches, i.e., template-based, stochastic mutation-based or synthesis-based APR). © 2019 Association for Computing Machinery.},
author_keywords={Automated program repair;  Empirical assessment;  Fix pattern},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cai2019352,
author={Cai, H. and Zhang, Z. and Li, L. and Fu, X.},
title={A large-scale study of application incompatibilities in android},
journal={ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2019},
pages={352-362},
doi={10.1145/3293882.3330564},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070616257&doi=10.1145%2f3293882.3330564&partnerID=40&md5=1c60e8d6635ecb87c88437958ec15d23},
affiliation={Washington State University, Pullman, United States; Monash University, Australia},
abstract={The rapid expansion of the Android ecosystem is accompanied by continuing diversification of platforms and devices, resulting in increasing incompatibility issues which damage user experiences and impede app development productivity. In this paper, we conducted a large-scale, longitudinal study of compatibility issues in 62,894 benign apps developed in the past eight years, to understand the symptoms and causes of these issues. We further investigated the incompatibilities that are actually exercised at runtime through the system logs and execution traces of 15,045 apps. Our study revealed that, among others, (1) compatibility issues were prevalent and persistent at both installation and run time, with greater prevalence of run-time incompatibilities, (2) there were no certain Android versions that consistently saw more or less app incompatibilities than others, (3) installation-time incompatibilities were strongly correlated with the minSdkVersion specified in apps, while run-time incompatibilities were most significantly correlated with the underlying platform’s API level, and (4) installation-time incompatibilities were mostly due to apps’ use of architecture-incompatible native libraries, while run-time incompatibilities were mostly due to API changes during SDK evolution. We offered further insights into app incompatibilities, as well as recommendations on dealing with the issues for bother developers and end users of Android apps. © 2019 Association for Computing Machinery.},
author_keywords={Android;  Compatibility;  Installation failure;  Run-time failure},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2019158,
author={Xie, X. and Ma, L. and Juefei-Xu, F. and Xue, M. and Chen, H. and Liu, Y. and Zhao, J. and Li, B. and Yin, J. and See, S.},
title={Deephunter: A coverage-guided fuzz testing framework for deep neural networks},
journal={ISSTA 2019 - Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2019},
pages={158-168},
doi={10.1145/3293882.3330579},
note={cited By 101},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070586358&doi=10.1145%2f3293882.3330579&partnerID=40&md5=0e801b8666e47926990998f9fc8d9bcd},
affiliation={Nanyang Technological University, Singapore; Kyushu University, Japan; Carnegie Mellon University, United States; University of Adelaide, Australia; Nanyang Technological University, Zhejiang Sci-Tech University, China; University of Illinois at Urbana-Champaign, United States; NVIDIA AI Tech Centre, Singapore},
abstract={The past decade has seen the great potential of applying deep neural network (DNN) based software to safety-critical scenarios, such as autonomous driving. Similar to traditional software, DNNs could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses. In this paper, we propose DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs. To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests, and leverage multiple extensible coverage criteria as feedback to guide the test generation. We further propose a seed selection strategy that combines both diversity-based and recency-based seed selection. We implement and incorporate 5 existing testing criteria and 4 seed selection strategies in DeepHunter. Large-scale experiments demonstrate that (1) our metamorphic mutation strategy is useful to generate new valid tests with the same semantics as the original seed, by up to a 98% validity ratio; (2) the diversity-based seed selection generally weighs more than recency-based seed selection in boosting the coverage and in detecting defects; (3) DeepHunter outperforms the state of the arts by coverage as well as the quantity and diversity of defects identified; (4) guided by corner-region based criteria, DeepHunter is useful to capture defects during the DNN quantization for platform migration. © 2019 Association for Computing Machinery.},
author_keywords={Coverage-guided fuzzing;  Deep learning testing;  Metamorphic testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rosvall2019342,
author={Rosvall, O.},
title={Using Norway spruce clones in Swedish forestry: Swedish forest conditions, tree breeding program and experiences with clones in field trials},
journal={Scandinavian Journal of Forest Research},
year={2019},
volume={34},
number={5},
pages={342-351},
doi={10.1080/02827581.2018.1562566},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066806200&doi=10.1080%2f02827581.2018.1562566&partnerID=40&md5=529804e189eb8a7be7840a6ae8722a61},
affiliation={Skogforsk, Uppsala Science Park, Uppsala, Sweden},
abstract={Conditions in Sweden for using vegetative propagation and clones for forest regeneration differ from many other parts of the world where clones are used. In this paper, we describe how Swedish forestry takes place under semi-natural conditions, which should be considered when planning for conservation of species genetic diversity. We also describe how the Swedish long-term breeding program is sustainable by combining genetic progress and gene conservation. It involves clonal replication as the standard field-test strategy. In this way, the breeding program continuously supplies a large number of tested genotypes to be used as parents for recurrent breeding, seed orchards and for vegetative propagation of nursery stock. By computer simulation, we illustrate the progress of genetic gain and diversity in reforestation material from the breeding program. Finally, we briefly refer to Swedish experiences of clones in field trials. © 2019, © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={Clonal forestry;  family forestry;  genetic diversity;  genetic gain;  Sweden;  tree breeding},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Linnosmaa2019369,
author={Linnosmaa, J. and Alanen, J.},
title={Demonstration of a conformity assessment data model},
journal={IEEE International Conference on Industrial Informatics (INDIN)},
year={2019},
volume={2019-July},
pages={369-373},
doi={10.1109/INDIN41052.2019.8972077},
art_number={8972077},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079038665&doi=10.1109%2fINDIN41052.2019.8972077&partnerID=40&md5=00b623841cfa815824e7c8b1174ad011},
affiliation={VTT Technical Research Centre of Finland, Tampere, Finland},
abstract={The purpose of this research was to test the applicability of our conformity assessment data model using a simple industrial conformity example and an industrial software tool. As a demonstration case, we stated a claim relating to diversity of a nuclear fuel cooling pool control system. With the Siemens' Polarion™ Application Lifecycle Management tool, we traced the claim to the diversity requirement, to the system model and to the dependency analysis result, which was used as the evidence for the claim. We managed to implement all the artefacts to be traced according to our data model into the industrial tool. We found out that it is rather straightforward to implement our conformity assessment data model in a software tool offering database-oriented repository and traceability support. The data model is thus suggested for industrial use to make the conformity assessment process more effective by providing traceability information, structured input and storage of compliance data and by facilitating automatic document generation. © 2019 IEEE.},
author_keywords={Conformity assessment;  Data model;  Demonstration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Omari201954,
author={Omari, M. and Chen, J. and Kwaku Kudjo, P. and Ackah-Arthur, H. and Huang, R.},
title={Random Border Mirror Transform: A Diversity Based Approach to an Effective and Efficient Mirror Adaptive Random Testing},
journal={Proceedings - 19th IEEE International Conference on Software Quality, Reliability and Security, QRS 2019},
year={2019},
pages={54-61},
doi={10.1109/QRS.2019.00020},
art_number={8854699},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073808773&doi=10.1109%2fQRS.2019.00020&partnerID=40&md5=139b6f36258799f770a24ecae308ef81},
affiliation={School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China},
abstract={Mirror Adaptive random testing (MART) is an overhead reduction strategy for adaptive random testing methods. Theoretically speaking, MART's advantage over ordinary ARTs is determined by the mirroring scheme selected. Incidentally, an inherent problem with MART relates to the difficulty in the choice of a scheme for any testing task. This is because a higher scheme (larger mirror domains) does not necessarily guarantee efficient utilization of testing resources due to lack of diversity of mirror generated test cases. The culprit has been identified as the mapping functions used as substitutes to complex ART methods. In this paper, we present a new method for generating diversified mirror test cases by randomly displacing the mirror partitions upon which the mapping functions of MART operates. The result of simulations and experiments conducted shows remarkable improvement over MART's effectiveness and efficiency across MART schemes, especially where program failures are unrelated to one or more input parameters. © 2019 IEEE.},
author_keywords={Adaptive random testing;  mirror adaptive random testing;  software testing;  test case diversity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Carrero2019535,
author={Carrero, J. and Krzeminska, A. and Härtel, C.E.J.},
title={The DXC technology work experience program: Disability-inclusive recruitment and selection in action},
journal={Journal of Management and Organization},
year={2019},
volume={25},
number={4},
pages={535-542},
doi={10.1017/jmo.2019.23},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071362478&doi=10.1017%2fjmo.2019.23&partnerID=40&md5=40edbc9208464e354dfadfc857a904db},
affiliation={University of Queensland, Brisbane, Australia; Macquarie University, Sydney, Australia},
abstract={With the rapid advancement of innovative technology, coupled with IT being a core function in contemporary business, there has been an upward trend of multi-national companies (MNCs) reporting a skill deficit in areas such as data analytics and cybersecurity (Columbus, 2017. IBM predicts demand for data scientists will soar 28% By 2020. Forbes; NeSmith, 2018. The cybersecurity gap is an industry crisis. Forbes). In a recent survey with over 3,000 CIOs, 65% indicated their organizations were unable to maintain par with the progression of technology in areas such as data analytics and security due to a lack of adequate talent (Harvey Nash & KPMG, 2018. CIO survey 2018). Although, organizations have recently started to expand their talent pipeline following a neurological breakthrough: research as well as anecdotal evidence suggests adults with mild forms of autism display above-average intelligence, increased attention focus, and high visual-spatial abilities; a combination in high market demand for roles such as software testing, data analysis, cybersecurity, and engineering due to their uncanny ability with pattern recognition, information processing, analytics, and attention to detail. These auspicious developments come at the helm of an increasing rate of governments around the world implementing provisions to their labour regulations towards equitable hiring of people with disabilities (Myors et al., 2017. Perspectives from 22 countries on the legal environment for selection. Handbook of Employee Selection. 659-677. Research Collection Lee Kong Chian School of Business.). Some, such as France, Japan, Kenya, Korea, and Taiwan, have gone so far as to set quota targets (Myors et al., 2017. Perspectives from 22 countries on the legal environment for selection. Handbook of Employee Selection. 659-677. Research Collection Lee Kong Chian School of Business.). The implication for organizations is that they need to develop disability-inclusive recruitment and selection systems along with work designs and environments that are disability friendly. But what does this mean in practice? What does a disability-inclusive recruitment and selection system look like? Enter DXC Technology (DXC): born out of a merger between global conglomerate Computer Science Corporation and Hewlett Packard Enterprise, generating close to 25 billion annually in revenue, with clients across more than 70 countries, they strategically became a pioneer in the digital transformation that was taking place globally. In the wake of the breakthrough in employment diversity, DXC recognized this as an opportunity to gain a critical edge within the increasingly competitive talent pool market. First, design a program of their own for recruiting and selecting adults with high functioning autism. Next, through a collaboration with various universities including the University of Queensland and Macquarie University, Neurodiversity Hubs were established; an initiative designed to assist neurodivergent students with obtaining work experience and internships. In doing so, they faced the following key challenges: How could they design a recruitment and selection strategy for neurodivergent individuals that was equitable, ethical, and efficient? In particular, where could they find suitable neurodivergent candidates, what criteria should they use to select them, and how should they handle unsuccessful candidates to ensure beneficial outcomes for all stakeholders? Copyright © Cambridge University Press and Australian and New Zealand Academy of Management 2019.},
author_keywords={disabilities;  human resource management;  Key wordsdiversity and inclusion;  recruitment;  selection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Emami2019227,
author={Emami, A. and Kunii, N. and Matsuo, T. and Shinozaki, T. and Kawai, K. and Takahashi, H.},
title={Autoencoding of long-term scalp electroencephalogram to detect epileptic seizure for diagnosis support system},
journal={Computers in Biology and Medicine},
year={2019},
volume={110},
pages={227-233},
doi={10.1016/j.compbiomed.2019.05.025},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066983110&doi=10.1016%2fj.compbiomed.2019.05.025&partnerID=40&md5=620e805008ae6570fb35a788d60b91ad},
affiliation={Research Center for Advanced Science and Technology, The University of Tokyo, Japan; Department of Neurosurgery, Graduate School of Medicine, The University of Tokyo, Japan; Tokyo Metropolitan Neurological Hospital, Japan; National Institute of Information and Communications Technology, Japan; Department of Neurosurgery, Jichi Medical University, Japan},
abstract={Introduction: Epileptologists could benefit from a diagnosis support system that automatically detects seizures because visual inspection of long-term electroencephalograms (EEGs) is extremely time-consuming. However, the diversity of seizures among patients makes it difficult to develop universal features that are applicable for automatic seizure detection in all cases, and the rarity of seizures results in a lack of sufficient training data for classifiers. Methods: To overcome these problems, we utilized an autoencoder (AE), which is often used for anomaly detection in the field of machine learning, to perform seizure detection. We hypothesized that multichannel EEG signals are compressible by AE owing to their spatio-temporal coupling and that the AE should be able to detect seizures as anomalous events from an interictal EEG. Results: Through experiments, we found that the AE error was able to classify seizure and nonseizure states with a sensitivity of 100% in 22 out of 24 available test subjects and that the AE was better than the commercially available software BESA and Persyst for half of the test subjects. Conclusions: These results suggest that the AE error is a feasible candidate for a universal seizure detection feature. © 2019 Elsevier Ltd},
author_keywords={Autoencoder;  Epilepsy;  Scalp electroencephalogram;  Seizure detection;  Unsupervised learning},
document_type={Article},
source={Scopus},
}

@ARTICLE{Haddad20191,
author={Haddad, S.M. and Souza, R.T. and Cecatti, J.G.},
title={Mobile technology in health (mHealth) and antenatal care–Searching for apps and available solutions: A systematic review},
journal={International Journal of Medical Informatics},
year={2019},
volume={127},
pages={1-8},
doi={10.1016/j.ijmedinf.2019.04.008},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064319719&doi=10.1016%2fj.ijmedinf.2019.04.008&partnerID=40&md5=c13edfe886fd9702c9cd5cb874c486a7},
affiliation={Department of Obstetrics and Gynecology, School of Medicine, University of Campinas (UNICAMP), Campinas, Brazil},
abstract={Background: Medical Information Technology may be understood as an interdisciplinary study of the conception, design, development, adoption and use of Information Technology (IT) innovations for healthcare provision, management and planning. Concerning the use of IT in reproductive health, the aim of the diverse range of currently available applications (apps) is to assist in family planning, antenatal, intrapartum and postpartum care, along with neonatal and infant healthcare. End users are healthcare workers or women. Studies evaluating the effectiveness of these solutions have demonstrated promising results reflecting adherence to healthcare services and recommendations, information on management and risk identification in pregnancy, improvement in women's satisfaction with healthcare received, in addition to financial benefits for the healthcare system. Objective: The aim of the present review was to identify main apps and software that are currently available in mHealth, designed for use by health professionals during antenatal care. Methods: A systematic review of the literature was conducted through a search for digital health solutions (mhealth/ehealth), apps and/or software, in publications after 2014, during antenatal care provision, in the Pubmed/Medline, Google Scholar databases and Google Play platform. Furthermore, relevant publications cited in bibliographic references of articles selected and unconventional sources (grey literature) were evaluated. Inclusion criteria for analysis of publications or tools were title or abstract descriptions of the following functions: use by health professionals during antenatal care provision, patient electronic record, integration of the app connecting the pregnant woman to the healthcare professional, clinical decision support system and use of mobile technology. The most recent article of duplicated information on apps or mobile health solutions was considered. Systematic review protocol (number CRD42017080501) was registered on PROSPERO in 2017. Results: A search in the Pubmed/Medline database produced 235 results between Jan 2014 and June 2018, 7840 publications in the Google Scholar database; 422 apps in Google Play. The first review of article abstracts and/or descriptors of products available resulted in the exclusion of 8483 sources of data, remaining 14 apps for detailed analysis. Of these, 5 were excluded for failing to meet inclusion criteria or lack of clarity or availability of sufficient data for inclusion. Conclusion: The systematic review demonstrated that it is an arduous task to search for mobile digital solutions that meet the guidelines for clinical use during antenatal care. Although the apps analyzed have great potential for use in different contexts, the bulk of these software systems are unavailable for “prompt delivery”, since the test version cannot be downloaded or access is restricted. © 2019 Elsevier B.V.},
author_keywords={Antenatal care;  Health app;  mHealth;  Prenatal care},
document_type={Review},
source={Scopus},
}

@ARTICLE{Qi2019110,
author={Qi, X.-F. and Hua, Y.-L. and Wang, P. and Wang, Z.-Y.},
title={Leveraging keyword-guided exploration to build test models for web applications},
journal={Information and Software Technology},
year={2019},
volume={111},
pages={110-119},
doi={10.1016/j.infsof.2019.03.016},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063720083&doi=10.1016%2fj.infsof.2019.03.016&partnerID=40&md5=04349d49460e6468d9771d3368040fae},
affiliation={School of Computer Science and Engineering, Southeast University, Nanjing, 211189, China; Key Laboratory of Computer Network and Information Integration, Ministry of Education, Southeast University, Nanjing, 211189, China; School of Computer Science and Technology, Nanjing University of Posts and Telecommunications, Nanjing, 210023, China},
abstract={Context: Dynamic exploration techniques, which automatically exercise possible user interface elements, have been used to explore user interface state flow graphs as test models for web applications. An exhaustive exploration may incur the well-known state explosion problem. In a limited amount of time, most existing dynamic exploration techniques tend to become mired in local or irrelevant regions of the web application due to not considering functionality semantics information. Hence, generated test models have often inadequate functionality coverage for deriving effective test cases. Objective: This paper proposes a keyword-guided exploration strategy for automatic construction of web application test models. The goal is to generate incomplete test models with adequate functionality coverage in a given time budget for deriving test cases w.r.t. specified functionalities. Method: Given very few keywords that describe specified functionalities, our strategy guides the exploration to discover user interface states and transitions among them that are relevant to the specified functionalities by computing similarity scores between text contents in web pages and given keywords. We use nine representative web applications to perform dynamic explorations in a given time budget and empirically evaluate functionality coverage, and other metrics, e.g., code coverage, the size of test model, the number of the test suite, path diversity, and DOM diversity. Results: Our keyword-guided exploration strategy achieves a higher functionality coverage as compared with the generic and feedback-directed exploration strategies. Yet the significant improvement of functionality coverage achieved by our strategy is not exchanged at the cost of other metrics. Conclusion: Our keyword-guided exploration strategy is more effective than the generic and feedback-directed exploration strategies in terms of functionality coverage. In a limited amount of time, test models generated with our strategy can be used to derive effective web application test cases. © 2019},
author_keywords={Automated testing;  Coverage;  Model generation;  Software testing;  Web apps},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wuryantoro2019,
author={Wuryantoro and Puspitawati, I.R. and Fitriyani, R.I. and Soni, P.},
title={Identification of a local variety of 'uwi' (Dioscorea alata Linn.) in four agro-climate regions of East-West Java-Indonesia based on tuber character},
journal={IOP Conference Series: Earth and Environmental Science},
year={2019},
volume={293},
number={1},
doi={10.1088/1755-1315/293/1/012040},
art_number={012040},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069056211&doi=10.1088%2f1755-1315%2f293%2f1%2f012040&partnerID=40&md5=320f5a3a7f271f57402b74cd9a4380f1},
affiliation={Faculty of Agriculture, Merdeka University of Madiun, Jl. Serayu 79, Madiun, 63133, Indonesia; Department of Agriculture and Food Engineering, Indian Institute of Technology Kharagpur, Kharagpur, West Bengal, 721302, India},
abstract={Indonesia is very well known for its genetic diversity of potential tubers plants for alternative and functional food sources. 'Uwi' (Dioscorea alata Linn.) as one of the tuber's plants needs to get attention, so there is no genetic erosion or even extinction. Seeing the high diversity of agro-climates and the high genetic diversity of 'uwi' plants, it is necessary to mapping and identify the suitability of the 'uwi' plant in line with its agro-climate character. The study aims to identify specific local varieties and the distribution of other types with broad adaptability. The analysis using the similarity interval function on the NTSys program showed that of the 47 accessions tested at 70 % similarity level there were four groups scattered in four different agro-climates. The first group enters climate types C2, C3 and B2; the second group enters C2; the third group enters C2 and C3; all fourth enter C2, C3, D3. Found three specific accession only in type C2 and C3 and most of the 'uwi' plants have distribution in all types of climates, showing broad adaptability and potential to be developed without high technology. The colours, shapes, flavours, and feathers of the tubers are the main differentiators in existing diversity. © 2019 Published under licence by IOP Publishing Ltd.},
author_keywords={Distribution;  diversity;  germplasm;  local variety;  uplands},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{CoelhodeCarvalho2019,
author={Coelho de Carvalho, L.M. and Dal Toé Casagrande, M.},
title={Mechanical behaviour of reinforced sand with natural curauá fibers through full scale direct shear tests},
journal={E3S Web of Conferences},
year={2019},
volume={92},
doi={10.1051/e3sconf/20199212003},
art_number={12003},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069745207&doi=10.1051%2fe3sconf%2f20199212003&partnerID=40&md5=72f833209b467643f3a2bee8545b3199},
affiliation={University of Brasília, Civil and Environmental Engineering Department, Brazil},
abstract={Inclusion of natural fibers (sisal, curauá, coco fiber and others) for soil improvement has been the study object in diverse geotechnical areas and it is a topic of growing interest, within the research area of new geotechnical materials. The state of the art in this subject highlights excellent results as soil strength parameters improve and post-cracking strength (toughness) increase. Soil reinforcement technique with fibers is established in the technology of composite materials, this being a combination of two or more materials presenting properties that the component materials do not possess on their own. The aim of this paper is to study the mechanical behaviour of sand-fiber composite by inserting natural curauá fibers into a sandy matrix, with different fiber contents. The fibers were randomly distributed in the soil mass. The experimental program included physical and mechanical characterization of the composites, using full-scale direct shear tests, with samples measuring 30 x 30 cm and 15 cm high. Direct shear tests were carried out using fibers with 25 mm length and 0.5 and 0.75% fiber content (relative to the soil dry weight). The specimens also presented a relative density of 50% and moisture content of 10%. It was sought to establish a pattern behaviour so that the addition of curauá fiber influence can be explained, thus, comparing with the sandy soil shear strength parameters. Inclusion of natural curauá fibers as soil reinforcement presented satisfactory results, as an increase in the soil shear strength parameters was observed when compared with sandy soil results. © The Authors, published by EDP Sciences.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frenkel2019,
author={Frenkel, M. and Bringardner, J. and Rajguru, S.B.},
title={WIP: Student to scholar: A learning community model for professional skills development},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2019},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078785517&partnerID=40&md5=25f326a630aff27690028933c9634fc2},
affiliation={New York University, United States},
abstract={This WorkInProgress paper documents the first steps in the creation of a cocurricular program, Student to Scholar (S2S), designed to assist students in their development of professional skills. A literature review, as well as an examination of local curriculum provides the evidence for the need of such a program. A fully realized S2S program will have a tiered structure involving faculty, graduate students, and undergraduate students. Graduate students entering the S2S program take a forcredit course exposing them to a number of professional skills, and helping them to understand how to teach those skills to undergraduates. Once trained the graduate students become the leaders of cohorts, or learning communities, of undergraduate students. The graduate students will meet with their cohorts several times a semester to engage with them in professional skill workshops. The S2S program aims to prepare both graduate and undergraduate students with the professional skills they will need after graduation regardless of if they are going into industry or academia. A multitude of skills are covered in the program that address the knowledge, skills, and abilities necessary for the Tshaped engineer, including but not limited to: information literacy, leadership, teamwork, diversity, time and project management, reflection scientific/written/oral communication, writing, career services, entrepreneurial mindset, and public speaking. To reinforce students development of these skills the tiered structure of the program transitions students from learners to teachers. This transition occurs at every level, with graduate students senior undergraduates, seniors teaching juniors, and so on down to the first year. Currently, this study aims to help develop the needed institutional support to implement the full S2S program. As that work takes place a pilot program has begun to test student reception to a variety of professional skills workshops. The results of this testing are presented here. Through a partnership between the library and the Vertically Integrated Projects (VIP) program, 5 workshops took place during the fall 2018 and 3 during spring 2019 semesters. Faculty and staff with expertise in various professional skills have been brought in to lead students through these workshops. The students were surveyed during the spring semester to evaluate each workshop. © American Society for Engineering Education, 2019.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2019134,
author={Zhang, Q. and Ma, H. and Wang, Y. and Shen, W. and Wu, C.},
title={Optimization of CPR1000 Charging Pump Shaft Power and Hydraulic Performance Study [CPR1000上充泵的轴功率优化及水力性能研究]},
journal={Hedongli Gongcheng/Nuclear Power Engineering},
year={2019},
volume={40},
number={3},
pages={134-137},
doi={10.13832/j.jnpe.2019.03.0134},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070400164&doi=10.13832%2fj.jnpe.2019.03.0134&partnerID=40&md5=b229c5b8c95b61009138bd9e2e4cd623},
affiliation={Nuclear and Radiation Safety Center, MEE, Beijing, 100082, China; Shanghai Apollo Machinery Co. Ltd., Shanghai, 201401, China},
abstract={Based on the analysis of CPR1000 charging pump hydraulic performance test, it is found that the large inlet area of the diversion body is the main reason for the high shaft power. Every piece of steel plate was welded to the inlet tongue respectively to improve the hydraulic performance of the charging pump. Firstly, with the help of Ansys CFX, the flow field inside the charging pump of the modified scheme was numerically simulated and compared with the measured hydraulic performance data before the scheme modification. Then the hydraulic performance of the charging pump was tested and verified. The results show that the shaft power of the charging pump is controlled below 650 kW, meeting the requirements of the evaluation test program and technical specification, and the safety and reliability of the charging pump is improved. © 2019, Editorial Board of Journal of Nuclear Power Engineering. All right reserved.},
author_keywords={Ansys CFX;  Charging pump;  Diversion body;  Hydraulic performance;  Shaft power},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mon-Nzongo2019576,
author={Mon-Nzongo, D.L. and Tang, J. and Ipoum-Ngome, P.G. and Jin, T. and Song-Manguelle, J.},
title={Design and development of an experimental test-bench based on multi-pulse and multilevel converters},
journal={PEDG 2019 - 2019 IEEE 10th International Symposium on Power Electronics for Distributed Generation Systems},
year={2019},
pages={576-581},
doi={10.1109/PEDG.2019.8807675},
art_number={8807675},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071942104&doi=10.1109%2fPEDG.2019.8807675&partnerID=40&md5=5e878dc1b7e756370d8428335006163c},
affiliation={Research and Development Department, Pearl Electric Corporation, Guangzhou, China; Electrical Engineering Department, Fuzhou University, Fuzhou, China; Facilities Engineering Department, ExxonMobil, TX, United States},
abstract={This paper describes an experimental test-bench based on multi-pulse and multilevel converters proposed for research and development projects related with classical power electronics converters for high-power applications. The developed system is rated 18-kW and regroups in a single set-up most used converter topologies in electric drive and grid-connected. It is consisting of a 3-level neutral-point clamped (NPC) inverter, an 18-pulse rectifier and a multilevel cascaded H-bridge system. The practical implementation of such system for laboratory use is a time-consuming task, mostly for beginners and students. Thus, this paper can serve as a reference design and complete our previous works where no details about this platform were given. The proposed system has been used to demonstrate reversible transfer of electrical and mechanical harmonics for pulsating torque analysis and advanced modulation and control schemes. Test results obtained confirm that, diverse advanced analysis related with classical converters can be achieved by using a single test rig and significantly save time and enables researchers to be focused on the system analysis and control software development. © 2019 IEEE.},
author_keywords={Electrical and mechanical harmonics;  Experimental test-bench multi-pulse rectifier;  Multilevel inverter},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Vansteenwegen2019,
author={Vansteenwegen, D. and Ruddick, K. and Cattrijsse, A. and Vanhellemont, Q. and Beck, M.},
title={The pan-and-tilt hyperspectral radiometer system (PANTHYR) for autonomous satellite validation measurements-Prototype design and testing},
journal={Remote Sensing},
year={2019},
volume={11},
number={11},
doi={10.3390/rs11111360},
art_number={1360},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067387994&doi=10.3390%2frs11111360&partnerID=40&md5=bf0540ef7756bda3f610d697eb52f41a},
affiliation={Flanders Marine Institute (VLIZ), Wandelaarkaai 7, Ostend, 8400, Belgium; Royal Belgian Institute of Natural Sciences (RBINS), Operational Directorate Natural Environment, 29 Rue Vautierstraat, Brussels, 1000, Belgium},
abstract={This paper describes a system, named "pan-and-tilt hyperspectral radiometer system" (PANTHYR) that is designed for autonomous measurement of hyperspectral water reflectance. The system is suitable for deployment in diverse locations (including offshore platforms) for the validation of water reflectance derived from any satellite mission with visible and/or near-infrared spectral bands (400-900 nm). Key user requirements include reliable autonomous operation at remote sites without grid power or cabled internet and only limited maintenance (1-2 times per year), flexible zenith and azimuth pointing, modularity to adapt to future evolution of components and different sites (power, data transmission, and mounting possibilities), and moderate hardware acquisition cost. PANTHYR consists of two commercial off-the-shelf (COTS) hyperspectral radiometers, mounted on a COTS pan-and-tilt pointing system, controlled by a single-board-computer and associated custom-designed electronics which provide power, pointing instructions, and data archiving and transmission. The variable zenith pointing improves protection of sensors which are parked downward when not measuring, and it allows for use of a single radiance sensor for both sky and water viewing. The latter gives cost reduction for radiometer purchase, as well as reduction of uncertainties associated with radiometer spectral and radiometric differences for comparable two-radiance-sensor systems. The system is designed so that hardware and software upgrades or changes are easy to implement. In this paper, the system design requirements and choices are described, including details of the electronics, hardware, and software. A prototype test on the Acqua Alta Oceanographic Tower (near Venice, Italy) is described, including comparison of the PANTHYR system data with two other established systems: the multispectral autonomous AERONET-OC data and a manually deployed three-sensor hyperspectral system. The test established that high-quality hyperspectral data for water reflectance can be acquired autonomously with this system. Lessons learned from the prototype testing are described, and the future perspectives for the hardware and software development are outlined. © 2019 by the authors.},
author_keywords={Autonomous measurements;  Ground-truth data;  Hyperspectral reflectance;  System design;  Validation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bokor2019202,
author={Bokor, B. and Sharma, A. and Hofmann, J.},
title={Experimental investigations on concrete cone failure of rectangular and non-rectangular anchor groups},
journal={Engineering Structures},
year={2019},
volume={188},
pages={202-217},
doi={10.1016/j.engstruct.2019.03.019},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063032594&doi=10.1016%2fj.engstruct.2019.03.019&partnerID=40&md5=1900f87b865bf4d28af8b08a360d0e12},
affiliation={Institute of Construction Materials, University of Stuttgart, Pfaffenwaldring 4, Stuttgart, 70569, Germany},
abstract={The current recommendations given in the codes and guidelines for design of anchorages to concrete are limited in scope to rectangular and regular anchor groups with a maximum of three anchors in a row. Furthermore, several assumptions are made, which may render the designs conservative but may also lead to unconservative results such as the required base plate thickness and the cracked concrete assumption. The aim of this experimental study was to investigate the load-displacement behaviour of tension loaded anchor group configurations that are either only partly covered or not covered by the current design provisions. Furthermore, this work aimed to generate an experimental database on the concrete cone failure of anchor groups under tension loading. The results of a comprehensive experimental program carried out on diverse anchor group configurations within and beyond the scope of the European Standard EN1992-4 are discussed focusing on several aspects such as (i) different geometric configurations, (ii) varying stiffness of the base plate, and (iii) influence of eccentricity. The experimental program contains tests on single anchors and on anchor groups in normal- and high-strength concrete. The testing concept, test setup and the test results are discussed in detail in this paper. Based on the evaluation of the test results presented here, a new analytical model is being developed for the concrete cone failure mode of tension loaded anchor groups that will be presented in a future paper. © 2019},
author_keywords={Anchor group;  Anchorage;  Concrete cone failure;  Experimental investigation;  Tension loading;  Test database},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kaur201956,
author={Kaur, A. and Kaur, K.},
title={Investigation on test effort estimation of mobile applications: Systematic literature review and survey},
journal={Information and Software Technology},
year={2019},
volume={110},
pages={56-77},
doi={10.1016/j.infsof.2019.02.003},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062221200&doi=10.1016%2fj.infsof.2019.02.003&partnerID=40&md5=9aadbc29c4adf5c07c2ce06035b4e91d},
affiliation={I.K. Gujral Punjab Technical University, Kapurthala, India; School of IT, Apeejay Institute of Management Technical Campus Jalandhar, India},
abstract={Context: In the last few years, the exigency of mobile devices has proliferated to prodigious heights. The process of developing the mobile software/application proceeds amidst testing phase to verify the correctness of the mobile app. The estimation of testing plays a vital role in the effective completion of testing. Objective: To identify how estimation of test effort for mobile applications is distinct from other software via published literature and from mobile software organizations. Second is to recognize different issues in adapting traditional test estimation methods to the mobile domain and if suggestions from survey results could be helpful in providing an improved test estimation model for mobile applications. Method: A systematic literature review is conducted followed by a survey through an online questionnaire filled from experienced mobile application developers and testers. Results: The results from SLR cover identification of mobile app specific characteristics and reports test effort estimation techniques in the mobile domain. Findings from survey corroborate that a) Function Point/Test Point Analysis is highly adapted traditional test estimation technique to mobile domain; b) Challenges like uncertain requirements, no tool support for test estimation, complexity in testing, client miscommunication etc. are reported; c)Suggestions to improve test estimation process include proper test planning, adoption of agile methodology, healthier communication among client, developer, and tester etc.; d) On the basis of responses, Analytical Hierarchical Process (AHP) identifies “Diverse Devices and OS” along with “Type of App” as highly influential mobile app characteristic on the test estimation process. Conclusion: Results conclude that the importance of identified mobile app characteristics from SLR cannot be ignored in the estimation process of mobile software testing. There might be a possibility to improve existing test estimation techniques for mobile apps by giving weight to mobile app specific characteristics and by considering suggestions from experienced developers and testers. © 2019 Elsevier B.V.},
author_keywords={Mobile applications;  Software engineering;  Survey;  Systematic literature review (SLR);  Test effort estimation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Harmain2019,
author={Harmain, R.M. and Dali, F.A. and Husain, R.},
title={Physical analyze and hedonic quality of ilabulo crackers skipjack (Katsuwonus pelamis) fortified nano calcium bone},
journal={IOP Conference Series: Earth and Environmental Science},
year={2019},
volume={278},
number={1},
doi={10.1088/1755-1315/278/1/012031},
art_number={012031},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067062343&doi=10.1088%2f1755-1315%2f278%2f1%2f012031&partnerID=40&md5=d4411cb17db9a901fc70ce11b8a11b23},
affiliation={State University of Gorontalo, Faculty of Fisheries and Marine Science, Gorontalo, Indonesia},
abstract={Ilabulo crackers of skipjack fish (Katsuwonus pelamis) is one of the traditional food diversification products. The aim of this research in Indonesia was to analyze the physical and hedonic quality of Ilabulo Skipjack Fish Crackers fortified nano calcium bone on a different formula. Physical testing using TA-XT2i and hedonic quality using Kruskall Wallis non parameter analyze and using SPSS 16 software. The data was using analysis of variance and a significant effect was continued by Duncan test. The results of this research showed physical analyze about the crispness that formulation B higher 17875.7/gf than formulation A 14366.2/gf, formulation C 10142.8/gf and formulation D 4884/gf. All results within different formulas obtained were significantly different. The hedonic quality of formulation A was chosen with the criteria of complete appearance, flat surface, neutral texture, brownish yellow color, slightly fishy and rather savory. © Published under licence by IOP Publishing Ltd.},
author_keywords={crackers;  formulation;  hedonic quality;  organoleptic;  physical analyze},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xu201954,
author={Xu, C. and Zhong, Z.W. and Choi, W.K.},
title={Evaluation of fan-out wafer level package strength},
journal={Microelectronics International},
year={2019},
volume={36},
number={2},
pages={54-61},
doi={10.1108/MI-06-2018-0040},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062546467&doi=10.1108%2fMI-06-2018-0040&partnerID=40&md5=755a57d4982f25164ac9532cbf359a1e},
affiliation={School of Mechanical and Aerospace Engineering, Nanyang Technological University, Singapore},
abstract={Purpose: The fan-out wafer level package (FOWLP) becomes more and more attractive and popular because of its flexibility to integrate diverse devices into a very small form factor. The strength of ultrathin FOWLP is low, and the low package strength often leads to crack issues. This paper aims to study the strength of thin FOWLP because the low package strength may lead to the reliability issue of package crack. Design/methodology/approach: This paper uses the experimental method (three-point bending test) and finite element method (ANSYS simulation software) to evaluate the FOWLP strength. Two theoretical models of FOWLP strength are proposed. These two models are based on the location of FOWLP initial fracture point. Findings: The results show that the backside protection tape does not have the ability to enhance the FOWLP strength, and the strength of over-molded structure FOWLP is superior to that of other structure FOWLPs with the same thickness level. Originality/value: There is ample research about the silicon strength and silicon die strength. However, there is little research about the package level strength and no research about the FOWLP strength. The FOWLP is made up of various materials. The effect of individual component and external environment on the FOWLP strength is uncertain. Therefore, the study of strength behavior of FOWLP is significant. © 2019, Emerald Publishing Limited.},
author_keywords={Advanced packaging;  Fan-out wafer level package;  Finite element method;  Microelectronics packaging;  Package strength;  Three-point bending test},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fu201969,
author={Fu, Y. and Terechko, A. and Bijlsma, T. and Cuijners, P.J.L. and Redegeld, J. and Ors, A.O.},
title={A Retargetable Fault Injection Framework for Safety Validation of Autonomous Vehicles},
journal={Proceedings - 2019 IEEE International Conference on Software Architecture - Companion, ICSA-C 2019},
year={2019},
pages={69-76},
doi={10.1109/ICSA-C.2019.00020},
art_number={8712351},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066503750&doi=10.1109%2fICSA-C.2019.00020&partnerID=40&md5=21fa2ead17fc50ac8e6899868761d46e},
affiliation={NXP Semiconductors, Eindhoven, Netherlands; TNO Embedded Systems Innovation, Eindhoven, Netherlands; Eindhoven University of Technology, Department of Mathematics and Computer Science, Eindhoven, Netherlands},
abstract={Autonomous vehicles use Electronic Control Units running complex software to improve passenger comfort and safety. To test safety of in-vehicle electronics, the ISO 26262 standard on functional safety recommends using fault injection during component and system-level design. A Fault Injection Framework (FIF) induces hard-to-trigger hardware and software faults at runtime, enabling analysis of fault propagation effects. The growing number and complexity of diverse interacting components in vehicles demands a versatile FIF at the vehicle level. In this paper, we present a novel retargetable FIF based on debugger interfaces available on many target systems. We validated our FIF in three Hardware-In-the-Loop setups for autonomous driving based on the NXP BlueBox prototyping platform. To trigger a fault injection process, we developed an interactive user interface based on Robot Operating System, which also visualized vehicle system health. Our retargetable debugger-based fault injection mechanism confirmed safety properties and identified safety shortcomings of various automotive systems. © 2019 IEEE.},
author_keywords={Automotive Systems;  Autonomous Driving;  Debugger Interface;  Fault Injection;  Functional Safety;  ISO 26262;  NXP BlueBox Prototyping Platform},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yu2019,
author={Yu, J.},
title={Analysis and Design of Course Website for Software Testing Based on SPOC},
journal={Journal of Physics: Conference Series},
year={2019},
volume={1187},
number={5},
doi={10.1088/1742-6596/1187/5/052015},
art_number={052015},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067701797&doi=10.1088%2f1742-6596%2f1187%2f5%2f052015&partnerID=40&md5=2a4c59a38e19cfdca51b55137ada2c29},
affiliation={College of Computer Engineering, Anhui SanLian University, Hefei, 230601, China},
abstract={Technologies of the architecture on SSH(Struts-Spring-Hibernate), database interface access, and web data mining are used to analyse and design of a lightweight course website for software testing which is based on SPOC(Small Private Online Course) for blended teaching on campus. Functions of learning topics releasing on the course, organizing and coordinating learners' discussion to promote the learning process can be achieved through the website for teachers, and the website can also provide learners with selecting some social learning tools(BBS, QQ, WeChat) to complete online communication for the course learning. Finally, further work is expected in the paper. Firstly, the targeted and distinctive mobile learning resources and smart education mode should be constructed which is based on SPOC. Secondly, the visualized, diversified third-party platforms and tools in the course website should be integrated effectively which are enable learners to complete the process of online exercising or examining for some types of subjective testing items. Thirdly, how to ensure the reliability, toughness, running performance, safety, stability for the course website which is formed by cutting out some non-essential functions from existed MOOC(Massive Open Online Course) platforms, and improve the functions of multiple data statistics with web data mining technology and motivational observation for the website development. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lei2019,
author={Lei, T. and Liu, Y. and Fu, H.J. and Zhang, X.},
title={The designing of integrated testing system for the electric power system in large civil aircraft},
journal={I2MTC 2019 - 2019 IEEE International Instrumentation and Measurement Technology Conference, Proceedings},
year={2019},
volume={2019-May},
doi={10.1109/I2MTC.2019.8826900},
art_number={8826900},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072834626&doi=10.1109%2fI2MTC.2019.8826900&partnerID=40&md5=d4b2451ad61763039245f04ef70420ad},
affiliation={Electrical Engineering Department of the Northwestern, Polytechnic University, Xi'an, China},
abstract={A integrated testing platform for verifying the function of Variable Frequency Alternating Current (VFAC) electrical power system(EPS) for Civil aircraft is presented in this papers. The diverse functions of testing system are elaborated for the Aircraft function testing such as the power quality analysis, power distribution logic checking, system control and protective ability verification, and the communication testing with the Avionic system by the Avionics Full-Duplex Switched Ethernet (AFDX) bus. In this paper, The hardware scheme which is the distributed and network measuring system based on PXI-bus was proposed. The testing software which verified the function of EPS was developed by Labview software. The integrated testing platform can be simulated as Aircraft copper bird experimental platform which can send the on/off command of emulated Electric load to power generation and distribution subsystem, receive the status or fault information from Electric load and controlling or protective terminal device during different flight profile such as taxing, climbing, cruising operation stage. The power quality analysis algorithm also was developed on the testing system to verify the performance of the EPS. The function of overall testing system proposed is verified by experimental results. © 2019 IEEE.},
author_keywords={AFDX;  Electric power system (EPS);  Integrated testing system;  Labview;  Power quality analysis;  PXI bus;  RPDU},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Semerath201943,
author={Semerath, O. and Babikian, A.A. and Pilarski, S. and Varro, D.},
title={VIATRA solver: A framework for the automated generation of consistent domain-specific models},
journal={Proceedings - 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion, ICSE-Companion 2019},
year={2019},
pages={43-46},
doi={10.1109/ICSE-Companion.2019.00034},
art_number={8802773},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071851531&doi=10.1109%2fICSE-Companion.2019.00034&partnerID=40&md5=b7a7af6e8151013b0f8b0f54758eee93},
affiliation={MTA-BME Lendulet Cyber-Physical Systems Research Group, Budapest, Hungary; Budapest University of Technolgy and Economics, Dept. of Measurement and Information Systems, Budapest, Hungary; McGill University, Dept. of Electrical and Computer Engineering, Montreal, QC, Canada},
abstract={Viatra Solver [1] is a novel open source software tool to automatically synthesize consistent and diverse domain-specific graph models to be used as a test suite for the systematic testing of CPS modelling tools. Taking a metamodel, and a set of well-formedness constraints of a domain as input, the solver derives a diverse set of consistent graph models where each graph is compliant with the metamodel, satisfies consistency constraints, and structurally different from each other. The tool is integrated into the Eclipse IDE or it is executable from the command line. © 2019 IEEE.},
author_keywords={Graph generation;  Logic solver;  Model based system engineering;  Test generation;  Tool testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jabbarvand20191119,
author={Jabbarvand, R. and Lin, J.-W. and Malek, S.},
title={Search-Based Energy Testing of Android},
journal={Proceedings - International Conference on Software Engineering},
year={2019},
volume={2019-May},
pages={1119-1130},
doi={10.1109/ICSE.2019.00115},
art_number={8812097},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071277035&doi=10.1109%2fICSE.2019.00115&partnerID=40&md5=447e28675d8f52da8a89b75eb46d5dd4},
affiliation={University of California, Irvine, United States},
abstract={The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efficiently use the device's battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app's energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app's energy behavior, COBWEB generates a test suite that can effectively find energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efficiently test energy behavior of apps, but also its superiority over prior techniques by finding a wider and more diverse set of energy defects. © 2019 IEEE.},
author_keywords={Android;  Energy Testing;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2019,
author={Zhang, S. and Shen, W. and Zhangt, M. and Cao, X. and Cheng, Y.},
title={Experience-Driven Wireless D2D Network Link Scheduling: A Deep Learning Approach},
journal={IEEE International Conference on Communications},
year={2019},
volume={2019-May},
doi={10.1109/ICC.2019.8761818},
art_number={8761818},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070217479&doi=10.1109%2fICC.2019.8761818&partnerID=40&md5=5574a92d9002c0849a7beddc9a7c594b},
affiliation={Department of Electrical and Computer Engineering, Illinois Institute of Technology, Chicago, IL  60616, United States; AT AND T Labs Research, Middletown, NJ  07748, United States; School of Automation, Southeast University, Nanjing, 210018, China},
abstract={The protocol design of device-to-device (D2D) networks have regained research interest in recent years, due to the increasing number of networking devices and the diverse deployment settings. Most of the network optimization tasks are fundamentally difficult NP-hard problems in wireless settings, because managing interference introduces combinatorial complexity. Existing approaches use general heuristic algorithms for the underlying graph problems. While efficient and simple, they are not adaptive to the changing requirement and priorities of the service providers, and make no use of the past data to recognize and exploit the information within. In this paper, we study a representative network optimization task of maximizing the throughput-based system utility through link scheduling in a single-radio, single-channel D2D networks, and propose a learning-based method to leverage past experience to generate a good scheduling policy. We combine the pattern matching capabilities provided from recurrent neural networks (RNN) and the flexibility in changing environment from reinforcement learning (RL). The algorithm is implemented with existing software frameworks and tested with numerical experiments. We find that its overall solution quality is comparable to existing heuristics with various network scales, and report an improved system throughput with significant lower computation time. © 2019 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tomassi2019339,
author={Tomassi, D.A. and Dmeiri, N. and Wang, Y. and Bhowmick, A. and Liu, Y.-C. and Devanbu, P.T. and Vasilescu, B. and Rubio-Gonzalez, C.},
title={BugSwarm: Mining and Continuously Growing a Dataset of Reproducible Failures and Fixes},
journal={Proceedings - International Conference on Software Engineering},
year={2019},
volume={2019-May},
pages={339-349},
doi={10.1109/ICSE.2019.00048},
art_number={8812141},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067958238&doi=10.1109%2fICSE.2019.00048&partnerID=40&md5=ecfd53ba8cd29b7ac3750475d14f4ac2},
affiliation={University of California, Davis, United States; Carnegie Mellon University, United States},
abstract={Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually. © 2019 IEEE.},
author_keywords={Bug Database;  Experiment Infrastructure;  Program Analysis;  Reproducibility;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Martynova201949,
author={Martynova, Y.Z. and Khairullina, V.R. and Biglova, Y.N. and Mustafin, A.G.},
title={Quantitative structure-property relationship modeling of the C 60 fullerene derivatives as electron acceptors of polymer solar cells: Elucidating the functional groups critical for device performance},
journal={Journal of Molecular Graphics and Modelling},
year={2019},
volume={88},
pages={49-61},
doi={10.1016/j.jmgm.2018.12.013},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060094664&doi=10.1016%2fj.jmgm.2018.12.013&partnerID=40&md5=1dddb27a8d52f375523cbf7c462d7365},
affiliation={Bashkir State University, 32 Z. Validi Str., Ufa, 450076, Russian Federation; Ufa Institute of Chemistry of Russian Academy of Sciences, 71 Prospect Oktyabrya, Ufa, 450054, Russian Federation},
abstract={Using the GUSAR 2013 program, we have performed a quantitative analysis of the “structure–power conversion efficiency (PCE)” on the series of 100 methano[60]fullerenes previously tested as acceptor components of bulk-heterojunction polymer organic solar cells (PSCs) utilizing the same donor polymer, viz. poly(3-hexylthiophene). Based on the MNA and QNA descriptors and self-consistent regression implemented in the program, six statistically significant consensus models for predicting the PCE values of the methano[60]fullerene-based PSCs have been constructed. The structural fragments of the fullerene compounds leading to an increase in the device performances are determined. Based on these structural descriptors, we have designed the three methano[60]fullerenes included in the training sets and characterized by poor optoelectrical properties is performed. As a result, two new compounds with potentially moderate efficiency have been proposed. This result opens opportunities of using the GUSAR 2013 program for modeling of the “structure–PCE” relationship for diverse compounds (not only fullerene derivatives). © 2019 Elsevier Inc.},
author_keywords={GUSAR 2013;  methano[60]fullerene;  Power conversion efficiency;  QNA- and MNA-descriptors;  Quantitative structure–property relationship models;  [60]PCBM},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jawadi201912,
author={Jawadi, F. and Ftiti, Z.},
title={Oil price collapse and challenges to economic transformation of Saudi Arabia: A time-series analysis},
journal={Energy Economics},
year={2019},
volume={80},
pages={12-19},
doi={10.1016/j.eneco.2018.12.003},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059857807&doi=10.1016%2fj.eneco.2018.12.003&partnerID=40&md5=92984d09671543cdb21c33ba9b8de385},
affiliation={University of Lille, 104 Avenue du Peuple Belge, Lille, 59043, France; EDC Paris Business School, OCRE-Lab, 70 Galerie des Damiers, Courbevoie, 92415, France},
abstract={This paper studies the impact of oil price changes on economic growth in Saudi Arabia to measure its dependency on the crude oil sector. To this end, an On/Off threshold regression is specified to allow the oil/GDP relationship to be asymmetric, nonlinear, and time-varying with regard to the business cycle phases. Further, we empirically test the diversification hypothesis put forward by the National Transformation Program (Saudi Vision 2030) to check whether the equity-energy investment initiative could boost economic growth in Saudi Arabia. First, our findings confirm the contribution of the oil sector to economic growth in the country, but also show that the oil/Saudi economy relationship exhibits nonlinearity and threshold effects, as the impact of oil price varies per regime depending on the state of the market. Second, in line with the Vision 2030 expectations, we are in favor of transforming the economy and opening its equity market. We also quantify a positive and significant impact of equity investment on the Saudi Arabian economy. Further, this diversification route will stimulate a beneficial oil effect on the real economy. © 2018 Elsevier B.V.},
author_keywords={Diversification;  Economic growth;  Nonlinearity;  Oil price;  Saudi vision 2030;  Threshold effect},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou20191,
author={Zhou, P. and Liu, J. and Liu, X. and Yang, Z. and Grundy, J.},
title={Is deep learning better than traditional approaches in tag recommendation for software information sites?},
journal={Information and Software Technology},
year={2019},
volume={109},
pages={1-13},
doi={10.1016/j.infsof.2019.01.002},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059820569&doi=10.1016%2fj.infsof.2019.01.002&partnerID=40&md5=09b27d2dc044b340bad678abbd1ea781},
affiliation={School of Computer Science, Wuhan University, Wuhan, China; Key Laboratory of Network Assessment Technology, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; School of Information Technology, Deakin University, Geelong, Australia; Department of Computer Science, Western Michigan University, Kalamazoo, MI, United States; Faculty of Information Technology, Monash University, Melbourne, Australia},
abstract={Context: Inspired by the success of deep learning in other domains, this new technique been gaining widespread recent interest in being applied to diverse data analysis problems in software engineering. Many deep learning models, such as CNN, DBN, RNN, LSTM and GAN, have been proposed and recently applied to software engineering tasks including effort estimation, vulnerability analysis, code clone detection, test case selection, requirements analysis and many others. However, there is a perception that applying deep learning is a ”silver bullet” if it can be applied to a software engineering data analysis problem. Object: This motivated us to ask the question as to whether deep learning is better than traditional approaches in tag recommendation task for software information sites. Method: In this paper we test this question by applying both the latest deep learning approaches and some traditional approaches on tag recommendation task for software information sites. This is a typical Software Engineering automation problem where intensive data processing is required to link disparate information to assist developers. Four different deep learning approaches – TagCNN, TagRNN, TagHAN and TagRCNN – are implemented and compared with three advanced traditional approaches – EnTagRec, TagMulRec, and FastTagRec. Results: Our comprehensive experimental results show that the performance of these different deep learning approaches varies significantly. The performance of TagRNN and TagHAN approaches are worse than traditional approaches in tag recommendation tasks. The performance of TagCNN and TagRCNN approaches are better than traditional approaches in tag recommendation tasks. Conclusion: Therefore, using appropriate deep learning approaches can indeed achieve better performance than traditional approaches in tag recommendation tasks for software information sites. © 2019 Elsevier B.V.},
author_keywords={Data analysis;  Deep learning;  Software information site;  Software object;  Tag recommendation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chaudhary2019583,
author={Chaudhary, A. and Agarwal, A.P. and Rana, A. and Kumar, V.},
title={Crow Search Optimization Based Approach for Parameter Estimation of SRGMs},
journal={Proceedings - 2019 Amity International Conference on Artificial Intelligence, AICAI 2019},
year={2019},
pages={583-587},
doi={10.1109/AICAI.2019.8701318},
art_number={8701318},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065603429&doi=10.1109%2fAICAI.2019.8701318&partnerID=40&md5=c231293a86e7d6dd12c583ffba711ab3},
affiliation={Department of Computer Science and Engineering, Amity University, Noida, India; Department of Computer Science and Engineering, Amity School of Engineering and Technology, Delhi, India; Department of Applied Mathematics, Amity University, Noida, India},
abstract={Software Reliability Growth Model (SRGM) is a mathematical relation amidst the diverse attributes of testing as it proceeds, and enables the estimation of the optimal time of release, stop time for testing and numerous findings related to software faults, failures and release policies. However, parameter estimation of SRGMs is a challenging task and plays an important role in predicting failure behavior. The parameter estimation for a SRGM needs to be addressed with utmost care as traditional parameter estimation technique does not reflect consistency in prediction. This paper presents an efficient approach for parameter estimation of SRGMs using Crow Search Optimization. Furthermore, we have compared the proposed approach with traditional and genetic algorithm based approaches. The results have been validated on a real data set. © 2019 IEEE.},
author_keywords={Crow Search Optimization;  Parameter estimation;  Software Reliability;  SRGM;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Huo2019,
author={Huo, W. and Pi, Y. and Feng, M. and Qi, Y. and Gao, Y. and Caracappa, P.F. and Chen, Z. and Xu, X.G.},
title={VirtualDose-IR: A cloud-based software for reporting organ doses in interventional radiology},
journal={Physics in Medicine and Biology},
year={2019},
volume={64},
number={9},
doi={10.1088/1361-6560/ab0bd5},
art_number={095012},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065295693&doi=10.1088%2f1361-6560%2fab0bd5&partnerID=40&md5=fd683c83539fc75a108e4e93944a1a35},
affiliation={School of Physical Sciences, University of Science and Technology of China, Hefei, China; Department of Medical Physics, Memorial Sloan Kettering Cancer Center, New York, NY, United States; Virtual Phantoms Inc., Albany, NY, United States; Nuclear Engineering Program, Rensselaer Polytechnic Institute, Troy, NY, United States},
abstract={A cloud-based software, VirtualDose-IR (Virtual Phantoms Inc., Albany, New York, USA), designed to report organ doses and effective doses for a diverse patient population from interventional radiology (IR) procedures has been developed and tested. This software is based on a comprehensive database of Monte Carlo-generated organ dose built with a set of 21 anatomically realistic patient phantoms. The patient types included in this database are both male and female people with different ages reflecting reference adults, obese people with different BMIs and pregnant women at different gestational stages. Selectable parameters such as patient type, tube voltage, filtration thickness, beam direction, field size, and irradiation site are also considered in VirtualDose-IR. The software has been implemented using the 'Software as a Service (SaaS)' delivery concept permitting simultaneous multi-user, multi-platform access without requiring local installation. The patient doses resulting from different target sites and patient populations were reported using the VirtualDose-IR system. The patient doses under different source to surface distances (SSD) and beam angles calculated by VirtualDose-IR and Monte Carlo simulations were compared. For most organs, the dose differences between VirtualDose-IR results and Monte Carlo results were less than 0.3 mGy at 15 000 mGy ∗ cm2 kerma-area product (KAP). The organ dose results were compared with measurement data previously reported in literatures. The doses to organs that were located within the irradiation field match closely with experimental measurement data. The differences in the effective dose values between calculated using VirtualDose-IR and those measured were less than 2.5%. The dose errors of most organs between VirtualDose-IR and literature results were less than 40%. These results validate the accuracy of organ doses reported by VirtualDose-IR. With the inclusion of pre-specified clinical IR examination parameters (such as beam direction, target location, field of view and beam quality) and the latest anatomically realistic patient phantoms in Monte Carlo simulations, VirtualDose-IR provides users with accurate dose information in order to systematically compare, evaluate, and optimize IR plans. © 2019 Institute of Physics and Engineering in Medicine.},
author_keywords={interventional radiology;  Monte Carlo;  software as a service (SaaS);  VirtualDose-IR},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Doležal201985,
author={Doležal, M. and Vlachos, M. and Secci, M. and Demesticha, S. and Skarlatos, D. and Liarokapis, F.},
title={UNDERSTANDING UNDERWATER PHOTOGRAMMETRY for MARITIME ARCHAEOLOGY THROUGH IMMERSIVE VIRTUAL REALITY},
journal={ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
year={2019},
volume={42},
number={2/W10},
pages={85-91},
doi={10.5194/isprs-archives-XLII-2-W10-85-2019},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065675552&doi=10.5194%2fisprs-archives-XLII-2-W10-85-2019&partnerID=40&md5=ebff541ac6b1618f450f4328892bd679},
affiliation={Masaryk University, Faculty of Informatics, HCI Lab, Botanická 554/68a, Ponava, Brno, 60200, Czech Republic; Cyprus University of Technology, Faculty of Engineering and Technology, Department of Civil Engineering and Geomatics, PO BOX 50329, Lemesos, 3603, Cyprus; University of Cyprus, Archaeological Research Unit, PoBox 20537, Nicosia, 1678, Cyprus},
abstract={Underwater archaeological discoveries bring new challenges to the field, but such sites are more difficult to reach and, due to natural influences, they tend to deteriorate fast. Photogrammetry is one of the most powerful tools used for archaeological fieldwork. Photogrammetric techniques are used to document the state of the site in digital form for later analysis, without the risk of damaging any of the artefacts or the site itself. To achieve best possible results with the gathered data, divers should come prepared with the knowledge of measurements and photo capture methods. Archaeologists use this technology to record discovered arteacts or even the whole archaeological sites. Data gathering underwater brings several problems and limitations, so specific steps should be taken to get the best possible results, and divers should well be prepared before starting work at an underwater site. Using immersive virtual reality, we have developed an educational software to introduce maritime archaeology students to photogrammetry techniques. To test the feasibility of the software, a user study was performed and evaluated by experts. In the software, the user is tasked to put markers on the site, measure distances between them, and then take photos of the site, from which the 3D mesh is generated offline. Initial results show that the system is useful for understanding the basics of underwater photogrammetry. © 2019 Copernicus GmbH. All righhts reserved.},
author_keywords={gamification;  Underwater archaeology;  underwater photogrammetry;  virtual reality},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Williams2019292,
author={Williams, A. and Rainer, A.},
title={Do software engineering practitioners cite software testing research in their online articles? A larger scale replication},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={292-297},
doi={10.1145/3319008.3319708},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064757189&doi=10.1145%2f3319008.3319708&partnerID=40&md5=aadabe10c16aae0216d3d5fbea4956fc},
affiliation={Department of Computer Science and Software Engineering, University of Canterbury, Christchurch, New Zealand},
abstract={Background: Software engineering (SE) research continues to study the degree to which practitioners perceive research as relevant to practice. Such studies typically comprise surveys of practitioner opinions. In a preliminary, and relatively small scale, study of online articles we previously found few explicit citations to software testing research. Our previous study provided an in situ complement to the typical survey study, however the findings of the previous study were limited by the size of our sample. Objective: To further investigate whether and how practitioners cite software testing research in the grey literature, by using a larger and more diverse dataset. Method: We analyse four distinct datasets totalling over 400,000 online articles with approx. 2M external citations. Two datasets were generated by crawling predefined domains and two were generated by applying heuristics, developed in prior research, in Google searches. Citations are classified and then analysed. Results: We find a (very) low percentage of citations to research. Conclusion: Our replication corroborates our preliminary study and findings from others. In relative terms, topic–specific searches appear to return results that contain articles with more citations. Our results and method provide a basis for benchmarking. © 2019 Association for Computing Machinery.},
author_keywords={Evidence;  Grey literature;  Research impact;  Research relevance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Raulamo-Jurvanen201957,
author={Raulamo-Jurvanen, P. and Hosio, S. and Mäntylä, M.V.},
title={Practitioner evaluations on software testing tools},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={57-66},
doi={10.1145/3319008.3319018},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064735016&doi=10.1145%2f3319008.3319018&partnerID=40&md5=eb1abca0d4a93d65435a753dd0fff264},
affiliation={M3S, University of Oulu, Oulu, Finland; UBICOMP, University of Oulu, Oulu, Finland},
abstract={In software engineering practice, evaluating and selecting the software testing tools that best fit the project at hand is an important and challenging task. In scientific studies of software engineering, practitioner evaluations and beliefs have recently gained interest, and some studies suggest that practitioners find beliefs of peers more credible than empirical evidence. To study how software practitioners evaluate testing tools, we applied online opinion surveys (n=89). We analyzed the reliability of the opinions utilizing Krippendorff’s alpha, intra-class correlation coefficient (ICC), and coefficients of variation (CV). Negative binomial regression was used to evaluate the effect of demographics. We find that opinions towards a specific tool can be conflicting. We show how increasing the number of respondents improves the reliability of the estimates measured with ICC. Our results indicate that on average, opinions from seven experts provide a moderate level of reliability. From demographics, we find that technical seniority leads to more negative evaluations. To improve the understanding, robustness, and impact of the findings, we need to conduct further studies by utilizing diverse sources and complementary methods. © 2019 Copyright held by the owner/author(s).},
author_keywords={Opinion survey;  Reliability;  Software testing tool;  Tool evaluation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Slhoub2019,
author={Slhoub, K. and Carvalho, M. and Nembhard, F.},
title={Evaluation and comparison of agent-oriented methodologies: A software engineering viewpoint},
journal={SysCon 2019 - 13th Annual IEEE International Systems Conference, Proceedings},
year={2019},
doi={10.1109/SYSCON.2019.8836962},
art_number={8836962},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073160489&doi=10.1109%2fSYSCON.2019.8836962&partnerID=40&md5=b608503215032cb08235f8e55dcbe693},
affiliation={College of Engineering and Sciences, Florida Institute of Technology, Melbourne, FL, United States},
abstract={Numerous agent-oriented methodologies that offer a rich pool of resources to support developers of agent-based systems have been proposed. However, the use of existing methodologies in industrial settings is still limited due to the large volume of methodologies, diversity of covered scopes, ambiguity in concepts, and lack of maturity. This makes it difficult for agent technology practitioners to choose the appropriate methodology that best fits their given development context. To eliminate such agent-based development bottleneck, it is important to introduce suitable methods for evaluating, comparing, and classifying agent-oriented methodologies in order to leverage their usage among practitioners. Having systems to evaluate methodologies can effectively help developers better understand existing methodologies, realize their benefits, outline their pros and cons, and assist practitioners with selecting the best-fit methodology for a specific agent-based project. In response, this paper proposes a novel criteria-based evaluation that is influenced by software engineering practices to assess and compare agentoriented methodologies. The proposed evaluation is derived from the software engineering body of knowledge (SWEBOK) and provides a simplified method to assess the coverage degree of an agent-oriented methodology with respect to major software knowledge areas such as the requirements and testing phases. We demonstrate the applicability of the proposed evaluation by applying it to three agent-oriented methodologies (PASSI, MaSE, and Prometheus) in the software engineering requirements and testing phases. © 2019 IEEE.},
author_keywords={Agent-Oriented Methodologies;  AgentOriented Software Engineering;  AOSE;  Multi-Agent Systems;  Software Requirements;  Software Testing;  SWEBOK},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Castro2019,
author={Castro, A. and Souza, J.P. and Rocha, L. and Silva, M.F.},
title={AdaptPack Studio: Automatic Offline Robot Programming Framework for Factory Environments},
journal={19th IEEE International Conference on Autonomous Robot Systems and Competitions, ICARSC 2019},
year={2019},
doi={10.1109/ICARSC.2019.8733626},
art_number={8733626},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068482677&doi=10.1109%2fICARSC.2019.8733626&partnerID=40&md5=4242a5e8a82ffe6bca2fb19a6f051873},
affiliation={INESC TEC, Porto, Portugal; INESC TEC, ISEP-IPP, Porto, Portugal},
abstract={The brisk and dynamic environment that factories are facing, both as an internal and an external level, requires a collection of handy tools to solve emerging issues in the industry 4.0 context. Part of the common challenges that appear are related to the increasing demand for high adaptability in the organizations' production lines. Mechanical processes are becoming faster and more adjustable to the production diversity in the Fast Moving Consumer Goods (FMCG). Concerning the previous characteristics, future factories can only remain competitive and profitable if they have the ability to quickly adapt all their production resources in response to inconstant market demands. Having previous concerns in focus, this paper presents a fast and adaptative framework for automated cells modeling, simulation and offline robot programming, focused on palletizing operations. Established as an add-on for the Visual Components (VC) 3D manufacturing simulation software, the proposed application allows performing fast layout modeling and automatic offline generation of robot programs. Furthermore, A∗ based algorithms are used for generating collision-free trajectories, discretized both in the robot joints space and in the Cartesian space. The software evaluation was tested inside the VC simulation world and in the real-world scenario. Results have shown to be concise and accurate, with minor displacement inaccuracies due to differences between the virtual model and the real world. © 2019 IEEE.},
author_keywords={Factory layout modeling;  Industry 4.0;  Offline programming;  Product palletizing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li2019,
author={Li, X. and Fang, X. and Chen, G. and Gong, Y. and Wang, J. and Li, J.},
title={Evaluating curb inlet efficiency for urban drainage and road bioretention facilities},
journal={Water (Switzerland)},
year={2019},
volume={11},
number={4},
doi={10.3390/w11040851},
art_number={851},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065011403&doi=10.3390%2fw11040851&partnerID=40&md5=05d7a0119521f5993cfe9acbc7ba7f1b},
affiliation={Department of Civil Engineering, Auburn University, Auburn, AL  36849-5337, United States; College of Hydrology and Water Resources, Hohai University, No. 1 Xikang Road, Nanjing, 210098, China; Key Laboratory of Urban Stormwater System and Water Environment, Ministry of Education, Beijing University of Civil Engineering and Architecture, Beijing, 100044, China},
abstract={An updated two-dimensional flow simulation program, FullSWOF-ZG, which fully (Full) solves shallow water (SW) equations for overland flow (OF) and includes submodules modeling infiltration by zones (Z) and flow interception by grate-inlet (G), was tested with 20 locally depressed curb inlets to validate the inlet efficiency (Eci), and with 80 undepressed curb inlets to validate the inlet lengths (LT) for 100% interception. Previous curb inlet equations were based on certain theoretical approximations and limited experimental data. In this study, 1000 road-curb inlet modeling cases from the combinations of 10 longitudinal slopes (S0, 0.1-1%), 10 cross slopes (Sx, 1.5-6%), and 10 upstream inflows (Qin, 6-24 L/s) were established and modeled to determine LT. The second 1000 modeling cases with the same 10 S0 and 10 Sx and 10 curb inlet lengths (Lci, 0.15-1.5 m) were established to determine Eci. The LT and Eci regression equations were developed as a function of input parameters (S0, Sx, and Qin) and Lci/LT with the multiple linear regression method, respectively. Newly developed regression equations were applied to 10,000 inlet design cases (10 S0, 10 Sx, 10 Qin, and 10 Lci combinations) and comprehensively compared with three equations in previous studies. The 100% intercepted gutter flow (Qg100) equations were derived, and over-prediction of Qg100 from previous methods was strongly correlated to smaller S0. Newly developed equations gave more accurate estimations of LT and Eci over a wide range of input parameters. These equations can be applied to designing urban drainage and road bioretention facilities, since they were developed using a large number of simulation runs with diverse input parameters, but previous methods often overpredict the gutter flow of total interception when the longitudinal slope S0 is small. © 2019 by the authors.},
author_keywords={Bioretention;  Curb inlet;  Inlet efficiency;  Intercepted flow;  Overland flow;  Two-dimensional simulation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tue2019217,
author={Tue, N.V. and Ehmann, R. and Betschoga, C. and Tung, N.D.},
title={Effect of low amounts of shear reinforcement on the shear resistance of reinforced concrete beams with different M/V-combinations [Einfluss geringer Querkraftbewehrung auf die Querkrafttragfähigkeit von Stahlbetonbalken unterschiedlicher M/V-Kombinationen]},
journal={Beton- und Stahlbetonbau},
year={2019},
volume={114},
number={4},
pages={217-230},
doi={10.1002/best.201800075},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058012479&doi=10.1002%2fbest.201800075&partnerID=40&md5=794b5d958bd5f3445d4021a470b1cd65},
affiliation={Technische Universität Graz, Institut für Betonbau, Lessingstraße 25, Graz, 8010, Austria; Bundesanstalt für Wasserbau, Referat B1 Massivbau, Kußmaulstraße 17, Karlsruhe, 76187, Germany},
abstract={Effect of low amounts of shear reinforcement on the shear resistance of reinforced concrete beams with different M/V-combinations. In this experimental program, shear tests were conducted on beams with shear reinforcement ratio in the range of 0.6–1.6 times the minimum shear reinforcement (ρ w,min ) according to EC2. The program consisted of four simply supported beams subjected to point loads, three simply supported beams and four cantilevers subjected to uniform loads. The test results showed a considerable influence of the static system and loading condition on the crack pattern and load bearing capacity of the tested beams. While a reinforcement ratio lower than ρ w,min had almost no influence on the shear resistance of beams with point loads, a low ration of shear reinforcement corresponding to 0.6ρ w,min could significantly enhance the shear resistance of beams with uniform loads. With an application of innovative measuring methods, changes in the flow of forces in beams at diverse loading levels could be analyzed. © 2019, Ernst und Sohn. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{NoAuthor2019,
title={Proceeding - 2018 International Symposium on Advanced Intelligent Informatics: Revolutionize Intelligent Informatics Spectrum for Humanity, SAIN 2018},
journal={Proceeding - 2018 International Symposium on Advanced Intelligent Informatics: Revolutionize Intelligent Informatics Spectrum for Humanity, SAIN 2018},
year={2019},
page_count={230},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064157399&partnerID=40&md5=4b5d1e41f5d66fbcbfce4597fde491f2},
abstract={The proceedings contain 37 papers. The topics discussed include: transformation of 3-D jerk chaotic system into parallel form; artificial neural network for predicting Indonesian economic growth using macroeconomics indicators; a heuristic network for predicting the percentage of gross domestic product distribution; data reduction using rough set theory and conditional entropy: case study on software testing; agent-based modeling and simulation of speciation and ecosystem diversity; a survey of graph-based algorithms for discovering business process models; supporting investment decision using socio-economic issues exploration and stock price prediction; and colorectal polyp detection using feedforward neural network with image feature selection.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Madeiral2019468,
author={Madeiral, F. and Urli, S. and Maia, M. and Monperrus, M.},
title={BEARS: An Extensible Java Bug Benchmark for Automatic Program Repair Studies},
journal={SANER 2019 - Proceedings of the 2019 IEEE 26th International Conference on Software Analysis, Evolution, and Reengineering},
year={2019},
pages={468-478},
doi={10.1109/SANER.2019.8667991},
art_number={8667991},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064151415&doi=10.1109%2fSANER.2019.8667991&partnerID=40&md5=664ffc6e21723bf279da626dc8662b6c},
affiliation={Federal University of Uberlândia, Brazil; INRIA University of Lille, France; KTH Royal Institute of Technology, Sweden},
abstract={Benchmarks of bugs are essential to empirically evaluate automatic program repair tools. In this paper, we present BEARS, a project for collecting and storing bugs into an extensible bug benchmark for automatic repair studies in Java. The collection of bugs relies on commit building state from Continuous Integration (CI) to find potential pairs of buggy and patched program versions from open-source projects hosted on GitHub. Each pair of program versions passes through a pipeline where an attempt of reproducing a bug and its patch is performed. The core step of the reproduction pipeline is the execution of the test suite of the program on both program versions. If a test failure is found in the buggy program version candidate and no test failure is found in its patched program version candidate, a bug and its patch were successfully reproduced. The uniqueness of Bears is the usage of CI (builds) to identify buggy and patched program version candidates, which has been widely adopted in the last years in open-source projects. This approach allows us to collect bugs from a diversity of projects beyond mature projects that use bug tracking systems. Moreover, BEARS was designed to be publicly available and to be easily extensible by the research community through automatic creation of branches with bugs in a given GitHub repository, which can be used for pull requests in the BEARS repository. We present in this paper the approach employed by BEARS, and we deliver the version 1.0 of BEARS, which contains 251 reproducible bugs collected from 72 projects that use the Travis CI and Maven build environment. © 2019 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Alférez2019307,
author={Alférez, M. and Acher, M. and Galindo, J.A. and Baudry, B. and Benavides, D.},
title={Modeling variability in the video domain: language and experience report},
journal={Software Quality Journal},
year={2019},
volume={27},
number={1},
pages={307-347},
doi={10.1007/s11219-017-9400-8},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043359422&doi=10.1007%2fs11219-017-9400-8&partnerID=40&md5=9093ba12a98ee3a7d09e741f16c4eeff},
affiliation={Interdisciplinary Centre for Security, Reliability and Trust (SnT), University of Luxembourg, 29, Avenue J.F Kennedy, Luxembourg, 1855, Luxembourg; DiverSE Team at Inria Rennes, University of Rennes, IRISA, CNRS, Rennes, France; Department of Computer Languages and Systems, University of Seville, Seville, Spain; EECS/SCS, KTH, Royal Institute of Technology, Stockholm, Sweden},
abstract={In an industrial project, we addressed the challenge of developing a software-based video generator such that consumers and providers of video processing algorithms can benchmark them on a wide range of video variants. This article aims to report on our positive experience in modeling, controlling, and implementing software variability in the video domain. We describe how we have designed and developed a variability modeling language, called VM, resulting from the close collaboration with industrial partners during 2 years. We expose the specific requirements and advanced variability constructs; we developed and used to characterize and derive variations of video sequences. The results of our experiments and industrial experience show that our solution is effective to model complex variability information and supports the synthesis of hundreds of realistic video variants. From the software language perspective, we learned that basic variability mechanisms are useful but not enough; attributes and multi-features are of prior importance; meta-information and specific constructs are relevant for scalable and purposeful reasoning over variability models. From the video domain and software perspective, we report on the practical benefits of a variability approach. With more automation and control, practitioners can now envision benchmarking video algorithms over large, diverse, controlled, yet realistic datasets (videos that mimic real recorded videos)—something impossible at the beginning of the project. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Automated reasoning;  Configuration;  Domain-specific languages;  Feature modeling;  Software product line engineering;  Variability modeling;  Video testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Suribabu20191,
author={Suribabu, C.R. and Renganathan, N.T. and Perumal, S. and Paez, D.},
title={Analysis of water distribution network under pressure-deficient conditions through emitter setting},
journal={Drinking Water Engineering and Science},
year={2019},
volume={12},
number={1},
pages={1-13},
doi={10.5194/dwes-12-1-2019},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062945498&doi=10.5194%2fdwes-12-1-2019&partnerID=40&md5=ae85b7698668717c59119598edd8ec71},
affiliation={Centre for Advanced Research in Environment, School of Civil Engineering, SASTRA Deemed University, Thanjavur, Tamil Nadu, 613 401, India; Department of Civil Engineering, Kalasalingam Academy of Research and Education, Krishnankoil, Tamil Nadu, 626126, India; Department of Civil Engineering, NERIST, Nirjuli, Itanagar, Arunachal Pradesh, 791109, India; Department of Civil Engineering, Queen's University, Kingston, ON, Canada},
abstract={Pressure-driven analysis (PDA) of water distribution networks necessitates an assessment of the supplying capacity of a network within the minimum and required pressure ranges. Pressure-deficient conditions happen due to the uncertainty of nodal demands, failure of electromechanical components, diversion of water, aging of pipes, permanent increase in the demand at certain supply nodes, fire demand, etc. As the demand-driven analysis (DDA) solves the governing equations without any bound on pressure head, it fails to replicate the real scenario, particularly when the network experiences pressure-deficient situations. Numerous researchers formulated different head-discharge relations and used them iteratively with demand-driven software, while some other approaches solve them by incorporating this relation within the analysis algorithms. Several attempts have been made by adding fictitious network elements like reservoirs, check valves (CVs), flow control valves (FCVs), emitters, dummy nodes and pipes of negligible length (i.e., negligible pressure loss) to assess the supplying capability of a network under pressure-deficient conditions using demand-driven simulation software. This paper illustrates a simple way of assessing the supplying capacity of demand nodes (DNs) under pressure-deficient conditions by assigning the respective emitter coefficient only for those nodes facing a pressure-deficit condition. The proposed method is tested with three benchmark networks, and it is able to simulate the network without addition of any fictitious network elements or changing the source code of the software like EPANET. Though the proposed approach is an iterative one, the computational burden of adding artificial elements in the other methods is avoided and is hence useful for analyzing large networks. © 2019 Author(s).},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lim2019,
author={Lim, G. and Ham, M. and Moon, J. and Song, W. and Woo, S. and Oh, S.},
title={TAOS-CI: Lightweight Modular Continuous Integration System for Edge Computing},
journal={2019 IEEE International Conference on Consumer Electronics, ICCE 2019},
year={2019},
doi={10.1109/ICCE.2019.8662017},
art_number={8662017},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063808660&doi=10.1109%2fICCE.2019.8662017&partnerID=40&md5=55f7648ae2772c515616a989eb0d5951},
affiliation={Artificial Intelligence Center, Samsung Research, Seoul, South Korea},
abstract={With the proliferation of IoT and edge devices, we are observing a lot of consumer electronics becoming yet another IoT and edge devices. Unlike traditional smart devices, such as smart phones, consumer electronics, in general, have significant diversities with fewer number of devices per product model. With such high diversities, the proliferation of edge devices requires frequent and seamless updates of consumer electronics, which makes the manufacturers prone to regressions because the manufacturers have less resource per an instance of software release; i.e., they need to repeat releases by the number of product models times the number of updates. Continuous Integration (CI) systems can help prevent regression bugs from actively developing software packages including the frequently updated device software platforms. The proposed CI system provides a portable and modular software platform automatically inspecting potential issues of incoming changes with the enabled modules: code format and style, performance regressions, static checks on the source code, build and packaging tests, and dynamic checks with the built binary before deploying a platform image on the IoT and edge devices. Besides, our proposed approach is lightweight enough to be hosted in normal desktop computers even for dozens of developers. As a result, it can be easily applied to a lot of various source code repositories. Evaluation results demonstrate that the proposed method drastically improves plugins execution time and memory consumption, compared with methods in previous studies. © 2019 IEEE.},
author_keywords={code review;  continuous integration;  continuous test;  platform build;  software regression},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gomes2019271,
author={Gomes, R.M. and Baunach, M.},
title={Code Generation from Formal Models for Automatic RTOS Portability},
journal={CGO 2019 - Proceedings of the 2019 IEEE/ACM International Symposium on Code Generation and Optimization},
year={2019},
pages={271-272},
doi={10.1109/CGO.2019.8661170},
art_number={8661170},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063775209&doi=10.1109%2fCGO.2019.8661170&partnerID=40&md5=49377a26e0e44f59f4c85f921e1bcf37},
affiliation={Institute of Technical Informatics, Graz University of Technology, Graz, Austria},
abstract={Current approaches for portability of real-time operating systems (RTOSs) for embedded systems are largely based on manual coding, which is arduous and error prone. With increasing dependability requirements for cyber physical systems, specially within the Internet of Things (IoT), along with the expected great diversity of hardware platforms, software platforms will only remain competitive in the long run if they guarantee correct operation and easy deployment to every hardware platform. In this scenario, a new approach to the development and portability of RTOSs that guarantees correct implementations for all current and future devices and hardware architectures becomes indispensable.We present a framework for automatic RTOS portability that integrates model-based design and formal methods into dependable embedded software development. We focus specially on modeling the interaction between software and hardware in order to generate low-level code. This enables automatic portability for hardware-related parts of the OS (i.e., context switching, memory management, security aspects, etc,) as well as for on-chip peripheral drivers (i.e., timers, I/O, etc.).With our framework we will be able to prove the consistency of the refinements as well as that the RTOS model fulfills various functional and non-functional requirements. Automatic code generation guarantees that the model is correctly translated to machine language, avoiding implementation mistakes common to manual coding. Changes on the software, for bug fixes or testing of new concepts, for example, do not require knowledge of the target architectures, since they are done on the model and are immediately reflected in all implementations upon code generation, assuring consistency across platforms. © 2019 IEEE.},
author_keywords={Code generation;  Event-B;  Formal modeling;  IoT;  RTOS portability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2019444,
author={Liu, B. and Nejati, S. and Lucia and Briand, L.C.},
title={Effective fault localization of automotive Simulink models: achieving the trade-off between test oracle effort and fault localization accuracy},
journal={Empirical Software Engineering},
year={2019},
volume={24},
number={1},
pages={444-490},
doi={10.1007/s10664-018-9611-z},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044245466&doi=10.1007%2fs10664-018-9611-z&partnerID=40&md5=062b43f565f7564bc762c09577efddab},
affiliation={SnT Centre, University of Luxembourg, Luxembourg},
abstract={One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify four test objectives that aim to increase test suite diversity. We use four objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) expanding test suites used for fault localization using any of our four test objectives, even when the expansion is small, can significantly improve the accuracy of fault localization, (2) varying test objectives used to generate the initial test suites for fault localization does not have a significant impact on the fault localization results obtained based on those test suites, and (3) we identify an optimal configuration for prediction models to help stop test generation when it is unlikely to be beneficial. We further show that our optimal prediction model is able to maintain almost the same fault localization accuracy while reducing the average number of newly generated test cases by more than half. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Fault localization;  Search-based testing;  Simulink models;  Supervised learning;  Test suite diversity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Al-Hajjaji2019499,
author={Al-Hajjaji, M. and Thüm, T. and Lochau, M. and Meinicke, J. and Saake, G.},
title={Effective product-line testing using similarity-based product prioritization},
journal={Software and Systems Modeling},
year={2019},
volume={18},
number={1},
pages={499-521},
doi={10.1007/s10270-016-0569-2},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006469879&doi=10.1007%2fs10270-016-0569-2&partnerID=40&md5=13fffa642302dbebfb755a5e2c14ed25},
affiliation={University of Magdeburg, Magdeburg, Germany; TU Braunschweig, Braunschweig, Germany; TU Darmstadt, Darmstadt, Germany; METOP GmbH, University of Magdeburg, Magdeburg, Germany},
abstract={A software product line comprises a family of software products that share a common set of features. Testing an entire product-line product-by-product is infeasible due to the potentially exponential number of products in the number of features. Accordingly, several sampling approaches have been proposed to select a presumably minimal, yet sufficient number of products to be tested. Since the time budget for testing is limited or even a priori unknown, the order in which products are tested is crucial for effective product-line testing. Prioritizing products is required to increase the probability of detecting faults faster. In this article, we propose similarity-based prioritization, which can be efficiently applied on product samples. In our approach, we incrementally select the most diverse product in terms of features to be tested next in order to increase feature interaction coverage as fast as possible during product-by-product testing. We evaluate the gain in the effectiveness of similarity-based prioritization on three product lines with real faults. Furthermore, we compare similarity-based prioritization to random orders, an interaction-based approach, and the default orders produced by existing sampling algorithms considering feature models of various sizes. The results show that our approach potentially increases effectiveness in terms of fault detection ratio concerning faults within real-world product-line implementations as well as synthetically seeded faults. Moreover, we show that the default orders of recent sampling algorithms already show promising results, which, however, can still be improved in many cases using similarity-based prioritization. © 2016, Springer-Verlag Berlin Heidelberg.},
author_keywords={Combinatorial interaction testing;  Model-based testing;  Product-line testing;  Software product lines;  Test-case prioritization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lovreto201948,
author={Lovreto, G. and Endo, A.T. and Nardi, P. and Durelli, V.H.S.},
title={Automated Tests for Mobile Games: An Experience Report},
journal={Brazilian Symposium on Games and Digital Entertainment, SBGAMES},
year={2019},
volume={2018-November},
pages={48-56},
doi={10.1109/SBGAMES.2018.00015},
art_number={8636923},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062729681&doi=10.1109%2fSBGAMES.2018.00015&partnerID=40&md5=b6a743054fe65a33b2582a9decad3ff0},
affiliation={Department of Computing, Federal University of Technology - Parana, Cornelio Procopio, Brazil; Department of Computer Science, Federal University of Sao Joao Del Rei, Sao Joao del Rei, Brazil},
abstract={As mobile gaming is an ever-growing, competitive and profitable market, there has been an increasing demand for better quality in video game software. While manual testing is still a common practice among mobile game developers, some repetitive and error-prone tasks could benefit from test automation. For instance, test scripts that perform sanity checks of the proper functioning of a mobile game would be desirable in an ecosystem with constant hotfixes and updates, as well as a diverse set of configurations (e.g., device hardware, screensizes, and platforms). In this context, this paper reports an experience on developing automated test scripts for mobile games. To this end, we randomly selected 16 mobile games, from different genres, among the popular ones from the Google Play Store. For each game, test scripts were developed using the Appium testing framework and the OpenCV library. Based on our results, we provide an in-depth discussion on the challenges and lessons learned. © 2018 IEEE.},
author_keywords={Mobile Apps;  Mobile Games;  Software Testing;  Test Cases;  Video Game Software},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ha2019469,
author={Ha, Q.-H. and Liu, D.-Y. and Chen, Y. and Liu, L.},
title={Approach to cross-company spacecraft software defect prediction based on transfer learning [基于迁移学习的跨公司航天软件缺陷预测]},
journal={Guangxue Jingmi Gongcheng/Optics and Precision Engineering},
year={2019},
volume={27},
number={2},
pages={469-478},
doi={10.3788/OPE.20192702.0469},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065208245&doi=10.3788%2fOPE.20192702.0469&partnerID=40&md5=7e636104355ba664c4848eb5e3859b2b},
affiliation={College of Computer Science and Technology, Jilin University, Changchun, 130012, China; Changchun Institute of Optics, Fine Mechanics and Physics, Chinese Academy of Science, Changchun, 130033, China; Key Laboratory of Symbolic Computation and Knowledge Engineering for the Ministry of Education, Jilin University, Changchun, 130012, China},
abstract={In order to improve the efficiency and quality of aerospace software testing, an approach to cross-company aerospace software defect prediction was proposed, especially for the scarcity of within-company software and the long cycle of development. Considering the complexity, large scale, and independent functions of aerospace software, the idea of building a defect prediction model based on static classification was proposed. In this paper, the transfer learning method was introduced. Using the nearest neighbor classifier and data gravity model, the distribution characteristics of training data were corrected to improve the similarity between training data and target data. In order to improve the generalization ability of the model to adapt to the diversity of target data, a small amount of target data was added to the training data for model training. The approach was applied to the test for aerospace software testing. The results of application show that, compared with existing software defect prediction methods, the proposed method can effectively improve the recall rate (close to 0.6) with a low false alarm rate (not higher than 0.3). The overall credibility is effectively enhanced (G-measure is over 0.6), and the method has high stability and strong generalization ability. This method can control the test scale in practical projects and improve testing efficiency. © 2019, Science Press. All right reserved.},
author_keywords={Data gravity;  Defect prediction;  Naive Bayes;  Nearest neighbor classifier;  Transfer learning},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhu2019252,
author={Zhu, G.-Y. and Xu, W.-J.},
title={Multi-objective flexible job shop scheduling method for machine tool component production line considering energy consumption and quality [考虑能耗与质量的机床构件生产线多目标柔性作业车间调度方法]},
journal={Kongzhi yu Juece/Control and Decision},
year={2019},
volume={34},
number={2},
pages={252-260},
doi={10.13195/j.kzyjc.2018.0131},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064451737&doi=10.13195%2fj.kzyjc.2018.0131&partnerID=40&md5=5ecb0faeefaefddc59d3f0337ac33e77},
affiliation={College of Mechanical Engineering and Automation, Fuzhou University, Fuzhou, 350116, China},
abstract={A multi-objective flexible job shop scheduling model aiming at the completion time, idle time, processing quality and machine tool energy consumption is established according to the characteristics of machine tool components in production such as multi-varieties, small batch and large production energy consumption. And a genetic algorithm based on intuitionistic fuzzy set similarity (IFS_GA) is proposed to solve this scheduling model. The intuitionistic fuzzy set similarity value is used as the fitness value to lead the evolution of the algorithm. The crowd distance is used to trim the external files to improve the diversity of the population. In order to improve the quality of the initial population, a weight-based heuristic rule is proposed. A new chromosome cross method is presented to improve the searching ability of the algorithm. The leader is selected by the intuitionistic fuzzy set similarity value to guide the cross. In the feasible Pareto optimal solution, the solution with the highest similarity value of the intuitionistic fuzzy set is selected as the most satisfactory solution. The proposed algorithm is tested with the verification methods of example simulation, instance simulation and QUEST software. The results show that the IFS_GA is effective, and it is better than the NSGAII. © 2019, Editorial Office of Control and Decision. All right reserved.},
author_keywords={Energy saving;  Flexible job shop scheduling;  Heuristic rules based on weights;  Multi-objective optimization;  QUEST simulation;  Similarity of intuitionistic fuzzy set},
document_type={Article},
source={Scopus},
}

@ARTICLE{Karthikeyan201987,
author={Karthikeyan, T. and Vamsi Krishna, V. and Basheer, S.},
title={Essential foundation concepts of manual software testing actions, common characteristics and procedure},
journal={International Journal of Civil Engineering and Technology},
year={2019},
volume={10},
number={2},
pages={87-95},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063558109&partnerID=40&md5=cb660d3951ff62bf021a7bb14dc2f01a},
affiliation={Department of Computer Science, Sri Balaji Chockalingam Engineering College, Arni, Tamilnadu, India; Dr.MGR Chockalingam Arts College, Arni, Tamilnadu, India; Department of Information System, Prince Nora Bint Abdul Rahman University, Riyadh, Saudi Arabia},
abstract={Programming testing is the stage which makes programming as usable quality scholarly amount. Programming testing under experiences distinctive stages. The accompanying stages according to the examination are investigation test, test arranging, experiment or test information or test condition creation, test execution, bugs logging, following and test strategy. Past research has been improved the situation advance test process in nature of programming. All accessible testing forms incorporate distinctive advancement models and diverse programming testing procedures are performed. Each organization chooses their testing procedure dependent on the basic condition of the applications each organization selects their testing procedure. The security, execution and utilitarian parts are most basic in every application these are altogether to be tried and carrying on obviously. This paper will clarify and guaranteeing about programming applications quality to do enhanced testing forms. The real programming testing systems are Security, Performance and Functional are handled by Analysis, Preparation and Execution will be finished up. ©IAEME Publication},
author_keywords={Execution and Closure;  Functional;  Performance and Security Testing Analysis;  Planning and Preparation;  Software Development Life Cycle (SDLC);  Software Testing Life Cycle (STLC);  Software Testing Techniques},
document_type={Article},
source={Scopus},
}

@ARTICLE{Hamidi-Razi201987,
author={Hamidi-Razi, H. and Mazaheri, M. and Carvajalino-Fernández, M. and Vali-Samani, J.},
title={Investigating the restoration of Lake Urmia using a numerical modelling approach},
journal={Journal of Great Lakes Research},
year={2019},
volume={45},
number={1},
pages={87-97},
doi={10.1016/j.jglr.2018.10.002},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055721207&doi=10.1016%2fj.jglr.2018.10.002&partnerID=40&md5=6680b8ffeab8e32e8e62933a9f4f2cf0},
affiliation={Tarbiat Modares University, Jalal AleAhmad Ave., P.O. Box: 14115-111, Tehran, Iran; Institute of Marine Research, Postboks 1870 Nordnes, Bergen, 5817, Norway},
abstract={We assessed the effectiveness of multiple hydrologic restoration scenarios for Lake Urmia, as well as the variation of its salinity regime under no intervention, using a 2D shallow water model. Tested scenarios, proposed by Urmia Lake Restoration Program Committee, include: Preservation of current lake status (no intervention), complete closing of Shahid Kalantari causeway, dyke construction in the southern part of Lake Urmia, water transfer from Zarrinehrood River to Siminehrood River and reduction of agricultural water consumption by best agricultural practices. Results indicate that neither the closure of the causeway nor the construction of the southern dyke would significantly improve lake conditions when compared to preservation of current lake status. The water transfer alternative doesn't seem to have any effect on the current lake conditions either. However, the reduction on water diversions by improving agricultural practices in the lake's basin leads to a partial restoration of the lake in terms of water level, surface area and volume. If current conditions persist, salinity in the northern part of Lake Urmia will reach supersaturation levels (340 g/L), generating further salt deposits. © 2018 International Association for Great Lakes Research},
author_keywords={Ecological level;  Lake Urmia;  Modelling;  Restoration;  Salinization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Setiani201991,
author={Setiani, N. and Ferdiana, R. and Santosa, P.I. and Hartanto, R.},
title={Literature review on test case generation approach},
journal={ACM International Conference Proceeding Series},
year={2019},
pages={91-95},
doi={10.1145/3305160.3305186},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063583393&doi=10.1145%2f3305160.3305186&partnerID=40&md5=0b724406d19c5acdffd262bc9238a689},
affiliation={Dept. Electrical Enginering and Informatics, Universitas Gadjah Mada, Yogyakarta, Indonesia; Informatics Engineering, Universitas Islam Indonesia, Yogyakarta, Indonesia},
abstract={Test case generation is a testing stage that requires the greatest resources among other stages so it has significant impact on the effectiveness and efficiency of software testing. Test case is a pair of input and output that will be executed by the tester whose aim is reveal the failures in software under test (SUT). For decades, this topic has become one of the most active topics in research on software testing. It has been proved by a variety of techniques and diverse tools proposed. In last decade, research in the field of test case generation experienced some progress. Nowadays, software testing is challenged to be able to test complex computation software that intensively interact with another system. The aim of this study is to give an up-to-date and overview of research in test case generation researches. © 2019 Association for Computing Machinery.},
author_keywords={Software testing;  Test case;  Test case generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Krusche20197592,
author={Krusche, S. and Seitz, A.},
title={Increasing the interactivity in software engineering MOOCs - A case study},
journal={Proceedings of the Annual Hawaii International Conference on System Sciences},
year={2019},
volume={2019-January},
pages={7592-7601},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108299366&partnerID=40&md5=9a08069e426c312795c5b3e2269e57be},
affiliation={Technische Universität München},
abstract={MOOCs differ from traditional university courses: instructors do not know the learners who have a diverse background and cannot talk to them in person due to the worldwide distribution. This has a decisive influence on the interactivity of teaching and the learning success in online courses. While typical online exercises such as multiple choice quizzes are interactive, they only stimulate basic cognitive skills and do not reflect software engineering working practices such as programming or testing. However, the application of knowledge in practical and realistic exercises is especially important in software engineering education. In this paper, we present an approach to increase the interactivity in software engineering MOOCs. Our interactive learning approach focuses on a variety of practical and realistic exercises, such as analyzing, designing, modeling, programming, testing, and delivering software stimulating all cognitive skills. Semi-automatic feedback provides guidance and allows reflection on the learned theory. We applied this approach in the MOOC software engineering essentials SEECx on the edX platform. Since the beginning of the course, more than 15,000 learners from more than 160 countries have enrolled. We describe the design of the course and explain how its interactivity affects the learning success. © 2019 IEEE Computer Society. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Spieker20197724,
author={Spieker, H. and Gotlieb, A. and Mossige, M.},
title={Rotational diversity in multi-cycle assignment problems},
journal={33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019},
year={2019},
pages={7724-7731},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090807023&partnerID=40&md5=3883465b1b4627c10a09c37b14ce42f9},
affiliation={Simula Research Laboratory, P.O. Box 134, Lysaker, 1325, Norway; University of Stavanger Stavanger, Norway; ABB Robotics, Bryne, Norway},
abstract={In multi-cycle assignment problems with rotational diversity, a set of tasks has to be repeatedly assigned to a set of agents. Over multiple cycles, the goal is to achieve a high diversity of assignments from tasks to agents. At the same time, the assignments' profit has to be maximized in each cycle. Due to changing availability of tasks and agents, planning ahead is infeasible and each cycle is an independent assignment problem but influenced by previous choices. We approach the multi-cycle assignment problem as a two-part problem: Profit maximization and rotation are combined into one objective value, and then solved as a General Assignment Problem. Rotational diversity is maintained with a single execution of the costly assignment model. Our simple, yet effective method is applicable to different domains and applications. Experiments show the applicability on a multi-cycle variant of the multiple knapsack problem and a real-world case study on the test case selection and assignment problem, an example from the software engineering domain, where test cases have to be distributed over compatible test machines. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Goyal2019,
author={Goyal, R. and Kushal, K.S. and Mistry, P.},
title={Standard Process for Establishment of ECU Virtualization as Integral Part of Automotive Software Development Life-Cycle},
journal={SAE Technical Papers},
year={2019},
number={October},
doi={10.4271/2020-01-5007},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084470711&doi=10.4271%2f2020-01-5007&partnerID=40&md5=8f89055cef8b8dbc4daded56ef569615},
affiliation={KPIT, India; KPIT Technologies Limited, India},
abstract={In recent year, Electronic Control Unit (ECU) virtualization is being promoted for development and validation of automotive software. ECU virtualization allows execution of integrated software on developer's computer, enabling faster algorithm testing. However, the challenge remains to establish a process to be followed at organization level and its integration in the existing development process/es. In this paper, an approach for integration of Virtual ECU (V-ECU) in Software Development Life-Cycle (SDLC) is discussed. The rationale of merging ECU virtualization in standard SDLC V-cycle supplements milestones in testing & validation. Addition of this milestone with the implementation of ECU virtualization is also presented. On the foundation of modified SDLC V-cycle, a standard process flow is created to establish ECU virtualization as integral part of software development in agile methodology. We have implemented the discussed process with diversified tool chains on various automotive controller i.e. TCM, ECM, BMS etc. The process is accomplished, and the benefits of ECU virtualization are accumulated by algorithm development and testing team. A case study of Battery Management System (BMS) application is presented in this paper, using dSPACE tool suite. A sprint plan is also presented to highlight implementation of ECU virtualization and uses of system by various stake holders. © 2020 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ching2019,
author={Ching, M. and Gill, T.R. and Cherice Moore, E. and Clawson, J.M. and Cross, A. and Kessler, P.D. and Dillard, M.A. and Craig, D.A.},
title={NextStEP habitat risk reduction for gateway},
journal={Proceedings of the International Astronautical Congress, IAC},
year={2019},
volume={2019-October},
art_number={IAC-19_B3_7_3_x54185},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079167238&partnerID=40&md5=9e744cbc52f16e2073daed60b3d0602c},
affiliation={Stellar Solutions, Inc, Palo Alto, CA  94306, United States; National Aeronautics and Space Administration, Kennedy Space CenterFL  32899, United States; National Aeronautics and Space Administration, Johnson Space Center, Texas, 77058, United States; National Aeronautics and Space Administration, Langley Research Center, Hampton, VA  23681, United States; National Aeronautics and Space Administration, Headquarters, Washington, DC  20546, United States},
abstract={This paper will provide an overview of the Next Space Technologies for Exploration Partnerships (NextSTEP) habitation prototype and test efforts, and how they provide risk reduction benefits to the NASA Gateway Program from multiple perspectives. The Gateway is envisioned as an outpost orbiting the Moon that provides vital support for a sustainable, long-term base for human return to the lunar surface, as well as a staging point for further deep space exploration. Implementation of this outpost will foster U.S. industry and international partnerships and enable multi-discipline utilization. In advance of formal establishment of the Gateway program, NASA has been developing campaign strategies, architectures, habitat systems, and subsystems through a number of targeted development projects. The NextSTEP-2 Phase 2 habitat contracts were awarded to five commercial partners to develop their concepts for a Gateway system and provide a high fidelity, full scale ground prototype for independent habitability testing. These partnerships provided valuable risk-reduction benefits to the future Gateway from multiple aspects to include: business and process; leveraging and stimulating commercial industry and technologies; innovative technology development and application; common standards and interface development and architecture; and systems/subsystems analysis. The partnerships were established through a Broad Agency Announcement (BAA) solicitation and contracting process which provides more flexibility in developing desired capabilities versus the traditional requirements driven acquisition. This foreshadows the Gateway acquisition approach. The NextSTEP contract work has enhanced the experience base of both NASA and its industry partners in design, development, and testing of space habitat systems, and is providing a foundation from the transition from a prototyping effort to an implementing program. Each contractor developed a different architecture approach incorporating diverse technologies along with the associated detailed lower level requirements, trade studies, and functional allocation to implement their concept. These activities are executed concurrently with NASA's internal reference architecture development and have provided additional aerospace industry expertise that informs - and in many cases - validates the NASA efforts. A consistently managed set of ground test evaluations occurs at the end of the contract, but the design process has allowed ongoing development to inform and evolve the Gateway architectural concepts through a series of design analysis cycles. The rapid prototyping and design cycles of the various concepts follow universal ground rules and assumptions driven by mission objectives. In addition, the industry partners' early review and inputs on draft common interfaces and standards have resulted in more robust and universally acceptable standards. Copyright © 2019 by the International Astronautical Federation (IAF). All rights reserved.},
author_keywords={Artemis;  Cislunar;  Commercial;  Gateway;  Habitat;  Lunar;  NextSTEP},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tao2019120164,
author={Tao, C. and Gao, J. and Wang, T.},
title={Testing and Quality Validation for AI Software-Perspectives, Issues, and Practices},
journal={IEEE Access},
year={2019},
volume={7},
pages={120164-120175},
doi={10.1109/ACCESS.2019.2937107},
art_number={8811507},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078911568&doi=10.1109%2fACCESS.2019.2937107&partnerID=40&md5=a83107370b3ab73e0ed49a602b40b88e},
affiliation={College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; Ministry Key Laboratory for Safety-Critical Software Development and Verification, Nanjing University of Aeronautics and Astronautics, Nanjing, 210016, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210093, China; Department of Computer Engineering, San José State University, San Jose, CA  95192-01809, United States},
abstract={With the fast growth of artificial intelligence and big data computing technologies, more and more software service systems have been developed using diverse machine learning models and technologies to make business and intelligent decisions based on their multimedia input to achieve intelligent features, such as image recognition, recommendation, decision making, prediction, etc. Nevertheless, there are increasing quality problems resulting in erroneous testing costs in enterprises and businesses. Existing work seldom discusses how to perform testing and quality validation for AI software. This paper focuses on quality validation for AI software function features. The paper provides our understanding of AI software testing for new features and requirements. In addition, current AI software testing categories are presented and different testing approaches are discussed. Moreover, test quality assessment and criteria analysis are illustrated. Furthermore, a practical study on quality validation for an image recognition system is performed through a metamorphic testing method. Study results show the feasibility and effectiveness of the approach. © 2013 IEEE.},
author_keywords={AI software quality validation;  AI testing;  testing AI software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Majerník2019106,
author={Majerník, J. and Gašpár, Š.},
title={Analysis of interaction between position of gate and selected properties of low-weight casts on the silumin basis},
journal={Archives of Foundry Engineering},
year={2019},
volume={19},
number={3},
pages={106-110},
doi={10.24425/afe.2019.129619},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078568177&doi=10.24425%2fafe.2019.129619&partnerID=40&md5=54896ed28a198107c6b39f5e2510cbf9},
affiliation={Institute of Technology and Business in České Budějovice, Okružní 517/10, České Budějovice, 370 01, Czech Republic; Technical University Of Košice, Faculty Of Manufacturing Technologies With The Seat In, Prešov Bayerova 1, Prešov, 080 01, Slovakia},
abstract={Final quality of casts produced in a die casting process represents a correlation of setting of technological parameters of die casting cycle, properties of alloy, construction of a die and structure of gating and of bleeding systems. Suitable structure of a gating system with an appertaining bleeding system of the die can significantly influence mechanical and structural properties of a cast. The submitted paper focuses on influence of position of outfall of an gate into the cast on its selected quality properties. Layout of the test casts in the die was designed to provide filling of a shaping cavity by the melt with diverse character of flowing. Setting of input technological parameters during experiment remained on a constant level. The only variable was the position of the gate. Homogeneity represented by porosity f and ultimate strength Rm were selected to be the assessed representative quality properties of the cast. The tests of the influence upon monitored parameters were realized in two stages. The test gating system was primarily subjected to numerical tests with the utilization of a simulation program NovaFlow&Solid. Consequently, the results were verified by the experimental tests carried out with the physical casts produced during operation. It was proved that diverse placement of the gate in relation to the cast influences the mode of the melt flowing through the shaping cavity which is reflected in the porosity of the casts. The experimental test proved correlation of porosity f of the cast with its ultimate strength Rm. At the end of the paper, the interaction dependencies between the gate position, the mode of filling the die cavity, porosity f and ultimate strength Rm. © 2019 Polish Academy of Sciences. All rights reserved.},
author_keywords={High pressure die casting;  Mechanical properties;  Product development},
document_type={Article},
source={Scopus},
}

@ARTICLE{Aranda2019128153,
author={Aranda, L.A. and Sanchez-Macian, A. and Maestro, J.A.},
title={ACME: A tool to improve configuration memory fault injection in SRAM-based FPGAS},
journal={IEEE Access},
year={2019},
volume={7},
pages={128153-128161},
doi={10.1109/ACCESS.2019.2939858},
art_number={8826250},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077984591&doi=10.1109%2fACCESS.2019.2939858&partnerID=40&md5=b9ff9e538e692e4f2e5921da744d2eb2},
affiliation={ARIES Research Center, Universidad Antonio de Nebrija, Madrid, 28040, Spain},
abstract={Circuits in harsh environments, as space, tend to suffer severe problems caused by radiation. In this scenario, where the behavior of the system can be jeopardized, it is critical to produce fault tolerant circuits that can operate correctly. An important task in this scenario is to effectively test the new fault tolerant designs to guarantee their correct operation. There exist several and diverse methods to achieve this task, from actual test flights to the use of particle accelerators. Fault injection emulation is one of the most popular methods, due to its low cost, availability and convenience. There are a number of tools to perform fault injection using a field-programmable gate array (FPGA) as a supporting platform for this task. However, most of these tools are very dependent on the FPGA version and technology, with limited capability to control the injection process in a precise way. In this paper we present ACME (Automatic Configuration Memory Error-injection), a new tool able to pinpoint fault injections in specific areas of the design under test, with great control and precision of the process. In addition, the methodology to configure the tool and make it work with new FPGA families is also provided. © 2013 IEEE.},
author_keywords={Configuration memory;  Emulation;  Fault injection;  Fault tolerance;  FPGA;  reliability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2019474,
author={Wang, R. and Artho, C. and Kristensen, L.M. and Stolz, V.},
title={Visualization and Abstractions for Execution Paths in Model-Based Software Testing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11918 LNCS},
pages={474-492},
doi={10.1007/978-3-030-34968-4_26},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077014675&doi=10.1007%2f978-3-030-34968-4_26&partnerID=40&md5=2973e61350bba528484a8537e4f27b1f},
affiliation={Department of Computing, Mathematics, and Physics, Western Norway University of Applied Sciences, Bergen, Norway; School of Computer Science and Communication, KTH Royal Institute of Technology, Stockholm, Sweden},
abstract={This paper presents a technique to measure and visualize execution-path coverage of test cases in the context of model-based software systems testing. Our technique provides visual feedback of the tests, their coverage, and their diversity. We provide two types of visualizations for path coverage based on so-called state-based graphs and path-based graphs. Our approach is implemented by extending the Modbat tool for model-based testing and experimentally evaluated on a collection of examples, including the ZooKeeper distributed coordination service. Our experimental results show that the state-based visualization is good at relating the tests to the model structure, while the path-based visualization shows distinct paths well, in particular linearly independent paths. Furthermore, our graph abstractions retain the characteristics of distinct execution paths, while removing some of the complexity of the graph. © 2019, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Clarisó201945,
author={Clarisó, R. and Gogolla, M.},
title={A feasibility study on using classifying terms in alloy},
journal={CEUR Workshop Proceedings},
year={2019},
volume={2513},
pages={45-57},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076485144&partnerID=40&md5=6e9e55bce160533be6beef71d61460ef},
affiliation={Universitat Oberta de Catalunya, Barcelona, Spain; University of Bremen, Bremen, Germany},
abstract={To perform the analysis of a structural model of a software system (like a class diagram), it is often necessary to compute sample valid instantiations (like object diagrams), for example, for testing purposes. Classifying terms (CTs) provide a technique for improving diversity in the instantiation generation process. CTs have been proposed and studied in the context of UML class diagrams annotated with OCL invariants. Nevertheless, they can also be employed in other declarative specification languages. This paper explores the feasibility of using CTs in the context of Alloy. The discussion considers both the Alloy notation and the integration with the Alloy Analyzer. Copyright © 2019 the author(s).},
author_keywords={Alloy;  Classifying term;  OCL;  Software modeling;  Testing;  Verification and validation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu20191805,
author={Xu, X. and Ghaffarinia, M. and Wang, W. and Hamlen, K.W. and Lin, Z.},
title={ConfirM: Evaluating compatibility and relevance of control-flow integrity protections for modern software},
journal={Proceedings of the 28th USENIX Security Symposium},
year={2019},
pages={1805-1821},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076348157&partnerID=40&md5=ae1cb0a3c5a1a6faf8f21d66a6759a8c},
affiliation={University of Texas, Dallas, United States; Ohio State University, United States},
abstract={CONFIRM (CONtrol-Flow Integrity Relevance Metrics) is a new evaluation methodology and microbenchmarking suite for assessing compatibility, applicability, and relevance of control-flow integrity (CFI) protections for preserving the intended semantics of software while protecting it from abuse. Although CFI has become a mainstay of protecting certain classes of software from code-reuse attacks, and continues to be improved by ongoing research, its ability to preserve intended program functionalities (semantic transparency) of diverse, mainstream software products has been under-studied in the literature. This is in part because although CFI solutions are evaluated in terms of performance and security, there remains no standard regimen for assessing compatibility. Researchers must often therefore resort to anecdotal assessments, consisting of tests on homogeneous software collections with limited variety (e.g., GNU Coreutils), or on CPU benchmarks (e.g., SPEC) whose limited code features are not representative of large, mainstream software products. Reevaluation of CFI solutions using CONFIRM reveals that there remain significant unsolved challenges in securing many large classes of software products with CFI, including software for market-dominant OSes (e.g., Windows) and code employing certain ubiquitous coding idioms (e.g., event-driven callbacks and exceptions). An estimated 47% of CFI-relevant code features with high compatibility impact remain incompletely supported by existing CFI algorithms, or receive weakened controls that leave prevalent threats unaddressed (e.g., return-oriented programming attacks). Discussion of these open problems highlights issues that future research must address to bridge these important gaps between CFI theory and practice. © 2019 by The USENIX Association. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Qerimi2019745,
author={Qerimi, F. and Behluli, A. and Borisov, P. and Atanasov, D. and Radev, T.},
title={Management effectivity of forests resources in heating, environmental protection and social awareness for forest of kosovo},
journal={International Multidisciplinary Scientific GeoConference Surveying Geology and Mining Ecology Management, SGEM},
year={2019},
volume={19},
number={3.2},
pages={745-752},
doi={10.5593/sgem2019/3.2/S14.096},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073316118&doi=10.5593%2fsgem2019%2f3.2%2fS14.096&partnerID=40&md5=87d1c8f3f80ac901910ee98994d97ac1},
affiliation={AAB college, Pristina, Kosovo, Serbia; Agricultural University of Plovdiv, Bulgaria},
abstract={Forests are not only ecologically important for the existence of ecosystems, air purification and its supply with oxygen as the Earth's lungs, but they also have social and economic significance that directly affects the quality of life and sustainable development in general. Given that about 40% of the entire territory of Kosovo is a forest area, and the fact that they are considered as part of national wealth, they continue to be affected by fires and illegal and uncontrolled logging of the human factor. If we look at the statistics of hectares with a forest area, every day we see that this number is falling. Therefore, they need to be managed to provide sustainable production as well as to protect species from their disappearance, thus protecting the biological diversity for a good collective future. As the forests are continuing to be misused and their number is falling, the idea of this paper is to make efficient management of wood and timber heating thus protecting forest areas and making more efficient use of energy. In addition to the research of this problematic, citizens' awareness on efficient heating management will be explored, where 200 family of the Republic of Kosovo will be part of the study. Data processing will be done through the SPSS program, where different statistical test scores will be made. This paper is of great importance, as mismanagement of forests has become the current problem of modern times. Therefore, finding alternatives to heat not only will enable forest protection and better use of energy but also environmental protection from pollution, as we rank among the highest polluting countries in the world. © SGEM2019 All Rights Reserved.},
author_keywords={Civil awareness;  Effectivity;  Environmental protection;  Management of natural resources},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Avery2019,
author={Avery, M. and Large, R. and Azhar, H. and Wong, K.S. and Yusoff, M.F.B.},
title={Asia's first rigless subsea stimulation using patented well access technology and a fully integrated service model},
journal={Society of Petroleum Engineers - SPE Subsea Well Intervention Symposium 2019, SSI 2019},
year={2019},
art_number={SPE-197073-MS},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072983336&partnerID=40&md5=ad46d8cbff975635c065dd6786a19fd9},
affiliation={Schlumberger, United States; Murphy Sarawak Oil Co. Ltd, Malaysia},
abstract={Asia's first rigless subsea stimulation was executed in 2018, with intervention performed upon three target wells offshore Sabah Malaysia, at a water depth of approximately 1400 m (4,593 ft). Significant changes in reservoir performance prompted an acid stimulation and scale squeeze treatment, designed to remedy fines migration and scaling issues within the well and production system. Treatment fluids were delivered subsea by an open-water hydraulic access system, using a hybrid coiled-tubing downline. Access to the subsea trees was permitted via a patented choke access technology, allowing for a flexible, opex-efficient, and low-risk intervention. The intervention system was installed upon a multi-service vessel, with the downline deployed via the vessel moonpool. A second support vessel was used as required to provide additional fluid capacity without disturbing primary intervention operations. This enhanced the flexibility of the operation, permitting changes in the treatment plan to be accommodated for without impact to critical path stimulation activities. The full intervention was delivered as an integrated service, with all elements supplied by a single provider, via one contract. An established network of in-house equipment, expertise, test laboratories, and operational bases supported the planning and execution of the project. This was complemented by select external providers for vessels, remotely operated vehicle services, and other specialist contractors. The challenges faced during this new market entry included completion of a comprehensive treatment fluid test program, importation and logistics of equipment from around the globe, and managing operational risks, all within a condensed timeline to satisfy a brief intervention window. By leveraging the diverse global network of the service provider, the technology and people required for the project were accessed and brought together to achieve a collaborative solution. This was enhanced by the inclusion of performance based elements within the contract. The provision of a highly efficient and flexible well access technology also supported rapid mobilization and operational risk reduction. Post-stimulation well testing confirmed an average increase in oil productivity of 86%, with a corresponding productivity index factor (PIF) gain of 3.4. These results, combined with the efficient execution of the campaign, confirm the appropriateness of open-water hydraulic access using coiled-tubing for performing cost-effective stimulations on complex subsea wells. Successful entry to the region was highly dependent upon the integrated nature of the service. Access to the service providers global network permitted a high degree of influence upon the ultimate performance of the stimulation. Examples include the PIF results achieved and the responsive actions taken to remedy offshore challenges such as reservoir lock-up on well #3. © 2019, Society of Petroleum Engineers},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{AlSardy201949,
author={Al Sardy, L. and Neubaum, A. and Saglietti, F. and Rudrich, D.},
title={Comparative Evaluation of Security Fuzzing Approaches},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11699 LNCS},
pages={49-61},
doi={10.1007/978-3-030-26250-1_4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072884159&doi=10.1007%2f978-3-030-26250-1_4&partnerID=40&md5=4734825bce148261fb10ba8fed48b74a},
affiliation={Software Engineering (Informatik 11), University of Erlangen-Nuremberg, Martensstr. 3, Erlangen, 91058, Germany},
abstract={This article compares security fuzzing approaches with respect to different characteristics commenting on their pro and cons concerning both their potential for exposing vulnerabilities and the expected effort required to do so. These preliminary considerations based on abstract reasoning and engineering judgement are subsequently confronted with experimental evaluations based on the application of three different fuzzing tools characterized by diverse data generation strategies on examples known to contain exploitable buffer overflows. Finally, an example inspired by a real-world application illustrates the importance of combining different fuzzing concepts in order to generate data in case fuzzing requires the generation of a plausible sequence of meaningful messages to be sent over a network to a software-based controller as well as the exploitation of a hidden vulnerability by its execution. © 2019, Springer Nature Switzerland AG.},
author_keywords={Buffer overflow;  Integer constraint analysis;  Random testing;  Security fuzzing;  Software vulnerability;  Structural coverage},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2019,
title={11th International Symposium on Search-Based Software Engineering, SSBSE 2019},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11664 LNCS},
page_count={189},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072865443&partnerID=40&md5=757831d2a4157e5c610bd9bc74a5df12},
abstract={The proceedings contain 14 papers. The special focus in this conference is on Search-Based Software Engineering. The topics include: Revisiting Hyper-Parameter Tuning for Search-Based Test Data Generation; towards Automated Boundary Value Testing with Program Derivatives and Search; code Naturalness to Assist Search Space Exploration in Search-Based Program Repair Methods; dorylus: An Ant Colony Based Tool for Automated Test Case Generation; software Improvement with Gin: A Case Study; a Systematic Comparison of Search Algorithms for Topic Modelling—A Study on Duplicate Bug Report Identification; constructing Search Spaces for Search-Based Software Testing Using Neural Networks; a Review of Ten Years of the Symposium on Search-Based Software Engineering; does Diversity Improve the Test Suite Generation for Mobile Applications?; PRICE: Detection of Performance Regression Introducing Code Changes Using Static and Dynamic Metrics; general Program Synthesis Using Guided Corpus Generation and Automatic Refactoring; A Search-Based Approach to Generate MC/DC Test Data for OCL Constraints.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Joffe201927,
author={Joffe, L. and Clark, D.},
title={Constructing Search Spaces for Search-Based Software Testing Using Neural Networks},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11664 LNCS},
pages={27-41},
doi={10.1007/978-3-030-27455-9_3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072859917&doi=10.1007%2f978-3-030-27455-9_3&partnerID=40&md5=96704311974afb2f720edb1574a5c5d9},
affiliation={University College London, Gower Street, London, WC1E 6BT, United Kingdom},
abstract={A central requirement for any Search-Based Software Testing (SBST) technique is a convenient and meaningful fitness landscape. Whether one follows a targeted or a diversification driven strategy, a search landscape needs to be large, continuous, easy to construct and representative of the underlying property of interest. Constructing such a landscape is not a trivial task often requiring a significant manual effort by an expert. We present an approach for constructing meaningful and convenient fitness landscapes using neural networks (NN) – for targeted and diversification strategies alike. We suggest that output of an NN predictor can be interpreted as a fitness for a targeted strategy. The NN is trained on a corpus of execution traces and various properties of interest, prior to searching. During search, the trained NN is queried to predict an estimate of a property given an execution trace. The outputs of the NN form a convenient search space which is strongly representative of a number of properties. We believe that such a search space can be readily used for driving a search towards specific properties of interest. For a diversification strategy, we propose the use of an autoencoder; a mechanism for compacting data into an n-dimensional “latent” space. In it, datapoints are arranged according to the similarity of their salient features. We show that a latent space of execution traces possesses characteristics of a convenient search landscape: it is continuous, large and crucially, it defines a notion of similarity to arbitrary observations. © Springer Nature Switzerland AG 2019.},
author_keywords={Fitness function;  Machine learning;  Neural networks;  Search-Based Software Testing;  Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{VanEekelen201929,
author={Van Eekelen, E.M.M. and Sittoni, L. and Van Der Goot, F. and Nieboer, H.E.},
title={Building with nature: More than 10 years of pre-competitive knowledge development},
journal={22nd World Dredging Congress, WODCON 2019},
year={2019},
pages={29-42},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071452192&partnerID=40&md5=edd2baf19f3f9c57a178adab0bcb3f0a},
affiliation={Stichting EcoShape, Spuiboulevard 210, Dordrecht, 3311 GR, Netherlands; Van Oord Dredging and Marine Contractors BV, PO Box 8574, Rotterdam, 3009 AN, Netherlands; Deltares, PO Box 177, Delft, 2600 MH, Netherlands; Royal Boskalis Westminster N.V., PO Box 43, Papendrecht, 3350 AA, Netherlands; Witteveen+Bos, PO Box 233, Deventer, 7400 AE, Netherlands},
abstract={Building with Nature (BwN) is an innovative design philosophy that optimizes the utilization of natural processes in the development of resilient and sustainable hydraulic infrastructure. BwN aligns the interests of human, economic and nature development. In 2008 an initial Building with Nature (BwN I) innovation program started, managed by the Ecoshape foundation, as a 30 mln Public-Private Partnership (PPP). After this first program, EcoShape and partners have established the on-going second Building with Nature program (BwN II). In the second program, the knowledge developed in the first program is tested in field pilot applications in a broad range of environments. The scope of the second program is nearly 47 mln in pilot projects and associated research initiatives. Contrary to the first program, this program is funded 'bottom-up' via a diversity of subsidies, research grants, project development funding as well as a significant cash and in kind contribution from partners of the EcoShape consortium. Anticipated end time for this research program is December 2020. Within the two research programs a variety of projects, research and pilots have taken place to demonstrate the applicability of BwN principles as integral part of the design of hydraulic infrastructure. During the period of activity of the EcoShape-consortium, the thinking about nature-based solutions has become more common. Already quite some projects have been realized that include BwN principles. Yet nature-based philosophies still require greater diffusion and mainstreaming worldwide to improve resilience of infrastructure and areas threatened by the effects of climate change and ecosystem degradation. This paper will discuss the activities of the EcoShape-consortium in the two BwN knowledge development programs, provides a detailed overview of the current BwN II program including some examples and highlights of pilot projects. Furthermore, it addresses questions regarding the wider application of the BwN-philosophy and the more general dissemination of nature-based solutions. © Environment Federation Technical Exhibition and Conference, WEFTEC 2014.All right reserved.},
author_keywords={Building with Nature;  Design philosophy;  Knowledge development;  Nature-based solutions},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jabeen2019349,
author={Jabeen, G. and Akram, J. and Ping, L. and Shah, A.A.},
title={An integrated software vulnerability discovery model based on artificial neural network},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2019},
volume={2019-July},
pages={349-354},
doi={10.18293/SEKE2019-168},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071361088&doi=10.18293%2fSEKE2019-168&partnerID=40&md5=a1a8dde4ee91f7fd1f80ee2784fc7f8d},
affiliation={State Key Laboratory of Information Security, School of Software Engineering, Tsinghua University, China; School of Economics and Management, University of Chinese Academy of Science, Beijing, China},
abstract={Quantitative approaches for software security are needed for effective testing, maintenance and risk assessment of software systems. Vulnerabilities that are present in a software system after its release represent a great risk. Vulnerability discovery models (VDMs) have been proposed to model vulnerability discovery and have has been fined to vulnerability data against calendar time. Though, these models have various shortcomings include changes and development of VDMs for different dataset due to diverse approaches and assumptions in their analytical formulation. There is a clear need for an intensive investigation on these models to enhance predictive accuracy of existing VDMs and adopt the actual behavior of software vulnerabilities which were not modeled previously. This study proposed an integrated model to predict a number of software vulnerabilities by hybridizing the Multi-Layer Perceptron (MLP) artifical neural network and Vulnerability Discovery Models. The proposed model is also widely applicable across various vulnerability datasets and models due to its input diversity by providing improved fitting and predictive accuracy. Further, the experimental results show that this model not only retained the properties of traditional parametric VDM models as well as MLP's good nonlinear mapping ability and useful generalization. © 2019 Knowledge Systems Institute Graduate School. All rights reserved.},
author_keywords={Artificial neural network;  Integerated model;  Multi-Layer Perceptron neural network;  Security;  Vulnerability discovery model},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tan2019385,
author={Tan, T.-B. and Cheng, W.-K.},
title={Software Testing Levels in Internet of Things (IoT) Architecture},
journal={Communications in Computer and Information Science},
year={2019},
volume={1013},
pages={385-390},
doi={10.1007/978-981-13-9190-3_40},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069745889&doi=10.1007%2f978-981-13-9190-3_40&partnerID=40&md5=7628187300cd3112b2932b186c283e3c},
affiliation={Universiti Tunku Abdul Rahman, Kampar, Malaysia},
abstract={Testing the Internet of Things (IoT) solution is complex as it involves a diversification of implementation of smart objects that adopt a diverse and complex communication protocols. It is doubtful whether tests done in IoT solution have been adequately sufficient and scalable. This paper proposed a mapping of the IoT architecture to the conventional software test levels. The test levels shall provide a better view for tester to conduct tests based on different focus of the level. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={Challenges of IoT tests;  Internet of Things;  Test levels;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dosaev201986,
author={Dosaev, R.V. and Kiy, K.I.},
title={A new real-time method for finding temporary and permanent road marking and its applications},
journal={CEUR Workshop Proceedings},
year={2019},
volume={2391},
pages={86-96},
doi={10.18287/1613-0073-2019-2391-86-96},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069630480&doi=10.18287%2f1613-0073-2019-2391-86-96&partnerID=40&md5=b07b5b6170895d12847195c55e5f252b},
affiliation={Keldysh Institute of Applied Mathematics of RAS, Miusskaya square 4, Moscow, 145047, Russian Federation},
abstract={In this paper, a new real-time method for finding temporary and permanent road marking is proposed. The method is based on the geometrized histograms method for segmenting and describing color images. This method is able to deal with both rectilinear and curvilinear marking, as well as with color temporary and permanent road marking. It also makes it possible to distinguish temporary road marking from white permanent road marking. The developed method is stable under illumination and is able to work even for partially disappearing road marking, typical for late winter and early spring. In contrast to many other methods, this method does not require any information about camera parameters and calibration and is able to find road marking in images taken under unknown conditions. The proposed method has been implemented by a program written in C++, operating under Windows and Linux. The program operation has been tested on video records shot on typical Russian roads during different seasons and under diverse whether and illumination conditions. The processing speed is about 20 fps for a standard modern computer. Using parallel computing, this speed is reduced considerably. The results of program operation are presented and discussed. The developed program is a part of the computer vision component of the control system of the AvtoNiva pilotless vehicle. © 2019 CEUR-WS. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{daSilva2019263,
author={da Silva, D.S. and del Rio, D.G. and de Melo, W.C. and Torné, I.G. and do Nascimento, L.B.F.},
title={Estimate of three-phase distribution transformer losses through artificial neural networks},
journal={Smart Innovation, Systems and Technologies},
year={2019},
volume={140},
pages={263-271},
doi={10.1007/978-3-030-16053-1_25},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068621675&doi=10.1007%2f978-3-030-16053-1_25&partnerID=40&md5=2035861e548a54c1cba8509228ba62ea},
affiliation={Universidade Do Estado Do Amazonas, Manaus, Amazonas  1200, Brazil},
abstract={This article presents a study of a neural network applied to estimate losses in core and winding of three-phase distribution network. The architecture of neural network used was the topology Multiple Layer Perceptron and training algorithm Levenberg-Marquard that use non-linear methods. From collate of data is create a database with all selected attribute for subsequently be used on simulation of artificial neural networks. All samples of learning are collected from transformers electric tests of automated software of routine testing used in diverse transformers industry and concessionaire of electric energy. The test stage represents 15% of samples, this part represents not supervision stage of training process where is possible observe ANN behavior after training stage (70% of samples) and validation (15% of samples). The evaluation of neural network was made by tools Mean Square Error, Linear Correlation Coefficient and graphic analyzer of cross-validation process. And in the training process obtain accuracy of 80 and 96% of data samples test of a transformer industry. © Springer Nature Switzerland AG 2019.},
author_keywords={Levenberg-Marquard;  Losses;  Neural network;  Three-phase distribution transformer},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Omari201971038,
author={Omari, M. and Chen, J. and Ackah-Arthur, H. and Kwaku Kudjo, P.},
title={Elimination by Linear Association: An Effective and Efficient Static Mirror Adaptive Random Testing},
journal={IEEE Access},
year={2019},
volume={7},
pages={71038-71060},
doi={10.1109/ACCESS.2019.2919160},
art_number={8725470},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067402832&doi=10.1109%2fACCESS.2019.2919160&partnerID=40&md5=7414d559a9a6700abf3792d6e7198e46},
affiliation={School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 212013, China},
abstract={Adaptive random testing (ART) is a software testing method which combines randomness with even distribution of test cases within the input domain of a program with the aim of improving the effectiveness of random testing (RT). It was established right from the onset that, ART is considerably less efficient compared to RT due to the overhead cost involved in filtering randomly generated test cases in order to achieve the even spread objective. Again, it has been observed that over-concentration on achieving better effectiveness at the expense of efficiency will make ART advantage over RT a superficial one. Besides, the ART is close to its theoretical bound in terms of effectiveness. Various algorithms have therefore emerged that seeks to minimize the efficiency deficit incurred by the ART. One of such strategies is mirror adaptive random testing (MART). Unfortunately, the MART's performance is generally unstable due to the lack of diversity in mirror generated test cases. The culprit has been identified as the mirroring functions used in place of complex ART computations. In this paper, we present elimination (E) by linear association (E-MART) as a solution to the problem of the MART that guarantees diversity in all dimension(s) of mirror test cases. By partitioning the source domain into multiple subdomains, we systematically isolate mirror partitions which are linearly associated with the source domains. The source domain is then iteratively partitioned whiles forgetting strategy is applied to select test cases. The simulations and experimental studies conducted indicate that the EMART has a more stable performance compared to the MART and compares favorably in terms of efficiency by reducing the quadratic time of the MART to linear. © 2013 IEEE.},
author_keywords={Adaptive random testing;  ART overhead challenge;  mirror adaptive random testing;  software testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zimmerer20196,
author={Zimmerer, P.},
title={Test architects at Siemens},
journal={CEUR Workshop Proceedings},
year={2019},
volume={2358},
pages={6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066024361&partnerID=40&md5=68abc8e91625e672876d9e8856990fd3},
affiliation={Siemens AG, Germany},
abstract={At Siemens we have invented and defined a new key role Test Architect to meet the diverse challenges of shorter time-to-market, increasing complexity and more agility while keeping quality and other key system properties high. In the real world our test systems increase in size, volume, flexibility, velocity, complexity and unpredictability: think about testing of autonomous systems or testing of AI systems (artificial intelligence). Additionally, digitalization (virtualization, cloud, mobile, big data, data analytics, internet of things, continuous delivery, DevOps) requires more than just a face lift in testing. This talk shares our motivations, decisions, achievements, and experiences on our journey since 2016 to establish this new key role Test Architect on eye level with the software architects within the company. © 2019 CEUR-WS. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kübler2019268,
author={Kübler, K. and Schwarz, E. and Verl, A.},
title={Test case generation for production systems with model-implemented fault injection consideration},
journal={Procedia CIRP},
year={2019},
volume={79},
pages={268-273},
doi={10.1016/j.procir.2019.02.065},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065438541&doi=10.1016%2fj.procir.2019.02.065&partnerID=40&md5=789c41a283e4b56d42f7e9de3541e278},
affiliation={Institute for Control Engineering of Machine Tools and Manufacturing Units (ISW), University of Stuttgart, Seidenstr. 36, Stuttgart, 70174, Germany},
abstract={Complex production systems, which have to handle product diversity and short product life cycles, can only be stable and efficient when successfully tested prior to their start-up. Also, changes in automation software within the life cycle of a production system should be verified through repetitive testing before applying the new software version. Therefore, test automation in combination with fault injection on a virtual model of the production system is seen as the next step to improve the verification and validation of a production system. This work presents an approach to achieve negative test cases for a test automation framework. © 2019 The Author(s).},
author_keywords={Hardware-in-the-loop;  Negative testing;  Test automation;  Virtual commissiong},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jovanovikj2019536,
author={Jovanovikj, I. and Yigitbas, E. and Grieger, M. and Sauer, S. and Engels, G.},
title={Modular construction of context-specific test case migration methods},
journal={MODELSWARD 2019 - Proceedings of the 7th International Conference on Model-Driven Engineering and Software Development},
year={2019},
pages={536-543},
doi={10.5220/0007690205360543},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064682541&doi=10.5220%2f0007690205360543&partnerID=40&md5=0f0936c6733f5d29dbf0dbc0497feb7d},
affiliation={Software Innovation Lab, Paderborn University, Paderborn, Germany; VHV Gruppe, Hannover, Germany},
abstract={Migration of test cases has a twofold benefit in software migration projects: reuse of valuable knowledge as well as time and cost savings. The diversity of software migration project contexts require a flexible and modular construction method to address several aspects like different system and test environments or the impact of the system changes on the test cases. When an inappropriate migration method is used, it may increase the effort and the costs and also decrease the overall software quality. Therefore, a critical task in test case migration is to provide a transformation method which fits the context. To address this problem, in this paper, we present a framework that enables a modular construction of context-specific migration methods for test cases by assembling predefined building blocks. Our approach builds upon an existing framework for modular construction of software transformation methods and consists of a method base and a method engineering process. Method fragments are the atomic building blocks of a migration method, whereas method patterns encode specific migration strategies. The guidance on development and enactment of migration methods is provided by the method engineering process. We evaluate our approach in an industrial case study where a part of the Eclipse Modeling Framework was migrated from Java to C#. Copyright © 2019 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved},
author_keywords={Method Engineering;  Software Co-evolution;  Software Reengineering;  Test Case Migration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nakajima201921,
author={Nakajima, S.},
title={Dataset Diversity for Metamorphic Testing of Machine Learning Software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11392 LNCS},
pages={21-38},
doi={10.1007/978-3-030-13651-2_2},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064049863&doi=10.1007%2f978-3-030-13651-2_2&partnerID=40&md5=0b0b220dd50a97e421739b8d1d4c6e43},
affiliation={National Institute of Informatics, Tokyo, Japan},
abstract={Machine learning software is non-testable in that training results are not available in advance. The metamorphic testing, using pseudo oracle, is promising for software testing of such machine learning programs. Machine learning software, indeed, works on a collection of a large number of data, and thus slight changes in the input training dataset have a large impact on training results. This paper proposes a new metamorphic testing method applicable to neural network learning models. Key ideas are dataset diversity as well as behavioral oracle. Dataset diversity takes into account the dataset dependency of training results, and provides a new way of generating follow-up test inputs. Behavioral oracle monitors changes of certain statistical indicators as training processes proceed and is a basis of metamorphic relations to be checked. The proposed method is illustrated with a case of software testing of neural network programs to classify handwritten numbers. © 2019, Springer Nature Switzerland AG.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mukanov2019355,
author={Mukanov, R. and Kasenov, A. and Itybayeva, G. and Musina, Z. and Strautmanis, G.},
title={Modeling of the cutting head for treating holes in the railway},
journal={Procedia Computer Science},
year={2019},
volume={149},
pages={355-359},
doi={10.1016/j.procs.2019.01.148},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063777596&doi=10.1016%2fj.procs.2019.01.148&partnerID=40&md5=215a23f38e7d0dd7e282bd792e46fb1b},
affiliation={S. Toraighyrov Pavlodar State University, Lomov Street 64, Pavlodar, 140008, Kazakhstan; Riga Technical University, Institute of Transport, 1 Kalku Street, Riga, LV-1658, Latvia},
abstract={Research of porting operations made by new metal-cutting instrument - a sectional tool head with asymmetrical hard alloyed plates of different width - of holes in railway rails. The tool head has an enhanced durability, productivity, precision and decrease the form deviation and the roughness of surface proceeded. In the article the principle of work and the design are described, and the engineer analysis of the proposed instrument is conducted via APM WinMachine. The application of modern software allow to design instruments with quality, reliability and competitiveness and to make functional decisions based on a comprehensive engineer analysis which enhances quality of design decisions, reduce the time for instrumental production tooling and research the diversity of cutting modes and parameters of the instrument on a model, and indicate the design weaknesses without a full-scale test. © 2019 The Author(s).},
author_keywords={Cutting head;  Discrepancy;  hole;  Turning},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pastusek2019,
author={Pastusek, P. and Payette, G. and Shor, R. and Cayeux, E. and Aarsnes, U.J. and Hedengren, J. and Menand, S. and Macpherson, J. and Gandikota, R. and Behounek, M. and Harmer, R. and Detournay, E. and Illerhaus, R. and Liu, Y.},
title={Creating open source models, test cases, and data for oilfield drilling challenges},
journal={SPE/IADC Drilling Conference, Proceedings},
year={2019},
volume={2019-March},
doi={10.2118/194082-ms},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063163152&doi=10.2118%2f194082-ms&partnerID=40&md5=66a22c1b78a68392207f0b1befd7e3f9},
affiliation={ExxonMobil Development Co., United States; University of Calgary, Canada; Norce, Norway; Brigham Young University, United States; DrillScan, United States; Baker Hughes GE, United States; MindMesh Inc., United States; Apache Corp, United States; Schlumberger, United States; University of Minnesota, United States; Integrity Directional, United States; Shell Development Co, United States},
abstract={The drilling industry has substantially improved performance based on knowledge from physics-based, statistical, and empirical models of components and systems. However, most models and source code have been recreated multiple times, which requires significant effort and energy with little additional benefit or step-wise improvements. The authors propose that it is time to form a coalition of industry and academic leaders to support an open source effort for drilling, to encourage the reuse of continuously improving models and coding efforts. The vision for this guiding coalition is to 1) set up a repository for source code, data, benchmarks, and documentation, 2) encourage good coding practices, 3) review and comment on the models and data submitted, 4) test, use and improve the code, 5) propose and collect anonymized real data, 6) attract talent and support to the effort, and 7) mentor those getting started. Those interested to add their time and talent to the cause may publish their results through peer-reviewed literature. Several online meetings are planned to create this coalition, establish a charter, and layout the guiding principles. Multiple support avenues are proposed to sustain the effort such as: annual user group meetings, create a SPE Technical Section, and initiating a Joint Industry Program (JIP). The Open Porous Media Initiative is just one example of how this could be organized and maintained. As a starting point, this paper reviews existing published drilling models and highlights the similarities and differences for commonly used drillstring hydraulics, dynamics, directional, and bit-rock interaction models. The key requirements for re-usability of the models and code are: 1) The model itself must be available as open source, well documented with the objective and expected outcomes, include commented code, and shared in a publicly available repository which can be updated, 2) A user's guide must include how to run the core software, how to extend software capabilities, i.e., plug in new features or elements, 3) Include a "theory" manual to explain the fundamental principles, the base equations, any assumptions, and the known limitations, 4) Data examples and formatting requirements to cover a diversity of drilling operations, and 5) Test cases to benchmark the performance and output of different proposed models. In May 2018 at "The 4th International Colloquium on Non-linear dynamics and control of deep drilling systems," the keynote question was, "Is it time to start using open source models?" The answer is "yes". Modeling the drilling process is done to help drill a round, ledge free hole, without patterns, with minimum vibration, minimum unplanned dog legs, that reaches all geological targets, in one run per section, and in the least time possible. An open source repository for drilling will speed up the rate of learning and automation efforts to achieve this goal throughout the entire well execution workflow, including planning, BHA design, real-time operations, and post well analysis. Copyright 2019, SPE/IADC International Drilling Conference and Exhibition.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mahanta2019189,
author={Mahanta, P. and Bischoff, G.},
title={Framework for collaborative software testing efforts between cross-functional teams aiming at high quality end product},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11231 LNCS},
pages={189-195},
doi={10.1007/978-3-030-11683-5_21},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062283676&doi=10.1007%2f978-3-030-11683-5_21&partnerID=40&md5=d896150b85f267f7db09829515f81f54},
affiliation={SAP Labs India Pvt Ltd., Bangalore, Karnataka  560076, India; SAP SE, Dietmar-Hopp-Allee 16, Walldorf, 69190, Germany},
abstract={Currently software testing has become critical, expensive (time and cost), effort intensive activity. There are times when software development teams keep the testing activities until a feature development completion which impacts the delivery and quality. To overcome certain friction in quality, there are methods like automation which are applied to various phases of development but often the regression element is skipped. The paper stresses on key elements like automation, quality metrics, feedback and collaboration between cross units in case of diverse team sets in a large organization. The paper tries to present a framework where collaboration, standardization of software testing approach at each level of development becomes critical for successful and high-quality software delivery. © IFIP International Federation for Information Processing 2019.},
author_keywords={Automation;  Coaching;  Collaboration;  Quality;  Software development;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Netti2019800,
author={Netti, A. and Kiziltan, Z. and Babaoglu, O. and Sîrbu, A. and Bartolini, A. and Borghesi, A.},
title={FINJ: A fault injection tool for HPC systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2019},
volume={11339 LNCS},
pages={800-812},
doi={10.1007/978-3-030-10549-5_62},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061750434&doi=10.1007%2f978-3-030-10549-5_62&partnerID=40&md5=3a5dbad72ff95cbe5f45b9e004797732},
affiliation={Department of Computer Science and Engineering, University of Bologna, Bologna, Italy; Department of Computer Science, University of Pisa, Pisa, Italy; Department of Electrical, Electronic and Information Engineering, University of Bologna, Bologna, Italy},
abstract={We present FINJ, a high-level fault injection tool for High-Performance Computing (HPC) systems, with a focus on the management of complex experiments. FINJ provides support for custom workloads and allows generation of anomalous conditions through the use of fault-triggering executable programs. FINJ can also be integrated seamlessly with most other lower-level fault injection tools, allowing users to create and monitor a variety of highly-complex and diverse fault conditions in HPC systems that would be difficult to recreate in practice. FINJ is suitable for experiments involving many, potentially interacting nodes, making it a very versatile design and evaluation tool. © Springer Nature Switzerland AG 2019.},
author_keywords={Benchmarking;  Exascale systems;  Fault detection;  Monitoring;  Open-source;  Resiliency},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tjønn2019,
author={Tjønn, A.F.},
title={Digital twin through the life of a field},
journal={Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference 2018, ADIPEC 2018},
year={2019},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059975426&partnerID=40&md5=07e47f4449ad18ff18f7dfbdd58ad123},
affiliation={Aker Solutions, Norway},
abstract={The global energy industry is being radically transformed by increased access to data and digitalization of tools and processes. When data is harnessed effectively and then combined with engineering insight it has the ability to radically improve how energy assets are designed, operated and optimized from concept and front end design all the way through the life of the field to decommissioning. Aker Solutions has developed a data platform and series of software applications which together form a digital twin capable of optimizing energy operations at a product, facility and field level integrating subsea and topside facilities. The digital twin, conceived during the early concept phase (or retrofit to existing assets), draws together diverse data sources into an integrated format which can be managed and enriched as the asset progresses through its lifecycle. Software applications built on this platform include: • Using knowledge-based and adaptable building blocks, our early phase software application accelerates the development of concepts with the ability to rapidly explore different options and adjust and optimize to different customer driven variables • During detailed design phase, software applications are used to drive engineering automation and ensure that safety is built inherently into the design • Simulation software applications are used to test and evaluate complex installation operations in a virtual environment before any offshore operations take place • Digital twin can add significant value during operations and maintenance activities with performance and integrity data collated and visualized helping to make informed decisions to ensure production optimization and continuity • At a product level, Aker Solutions' connected subsea equipment uses software applications and analytics to perform condition monitoring and enable predictive maintenance, maximizing production uptime This digital thread allows data to seamlessly transition across asset phases enabling engineers, operators and asset managers to make informed and confident data-driven decisions; increasing efficiency, reducing cost and reducing risk. © Copyright 2018, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Azam2019571,
author={Azam, M. and Atta-ur-Rahman and Sultan, K. and Dash, S. and Khan, S.N. and Khan, M.A.A.},
title={Automated testcase generation and prioritization using GA and FRBS},
journal={Communications in Computer and Information Science},
year={2019},
volume={955},
pages={571-584},
doi={10.1007/978-981-13-3140-4_52},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059031933&doi=10.1007%2f978-981-13-3140-4_52&partnerID=40&md5=faefc6a73261dfb3c0b4fce852cea73c},
affiliation={Barani Institute of Information Technology (BIIT), PMAS-AA University, Rawalpindi, Pakistan; College of Computer Science and Information Technology, Department of Computer Science, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, Dammam, Saudi Arabia; Department of CIT, JCC, King AbdulAziz University, Jeddah, Saudi Arabia; Department of Computer Science, North Orissa University, Baripada, India; Faculty of Computer Science and Information Technology, Universiti Tun Hussein Onn Malaysia, Parit Raja, Johor  86400, Malaysia},
abstract={Software Quality Assurance (SQA) is a process in which the quality of software is assured by adequate software testing techniques that mainly comprise of verification and validation of the software. Software testing is the process of assessing the features of a software item and evaluating it to detect differences between given input and expected output. This process is done during the development process just prior to deployment. The SQA process is usually a manual process due to the diverse and versatile nature of the software products. That means a technique devised to test one type of software may not work that efficiently while testing another kind of software etc. Moreover, it is a time consuming process; according to a survey it consumes almost half of the total development cost and around two third of the total development time. To address the above-mentioned issues, in this research an intelligent toolkit for automated SQA is proposed and compared them with the existing famous tools like Selenium. This research focuses on automated test case/test data generation and prioritization of test cases. For this purpose, Genetic Algorithm is investigated for automatic test case generation and a fuzzy based system is proposed for test case prioritization. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={Automated testing;  FRBS;  GA;  SQA;  Testcase generation;  Testcase prioritization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shin2019,
author={Shin, D. and Yoo, S. and Papadakis, M. and Bae, D.-H.},
title={Empirical evaluation of mutation-based test case prioritization techniques},
journal={Software Testing Verification and Reliability},
year={2019},
volume={29},
number={1-2},
doi={10.1002/stvr.1695},
art_number={e1695},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058967207&doi=10.1002%2fstvr.1695&partnerID=40&md5=93d9f8b42ed74e17b5a27a9e7504fd13},
affiliation={KAIST, 291 Daehak-ro Yuseong-gu, Daejeon, South Korea; Interdisciplinary Center for Security, Reliability and Trust (SnT), University of Luxembourg, Luxembourg},
abstract={In this paper, we propose a new test case prioritization technique that combines both mutation-based and diversity-aware approaches. The diversity-aware mutation-based technique relies on the notion of mutant distinguishment, which aims to distinguish one mutant's behaviour from another, rather than from the original program. The relative cost and effectiveness of the mutation-based prioritization techniques (i.e., using both the traditional mutant kill and the proposed mutant distinguishment) are empirically investigated with 352 real faults and 553,477 developer-written test cases. The empirical evaluation considers both the traditional and the diversity-aware mutation criteria in various settings: single-objective greedy, hybrid, and multi-objective optimization. The results show that there is no single dominant technique across all the studied faults. To this end, the reason why each one of the mutation-based prioritization criteria performs poorly is discussed, using a graphical model called Mutant Distinguishment Graph that demonstrates the distribution of the fault-detecting test cases with respect to mutant kills and distinguishment. © 2018 John Wiley & Sons, Ltd. © 2018 John Wiley & Sons, Ltd.},
author_keywords={mutation testing;  regression testing;  test case prioritization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Esteban2019111,
author={Esteban, O. and Markiewicz, C.J. and Blair, R.W. and Moodie, C.A. and Isik, A.I. and Erramuzpe, A. and Kent, J.D. and Goncalves, M. and DuPre, E. and Snyder, M. and Oya, H. and Ghosh, S.S. and Wright, J. and Durnez, J. and Poldrack, R.A. and Gorgolewski, K.J.},
title={fMRIPrep: a robust preprocessing pipeline for functional MRI},
journal={Nature Methods},
year={2019},
volume={16},
number={1},
pages={111-116},
doi={10.1038/s41592-018-0235-4},
note={cited By 491},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058146942&doi=10.1038%2fs41592-018-0235-4&partnerID=40&md5=a31c66e49cfb657ffd359b872c972c86},
affiliation={Department of Psychology, Stanford University, Stanford, CA, United States; Max Planck Institute for Empirical Aesthetics, Hesse, Germany; Computational Neuroimaging Lab, Biocruces Health Research Institute, Bilbao, Spain; Neuroscience Program, University of Iowa, Iowa City, IA, United States; McGovern Institute for Brain Research, Massachusetts Institute of Technology (MIT), Cambridge, MA, United States; Montreal Neurological Institute, McGill University, Montreal, QC, Canada; Department of Psychiatry, Stanford Medical School, Stanford University, Stanford, CA, United States; Department of Neurosurgery, University of Iowa Health Care, Iowa City, IA, United States; Department of Otolaryngology, Harvard Medical School, Boston, MA, United States},
abstract={Preprocessing of functional magnetic resonance imaging (fMRI) involves numerous steps to clean and standardize the data before statistical analysis. Generally, researchers create ad hoc preprocessing workflows for each dataset, building upon a large inventory of available tools. The complexity of these workflows has snowballed with rapid advances in acquisition and processing. We introduce fMRIPrep, an analysis-agnostic tool that addresses the challenge of robust and reproducible preprocessing for fMRI data. fMRIPrep automatically adapts a best-in-breed workflow to the idiosyncrasies of virtually any dataset, ensuring high-quality preprocessing without manual intervention. By introducing visual assessment checkpoints into an iterative integration framework for software testing, we show that fMRIPrep robustly produces high-quality results on a diverse fMRI data collection. Additionally, fMRIPrep introduces less uncontrolled spatial smoothness than observed with commonly used preprocessing tools. fMRIPrep equips neuroscientists with an easy-to-use and transparent preprocessing workflow, which can help ensure the validity of inference and the interpretability of results. © 2018, The Author(s), under exclusive licence to Springer Nature America, Inc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stalidzans201925,
author={Stalidzans, E. and Landmane, K. and Sulins, J. and Sahle, S.},
title={Misinterpretation risks of global stochastic optimisation of kinetic models revealed by multiple optimisation runs},
journal={Mathematical Biosciences},
year={2019},
volume={307},
pages={25-32},
doi={10.1016/j.mbs.2018.11.002},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056481860&doi=10.1016%2fj.mbs.2018.11.002&partnerID=40&md5=09203fbb37d49b28ae6f67e357a8b852},
affiliation={Department of Computer Systems, Faculty of Information Technologies, Latvia University of Life Sciences and Technologies, Liela iela 2, Jelgava, LV-3001, Latvia; Institute of Microbiology and Biotechnology, University of Latvia, Jelgavas iela 1, Riga, LV1004, Latvia; Dept. Modeling of Biological Processes, COS Heidelberg/BioQuant, University of Heidelberg, Im Neuenheimer Feld 267, Heidelberg, 69120, Germany},
abstract={One of use cases for metabolic network optimisation of biotechnologically applied microorganisms is the in silico design of new strains with an improved distribution of metabolic fluxes. Global stochastic optimisation methods (genetic algorithms, evolutionary programing, particle swarm and others) can optimise complicated nonlinear kinetic models and are friendly for unexperienced user: they can return optimisation results with default method settings (population size, number of generations and others) and without adaptation of the model. Drawbacks of these methods (stochastic behaviour, undefined duration of optimisation, possible stagnation and no guaranty of reaching optima) cause optimisation result misinterpretation risks considering the very diverse educational background of the systems biology and synthetic biology research community. Different methods implemented in the COPASI software package are tested in this study to determine their ability to find feasible solutions and assess the convergence speed to the best value of the objective function. Special attention is paid to the potential misinterpretation of results. Optimisation methods are tested with additional constraints that can be introduced to ensure the biological feasibility of the resulting optimised design: (1) total enzyme activity constraint (called also amino acid pool constraint) to limit the sum of enzyme concentrations and (2) homeostatic constraint limiting steady state metabolite concentration corridor around the steady state concentrations of metabolites in the original model. Impact of additional constraints on the performance of optimisation methods and misinterpretation risks is analysed. © 2018},
author_keywords={COPASI;  Homeostatic constraint;  Metabolic engineering;  Optimisation;  Parallel optimisation runs},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kamalakannan2019607,
author={Kamalakannan, J. and Chakrabortty, A. and Basak, D. and Dhinesh Babu, L.D.},
title={Emerging trends for effective software project management practices},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={828},
pages={607-613},
doi={10.1007/978-981-13-1610-4_61},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054878235&doi=10.1007%2f978-981-13-1610-4_61&partnerID=40&md5=63d49022133a03f9c59ed91ddff62c16},
affiliation={School of Information Technology and Engineering, Vellore Institute of Technology, Vellore, India},
abstract={Developing software projects nowadays is not much of an errand. With several tools springing up, it is easy to develop software projects and if we go down with a sneak-peek into the overall project scenario, what ultimately matters is the management. Management serves as upright in whatever we do, and for software projects, it is none the less. Software projects can be developed and tested with several software and automated tools in hand, but efficient and successful management is needed, satisfying customers, stakeholders and those involved in the development and other tasks incorporated in developing software projects. This paper of ours aims to bring out newer trends, solutions, and practices in the field of management involved in software projects so as to cope up with the ever increasing diversification of projects, customer needs, demands, and preferences. © 2019, Springer Nature Singapore Pte Ltd.},
author_keywords={Efficient management;  IT world;  Management practices;  Management trends;  Newer management trends;  Software project development;  Software project management},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zheng2019469,
author={Zheng, T.},
title={Automatic test case generation method of parallel multi-population self-adaptive ant colony algorithm},
journal={Advances in Intelligent Systems and Computing},
year={2019},
volume={752},
pages={469-476},
doi={10.1007/978-981-10-8944-2_54},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053280720&doi=10.1007%2f978-981-10-8944-2_54&partnerID=40&md5=8b14855ad5330eb12ac014bb7a50d2a0},
affiliation={A College of Information, Zhejiang Sci-Tech University, Hangzhou, 310018, China},
abstract={The design of test case automatic generation technology is an important step in the implementation of software automated testing. It plays an important role in guiding the later testing work and is the fundamental guarantee for improving software quality and reliability. Ant colony optimization (ACA), as a robust optimization algorithm, has a strong ability of global optimization. In this paper, an automatic test case generation method of parallel multi-group adaptive ant colony algorithm is presented. This algorithm adopts multi-group parallel search, adopts adaptive migration rule based on population diversity, and updates pheromone strategy adaptively according to fitness function in population. Theoretical analysis and simulation experiments show that the application of this algorithm is superior to the generation of test cases. © Springer Nature Singapore Pte Ltd. 2019.},
author_keywords={Adaptive method;  Fitness function;  Pheromone;  Population diversity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hemmati2019185,
author={Hemmati, H.},
title={Advances in Techniques for Test Prioritization},
journal={Advances in Computers},
year={2019},
volume={112},
pages={185-221},
doi={10.1016/bs.adcom.2017.12.004},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041909824&doi=10.1016%2fbs.adcom.2017.12.004&partnerID=40&md5=0466fea9288a468c768b4788f7a5f469},
affiliation={University of Calgary, Calgary, AB, Canada},
abstract={With the increasing size of software systems and the continuous changes that are committed to the software's codebase, regression testing has become very expensive for real-world software applications. Test case prioritization is a classic solution in this context. Test case prioritization is the process of ranking existing test cases for execution with the goal of finding defects sooner. It is useful when the testing budget is limited and one needs to limit their test execution cost, by only running top n test cases, according to the testing budget. There are many heuristics and algorithms to rank test cases. In this chapter, we will see some of the most common test case prioritization techniques from software testing literature as well as trends and advances in this domain. © 2019 Elsevier Inc.},
author_keywords={Code coverage;  Diversity;  Execution cost;  Fault detection;  Multiobjective;  Regression testing;  Scalability;  Search-based testing;  Test case prioritization},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Liu201856,
author={Liu, C. and Du, X. and Zhang, F. and Ma, T. and Zhang, H. and Li, Y.},
title={Design and Test of Cone Diversion Type Horizontal Plate Wheat Precision Seed-metering Device [锥面导流水平盘式小麦精量排种器设计与试验]},
journal={Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery},
year={2018},
volume={49},
number={12},
pages={56-65},
doi={10.6041/j.issn.1000-1298.2018.12.007},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061254605&doi=10.6041%2fj.issn.1000-1298.2018.12.007&partnerID=40&md5=5df4d92d9ca43c2c59c403890b3f72c2},
affiliation={Key Laboratory of Soil-Machine-Plant System Technology, Ministry of Agriculture and Rural Affairs, China Agricultural University, Beijing, 100083, China},
abstract={In order to achieve precision seeding of low sowing quantity of wheat and improve the accuracy of precision seeding of wheat, a kind of cone diversion type horizontal plate wheat precision seed-metering device was proposed. The design and theoretical analysis of key parameters were carried out. Single factor test on the effects of the guide bar form, number of holes, cone disc speed and cone disc angle on seed-filling performance were studied by EDEM discrete element software, and then the parameters were determined. On this basis, the multiple quadratic regression rotation orthogonal combination test was carried out with cone speed, seed-layer thickness and length of hole, and then the test data was analyzed by Design-Expert 8.0.6 software. The regression model and the influence of factors on the index were obtained. The influence relation of factors on the single rate was determined and the order of importance was the cone disc speed, length of hole and seed-layer thickness in turn. The order of importance of the qualified rate was the cone disc speed, seed-layer thickness and length of hole in turn. Interaction between the length of a hole and the seed-layer thickness, cone disc speed and seed-layer thickness cannot be ignored. Based on the regression model, the parameters were optimized and the seed metering device under the optimized parameter combination was tested for seed-metering performance. The results showed that the qualified seed-metering rate, the leakage rate and single rate was 90.13%, 9.87% and 49.50%, raspectively, and the test results matched the simulation optimization results. The reliability of the simulation optimization result was verified. The performance comparison tests with the original cone seed-metering device showed that the designed cone diversion type horizontal plate wheat precision seed-metering device was better than the original cone disc seed-metering device in both metering performance and breakage index. The qualified rate was increased by 3.4 percentage points, 2.1 percentage points and 1.9 percentage points at 3 km/h, 4 km/h and 5 km/h, respectively, and the damage rate was decreased by 0.2 percentage point at 3 km/h. The designed cone diversion type horizontal plate wheat precision seed-metering device can effectively improve seed filling performance and qualification index and reduce sowing quantity and breakage rate. The research results provided a reference for the design of wheat precision seeder used in wide seedling strip. © 2018, Chinese Society of Agricultural Machinery. All right reserved.},
author_keywords={Design;  Discrete element method;  Involute guide bar;  Seed-metering device;  Test;  Wheat},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pudjihastuti2018,
author={Pudjihastuti, I. and Sumardiono, S. and Kusumayanti, H.},
title={Analog Rice Development as Alternative Food Made of Raw Composite Flour Enriched Protein Canavalia ensiformis},
journal={E3S Web of Conferences},
year={2018},
volume={73},
doi={10.1051/e3sconf/20187313017},
art_number={13017},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059617247&doi=10.1051%2fe3sconf%2f20187313017&partnerID=40&md5=be2579d36f6dbc3484c43b8d5e7d7eb9},
affiliation={Vocational Program Study of Chemical Engineering, Diponegoro University, Semarang, Central Java, Indonesia; Chemical Engineering Department, Faculty of Engineering, Diponegoro University, Semarang, Central Java, Indonesia; Chemical Engineering, Faculty of Engineering, Diponegoro University, Semarang, Central Java, Indonesia},
abstract={Food is an important factor in human life. Indonesia's main food need is rice. Domestic rice demand continues to increase along with population growth. One of the businesses that can increase the availability of food, especially rice, is to utilize the existing agricultural products even though they have not been utilized economically and intensified the excavation of new food sources. Analog rice is a form of food diversification by utilizing local carbohydrate sources. In this study analog rice made from composite flour mixture of cassava flour, Dioscorea esculenta L, corn enriched with protein Canavalia ensiformis. The purpose of this research is to develop the method of production of analog rice and to test the physicochemical properties including water content, water, protein, and amylose so that it can be applied as food substitute for rice in realizing Indonesia food security program. The largest protein content of 9.156%, the highest water content of 12.431%, water absorption 187%. The amylose content of analog rice amounted to 19.677% included in the low-octane rice. © The Authors, published by EDP Sciences, 2018.},
author_keywords={Alternative food;  Analog rice;  Composite;  Protein},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Perna2018,
author={Perna, M.G. and Gonzalez, N. and Karp, J.A. and Iriso, P. and Duran, R. and Guevara, J. and Salamida, D.},
title={First approach to image stacking using a Single-Board Computer - A small study of strengths, opportunities, weaknesses and threats},
journal={Congreso Argentino de Ciencias de la Informatica y Desarrollos de Investigacion, CACIDI 2018},
year={2018},
doi={10.1109/CACIDI.2018.8584353},
art_number={8584353},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060739607&doi=10.1109%2fCACIDI.2018.8584353&partnerID=40&md5=f6cc8bc5efa003dda790f53ad41e0676},
affiliation={UNSAM CIDI, Buenos Aires, Argentina},
abstract={Since the appearance in the market, low-cost computers (SBC) are used for the most diverse tasks: from personal scale projects to clusters for data processing or web servers. However, until now there have been few applications of a SBC aimed at stacking and postprocessing of images.The following work details the approach, the analysis and the tests carried out on the third generation of one of the most popular SBCs, the Raspberry Pi, in order to evaluate its performance in the image stacking processing, using a customized software, developed specifically for this study. © 2018 IEEE.},
author_keywords={Image enhancement;  Image processing;  Microcomputers;  Stacking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Suranata2018,
author={Suranata, K. and Rangka, I.B. and Ifdil, I. and Ardi, Z. and Susiani, K. and Prasetyaningtyas, W.E. and Daharnis, D. and Alizamar, A. and Erlinda, L. and Rahim, R.},
title={Diagnosis of students zone proximal development on math design instruction: A Rasch analysis},
journal={Journal of Physics: Conference Series},
year={2018},
volume={1114},
number={1},
doi={10.1088/1742-6596/1114/1/012034},
art_number={012034},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058382048&doi=10.1088%2f1742-6596%2f1114%2f1%2f012034&partnerID=40&md5=ef5ea3a8f20ab66437ce7d60b1332eb7},
affiliation={Universitas Pendidikan Ganesha, Bali, Indonesia; Universitas Indraprasta PGRI, Jakarta, Indonesia; Universitas Negeri Padang, Padang, Indonesia; Sekolah Tinggi Ilmu Manajemen Sukma, Medan, Indonesia},
abstract={Every student could be studying optimally when movements within the zone of proximal development (ZPD). The ZPD among students in a class may diverge. This research tried to analysis of students ZPD in numerical skills on math learning. The study covered 28 third-grade students in elementary school at Padangkeling, Singaraja, Bali. Numerical data understanding of students obtained through the 10 items essays numerical understanding test for elementary school students. Rasch Model analysis is used to examine the properties and quality of numerical understanding tests and to describe each student's ZPD in numeric understanding. This study basically shows that through Rasch analysis it is known that ZPD of students in one class are absolutely different and diverse. There are 11 students identified as having ZPD under the control of the material based on the numerical understanding test so that it requires a special learning program. Four students have ZPD zones that are under some materials requiring more intelligent adult tutors or peers. The other fourth students have ZPD that suited to the comprehensive work in the test, and nine students are students with specific skills who require to look at enrichment in learning program. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cui20183648,
author={Cui, Q. and Wang, J.-J. and Xie, M. and Wang, Q.},
title={Towards Crowd Worker Selection for Crowdsourced Testing Task [众测中的工作者选择方法研究]},
journal={Ruan Jian Xue Bao/Journal of Software},
year={2018},
volume={29},
number={12},
pages={3648-3664},
doi={10.13328/j.cnki.jos.005329},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061351372&doi=10.13328%2fj.cnki.jos.005329&partnerID=40&md5=15d54a3614d0bac497d07a54d34077e7},
affiliation={Laboratory for Internet Software Technologies, Institute of Software, The Chinese Academy of Sciences, Beijing, 100190, China; State Key Laboratory of Computer Science, Institute of Software, The Chinese Academy of Sciences, Beijing, 100190, China; University of Chinese Academy of Sciences, Beijing, 100049, China},
abstract={Crowdsourced testing is an emerging trend in software testing, which relies on crowd workers to accomplish test tasks. Thus, who performs a test task is extremely important for detecting bugs and covering key points of test requirements in crowdsourced testing. There are a lot of candidate crowd workers who may have different testing experience but can also produce duplicate test reports for the same task due to the lack of cooperation. As crowd workers can freely participate in a test task, high quality of testing in terms of bug detection and coverage of key points of test requirements is not guaranteed. Thus, to improve bug detection and coverage of key points of test requirements, selecting an appropriate subset of workers to perform a test task is becoming an important problem. In this paper, three motivating studies are first conducted to investigate important aspects of workers in detecting bugs and covering key points of test requirements. Accordingly, the studies identify three aspects: initiative, relevance and diversity are identified, and produce a novel approach for selecting workers considering all these three aspects. This new approach is evaluated based on 46 real test tasks from Baidu CrowdTest, and the experimental results show the effectiveness of the approach. © Copyright 2018, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
author_keywords={Bug detection;  Crowdsourced testing;  Requirement coverage;  Worker selection},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Raza201899,
author={Raza, A. and Navaie, K. and Nicholson, R.},
title={An architecture for dependable connectivity in OSGi-enabled dynamic distributed systems},
journal={2018 5th International Conference on Internet of Things: Systems, Management and Security, IoTSMS 2018},
year={2018},
pages={99-106},
doi={10.1109/IoTSMS.2018.8554682},
art_number={8554682},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059979627&doi=10.1109%2fIoTSMS.2018.8554682&partnerID=40&md5=87d7e3d64a55d6a2baf38638f99e5c96},
affiliation={Lancaster University, School of Computing and Communication, Lancaster, United Kingdom; Paremus Ltd, London, United Kingdom},
abstract={From air pollution monitoring to debatable surveillance for better security, dynamic distributed systems led to the birth of versatile smart environments. Dependability of such systems is challenged by the reliability of the communication links between various sub-systems. In this paper, we address this issue by designing a TCP/I P based Client-Server architecture using diverse channels, to ensure zero tolerance with regards to outage of service. The prototype system uses Raspberry Pi 3, as a remote client, to intelligently communicate with the server node by choosing one or a set of available communication links, e.g., Ethernet (LAN), Wireless-LAN (Wi-Fi), and Bluetooth channels. We further implement the system using Open Services Gateway Initiative (OSGi), a modular and interpolate-able code foundation, to withstand the challenges faced by the modern software industry e.g. complexity and scalability. Prototype system was successfully tested for hardware and software fault tolerance using different test scenarios to ensure uninterrupted service delivery. In the end, we also present a machine learning technique to mitigate the effects of severe channel hostilities for diverse channels system. The results show improvement in the quality of data transmission by exploiting the flexibility of alternate channels. We demonstrate this intelligent and seamless communication link switching technique using Support Vector Machine (SVM) in MATLAB. © 2018 IEEE.},
author_keywords={dependability;  Dynamic distributed systems;  modular programing;  reliability;  security;  software complexity;  Support Vector Machine (SVM)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Alrawy201836,
author={Alrawy, S.N. and Hamid Ali, F.},
title={GPU Accelerated Rotation about an Arbitrary Axis},
journal={ICOASE 2018 - International Conference on Advanced Science and Engineering},
year={2018},
pages={36-41},
doi={10.1109/ICOASE.2018.8548793},
art_number={8548793},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060036801&doi=10.1109%2fICOASE.2018.8548793&partnerID=40&md5=3664e8c77905e75b89b23465b8a096af},
affiliation={Dept. of Computer Engineering, University of Mosul, Mosul, Iraq},
abstract={The three-dimensional (3D) rotation about any axis is essential in diverse applications and fields, particularly in computer graphics. This paper focuses on accelerating the operation of this transform using GPU in a real-Time environment. This special type of rotation is complicated compared to rotation about the conventional axes due to having many matrix operations, so accelerating such a transform with parallel techniques is an important issue to reduce the execution time that is important to ensure the realistic view of 3D animation scene. In addition to that, concatenating these many operations in a single rotation matrix also gives a significant reduction in computation time required to perform the rotation. The rotation transform is applied to complex models with hundreds or even millions of vertices, so standard 3D objects with different resolutions are used for testing the rotation about a selected axis that created interactively using LabVIEW and Visual Studio software environments. The experimental results showed the significant speedup on CUDA/C ++ compared to LabVIEW computations for the same model complexity. © 2018 IEEE.},
author_keywords={3D object;  arbitrary axis;  GPU;  rotation;  vertices},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Srinivasan2018162,
author={Srinivasan, M.},
title={Prioritization of Metamorphic Relations Based on Test Case Execution Properties},
journal={Proceedings - 29th IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2018},
year={2018},
pages={162-165},
doi={10.1109/ISSREW.2018.000-5},
art_number={8539189},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059843648&doi=10.1109%2fISSREW.2018.000-5&partnerID=40&md5=c89fb4bf05ecc0a49abf4cc61c95366a},
affiliation={Gianforte School of Computing, Montana State University, Bozeman, United States},
abstract={A test oracle is essential for software testing. In certain complex systems, it is hard to distinguish between correct and incorrect behavior. Metamorphic testing is one of the solution to solve the test oracle problem. In metamorphic testing, metamorphic relations (MRs) are derived based on the properties exhibited by the program under test (PUT). These MRs play a major role in the generation of test data for conducting MT. The effectiveness of MRs can be determined based on the ability to detect considerable faults for the given PUT. Many metamorphic relations with different fault finding capability can be used to test the PUT and it is important to identify and prioritize the MRs based on its fault finding effectiveness. In order to answer this challenge, we propose to prioritize the MRs based on the diversity in the execution path of the source and follow-up test cases of the MRs. We propose four metrics to capture different levels of diversity in the execution behavior of the test cases for each of the derived MRs. The total weight calculated for each of the MRs using the metrics is used to prioritize the MRs. © 2018 IEEE.},
author_keywords={Metamorphic Relations;  Metamorphic Testing;  MR Prioritization;  Test case diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Azizi2018144,
author={Azizi, M. and Do, H.},
title={ReTEST: A Cost Effective Test Case Selection Technique for Modern Software Development},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2018},
volume={2018-October},
pages={144-154},
doi={10.1109/ISSRE.2018.00025},
art_number={8539077},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059652407&doi=10.1109%2fISSRE.2018.00025&partnerID=40&md5=62a8634ece98236a4870792783dfeb57},
affiliation={Departemant of Computer Sceince and Engineering, University of North Texas, United States},
abstract={Regression test selection offers cost savings by selecting a subset of existing tests when testers validate the modified version of the application. The majority of test selection approaches utilize static or dynamic analyses to decide which test cases should be selected, and these analyses are often very time consuming. In this paper, we propose a novel language-independent Regression TEst SelecTion (ReTEST) technique that facilitates a lightweight analysis by using information retrieval. ReTEST uses fault history, test case diversity, and program change history information to select test cases that should be rerun. Our empirical evaluation with four open source programs shows that our approach can be effective and efficient by selecting a far smaller subset of tests compared to the existing techniques. © 2018 IEEE.},
author_keywords={Regression},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{An2018446,
author={An, W. and Zhu, X. and Zhang, Q. and Shen, B. and Wang, Q.},
title={Optimization design of space heat flow simulation software for spacecraft},
journal={Proceedings - 2018 3rd International Conference on Mechanical, Control and Computer Engineering, ICMCCE 2018},
year={2018},
pages={446-449},
doi={10.1109/ICMCCE.2018.00100},
art_number={8537598},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059840672&doi=10.1109%2fICMCCE.2018.00100&partnerID=40&md5=a5ecbac8204a89d4c85a1ebc73ea075b},
affiliation={Beijing Institute of Spacecraft Environment Engineering, Beijing, China},
abstract={In this paper, the principle of spacecraft space heat flow simulation control system and its hardware and software structure are introduced. Then the algorithm and control mode of the software program used in the control system are analyzed simply. In the end, the optimization design of heat flow simulation software and its configuration file is expounded according to the complexity and diversity of control mode in vacuum thermal test, the different driving mode and communication interface of different program-controlled power supply. At present, the reliability of the optimized software has been verified in the thermal tests of the spacecraft components. © 2018 IEEE.},
author_keywords={Core algorithm;  Heat flux simulation;  Optimum design;  Test verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khan2018,
author={Khan, A.I. and Al-Badi, A.},
title={Ubiquitous Application Testing on Cloud},
journal={2018 International Conference on Smart Computing and Electronic Enterprise, ICSCEE 2018},
year={2018},
doi={10.1109/ICSCEE.2018.8538412},
art_number={8538412},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059434912&doi=10.1109%2fICSCEE.2018.8538412&partnerID=40&md5=d264c3645b36951965e89a8a5f97c34e},
affiliation={Department of Information Systems, Sultan Qaboos University, Muscat, Oman},
abstract={Nowadays desktop-based application are continuously evolving into pervasive or ubiquitous applications. There are millions of ubiquitous applications developed and used, but the testing of these applications are very complex tasks due to diversity in ubiquitous devices. Cloud computing is the future which has brought cost reduction, improved business, and processes in organizations through minimum management efforts. Software testing is a process to make sure the software is of acceptable quality. The traditional testing processes are incompatible with the changing technology and environment such as in ubiquitous applications. The availability of 5G network has reduced the data streaming dependence on the network. Thus under the presence of 5 G network cloud based testing can be successfully carried out for ubiquitous applications. Therefore, this research proposes a testing process for ubiquitous applications on cloud consisting of five steps. The testing on cloud would include stress testing, performance testing, compatibility testing, functional testing, web browser testing, load testing, and latency testing etc. The research mainly relate to ubiquitous application testing. © 2018 IEEE.},
author_keywords={cloud based testing;  cloud computing;  cloud testing;  mobile application;  pervasive application;  ubiquitous application;  ubiquitous devices},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yildirim2018,
author={Yildirim, A.S. and Berker, E. and Kayakesen, M.E.},
title={System Level Test Automation in UAV Development},
journal={AUTOTESTCON (Proceedings)},
year={2018},
volume={2018-September},
doi={10.1109/AUTEST.2018.8532551},
art_number={8532551},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058217730&doi=10.1109%2fAUTEST.2018.8532551&partnerID=40&md5=0b540dc73259b39a873e9cb47fea1726},
affiliation={UAV Test and Integration Department, Turkish Aerospace Industries, Ankara, Turkey},
abstract={As the complexity of defense systems have increased in recent years; avionics and automated test systems have become more complex. Consequently, system engineering requirements demand robust requirement verification for the customer specifications and product quality. Traditional test system does not meet the demands like inconvenient data format, difficulty in test programs' reuse, inefficient use of available system resources, difficult error findings. The testing technology is growing continuously and rapidly. Frequently used automated test strategies are mainly based on software testing in software verification level. A software insensitive avionic system mostly comprises software modules at the unit level. When it comes to testing from subsystem to system level different circumstances emerge. System level testing has always been heavily dependent on human intervention and human judgment. Before emergence of system of systems concept most of the systems have their own boundaries for external interaction at human machine interface (HMI) level. Hence it has been natural that testing system functionality as a whole at HMI boundaries were carried out by human testers. However due to developments in software technologies and by approaching to automated system level testing problem as a collection of many self-containing diverse sub-problems; it can be seen that software industry has already created lots of tools to address each of these sub-problems without even aiming to solve them for automated system test approach. In this paper automated UAV system test approach will be given by definition and analysis of each problem and solution addressing this problem. New automated testing model is presented to be functional on system level with a combination of hardware and software. The automated testing will handle the testing complexity with faster execution time, reduced testing costs, eliminating user errors and will also to increased probability of detecting failures. Test automation with both simulators and real devices is used for execution of the tests, and for the comparison of actual outcomes with predicted outcomes. This paper introduces a novel approach for test automation implementation for avionic system validation at system level in Unmanned Air Vehicle (UAV) development with different scenarios. © 2018 IEEE.},
author_keywords={Integration Test;  Smoke Test;  System Level Test;  Test Automation;  UAV Verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Linan2018549,
author={Linan, S. and Bello-Jimenez, L. and Arevalo, M. and Linares-Vasquez, M.},
title={Automated extraction of augmented models for android apps},
journal={Proceedings - 2018 IEEE International Conference on Software Maintenance and Evolution, ICSME 2018},
year={2018},
pages={549-553},
doi={10.1109/ICSME.2018.00065},
art_number={8530063},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058285313&doi=10.1109%2fICSME.2018.00065&partnerID=40&md5=9b463ccc71dbaf367747ebe9621158ef},
affiliation={Systems and Computing Engineering Department, Universidad de Los Andes, Bogotá, Colombia},
abstract={Mobile software development involves significant challenges to developers such as device fragmentation (i.e., enormous hardware and software diversity), event-driven programming (i.e., programming based on user interactions, sensor readings and other events where the program must react) and continuous evolving platforms (i.e., fast changing mobile frameworks and technologies). This can lead programmers to error-prone code, because of the multiple combinations of external variables that must be taken into account in an app development process. Thus, testing is an underlying necessity in mobile applications to deliver high quality apps. However, defining tests suites for app development is a difficult task that requires a lot of effort, because it must consider all the possible states of an app, its context (e.g., device in which is running, sensors, touch gestures, screen proportions, connectivity), and a large combination of mobile devices and operating systems. Previous efforts have been done to extract models that support automated testing. However, as of today there is not a single model that synthesizes different aspects in mobile apps such as domain, usage, context and GUI-related information. These aspects represent complementary information that can be mixed into a single and enriched model. In this paper, we propose a multi-model representation that combines information extracted statically and dynamically from Android apps. Our approach allows practitioners to automatically extract augmented models that combine different types of information, and could help them during comprehension and testing tasks. © 2018 IEEE.},
author_keywords={Android testing;  Augmented Model;  gui model;  Mobile software development;  Mobile testing;  Model testing;  Multi Model;  Multi model testing;  Software modeling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ruland2018119,
author={Ruland, S. and Lity, S. and Luthmann, L. and Thüm, T. and Ribeiro, M. and Bürdek, J. and Lochau, M.},
title={Measuring effectiveness of sample-based product-line testing},
journal={GPCE 2018 - Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences, co-located with SPLASH 2018},
year={2018},
pages={119-133},
doi={10.1145/3278122.3278130},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059045225&doi=10.1145%2f3278122.3278130&partnerID=40&md5=fefe4a2ab3ffafb0b1bf343a282acb0c},
affiliation={Real-Time Systems Lab, TU Darmstadt, Germany; Institute of Software Engineering and, Automotive Informatics, TU Braunschweig, Germany; Computing Institute, Federal University of Alagoas, Brazil},
abstract={Recent research on quality assurance (QA) of configurable software systems (e. g., software product lines) proposes different analysis strategies to cope with the inherent complexity caused by the well-known combinatorial-explosion problem. Those strategies aim at improving efficiency of QA techniques like software testing as compared to brute-force configuration-by-configuration analysis. Sampling constitutes one of the most established strategies, defining criteria for selecting a drastically reduced, yet sufficiently diverse subset of software configurations considered during QA. However, finding generally accepted measures for assessing the impact of sample-based analysis on the effectiveness of QA techniques is still an open issue. We address this problem by lifting concepts from single-software mutation testing to configurable software. Our framework incorporates a rich collection of mutation operators for product lines implemented in C to measure mutation scores of samples, including a novel family-based technique for product-line mutation detection. Our experimental results gained from applying our tool implementation to a collection of subject systems confirms the widely-accepted assumption that pairwise sampling constitutes the most reasonable efficiency/effectiveness trade-off for sample-based product-line testing. © 2018 Association for Computing Machinery.},
author_keywords={Mutation Testing;  Sample-Based Testing;  Software Product Lines},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kureichik2018,
author={Kureichik, V.M. and Logunova, J.A.},
title={Some of the New Indicators in Genetic Algorithms for the Traveling Salesman Problem},
journal={Proceedings of 2018 IEEE East-West Design and Test Symposium, EWDTS 2018},
year={2018},
doi={10.1109/EWDTS.2018.8524713},
art_number={8524713},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057980446&doi=10.1109%2fEWDTS.2018.8524713&partnerID=40&md5=d848af7009df7b69d57b3a3e3feb41f7},
affiliation={Southern Federal University, Autonomous Federal State Institution of Higher Education, Taganrog, Russian Federation},
abstract={The Traveling Salesman Problem (TSP) is a classic NP-hard problem example. In this regard, the development of new methods for solving it, is an urgent task. Considering that the time complexity finding the exact solution is a factorial or exponential dependence on the input data, there are many methods for approximate solution of TSP. In this case, algorithms based on probabilistic-directed search are popular. Among these are genetic and bio-inspired algorithms. This paper presents two new indicators in genetic algorithms (GA) for analyzing the degradation degree of the population. Special software was developed for the GA analysis, which was tested on the well-known benchmark: bier 127. A number of representation issues are discussed along with genetic Edge Recombination Crossover (ERX) and Partially-mapped crossover (PMX). Test results indicate that the GA with ERX gives an advantage in the diversity of the population in front of the GA with the PMX. The obtained information is useful for further genetic algorithm parameters settings. As a result, developed indicators can be used for forward estimation of the GA prospects even before applying it to a real task. They can be also used for parameter settings of the GA. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Das2018235,
author={Das, S. and Mukhopadhyay, S.},
title={FIL-DGA based hardware optimization system},
journal={Applied Soft Computing Journal},
year={2018},
volume={72},
pages={235-260},
doi={10.1016/j.asoc.2018.07.037},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051633338&doi=10.1016%2fj.asoc.2018.07.037&partnerID=40&md5=6f6f7ee274836739a64270f2c92fb9ed},
affiliation={Institute of Radio Physics and Electronics, University of Calcutta, India},
abstract={This paper presents a new algorithm entitled as dominant character genetic algorithm (DGA) along with its hardware architecture entitled as dominant character genetic algorithm hardware architecture (DGA-Arch) for real parameter optimization problem. In DGA, the evolution process is inspired from the dominant characteristics present in human cognizance and it is realized by varying the mutation probability of the genes. On the other hand, DGA-Arch is a resource efficient, highly flexible architecture which is designed and integrated with field programmable gate array-in-loop (FIL) environment and an overall FIL based DGA (FIL-DGA) optimization system is developed. The DGA-Arch was implemented on Virtex IV (ML401, XC4VLX25) field programmable gate array (FPGA) chip with maximum of 5% logic slice utilization and tested for 18 benchmark problems. On an average, the proposed hardware manifested speedup of about 130× over software genetic algorithm (GA) implementation for the test problems. The performance is also compared using 5 modified functions with different GA based hardware reported in existing literature and is found to optimize problems more accurately with greater repeatability and diversity. The DGA-Arch reached convergence within 0.0005–0.009% of function evaluations compared to the total search space and requires almost no repeated synthesis in different problem environment. Later, the FIL-DGA system has been employed to adapt the parameters of few classical engineering problems and a real world application in cognitive radio environment. © 2018 Elsevier B.V.},
author_keywords={Dominant character genetic algorithm;  Dynamic spectrum sensing;  Field programmable gate array;  FPGA-in-loop;  Genetic algorithm},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ma2018162,
author={Ma, L. and Wu, P. and Chen, T.Y.},
title={Diversity driven adaptive test generation for concurrent data structures},
journal={Information and Software Technology},
year={2018},
volume={103},
pages={162-173},
doi={10.1016/j.infsof.2018.07.001},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049561393&doi=10.1016%2fj.infsof.2018.07.001&partnerID=40&md5=289251b9cc1ac7cf5ecf4386d26672c5},
affiliation={State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China; University of Chinese Academy of Sciences, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Australia; Department of Computer Science, University of Miami, United States},
abstract={Context: Testing concurrent data structures remains a notoriously challenging task, due to the nondeterminism of multi-threaded tests and the exponential explosion on the number of thread schedules. Objective: We propose an automated approach to generate a series of concurrent test cases in an adaptive manner, i.e., the next test cases are generated with the guarantee to discover the thread schedules that have not yet been activated by the previous test cases. Method: Two diversity metrics are presented to induce such adaptive test cases from a static and a dynamic perspective, respectively. The static metric enforces the diversity in the program structures of the test cases; while the dynamic one enforces the diversity in their capabilities of exposing untested thread schedules. We implement three adaptive test generation approaches for C/C++ concurrent data structures, based on the state-of-the-art active testing engine Maple. Results: We then report an empirical study with 9 real-world C/C++ concurrent data structures, which demonstrates the efficiency of our test generation approaches in terms of the number of thread schedules discovered, as well as the time and the number of tests required for testing a concurrent data structure. Conclusion: Hence, by using diverse test cases derived from the static and dynamic perspectives, our adaptive test generation approaches can deliver a more efficient coverage of the thread schedules of the concurrent data structure under test. © 2018 Elsevier B.V.},
author_keywords={Active testing;  Adaptive random testing;  Concurrent data structures;  Test case diversity;  Test case generation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liang2018809,
author={Liang, J. and Jiang, Y. and Chen, Y. and Wang, M. and Zhou, C. and Sun, J.},
title={PAFL: Extend fuzzing optimizations of single mode to industrial parallel mode},
journal={ESEC/FSE 2018 - Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2018},
pages={809-814},
doi={10.1145/3236024.3275525},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058275544&doi=10.1145%2f3236024.3275525&partnerID=40&md5=01d3b3ec35f5b6e7263e8bee133ea9cc},
affiliation={KLISS, School of Software, Tsinghua University, China},
abstract={Researchers have proposed many optimizations to improve the efficiency of fuzzing, and most optimized strategies work very well on their targets when running in single mode with instantiating one fuzzer instance. However, in real industrial practice, most fuzzers run in parallel mode with instantiating multiple fuzzer instances, and those optimizations unfortunately fail to maintain the efficiency improvements. In this paper, we present PAFL, a framework that utilizes efficient guiding information synchronization and task division to extend those existing fuzzing optimizations of single mode to industrial parallel mode. With an additional data structure to store the guiding information, the synchronization ensures the information is shared and updated among different fuzzer instances timely. Then, the task division promotes the diversity of fuzzer instances by splitting the fuzzing task into several sub-tasks based on branch bitmap. We first evaluate PAFL using 12 different real-world programs from Google fuzzer-test-suite. Results show that in parallel mode, two AFL improvers-AFLFast and FairFuzz do not outperform AFL, which is different from the case in single mode. However, when augmented with PAFL, the performance of AFLFast and FairFuzz in parallel mode improves. They cover 8% and 17% more branches, trigger 79% and 52% more unique crashes. For further evaluation on more widely-used software systems from GitHub, optimized fuzzers augmented with PAFL find more real bugs, and 25 of which are security-critical vulnerabilities registered as CVEs in the US National Vulnerability Database. © 2018 Association for Computing Machinery.},
author_keywords={Fuzzing;  Parallel;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Akour201866,
author={Akour, M. and Abuwardih, L. and Alhindawi, N. and Alshboul, A.},
title={Test Case Minimization using Genetic Algorithm: Pilot Study},
journal={2018 8th International Conference on Computer Science and Information Technology, CSIT 2018},
year={2018},
pages={66-70},
doi={10.1109/CSIT.2018.8486190},
art_number={08486190},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056716694&doi=10.1109%2fCSIT.2018.8486190&partnerID=40&md5=870c04adb9477f0ac7a8b4e3a6ef9197},
affiliation={Department of Software Engineering, Yarmouk University, Irbid, Jordan; Computer Information Systems Department, Jordan University of Science and Technology, Irbid, Jordan; Department of Software Engineering, Jadara University, Irbid, Jordan; Computer Information Systems Department, Yarmouk University, Irbid, Jordan},
abstract={Due to the large number of test cases that are required to perform sufficient testing of the desired software; the diverse methods to reduce the test suite is needed. One of the common studied methods is removing the redundant test cases. In this research paper a short study is conducted to address the effectiveness of genetic algorithm in order to reduce the number of test cases that do not added tangible value in the mean of test coverage. Genetic algorithm is utilized in this study to help in minimizing the test cases, where the genetic algorithm generates the preliminary population, calculates the fitness value using coverage, and then selective the offspring in consecutive generations using genetic operations. This process of generation is repeated until a minimized test case is achieved. The results of study demonstrate that, genetic algorithms can significantly reduce the size of the test cases. © 2018 IEEE.},
author_keywords={evolutionary algorithm;  genetic algorithm;  test case minimization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murad201817,
author={Murad, G. and Badarneh, A. and Quscf, A. and Almasalha, F.},
title={Software Testing Techniques in IoT},
journal={2018 8th International Conference on Computer Science and Information Technology, CSIT 2018},
year={2018},
pages={17-21},
doi={10.1109/CSIT.2018.8486149},
art_number={08486149},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056698944&doi=10.1109%2fCSIT.2018.8486149&partnerID=40&md5=8fa52bda0e656a6560f8b5f482369592},
affiliation={Computer Science Department, Applied Science University, Amman, Jordan; Jordanian Engineers Association, Amman, Jordan; Software Engineering Department, Princess Sumaya University for Technology, Amman, Jordan},
abstract={As well as the important industry 4.0 revolution, based on of the serves that IoT provides in our society in different fields such as smart building, factory, mobility, health care. Previous studies explore various technology solution to find the best technique to test IoT applications in order to ensure quality for IoT devices. The paper surveys diverse of aspects of multiples software testing and tools for loT devices, and provides details in use case testing for IoT environment and test different accepts such as usability, security, connectivity. The paper considers various security requirement and challenges an unveils different research problems in testing IoT applications and proposes multiple tools and software techniques that help to enhance IoT applications quality. © 2018 IEEE.},
author_keywords={IoT;  Software Testing;  Testing Methodologies;  Testing Tools},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sun20182441,
author={Sun, J. and Chen, J. and Wang, G.},
title={Multi-objective test case prioritization based on epistatic particle swarm optimization},
journal={International Journal of Performability Engineering},
year={2018},
volume={14},
number={10},
pages={2441-2448},
doi={10.23940/ijpe.18.10.p20.24412448},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056585774&doi=10.23940%2fijpe.18.10.p20.24412448&partnerID=40&md5=d4ffaa3be519755d185e7071669217a6},
affiliation={School of Computer Science and Technology, Xi’an University of Posts and Telecommunications, Xi’an, 710121, China; Shaanxi Key Laboratory of Network Data Analysis and Intelligent Processing, Xi’an University of Posts and Telecommunications, Xi’an, 710121, China},
abstract={To address the Multi-Objective Test Case Prioritization (MOTCP) problem, an Epistatic Particle Swarm Optimization (EPSO) algorithm is presented. The epistasis in biology is introduced into the new algorithm, and the particles are updated based on the crossover of Epistatic Test Case Segment (ETS) in the test case sequence. The average coverage percentage of program entity and effective execution time of the test case sequence are set as two objective fitness functions in EPSO. The experiment selects four typical open12 source projects as benchmark programs. We adopted Average Percentage of Branch Coverage (APBC) and Effective Execution Time (EET) as objective fitness. The four classical Java testing projects results show that the EPSO is more effective and more diverse than single-point PSO and order PSO. The EPSO algorithm efficiently solves the MOTCP problem by promoting early detection of software defects and reducing software testing costs in regression testing. © 2018 Totem Publisher, Inc. All rights reserved.},
author_keywords={Average percentage of branch coverage;  Epistatic test case segment;  Multi-objective test case prioritization;  Particle swarm optimization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shin2018914,
author={Shin, D. and Yoo, S. and Bae, D.-H.},
title={A Theoretical and Empirical Study of Diversity-Aware Mutation Adequacy Criterion},
journal={IEEE Transactions on Software Engineering},
year={2018},
volume={44},
number={10},
pages={914-931},
doi={10.1109/TSE.2017.2732347},
art_number={7994647},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055528836&doi=10.1109%2fTSE.2017.2732347&partnerID=40&md5=36b231825d11cdfc1ccfefbc374d064f},
affiliation={KAIST, Daejeon, 34141, South Korea},
abstract={Diversity has been widely studied in software testing as a guidance towards effective sampling of test inputs in the vast space of possible program behaviors. However, diversity has received relatively little attention in mutation testing. The traditional mutation adequacy criterion is a one-dimensional measure of the total number of killed mutants. We propose a novel, diversity-Aware mutation adequacy criterion called distinguishing mutation adequacy criterion, which is fully satisfied when each of the considered mutants can be identified by the set of tests that kill it, thereby encouraging inclusion of more diverse range of tests. This paper presents the formal definition of the distinguishing mutation adequacy and its score. Subsequently, an empirical study investigates the relationship among distinguishing mutation score, fault detection capability, and test suite size. The results show that the distinguishing mutation adequacy criterion detects 1.33 times more unseen faults than the traditional mutation adequacy criterion, at the cost of a 1.56 times increase in test suite size, for adequate test suites that fully satisfies the criteria. The results show a better picture for inadequate test suites; on average, 8.63 times more unseen faults are detected at the cost of a 3.14 times increase in test suite size. © 1976-2012 IEEE.},
author_keywords={diversity;  Mutation testing;  test adequacy criteria},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jianping2018,
author={Jianping, C. and Wenzhu, S. and Xiuzhi, Z. and Guotao, Z. and Wenting, H.},
title={Interface Byte Stream Parsing Algorithm for Black Box Test Cases Automatic Generation},
journal={IOP Conference Series: Materials Science and Engineering},
year={2018},
volume={428},
number={1},
doi={10.1088/1757-899X/428/1/012001},
art_number={012001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054782787&doi=10.1088%2f1757-899X%2f428%2f1%2f012001&partnerID=40&md5=52093e7b52d139c2a07df7e10ba514b6},
affiliation={Naval Aviation University Qingdao Branch, QingDao, 266041, China},
abstract={Traditional black box test cases are mostly generated by manual methods and are difficult to automate. This is caused by the diversification of software module's input interface. In this paper, an intelligent black box test case automatic generation algorithm referred to as Interface Byte Stream Parsing (IBSP) is proposed. In this algorithm, the input interface of the software module to be tested is firstly abstracted as the number of bytes occupied by the input parameters of the module, and a section of dynamic memory is applied, then a binary sequence is generated in the dynamic memory by using Dynamic Memory Increment (DMI) or Random Sampling (RS) method. Follow that, the type information contained in the interface is inputted through the software module to be tested. The binary sequence is parsed into the input data of the software module to be tested, and the input data is filtered by using the filter conditions. Finally, the test expectations given by the user and the filtered input data constitute the test cases. For improving the performance of the algorithm, a testing experiment is conducted. The experimental result shows that the algorithm facilitates the automatic generation of test cases in software testing, improves the efficiency of test case generation, and has been initially applied. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{MuñumerHerrero2018119,
author={Muñumer Herrero, E. and Ellul, C. and Morley, J.},
title={Testing the impact of 2D generalisation on 3D models - Exploring analysis options with an off-the-shelf software package},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2018},
volume={42},
number={4/W10},
pages={119-126},
doi={10.5194/isprs-archives-XLII-4-W10-119-2018},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056172811&doi=10.5194%2fisprs-archives-XLII-4-W10-119-2018&partnerID=40&md5=a442c16fb17cd2db892f3540e484e68a},
affiliation={Department of Civil, Environmental and Geomatic Engineering, University College London, United Kingdom; Ordnance Survey, Southampton, United Kingdom},
abstract={Popularity and diverse use of 3D city models has increased exponentially in the past few years, providing a more realistic impression and understanding of cities. Often, 3D city models are created by elevating the buildings from a detailed 2D topographic base map and subsequently used in studies such as solar panel allocation, infrastructure remodelling, antenna installations or even tourist guide applications. However, the large amount of resulting data slows down rendering and visualisation of the 3D models, and can also impact the performance of any analysis. Generalisation enables a reduction in the amount of data - however the addition of the third dimension makes this process more complex, and the loss of detail resulting from the process will inevitably have an impact on the result of any subsequent analysis. While a few 3D generalization algorithms do exist in a research context, these are not available commercially. However, GIS users can create the generalised 3D models by simplifying and aggregating the 2D dataset first and then extruding it to the third dimension. This approach offers a rapid generalization process to create a dataset to underpin the impact of using generalised data for analysis. Specifically, in this study, the line of sight from a tall building and the sun shadow that it creates are calculated and compared, in both original and generalised datasets. The results obtained after the generalisation process are significant: both the number of polygons and the number of nodes are minimized by around 83% and the volume of 3D buildings is reduced by 14.87 %. As expected, the spatial analyses processing times are also reduced. The study demonstrates the impact of generalisation on analytical results - which is particularly relevant in situations where detailed data is not available and will help to guide the development of future 3D generalisation algorithms. It also highlights some issues with the overall maturity of 3D analysis tools, which could be one factor limiting uptake of 3D GIS. © Authors 2018. CC BY 4.0 License.},
author_keywords={3D buildings;  3D generalisation;  3D models;  Aggregation;  Performance;  Simplification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Basyuni2018,
author={Basyuni, M. and Baba, S. and Oku, H. and Wati, R. and Fitri, A.},
title={Microsatellite analysis of the mating system of Bruguiera gymnorrhiza and Kandelia obovata},
journal={MATEC Web of Conferences},
year={2018},
volume={197},
doi={10.1051/matecconf/201819706003},
art_number={06003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053775255&doi=10.1051%2fmatecconf%2f201819706003&partnerID=40&md5=cb893cbcc8ecb982140e91e924b5d6fe},
affiliation={Universitas Sumatera Utara, Department of Forestry, Faculty of Forestry, Jl. Tri Dharma Ujung No. 1 Medan, North Sumatera, 20155, Indonesia; Universitas Sumatera Utara, Mangrove and Bio-Resources Group, Center of Excellence for Natural Resources Based Technology, Medan North Sumatera, 20155, Indonesia; University of the Ryukyus, International Society for Mangrove Ecosystems, Faculty of Agriculture, 1 Senbaru, Nishihara, Okinawa, 903-0213, Japan; University of the Ryukyus, Molecular Biotechnology Group, Tropical Biosphere Research Center, 1 Senbaru, Nishihara, Okinawa, 903-0213, Japan},
abstract={Microsatellite loci were used for estimating mating system for three populations of B. gymnorrhiza and K. obovata (Rhizophoracea) in Okinawa, Japan. Mother trees and thirty offspring of individual samples representing the population of both species were genotyped at five microsatellites. The mating system was examined using two approaches: a mixed mating model of multilocus testing, implemented by MLTR program and outcrossing rate from the level of inbreeding. Mating system analysis showed multilocus outcrossing rates (tm) for both species was 0.850-1.000 and 0.780-0.938 respectively. By contrast, according to inbreeding level, tm was lower than MLRT: 0.495-1.028 and 0.480-0.612 of both species respectively. However, biparental inbreeding (tm- ts) was diverse from zero both species for all three populations, showing that cross-fertilization events may ensue between the relatives both species. This data as well means the genetic relatedness (r) for B. gymnorrhiza and K. obovata were 0.108±0.025 and 0.032±0.09 respectively. Average relatedness was below 0.25, the value for a half-sib relationship. These results suggest that postulation of a half-sib relationship among progeny of open-pollinated families is opposed for both mangrove tree species. © The Authors, published by EDP Sciences, 2018.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fletcher2018135,
author={Fletcher, K.},
title={ITS advanced support team: Swiss army knife or ticket dumpster?},
journal={Proceedings ACM SIGUCCS User Services Conference},
year={2018},
pages={135-138},
doi={10.1145/3235715.3235746},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057949073&doi=10.1145%2f3235715.3235746&partnerID=40&md5=79cd90f3aedf619274624dc9c3fd8b75},
affiliation={West Virginia University, Information Technology Services, One Waterfront Place, Morgantown, WV  26506-6500, United States},
abstract={At West Virginia University, Information Technology Services provides centralized enterprise resources and services, while additional independent IT groups provide support for individual colleges and units. ITS Advanced Support is a second-tier specialized support team of eleven individuals with diverse skills that supports over 80 software applications. In addition to responding to escalated and specialized support tickets, we serve as application administrators for select systems and participate in testing new applications and software upgrades. In this paper, I will summarize the creation of our multi-purpose Advanced Support Team, describe what we do, and explain how our team fits in the ITS organization chart and support processes. I will describe what is working well and the challenges we face juggling multiple roles and support topics. © 2018 Association for Computing Machinery.},
author_keywords={Application administration;  end-user support;  reorganization;  team building;  ticket escalation procedures},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zaman20181,
author={Zaman, T.S. and Tingting, Y.T.},
title={Extracting implicit programming rules: Comparing static and dynamic approaches},
journal={SoftwareMining 2018 - Proceedings of the 7th International Workshop on Software Mining, co-located with ASE 2018},
year={2018},
pages={1-7},
doi={10.1145/3242887.3242889},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063597937&doi=10.1145%2f3242887.3242889&partnerID=40&md5=6a960948f54ca1900feb6eb484842068},
affiliation={University of Kentucky, United States},
abstract={Programs often follow implicit programming rules, such as, function call A must be followed by function call B. Rules of such kinds are rarely documented by developers. Nevertheless, programming rules play an important role in software testing and maintenance. For example, the rules can be used as test oracles to detect violations. If a programmer can be notified of these rules before updating the source code, the chances of generating defects due to rule violations might be minimized. Prior works have used static and dynamic analysis techniques to extract implicit programming rules, but none compares the effectiveness of the two techniques. In this paper, we have undertaken an empirical study to compare the two techniques when they are being used for extracting programming rules. Our results indicate that the performance of the dynamic analysis technique depends on the number and the diversity of the traces. Moreover, the dynamic analysis technique generates more precise rules than the static analysis technique if a diverse and sufficient number of test cases are provided. © 2018 ACM.},
author_keywords={Dynamic analysis;  Empirical study;  Implicit Programming Rules;  Static Analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2018132,
author={Zhang, M. and Zhang, Y. and Zhang, L. and Liu, C. and Khurshid, S.},
title={Deeproad: GaN-based metamorphic testing and input validation framework for autonomous driving systems},
journal={ASE 2018 - Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
year={2018},
pages={132-142},
doi={10.1145/3238147.3238187},
note={cited By 186},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056509092&doi=10.1145%2f3238147.3238187&partnerID=40&md5=1d01dd8945cf7323da3424fc0a43a6ff},
affiliation={University of Texas, Austin, United States; Shenzhen Key Laboratory of Computational Intelligence, Department of Computer Science and Engineering, Southern University of Science and Technology, China; University of Texas, Dallas, United States},
abstract={While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness. In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well. © 2018 Association for Computing Machinery.},
author_keywords={Deep neural networks;  Input validation;  Software testing;  Test generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Clarisó20181,
author={Clarisó, R. and Cabot, J.},
title={Applying graph kernels to model-driven engineering problems},
journal={MASES 2018 - Proceedings of the 1st International Workshop on Machine Learning and Software Engineering in Symbiosis, co-located with ASE 2018},
year={2018},
pages={1-5},
doi={10.1145/3243127.3243128},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055870626&doi=10.1145%2f3243127.3243128&partnerID=40&md5=f60e7d2bc516068c54a260ef71edaf2f},
affiliation={Multimedia and Telecommunication Dept. Barcelona, Universitat Oberta de Catalunya (UOC) IT, Spain; SOM Research Lab, ICREA, Barcelona, Spain},
abstract={Machine Learning (ML) can be used to analyze and classify large collections of graph-based information, e.g. images, location information, the structure of molecules and proteins, . . . Graph kernels is one of the ML techniques typically used for such tasks. In a software engineering context, models of a system such as structural or architectural diagrams can be viewed as labeled graphs. Thus, in this paper we propose to employ graph kernels for clustering software modeling artifacts. Among other benefits, this would improve the efficiency and usability of a variety of software modeling activities, e.g., design space exploration, testing or verification and validation. © 2018 Association for Computing Machinery.},
author_keywords={Clustering;  Graph kernel;  Machine learning;  Model diversity;  Model-driven engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhou201823751,
author={Zhou, K. and Zhao, L. and Lin, T.},
title={A flexible and uniform string matching technique for general screen content coding},
journal={Multimedia Tools and Applications},
year={2018},
volume={77},
number={18},
pages={23751-23775},
doi={10.1007/s11042-018-5624-2},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051677707&doi=10.1007%2fs11042-018-5624-2&partnerID=40&md5=b47c23961b108e4ba11cbf434e0f13a1},
affiliation={Institute of VLSI, College of Electronics and Information Engineering, Tongji University, Shanghai, 200092, China; Department of Computer Science and Engineering, Shaoxing University, Shaoxing, 312000, China},
abstract={This paper proposes a flexible and uniform string matching technique named universal string matching (USM) for general screen content coding (SCC). USM uses two reference buffers for string matching: primary reference buffer (PRB) and secondary reference buffer (SRB), and includes three modes: general string (GS) mode, constrained string 1 (CS1) mode, and constrained string 2 (CS2) mode. PRB is used in GS mode and CS1 mode and SRB is used in GS mode and CS2 mode. Each of the three modes plays an essential role in SCC due to the diversity and comprehensiveness of the screen content. The experiments use HEVC SCC common test condition (CTC) for lossy coding. Compared with HEVC HM-16.6 + SCM-5.2 reference software of full frame search range for IBC and with ACT off, USM achieves an average Y BD-rate of −25.5% for four TGM (text and graphics with motion) test sequences from the SCC verification test suite and −5.5% for eight TGM test sequences from the HEVC SCC CTC test suite in all intra configuration with a small increase of encoding runtime and a small decrease of decoding runtime. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Audio video coding standard (AVS);  High efficiency video coding (HEVC);  Screen content coding (SCC);  String matching},
document_type={Article},
source={Scopus},
}

@ARTICLE{Appelt2018733,
author={Appelt, D. and Nguyen, C.D. and Panichella, A. and Briand, L.C.},
title={A Machine-Learning-Driven Evolutionary Approach for Testing Web Application Firewalls},
journal={IEEE Transactions on Reliability},
year={2018},
volume={67},
number={3},
pages={733-757},
doi={10.1109/TR.2018.2805763},
art_number={8395015},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049085424&doi=10.1109%2fTR.2018.2805763&partnerID=40&md5=78835877c45441bc6a84a26c675e71fb},
affiliation={SnT Centre, University of Luxembourg, Luxembourg City, L-2721, Luxembourg},
abstract={Web application firewalls (WAFs) are an essential protection mechanism for online software systems. Because of the relentless flow of new kinds of attacks as well as their increased sophistication, WAFs have to be updated and tested regularly to prevent attackers from easily circumventing them. In this paper, we focus on testing WAFs for SQL injection attacks, but the general principles and strategy we propose can be adapted to other contexts. We present ML-Driven, an approach based on machine learning and an evolutionary algorithm to automatically detect holes in WAFs that let SQL injection attacks bypass them. Initially, ML-Driven automatically generates a diverse set of attacks and submits them to the system being protected by the target WAF. Then, ML-Driven selects attacks that exhibit patterns (substrings) associated with bypassing the WAF and evolves them to generate new successful bypassing attacks. Machine learning is used to incrementally learn attack patterns from previously generated attacks according to their testing results, i.e., if they are blocked or bypass the WAF. We implemented ML-Driven in a tool and evaluated it on ModSecurity, a widely used open-source WAF, and a proprietary WAF protecting a financial institution. Our empirical results indicate that ML-Driven is effective and efficient at generating SQL injection attacks bypassing WAFs and identifying attack patterns. © 1963-2012 IEEE.},
author_keywords={Software security testing;  SQL injection (SQLi);  web application firewall (WAF)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Silva201832,
author={Silva, D.B. and Eler, M.M. and Durelli, V.H.S. and Endo, A.T.},
title={Characterizing mobile apps from a source and test code viewpoint},
journal={Information and Software Technology},
year={2018},
volume={101},
pages={32-50},
doi={10.1016/j.infsof.2018.05.006},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047254061&doi=10.1016%2fj.infsof.2018.05.006&partnerID=40&md5=1be6788c93ebd38e172effe604b41d40},
affiliation={Universidade Tecnologica Federal do Parana (UTFPR), Cornelio Procopio, Brazil; Instituto Federal de Santa Catarina (IFSC), Geraldo Werninghaus, Brazil; Universidade de Sao Paulo (USP), Sao Paulo, Brazil; Universidade Federal de Sao Joao del Rei (UFSJ), Sao Joao del Rei, Brazil},
abstract={Context: while the mobile computing market has expanded and become critical, the amount and complexity of mobile apps have also increased. To assure reliability, these apps require software engineering methods, mainly verification, validation, and testing. However, mobile app testing is a challenging activity due to the diversity and limitations found in mobile devices. Thus, it would be interesting to characterize mobile apps in hopes of assisting in the definition of more efficient and effective testing approaches. Objective: this paper aims to identify and quantify the specific characteristics of mobile apps so that testers can draw from this knowledge and tailor software testing activities to mobile apps. We investigate the presence of automated tests, adopted frameworks, external connectivity, graphical user interface (GUI) elements, sensors, and different system configurations. Method: we developed a tool to support the automatic extraction of characteristics from Android apps. We conducted an empirical study with a sample of 663 open source mobile apps. Results: we found that one third of the projects perform automated testing. The frameworks used in these projects can be divided into three groups: unit testing, GUI testing, and mocking. There is a medium correlation between project size and test presence. Specific features of mobile apps (connectivity, GUI, sensors, and multiple configurations) are present in the projects, however, they are fully covered by tests. Conclusion: automated tests are still not developed in a systematic way. Interestingly, measures of app popularity (number of downloads and rating) do not seem to be correlated with the presence of tests. However, the results show a correlation of the project size and more critical domains with the existence of automated tests. Although challenges such as connectivity, sensors, and multiple configurations are present in the examined apps, only one tool has been identified to support the testing of these challenges. © 2018 Elsevier B.V.},
author_keywords={Android;  Automated tests;  Mobile computing;  Open source;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Treacy2018489,
author={Treacy, P. and Jagger, P. and Song, C. and Zhang, Q. and Bilsborrow, R.E.},
title={Impacts of China’s Grain for Green Program on Migration and Household Income},
journal={Environmental Management},
year={2018},
volume={62},
number={3},
pages={489-499},
doi={10.1007/s00267-018-1047-0},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046672782&doi=10.1007%2fs00267-018-1047-0&partnerID=40&md5=e138f30728ee6326c27999836044cbbb},
affiliation={Department of Public Policy, University of North Carolina, Abernethy Hall, CB #3435, Chapel Hill, NC  27599-3435, United States; Department of Geography, University of North Carolina, 205 Saunders Hall, CB #3220, Chapel Hill, NC  25799-3220, United States; Gillings School of Global Public Health, University of North Carolina, 206W Franklin Street, CB #8120, Chapel Hill, NC  27516, United States},
abstract={In the late 1990s, China’s Yangtze and Yellow River Basins suffered devastating natural disasters widely attributed to the degradation of soil and water resources. The Government of China responded with a number of major environmental programs, the most expensive and influential of which, the Grain for Green (GfG) Program, was implemented widely from 1999. Under the GfG Program—also known as the Sloping Land Conversion or Conversion of Cropland to Forest Program—the central government compensates farmers to convert cropland on steep slopes or otherwise ecologically sensitive areas to forest or grassland. Its long-term success depends on households’ ability to make sustainable changes to their household income streams and income diversification strategies. In this paper, we use a difference-in-difference estimation approach to examine the role of migration as a household-level response to the GfG Program, testing the extent to which individuals migrate following a reduction in land available for farming. Importantly, we exploit 15 years of data on migration decisions and establish that participating and non-participating households were on parallel migration paths before the program, thus refuting a key threat to causality in a difference-in-difference model. We find that participating families do, in fact, choose migration as an income diversification strategy more frequently than non-participants. The program effects varied over time but peaked post-Great Recession in 2011 when migration rates in GfG households exceeded those of non-GfG households by 5.9% points (p = 0.003) or about 26%. Our findings should encourage policymakers that families are making long-term adjustments to their livelihood strategies to avoid poverty in anticipation of the eventual withdrawal of government supports. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={China;  Grain for green program;  Income diversification;  Migration;  Payment for ecosystem services;  Sloping land conversion program},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shahbazi2018531,
author={Shahbazi, A. and Panahandeh, M. and Miller, J.},
title={Black-box tree test case generation through diversity},
journal={Automated Software Engineering},
year={2018},
volume={25},
number={3},
pages={531-568},
doi={10.1007/s10515-018-0232-y},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044069327&doi=10.1007%2fs10515-018-0232-y&partnerID=40&md5=590f3abf097e8641300bb6612662bc6b},
affiliation={Department Electrical and Computer Engineering, University of Alberta, Edmonton, Canada},
abstract={To identify defects and security risks in many real-world applications structured test cases, including test cases structured as trees are required. A simple approach is to generate random trees as test cases [random testing (RT)]; however, the RT approach is not very effective. In this work, we investigate and extend the black-box tree test case generation approaches. We introduce a novel model to produce superior test case generation based around the idea of measuring the diversity of a tree test set. This initial approach is further extended by adding a second model which describes the distribution of tree sizes. Both models are realized via a multi-objective optimization algorithm. An empirical study is performed with four real-world programs indicating that the generated tree test cases outperform test cases generated by other methods. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Automated test case generation;  Black-box testing;  Random testing;  Software testing;  Structured input;  Tree distance;  Tree test cases;  Trees},
document_type={Article},
source={Scopus},
}

@ARTICLE{Herbold2018811,
author={Herbold, S. and Trautsch, A. and Grabowski, J.},
title={A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches},
journal={IEEE Transactions on Software Engineering},
year={2018},
volume={44},
number={9},
pages={811-833},
doi={10.1109/TSE.2017.2724538},
art_number={7972992},
note={cited By 106},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023163956&doi=10.1109%2fTSE.2017.2724538&partnerID=40&md5=31c7facb1cda8458ce7091e93910365a},
affiliation={University of Goettingen, Göttingen, 37073, Germany},
abstract={Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice. © 1976-2012 IEEE.},
author_keywords={benchmark;  comparison;  Cross-project defect prediction;  replication},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hayashi2018160,
author={Hayashi, K. and Aoyama, M.},
title={A multiple product line development method based on variability structure analysis},
journal={ACM International Conference Proceeding Series},
year={2018},
volume={1},
pages={160-169},
doi={10.1145/3233027.3233048},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055521064&doi=10.1145%2f3233027.3233048&partnerID=40&md5=a570512845000562c53b0955e44d7682},
affiliation={DENSO CORPORATION, Kariya, 448-8661, Japan; Nanzan University, Nagoya, 466-8673, Japan},
abstract={This article proposes a multiple product line development method based on variability structure analysis. In product line development, the problem area is divided into the domain engineering and application engineering for delivering diverse products. Now, the development of automotive software requires to meet both agility and extreme diversity, which is a big challenge. We developed a structural analysis method of variability for multiple product lines using an extended model of OVM (Orthogonal Variability Model). Together with the variability analysis method, we propose an agile application development method to refine development items according to variability dependency based on the analysis, and develop them incrementally. We applied the proposed method to the development of the multiple product lines of automotive software systems, and demonstrated to reduce the volatility of the test efforts and usage of the test environment, and higher velocity and better manageability of the value stream. © 2018 Association for Computing Machinery.},
author_keywords={Agile development;  Automotive software;  Multiple product lines;  Software product line;  Variability analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Robu2018473,
author={Robu, A. and Robu, N. and Neagu, A.},
title={New software tools for hydrogel-based bioprinting},
journal={SACI 2018 - IEEE 12th International Symposium on Applied Computational Intelligence and Informatics, Proceedings},
year={2018},
pages={473-478},
doi={10.1109/SACI.2018.8440971},
art_number={8440971},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053433342&doi=10.1109%2fSACI.2018.8440971&partnerID=40&md5=cdbf3e1fbc4138acce0cd599e3139dab},
affiliation={Politehnica University of Timisoara, Department of Automation and Applied Informatics, Timisoara, Romania; Victor Babeş University of Medicine and Pharmacy, Center for Modeling Biological Systems and Data Analysis, Timişoara, Romania},
abstract={Tissue engineering seeks to solve the problem of donor organ shortage by building tissues and organs in the laboratory. A powerful approach to tissue engineering is three-dimensional (3D) bioprinting-a numerically controlled deposition of cells and biomaterials. The outcome of bioprinting, however, depends on subsequent cell movements governed by mechanisms known from developmental biology. This work presents new informatics tools for predicting post-printing structure formation in bioprinted constructs. We extended the SIMMMC application, developed for simulating cell rearrangements in scaffold-based tissue engineering, with new modules for modeling and simulating the evolution of bioprinted tissue constructs, composed of cells, hydrogels, and cell culture medium. We adapted the Metropolis Monte Carlo algorithm, which lies at the basis of our simulations, such that it takes into consideration the energies of interaction between all the elements of the system. We also created a module that generates automatically the 3D models of fabricated tissue constructs, after loading an XYZ file. Thus, we have the possibility to integrate in our platform many architectures of bioprinted tissue constructs. To validate our new software components, we generated two models used in experiments and simulated their evolution, finding a qualitative agreement between experiments and simulations. The software packages presented here might be used to optimize extrusion-based bioprinting by testing various paths of the print head and diverse cell-cell and cell-hydrogel interactions. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hilger2018,
author={Hilger, L. and Schneiders, T. and Meyer, F.P. and Kroll, J.-P.},
title={Use of smart technologies for energy efficiency, energy- and load management in small and medium sized enterprises (SMEs)},
journal={2018 7th International Energy and Sustainability Conference, IESC 2018},
year={2018},
doi={10.1109/IESC.2018.8439992},
art_number={8439992},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053122871&doi=10.1109%2fIESC.2018.8439992&partnerID=40&md5=f65362cc12034f1287022d9bbfe32c66},
affiliation={Cologne Institute for Renewable Energy (CIRE), TH Köln (University of Applied Sciences), Cologne, Germany},
abstract={Currently, the impact of digitalisation on the energy sector enables new approaches of products and business models on the market. The use of smart technologies can create added value such as lowering the energy costs by increasing energy efficiency in enterprises. While the topic of using digital and smart technologies for energy efficiency already plays an essential role in large enterprises, small and medium sized enterprises (SMEs) are still not aware of the topic in general as well as the benefits of investing in energy efficiency measures.This paper presents an application-oriented scientific approach of identifying the application potential of smart technologies for energy efficiency as well as energy- and load-management in SMEs. Due to the diversity of smart technologies which are actually offered on the market, a technology-screening classifies the different systems into specific categories. These categories are evaluated and assessed in a cross-system comparison regarding the application potential in SMEs. Based on onsite investigations in SMEs, the results show the practical application potential for each technology category according to specific criteria. According to these criteria, smart technologies can be used in different sections of the enterprise, which are also developed during onsite investigations. The key-findings of the onsite investigations will feed into the foreseen application test of smart technologies in SMEs, analysing technical, software-specific and socio-economic aspects of smart technologies close to and in dialogue with the stakeholders. © 2018 IEEE.},
author_keywords={Application potential;  Energy efficiency;  Onsite investigations;  Smart technologies;  SMEs},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Prabhu201846,
author={Prabhu, S. and Chaudhry, G.I. and Godfrey, B. and Caesar, M.},
title={High-coverage testing of softwarized networks},
journal={SecSoN 2018 - Proceedings of the 2018 Workshop on Security in Softwarized Networks: Prospects and Challenges, Part of SIGCOMM 2018},
year={2018},
pages={46-52},
doi={10.1145/3229616.3229617},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056379407&doi=10.1145%2f3229616.3229617&partnerID=40&md5=39a2c9ddb7b0ea5368e9e6167d07ab53},
affiliation={University of Illinois, Urbana-Champaign, United States},
abstract={Network operators face a challenge of ensuring correctness as networks grow more complex, in terms of scale and increasingly in terms of diversity of software components. Network-wide verification approaches can spot errors, but assume a simplified abstraction of the functionality of individual network devices, which may deviate from the real implementation. In this paper, we propose a technique for high-coverage testing of end-to-end network correctness using the real software that is deployed in these networks. Our design is effectively a hybrid, using an explicit-state model checker to explore all network-wide execution paths and event orderings, but executing real software as subroutines for each device. We show that this approach can detect correctness issues that would be missed both by existing verification and testing approaches, and a prototype implementation suggests the technique can scale to larger networks with reasonable performance. © 2018 Copyright held by the owner/author(s).},
author_keywords={Correctness;  Network verification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2018872,
author={Wang, J. and Dong, J. and Chen, S.},
title={Multi-parameters Inversion of Stress-Seepage-Damage Coupling Model Based on DEPSO Intelligent Algorithm [基于DEPSO混合智能算法的岩土体应力-渗流-损伤耦合模型多参数反演研究]},
journal={Yingyong Jichu yu Gongcheng Kexue Xuebao/Journal of Basic Science and Engineering},
year={2018},
volume={26},
number={4},
pages={872-887},
doi={10.16058/j.issn.1005-0930.2018.04.017},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054864348&doi=10.16058%2fj.issn.1005-0930.2018.04.017&partnerID=40&md5=c1d3f76f175ff2933882b9fbbeb23528},
affiliation={School of Architecture and Civil Engineering, Shenyang University of Technology, Shenyang, 110870, China; Key Laboratory of Disaster Prevention and Mitigation in Civil Engineering of Gansu Province, Lanzhou University of Technology, Lanzhou, 730050, China},
abstract={Research on the stress-seepage-damage parameter inversion problem of the rock is a great significance with the development of the complex geotechnical engineering problems.DEPSO hybrid intelligent algorithm is introduced into the stress-seepage-damage coupling parameters inversion based on differential evolution algorithm and particle swarm optimization.The hybrid intelligent method can make sure the two interact with each other coordinated development of population of balance between ability and the ability to explore,to maintain the diversity of the whole population and reduce the risk of fall into local optimum.Two classical test functions are used in view of the convergence and robustness of DEPSO.PSO,DE algorithm and the control parameters and the importance of the difference strategy of DEPSO algorithm are discussed in details.The coupling multi-parameters method is set up combined the stress-seepage-damage coupling model with DEPSO hybrid intelligent algorithm,and the coupling parameters back analysis program is compiled using C++ programming based on DEPSO.A hypothetical measured values are calculated based on the stress-seepage-damage coupling program to analysis multi-parameter inversion for the method and program validation.Research results show that the multi-mechanics parameters and penetration can be inversion by the coupling parameter back analysis method based on the established DEPSO algorithm at the same time,and the problem of multi-parameters inversion can be well solved in geotechnical engineering.In terms of computation efficiency is higher than the single intelligent algorithm.It is a new and efficient back analysis method which has the good inversion precision and robustness.To provide the help and basis for dynamic construction in complex environmental geotechnical engineering. © 2018, The Editorial Board of Journal of Basic Science and Engineering. All right reserved.},
author_keywords={Back analysis;  DEPSO hybrid intelligence algorithm;  Differential evolution algorithm;  Geotechnical engineering;  Stress-seepage-damage coupling model},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang2018318,
author={Jiang, S. and Chen, J. and Zhang, Y. and Qian, J. and Wang, R. and Xue, M.},
title={Evolutionary approach to generating test data for data flow test},
journal={IET Software},
year={2018},
volume={12},
number={4},
pages={318-323},
doi={10.1049/iet-sen.2018.5197},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051745108&doi=10.1049%2fiet-sen.2018.5197&partnerID=40&md5=da85088606460e3cae24965540a408ab},
affiliation={School of Computer Science and Technology, China University of Mining and Technology, Xuzhou, China; Guangxi Key Laboratory of Trusted Software, Guilin University of Electronic Technology, Guilin, China},
abstract={Software testing consumes a significant portion of software effort. Program entities such as branch or definition-use pairs (DUPs) are used in diverse software development tasks. In this study, the authors present a novel evolution-based approach to generating test data for all definition-use coverage. First, the subset of DUPs, which can ensure the coverage adequacy, is computed by a reduction algorithm for the whole DUPs. Then they apply a genetic algorithm to generate test data for the subset of DUPs. Furthermore, the fitness of an individual depends on the matching degree between the traversed path and the definition-clear path of each target DUP. They also investigate the coverage and the size of test cases of test data generation by applying the authors' approach on 15 widely used subject programs. The experimental results show that their approach can reduce the size of test cases that generated without affecting the coverage rate. © 2018 The Institution of Engineering and Technology.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kumar2018,
author={Kumar, V. and Singh, V.B. and Dhamija, A. and Srivastav, S.},
title={Cost-Reliability-Optimal Release Time of Software with Patching Considered},
journal={International Journal of Reliability, Quality and Safety Engineering},
year={2018},
volume={25},
number={4},
doi={10.1142/S0218539318500183},
art_number={1850018},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045308672&doi=10.1142%2fS0218539318500183&partnerID=40&md5=f72833230821d3966db13c3d57199c21},
affiliation={Department of Mathematics, Amity School of Engineering and Technology, New Delhi, 110061, India; Delhi College of Arts and Commerce, University of Delhi, Delhi, India; Department of Computer Science and Engineering, Amity School of Engineering and Technology, New Delhi, 110061, India},
abstract={Testing life cycle poses a problem of achieving a high level of software reliability while achieving an optimal release time for the software. To enhance the reliability of the software, retain the market potential for the software and reduce the testing cost, the enterprise needs to know when to release the software and when to stop testing. To achieve this, enterprises usually release their product earlier in market and then release patches subsequently. Software patching is a process through which enterprises debug, update, or enhance their software. Software patching when used as a debugging process ensures an optimal release for the product, increasing the reliability of the software while reducing the economic overhead of testing. Today, due to the diverse and distributed nature of software, its journey in the market is dynamic, making patching an inherent aspect of testing. A patch is a piece of software designed to update a computer program or its supporting data to fix or improve it. Researchers have worked in the field to minimize the testing cost, but so far, reliability has not been considered in the models for optimal time scheduling using patching. In this paper, we discuss reliability, which is a major attribute of the quality of software. Thus, to address the issues of testing cost, release time of software, and a desirable reliability level, we propose a reliability growth model implementing software patching to make the software system reliable and cost effective. The numeric illustration has been implemented using real-life software failure data set. © 2018 World Scientific Publishing Company.},
author_keywords={release time;  reliability;  software patch;  SRGM;  Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rahimi20182198,
author={Rahimi, M. and Cleland-Huang, J.},
title={Evolving software trace links between requirements and source code},
journal={Empirical Software Engineering},
year={2018},
volume={23},
number={4},
pages={2198-2231},
doi={10.1007/s10664-017-9561-x},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032801677&doi=10.1007%2fs10664-017-9561-x&partnerID=40&md5=ec9d4622a811c321c5101e4d294fbfbc},
affiliation={School of Computing, DePaul University, 243 S Wabash, Chicago, IL  60604, United States; University of Notre Dame, Notre Dame, IN  46556, United States},
abstract={Traceability provides support for diverse software engineering activities including safety analysis, compliance verification, test-case selection, and impact prediction. However, in practice, there is a tendency for trace links to degrade over time as the system continually evolves. This is especially true for links between source-code and upstream artifacts such as requirements – because developers frequently refactor and change code without updating the links. In this paper we present TLE (Trace Link Evolver), a solution for automating the evolution of bidirectional trace links between source code classes or methods and requirements. TLE depends on a set of heuristics coupled with refactoring detection tools and informational retrieval algorithms to detect predefined change scenarios that occur across contiguous versions of a software system. We first evaluate TLE at the class level in a controlled experiment to evolve trace links for revisions of two Java applications. Second, we comparatively evaluate several variants of TLE across six releases of our in-house Dronology project. We study the results of integrating human analyst feed back in the evolution cycle of this emerging project. Additionally, in this system, we compare the efficacy of class-level versus method-level evolution of trace links. Finally, we evaluate TLE in a larger scale across 27 releases of the Cassandra Database System and show that the evolved trace links are significantly more accurate than those generated using only information retrieval techniques. © 2017, Springer Science+Business Media, LLC.},
author_keywords={Evolution;  Maintenance;  Traceability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Palsberg2018100,
author={Palsberg, J. and Lopes, C.V.},
title={NJR: A normalized Java resource},
journal={Companion Proceedings for the ISSTA/ECOOP 2018 Workshops},
year={2018},
pages={100-106},
doi={10.1145/3236454.3236501},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060973293&doi=10.1145%2f3236454.3236501&partnerID=40&md5=71e57abd24ff8ccf3d2e35247d83035f},
affiliation={University of California, Los Angeles, United States; University of California, Irvine, United States},
abstract={We are on the cusp of a major opportunity: software tools that take advantage of Big Code. Specifically, Big Code will enable novel tools in areas such as security enhancers, bug finders, and code synthesizers. What do researchers need from Big Code to make progress on their tools? Our answer is an infrastructure that consists of 100,000 executable Java programs together with a set of working tools and an environment for building new tools. This Normalized Java Resource (NJR) will lower the barrier to implementation of new tools, speed up research, and ultimately help advance research frontiers. Researchers get significant advantages from using NJR. They can write scripts that base their new tool on NJR's already-working tools, and they can search NJR for programs with desired characteristics. They will receive the search result as a container that they can run either locally or on a cloud service. Additionally, they benefit from NJR's normalized representation of each Java program, which enables scalable running of tools on the entire collection. Finally, they will find that NJR's collection of programs is diverse because of our efforts to run clone detection and near-duplicate removal. In this paper we describe our vision for NJR and our current prototype. © 2018 ACM.},
author_keywords={000 Java programs;  100;  plug-And-play environment;  reproducible results;  software tools;  static and dynamic analyses},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tanno2018356,
author={Tanno, H. and Adachi, Y.},
title={Support for finding presentation failures by using computer vision techniques},
journal={Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2018},
year={2018},
pages={356-363},
doi={10.1109/ICSTW.2018.00073},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050960024&doi=10.1109%2fICSTW.2018.00073&partnerID=40&md5=f1172d5e91f2bf1f9d008ab2e2d8cdca},
affiliation={NTT Laboratories, Tokyo, Japan},
abstract={When testing applications that run on diverse client devices, operating systems, and browsers, it is necessary to check whether application screens display correctly in various environments and ensure that there is no presentation failure. Applications are typically composed of many screens, and there are many types and combinations of environments in which they may run. Accordingly, it is very labor intensive to visually confirm large numbers of such screens, and there is a strong possibility of missing presentation failures. Finding a way of efficiently confirming large numbers of application screen images is hence a challenge facing real-world testing and industry. Here, we propose a method to support discovery of presentation failures. It detects differences in screen elements, e.g., a disappearance of or change in the position of a button, by comparing the images of the correct screen and the target screen by utilizing computer vision techniques and displays those differences to the tester in an easy-to-understand way. As a result, the tester can quickly confirm the differences and find presentation failures efficiently. Our method uses only images of the application screen, therefore it does not depend on a specific implementation technology. Thus, it can be utilized for confirming the test results of various implementations (Web, android, iOS, etc.). In experiments with application screens in which presentation failures were intentionally embedded, we measured how much the proposed method increases failure discovery rate and determined whether it reduces the amount of labor. © 2018 IEEE.},
author_keywords={Image processing;  Presentation failure;  Test oracle;  Test result verification;  Visual GUI testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Strandberg201873,
author={Strandberg, P.E. and Ostrand, T.J. and Weyuker, E.J. and Sundmark, D. and Afzal, W.},
title={Automated test mapping and coverage for network topologies},
journal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2018},
pages={73-83},
doi={10.1145/3213846.3213859},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051515196&doi=10.1145%2f3213846.3213859&partnerID=40&md5=a0162ded3c55605186099f2616dd6643},
affiliation={Westermo RandD AB, Sweden; Mälardalen University, Sweden},
abstract={Communication devices such as routers and switches play a critical role in the reliable functioning of embedded system networks. Dozens of such devices may be part of an embedded system network, and they need to be tested in conjunction with various computational elements on actual hardware, in many different configurations that are representative of actual operating networks. An individual physical network topology can be used as the basis for a test system that can execute many test cases, by identifying the part of the physical network topology that corresponds to the configuration required by each individual test case. Given a set of available test systems and a large number of test cases, the problem is to determine for each test case, which of the test systems are suitable for executing the test case, and to provide the mapping that associates the test case elements (the logical network topology) with the appropriate elements of the test system (the physical network topology). We studied a real industrial environment where this problem was originally handled by a simple software procedure that was very slow in many cases, and also failed to provide thorough coverage of each network's elements. In this paper, we represent both the test systems and the test cases as graphs, and develop a new prototype algorithm that a) determines whether or not a test case can be mapped to a subgraph of the test system, b) rapidly finds mappings that do exist, and c) exercises diverse sets of network nodes when multiple mappings exist for the test case. The prototype has been implemented and applied to over 10,000 combinations of test cases and test systems, and reduced the computation time by a factor of more than 80 from the original procedure. In addition, relative to a meaningful measure of network topology coverage, the mappings achieved an increased level of thoroughness in exercising the elements of each test system. © 2018 Association for Computing Machinery.},
author_keywords={Network topology;  Subgraph isomorphism;  Test coverage;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Noller2018322,
author={Noller, Y. and Kersten, R. and Pǎsǎreanu, C.S.},
title={Badger: Complexity analysis with fuzzing and symbolic execution},
journal={ISSTA 2018 - Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2018},
pages={322-332},
doi={10.1145/3213846.3213868},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051511569&doi=10.1145%2f3213846.3213868&partnerID=40&md5=e1491c832cf1841404c8ca8298cd4c16},
affiliation={Humboldt University of Berlin, Berlin, Germany; Synopsys, Inc., San Francisco, United States; Carnegie Mellon University Silicon Valley, NASA Ames Research Center, Moffet-Field, United States},
abstract={Hybrid testing approaches that involve fuzz testing and symbolic execution have shown promising results in achieving high code coverage, uncovering subtle errors and vulnerabilities in a variety of software applications. In this paper we describe Badger-a new hybrid approach for complexity analysis, with the goal of discovering vulnerabilities which occur when the worst-case time or space complexity of an application is significantly higher than the average case. Badger uses fuzz testing to generate a diverse set of inputs that aim to increase not only coverage but also a resource-related cost associated with each path. Since fuzzing may fail to execute deep program paths due to its limited knowledge about the conditions that influence these paths, we complement the analysis with a symbolic execution, which is also customized to search for paths that increase the resource-related cost. Symbolic execution is particularly good at generating inputs that satisfy various program conditions but by itself suffers from path explosion. Therefore, Badger uses fuzzing and symbolic execution in tandem, to leverage their benefits and overcome their weaknesses. We implemented our approach for the analysis of Java programs, based on Kelinci and Symbolic PathFinder. We evaluated Badger on Java applications, showing that our approach is significantly faster in generating worst-case executions compared to fuzzing or symbolic execution on their own. © 2018 Association for Computing Machinery.},
author_keywords={Complexity Analysis;  Denial-of-Service;  Fuzzing;  Symbolic Execution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeOliveiraNeto2018149,
author={De Oliveira Neto, F.G. and Feldt, R. and Erlenhov, L. and Nunes, J.B.D.S.},
title={Visualizing Test Diversity to Support Test Optimisation},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2018},
volume={2018-December},
pages={149-158},
doi={10.1109/APSEC.2018.00029},
art_number={8719537},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066790411&doi=10.1109%2fAPSEC.2018.00029&partnerID=40&md5=98808a404dcf478a55800c515dc7fdfa},
affiliation={Dept. of Computer Science and Engineering Chalmers, University of Gothenburg, Gothenburg, Sweden; Department of Computing Systems Federal, University of Campina Grande Campina, Grande, Brazil},
abstract={Diversity has been used as an effective criteria to optimise test suites for cost-effective testing. Particularly, diversity-based (alternatively referred to as similarity-based) techniques have the benefit of being generic and applicable across different Systems Under Test (SUT), and have been used to automatically select or prioritise large sets of test cases. However, there is a challenge in how to present diversity information to developers and testers since results are typically many-dimensional. Furthermore, the generality of diversity-based approaches makes it harder to choose when and where to apply them. In this paper we address these challenges by investigating: i) what are the trade-offs in using different sources of diversity (e.g., diversity of test requirements or test scripts) to optimise large test suites, and ii) how visualisation of test diversity data can assist testers for test optimisation and improvement. We perform a case study on three industrial projects and present quantitative results on the fault detection capabilities and redundancy levels of different sets of test cases. Our key result is that test similarity maps, based on pair-wise diversity calculations, helped industrial practitioners identify issues with their test repositories and decide on actions to improve. We conclude that the visualisation of diversity information can assist testers in their maintenance and optimisation activities. © 2018 IEEE.},
author_keywords={Diversity;  Empirical Study;  Search based Software Testing;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeSouza20181443,
author={De Souza, E.F. and Le Goues, C. and Camilo-Junior, C.G.},
title={A novel fitness function for automated program repair based on source code checkpoints},
journal={GECCO 2018 - Proceedings of the 2018 Genetic and Evolutionary Computation Conference},
year={2018},
pages={1443-1450},
doi={10.1145/3205455.3205566},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050588008&doi=10.1145%2f3205455.3205566&partnerID=40&md5=f5d729f664b5f2a15abee33e79b6b931},
affiliation={Instituto de Informática, Universidade Federal de Goiás, Goiânia, Goiás, Brazil; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={Software maintenance, especially bug fixing, is one of the most expensive problems in software practice. Bugs have global impact in terms of cost and time, and they also reflect negatively on a company's brand. GenProg is a method for Automated Program Repair based on an evolutionary approach. It aims to generate bug repairs without human intervention or a need for special instrumentation or source code annotations. Its canonical fitness function evaluates each variant as the weighted sum of the test cases that a modified program passes. However, it evaluates distinct individuals with the same fitness score (plateaus). We propose a fitness function that minimizes these plateaus using dynamic analysis to increase the granularity of the fitness information that can be gleaned from test case execution, increasing the diversity of the population, the number of repairs found (expressiveness), and the efficiency of the search. We evaluate the proposed fitness functions on two standard benchmarks for Automated Program Repair: IntroClass and ManyBugs. We find that our proposed fitness function minimizes plateaus, increases expressiveness, and the efficiency of the search. © 2018 Association for Computing Machinery.},
author_keywords={Fitness function;  Genetic programming;  Program repair;  Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gong2018144,
author={Gong, L. and Li, Y. and Jin, C.},
title={Numerical simulation and verification on impact damage mechanical property of drift ice on diversion tunnel [流冰对引水隧洞撞击破坏力学特性数值分析与验证]},
journal={Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
year={2018},
volume={34},
number={13},
pages={144-151},
doi={10.11975/j.issn.1002-6819.2018.13.017},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055578986&doi=10.11975%2fj.issn.1002-6819.2018.13.017&partnerID=40&md5=691d3a6708ec9516cbb04084516ede24},
affiliation={School of Civil Engineering, Lanzhou Jiaotong University, Lanzhou, 730070, China},
abstract={In high latitude region of western China, the environment is in harsh condition that it is cold and dry in winter and has long ice period, which causes many water resource problems. In order to relieve the serious water shortage condition in cold and dry region, a large number of long distance water diversion projects were established to improve the water resource condition, such as increasing farm irrigation, human and animal drinking. While the ice damage occurs frequently under severe ice conditions in cold and dry region, especially in ice period in winter and thawing period in spring, it is easy to form drift ice with different velocities, different plan sizes and different thicknesses, which produces different extrusion forces or impact forces to damage tunnel lining, causing project failure. The failure project could not realize the original planning and construction goal, giving rise to the water allocation pressure. The water allocation would cause water shortage which influences diversion irrigation and farming production in spring. Based on the intense researches on the collision simulation problem of the interaction between drift ice and diversion tunnel, this paper used the symmetric penalty function in the finite element contact-impact algorithm to conduct the theoretical study on collision simulation problem between drift ice and water diversion tunnel. ANSYS/LS-DYNA was adopted as the platform to establish tunnel model and drift ice model. LS-DYNA SOLVER was used as the solver to solve and analyze the damage degrees of drift ice on tunnel. The physical model tests were conducted to verify and reveal the impact damage mechanism of drift ice on diversion tunnel. The physical model was constructed by the geometric scale of 28, which is the ratio of the experiment facility to the prototype in the test. The results show that tunnel lining surface will form varying degrees of deformation and failure when the tunnel lining is impacted by the drift ice with different velocities, different plane sizes and different thicknesses. It is also discovered that the impact stress increases with the flow velocity and their relationship presents linear variation. The impact stress also increases with the drift ice's plane size and their relationship presents nonlinear variation. The impact stress increases with the drift ice thickness when the drift ice thickness is less than 0.5 m. While the drift ice thickness is greater than 0.5 m, the maximum stress value shows little change. The relationship between drift ice's plane size and maximum stress shows approximately linear variation. Meanwhile, the software simulation and test observation results are almost the same. The impact of drift ice on the tunnel lining would cause the deformation of lining, but the deformation has little influence on the tunnel stability. The drift ice's long time erosion would cause the tunnel lining surface to fall off, and further break the strength and stability of the tunnel structures. The study supplies theoretical support and technical guarantee for water diversion project security in cold and dry region of western China. © 2018, Editorial Department of the Transactions of the Chinese Society of Agricultural Engineering. All right reserved.},
author_keywords={Diversion tunnel;  Ice engineering;  Impact force;  Mechanical performance;  Models;  Numerical simulation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Böhme2018,
author={Böhme, M.},
title={STADS: Software testing as species discovery},
journal={ACM Transactions on Software Engineering and Methodology},
year={2018},
volume={27},
number={2},
doi={10.1145/3210309},
art_number={7},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053872377&doi=10.1145%2f3210309&partnerID=40&md5=287e999c5e908a5cf8a0a44ada688e90},
affiliation={National University of Singapore, Rm 131, 25 Exhibition Walk, Clayton, VIC  3800, Singapore; Monash University, Australia},
abstract={A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (1) to estimate the total number of feasible program branches, given that only a fraction has been covered so far; (2) to estimate the additional time required to cover 10% more branches (or to estimate the coverage achieved in one more day, respectively); or (3) to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability does not mean that none exists-even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees. In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimations (1) of the total number of species, (2) of the additional sampling effort required to discover 10% more species, or (3) of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias-AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy. © 2018 ACM.},
author_keywords={Code coverage;  Discovery probability;  Extrapolation;  Fuzzing;  Measure of confidence;  Measure of progress;  Reliability;  Security;  Species coverage;  Statistical guarantees;  Stopping rule},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mokhtari201892,
author={Mokhtari, N.-A. and Ghezavati, V.},
title={Integration of efficient multi-objective ant-colony and a heuristic method to solve a novel multi-objective mixed load school bus routing model},
journal={Applied Soft Computing Journal},
year={2018},
volume={68},
pages={92-109},
doi={10.1016/j.asoc.2018.03.049},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044931181&doi=10.1016%2fj.asoc.2018.03.049&partnerID=40&md5=979043e069013ac17995e592fe9afa10},
affiliation={School of Industrial Engineering, Islamic Azad University, South Tehran Branch, Tehran, Iran},
abstract={In this paper, a novel mixed-load school bus routing problem (MLSBRP) is introduced. MLSBRP assumes that several students from different schools can simultaneously take a ride on the same bus. A bi-objective mixed integer linear programming (BO-MILP) formulation is proposed to model MLSBRP. The objectives are: a) minimizing the number of the buses; and b) minimizing the average riding time of the students. A hybrid multi-objective ant colony optimization (h-MOACO) algorithm, incorporating a novel routing heuristic algorithm, is developed to solve the associated BO-MILP. Performance of the proposed h-MOACO is compared with those achieved by commercial operation research software called CPLEX, and a customized NSGAII algorithm through multi-objective diversity and accuracy metrics over several small-size and large-size test problems, respectively. Sensitivity analysis are conducted on the main parameters of the MLSBRP. The computational results indicate the capability of the new proposed MLSBRP and suitability of the proposed h-MOACO algorithm. © 2018 Elsevier B.V.},
author_keywords={Heuristics;  Hybrid multi-objective ant colony optimization algorithm;  Mixed load;  NSGAII;  School bus routing problem},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Stratis2018,
author={Stratis, P. and Brown, G.},
title={Assessing the effect of device-based test scheduling on heterogeneous test suite execution},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3210459.3210481},
art_number={3210481},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053684993&doi=10.1145%2f3210459.3210481&partnerID=40&md5=e24378001bcab6f896bda2da4e253404},
affiliation={University of Edinburgh - Codeplay Software, United Kingdom; Codeplay Software, United Kingdom},
abstract={As software takes on more responsibility, it gets increasingly complex, requiring an extremely large number of tests for effective validation. This problem is more serious in heterogeneous system testing, where an application is required to be tested against a diverse collection of processors before is accepted for release. Test scheduling methodologies are concerned with scheduling the execution of tests in an order which enhances performance. In this paper we assess the effect of device-based test scheduling on the execution time of heterogeneous test suites. We utilize a test scheduling algorithm which improves load balance between the multiple devices of a heterogeneous system during test suite execution in an attempt to reduce its execution time. We conduct an empirical evaluation on a large-scaled test suite targeting implementations of the SYCL standard which has been developed by Codeplay Software. We found that a maximum of 25.42% speed-up is achieved by our test scheduling algorithm when compared to parallel test execution without test scheduling. © 2018 Association for Computing Machinery.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hall20181,
author={Hall, H. and Donitz, B. and Kim, L. and Srivastava, D. and Albee, K. and Eisner, S. and Pierce, D. and Villapudua, Y. and Stoica, A.},
title={Project Zephyrus: Developing a rapidly reusable high-altitude flight test platform},
journal={IEEE Aerospace Conference Proceedings},
year={2018},
volume={2018-March},
pages={1-17},
doi={10.1109/AERO.2018.8396809},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049863724&doi=10.1109%2fAERO.2018.8396809&partnerID=40&md5=9ef3e25a56e091b033a95aa40a0455b6},
affiliation={University of California, Berkeley, Department of Physics, United States; University of Michigan, Ann Arbor, Department of Aerospace Engineering, United States; Columbia University, Department of Mechanical Engineering, United States; Rutgers University, Department of Mechanical Engineering, United States; Massachusetts Institute of Technology, Department of Aerospace Engineering, United States; University of California, Los Angeles, Department of Computer Science, United States; University of California, Irvine, Department of Mechanical Engineering, United States; NASA Jet Propulsion Laboratory, 347E: Robotics and Mobility Systems Section, United States},
abstract={Reusable, inexpensive, and rapid testing of payloads in near-space conditions is an unfilled niche that could speed development of flight-ready projects. With growing availability of commercial off-the-shelf (COTS) hardware and open source software, high-altitude ballooning (HAB) has become a viable option for rapid prototyping of ideas in near-space conditions. A product of the Jet Propulsions Laboratory's (JPL) collaborative Innovation to Flight (i2F) program, the Zephyrus system is a proposed HAB solution for high-altitude testing and atmospheric observation, a cheaper and easy-to-use alternative to traditional HAB systems. The Zephyrus system permits testing of small-to mid-sized instruments in the upper atmosphere (approximately 35,000 m maximum altitude) using a reusable common architecture. Over the summer of 2017, the i2F team demonstrated all of these key features of the Zephyrus platform via four test flights. Zephyrus I, II, III, and IV were successfully launched, tracked, and recovered over groundtracks of up to approximately 50 km and altitudes of approximately 30,000 m by a team averaging eight individuals, while accomplishing a majority of the intended science. This included a turnaround time of less than two weeks between two flight tests with differing instrumentation. Lessons from the Zephyrus I flight were successfully used in the Zephyrus II flight to speed up launch time and operations procedures (e.g. rigorous pre-launch preparation). Likewise, the Zephyrus III flight brought the cost of relaunching a mid-sized payload to 28,500 m down significantly. Many diverse instruments and experiments were tested during the Zephyrus missions (e.g. inflatable origami reflectors, flexible solar panels), demonstrating the system's versatility. From concept to first flight of Zephyrus I in just five weeks, followed by turnarounds as short as two weeks, the team demonstrated that it is entirely possible to bring to bear cost-effective, hobbyist HAB techniques in meaningful high-altitude testing using a new reusable test platform. © 2018 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jusmawati2018113,
author={Jusmawati and Arsunan Arsin, A. and Naiem, F.},
title={Risk factors of the occurance of decompression sickness among fishermen community to traditional divers of Saponda Island Konawe regency southeast Sulawesi Province in 2016},
journal={ACM International Conference Proceeding Series},
year={2018},
pages={113-116},
doi={10.1145/3242789.3242820},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055499870&doi=10.1145%2f3242789.3242820&partnerID=40&md5=dff72645358e003a5a2d9928def1963e},
affiliation={Faculty of Public Health, Hasanuddin University, Jl. Perintis Kemerdekaan Km. 10, Kota Makassar, Sulawesi Selatan, 90245, Indonesia},
abstract={The aim of this study to examine risk factors of decompression sickness among traditional fishermen community in Saponda Island of Konawe Regency of Southeast Sulawesi Province.The study was used the observation analysis with case control study. The location for this study was in Saponda island of Konawe regency of Sulawesi Province. There were 174 samples which divided on two group: intervention and control group. Both groups had 87 samples for each group where intervention group included all traditional fishermen experienced decompression sickness (DCS) that diagnosed by the doctor, while control group was traditional fishermen who did not suffered DCS based on the doctor's diagnosis. The fishermen were selected used purposive sampling method. The data were analyzed by using SPSS computer program with odds ratio test and multiple logistic regression test. There were three variables that significant with decompression sick ness incidence such as diving depth (OR=46.3 CI:2.99-715.72), disease background (OR=20.3, CI= 8.43-48.7), diving duration (OR=19.6, CI= 3.74-103.24) and high risk factor was diving depth. © 2018 Copyright is held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Age;  Decompression sickness;  Depth;  Disease history;  Duration;  Frequency},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Renzullo20181,
author={Renzullo, J. and Weimer, W. and Moses, M. and Forrest, S.},
title={Neutrality and epistasis in program space},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={1-8},
doi={10.1145/3194810.3194812},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054619081&doi=10.1145%2f3194810.3194812&partnerID=40&md5=96dfb9a447447df9dabedbdbada544ac},
affiliation={Arizona State University, Tempe, AZ, United States; University of Michigan, Ann Arbor, MI, United States; University of New Mexico, Albuquerque, NM, United States; Santa Fe Institute, United States},
abstract={Neutral networks in biology often contain diverse solutions with equal fitness, which can be useful when environments (requirements) change over time. In this paper, we present a method for studying neutral networks in software. In these networks, we find multiple solutions to held-out test cases (latent bugs), suggesting that neutral software networks also exhibit relevant diversity. We also observe instances of positive epistasis between random mutations, i.e. interactions that collectively increase fitness. Positive epistasis is rare as a fraction of the total search space but significant as a fraction of the objective space: 9% of the repairs we found to look (and 4.63% across all programs analyzed) were produced by positive interactions between mutations. Further, the majority (62.50%) of unique repairs are instances of positive epistasis. © 2018 ACM.},
author_keywords={automated software engineering;  biological networks;  network science;  software evolution;  software testing and debugging},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Qi20181284,
author={Qi, R.-Z. and Wang, Z.-J. and Huang, Y.-H. and Li, S.-Y.},
title={Generating Combinatorial Test Suite with Spark Based Parallel Approach [基于Spark的并行化组合测试用例集生成方法]},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2018},
volume={41},
number={6},
pages={1284-1299},
doi={10.11897/SP.J.1016.2018.01284},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054680397&doi=10.11897%2fSP.J.1016.2018.01284&partnerID=40&md5=da391bbc458c9aebb2f98cd6149aa5fd},
affiliation={College of Computer and Information, Hohai University, Nanjing, 211106, China; National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210046, China; College of Science, Hohai University, Nanjing, 211106, China},
abstract={The normal operation of computer system is influenced by many factors. Failures may be triggered by various factors and interactions of these factors in software. In order to detect these interaction failures, the researchers try to design suitable test cases. If the quantity of factors is larger and the values of these factors are more complex, the number of test cases may become very huge. How to design a small number of test cases, which can achieve full coverage of combinations of factor values of the software, is a key issue of the research of test case generation. Combinatorial testing can generate small test suite from large scale combinatorial space of the software under test. The generated test suite can cover combinations of factor values of the software fully. It has been proven that generating minimum test suite is an NP-complete problem, so, heuristic search algorithms have been used for generating smaller test suite by many researchers in recent years. Heuristic search algorithms formulate combinatorial test generation problem as a search problem, and apply metaheuristic algorithms to generate combinatorial test suite. Heuristic search algorithms can often produce smaller test suite than other approaches, but require a longer computation. To solve this problem, in this paper, we propose a Spark based parallel genetic algorithm based on the island model. This algorithm uses Hadoop distributed file system to implement the method of exchanging information among the running nodes of Spark, and implement the migration of individuals among various subpopulations. The Spark resilient distributed dataset is first generated from initial population. Then the resilient distributed dataset is split into several subpopulations. These subpopulations are then distributed into the nodes of the Spark cluster. Each subpopulation calculates the fitness function on its own node, and evolves separately. During the process of evolution, some individuals will be selected according to the migration strategy every other generation. Then these selected individuals will migrate among various subpopulations on the nodes of the Spark cluster. The migration of individuals improves the diversity of the whole population, and improves the effectiveness and performance on searching the optimal results. Finally, the algorithm returns the optimal combinatorial test suite that satisfies the desired coverage criterion. The Spark based parallel genetic algorithm is an effective attempt in applying massive parallelization to combinatorial test suite generation. In the experimental section, parameter tuning is first conducted systematically to find recommended settings for combinatorial test suite generation. Then the proposed parallel genetic algorithm is evaluated against the sequential genetic algorithm and the isolated running genetic algorithm using 20 benchmarks. Experiment results show that the proposed parallel genetic algorithm outperforms the sequential genetic algorithm and the isolated running genetic algorithm in both generated test suite sizes and computational effort. It can achieve about 4× to 30× speedup than the sequential genetic algorithm, and about 2× to 3× than the isolated running genetic algorithm for the selected benchmarks. The proposed parallel genetic algorithm also has significant advantages on the sizes of generated test suite when compared with other existing approaches. © 2018, Science Press. All right reserved.},
author_keywords={Combinatorial testing;  Island model;  Parallel genetic algorithm;  Spark;  Test suite generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sugave2018271,
author={Sugave, S.R. and Patil, S.H. and Eswara Reddy, B.},
title={DIV-TBAT algorithm for test suite reduction in software testing},
journal={IET Software},
year={2018},
volume={12},
number={3},
pages={271-279},
doi={10.1049/iet-sen.2017.0130},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048078228&doi=10.1049%2fiet-sen.2017.0130&partnerID=40&md5=19a4f259e9c8dbb5efb080f255dfd1ee},
affiliation={JNTUA, Ananthapuramu, Andhra Pradesh, India; Bharati Vidyapeeth University College of Engineering, Pune, Maharashtra, India; JNTUA College of Engineering, Kalikiri, Chittor District, Andhra Pradesh, India},
abstract={Researchers have investigated different approaches to maintain the minimum cost and effort in regression testing. Here, test suite reduction is a common technique to decrease the cost of regression testing by removing the redundant test cases from the test suite and then, obtaining a representative set of test cases that still yield a high level of code coverage. Accordingly, here, the authors have developed two various techniques for test suite reduction. In the first technique, ATAP measure is newly developed to find the reduced test suite with the help of greedy search algorithm. In the second technique, DIV-TBAT (DIVersity-based BAT) algorithm is newly devised based on the mechanisms of Boolean logic within BAT algorithm which improve diversity during the search process. The proposed techniques are experimented using eight programs from SIR subject programs and the performance study is conducted using nine different evaluation metrics based on different research questions. The comparative analysis is performed with the existing algorithms like GreedyRatio, GreedyEIrreplaceability, diversity-based genetic algorithm, TBAT, and TAP, to prove the performance improvement over the eight software programs considered. © The Institution of Engineering and Technology 2018.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2018378,
author={Wang, S. and Ali, S. and Yue, T. and Liaaen, M.},
title={Integrating Weight Assignment Strategies With NSGA-II for Supporting User Preference Multiobjective Optimization},
journal={IEEE Transactions on Evolutionary Computation},
year={2018},
volume={22},
number={3},
pages={378-393},
doi={10.1109/TEVC.2017.2778560},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036514630&doi=10.1109%2fTEVC.2017.2778560&partnerID=40&md5=915245d83c6b428b1077ac1262bd2ec5},
affiliation={Software Engineering Department, Simula Research Laboratory, Fornebu, Norway; Software Quality Assurance Group, Cisco Systems, Lysaker, Norway},
abstract={Driven by the needs of several industrial projects on the applications of multiobjective search algorithms, we observed that user preferences must be properly incorporated into optimization objectives. However, existing algorithms usually treat all the objectives with equal priorities and do not provide a mechanism to reflect user preferences. To address this, we propose an extension - user-preference multiobjective optimization algorithm (UPMOA), to the most commonly applied, nondominated sorting genetic algorithm II by introducing a user preference indicator {\wp } , based on existing weight assignment strategies [e.g., uniformly distributed weights (UDW)]. We empirically evaluated UPMOA using four industrial problems from three diverse domains (i.e., communication, maritime, and subsea oil and gas). We also performed a sensitivity analysis for UPMOA with 625 algorithm parameter settings. To further assess the performance and scalability, 103 500 artificial problems were created and evaluated representing 207 sets of user preferences. Results show that the UDW strategy with UPMOA achieves the best performance and UPMOA significantly outperformed other three multiobjective search algorithms, and has the ability to solve problems with a wide range of complexity. We also observed that different parameter settings led to the varied performance of UPMOA, thus suggesting that configuring proper parameters is highly problem-specific. © 1997-2012 IEEE.},
author_keywords={Multiobjective optimization;  Search algorithms;  User preference;  Weight assignment strategies},
document_type={Article},
source={Scopus},
}

@ARTICLE{Petke2018415,
author={Petke, J. and Haraldsson, S.O. and Harman, M. and Langdon, W.B. and White, D.R. and Woodward, J.R.},
title={Genetic Improvement of Software: A Comprehensive Survey},
journal={IEEE Transactions on Evolutionary Computation},
year={2018},
volume={22},
number={3},
pages={415-432},
doi={10.1109/TEVC.2017.2693219},
note={cited By 87},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026851053&doi=10.1109%2fTEVC.2017.2693219&partnerID=40&md5=cc2ec8d4506e62ebdf30433caef7ed23},
affiliation={University College London, London, WC1E 6BT, United Kingdom; University of Stirling, Stirling, FK94LA, United Kingdom},
abstract={Genetic improvement (GI) uses automated search to find improved versions of existing software. We present a comprehensive survey of this nascent field of research with a focus on the core papers in the area published between 1995 and 2015. We identified core publications including empirical studies, 96% of which use evolutionary algorithms (genetic programming in particular). Although we can trace the foundations of GI back to the origins of computer science itself, our analysis reveals a significant upsurge in activity since 2012. GI has resulted in dramatic performance improvements for a diverse set of properties such as execution time, energy and memory consumption, as well as results for fixing and extending existing system functionality. Moreover, we present examples of research work that lies on the boundary between GI and other areas, such as program transformation, approximate computing, and software repair, with the intention of encouraging further exchange of ideas between researchers in these fields. © 1997-2012 IEEE.},
author_keywords={Genetic improvement (GI);  survey},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ros201835,
author={Ros, R. and Runeson, P.},
title={Continuous experimentation and A/B testing: A mapping study},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
pages={35-41},
doi={10.1145/3194760.3194766},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051206831&doi=10.1145%2f3194760.3194766&partnerID=40&md5=3cf3d30bf9a6c6e8713e741ca34594bc},
affiliation={Lund University, Sweden},
abstract={Background. Continuous experimentation (CE) has recently emerged as an established industry practice and as a research subject. Our aim is to study the application of CE and A/B testing in various industrial contexts. Objective. We wanted to investigate whether CE is used in different sectors of industry, by how it is reported in academic studies. We also wanted to explore the main topics researched to give an overview of the subject and discuss future research directions. Method. We performed a systematic mapping study of the published literature and included 62 papers, using a combination of database search and snowballing. Results. Most reported software experiments are done online and with software delivered as a service, although varied exemptions exist for e.g., financial software and games. The most frequently researched topics are challenges to conduct experiments and statistical methods for software experiments. Conclusions. The software engineering research on CE is still in its infancy. There are future research opportunities in evaluation research of technical topics and investigations of ethical experimentation. We conclude that the included studies show that A/B testing is applicable to a diversity of software and organisations. © 2018 ACM.},
author_keywords={A/B testing;  continuous experimentation;  mapping study},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2018,
title={Proceedings - International Conference on Software Engineering},
journal={Proceedings - International Conference on Software Engineering},
year={2018},
volume={Part F137812},
page_count={60},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051173471&partnerID=40&md5=1a940e1764a6c770754efc5a83664927},
abstract={The proceedings contain 12 papers. The topics discussed include: computer science identity and sense of belonging: a case study in Ireland; issues in gender diversity and equality in the UK; gender bias in artificial intelligence: the need for diversity and gender theory in machine learning; software testing conferences and women; gender in open source software: what the tools tell; observations of computing students on the homogeneity of their classrooms; digital girls program - disseminating computer science to girls in Brazil; gender equality in software engineering; perceptions on diversity in Brazilian agile software development teams: a survey; and improving diversity in computing research: an overview of CRA-W activities.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Durieux2018139,
author={Durieux, T. and Hamadi, Y. and Yu, Z. and Baudry, B. and Monperrus, M.},
title={Exhaustive Exploration of the Failure-Oblivious Computing Search Space},
journal={Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018},
year={2018},
pages={139-149},
doi={10.1109/ICST.2018.00023},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048441080&doi=10.1109%2fICST.2018.00023&partnerID=40&md5=d675525025e8897d16a50794278d044a},
affiliation={University of Lille and Inria, France; Ecole Polytechnique, France; Royal Institute of Technology, Stockholm, Sweden},
abstract={High-availability of software systems requires automated handling of crashes in presence of errors. Failure-oblivious computing is one technique that aims to achieve high availability. We note that failure-obliviousness has not been studied in depth yet, and there is very few study that helps understand why failure-oblivious techniques work. In order to make failure-oblivious computing to have an impact in practice, we need to deeply understand failure-oblivious behaviors in software. In this paper, we study, design and perform an experiment that analyzes the size and the diversity of the failure-oblivious behaviors. Our experiment consists of exhaustively computing the search space of 16 field failures of large-scale open-source Java software. The outcome of this experiment is a much better understanding of what really happens when failure-oblivious computing is used, and this opens new promising research directions. © 2018 IEEE.},
author_keywords={failure oblivious;  repair search space;  runtime repair},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ryou2018182,
author={Ryou, Y. and Ryu, S.},
title={Automatic Detection of Visibility Faults by Layout Changes in HTML5 Web Pages},
journal={Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018},
year={2018},
pages={182-192},
doi={10.1109/ICST.2018.00027},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048436434&doi=10.1109%2fICST.2018.00027&partnerID=40&md5=6a3e9fa072a01725860b66648a4d4cb3},
affiliation={School of Computing, KAIST, South Korea},
abstract={Modern HTML5 web pages (pages) often change various elements of their documents dynamically to provide rich functionality to users interactively. As users interact with a document via events, the combination of HTML, CSS, and JavaScript dynamically changes the document layout, which is the arrangement of the document elements visualized to the users. Web pages change their layouts not only to support user interaction but also to react to different screen sizes being used to run the pages. To support diverse devices with different screen sizes using a single web page document, developers use Responsive Web Design, which enables web page layouts to change when the sizes of the underlying devices change. While such dynamic features of web pages provide powerful experiences to users, they also make development of web pages more difficult. Even expert developers find it difficult to write HTML5 web pages correctly. In this paper, we first define the problem that functionalities of HTML5 web pages may become unusable due to layout changes, and propose a technique to detect the problem automatically. We show that our implementation detects such problems in real-world HTML5 web pages. © 2018 IEEE.},
author_keywords={automatic bug detection;  HTML5 web pages;  layout changes;  visibility faults},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim2018239,
author={Kim, Y. and Hong, S. and Ko, B. and Phan, D.L. and Kim, M.},
title={Invasive Software Testing: Mutating Target Programs to Diversify Test Exploration for High Test Coverage},
journal={Proceedings - 2018 IEEE 11th International Conference on Software Testing, Verification and Validation, ICST 2018},
year={2018},
pages={239-249},
doi={10.1109/ICST.2018.00032},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048388101&doi=10.1109%2fICST.2018.00032&partnerID=40&md5=2f23af0901c4dffcc4c3319cbad94d0a},
affiliation={School of Computing, KAIST, South Korea; School of CSEE, Handong Global University, South Korea},
abstract={Software testing techniques have advanced significantly over several decades; however, most of current techniques still test a target program as it is, and fail to utilize valuable information of diverse test executions on many variants of the original program in test generation. This paper proposes a new direction for software testing-Invasive Software Testing (IST). IST first generates a set of target program variants m1; :::;mn from an original target program p by applying mutation operations 1; :::; n. Second, given a test suite T, IST executes m1; :::;mn with T and records the test runs which increase test coverage compared to p with T. Based on the recorded information, IST generates guideposts for automated test generation on p toward high test coverage. Finally, IST generates test inputs on p with the guideposts and achieves higher test coverage. We developed DEMINER which implements IST for C programs through software mutation and concolic testing. Further, we showed the effectiveness of DEMINER on three real-world target programs Busybox-ls, Busybox-printf, and GNU-find. The experiment results show that the amount of the improved branch coverage by DEMINER is 24.7% relatively larger than those of the conventional concolic testing techniques on average. © 2018 IEEE.},
author_keywords={C programs;  Mutation analysis;  Practical mutation tool},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sunarmani2018,
author={Sunarmani, S. and Setyadjit, S. and Ermi, S.},
title={Optimization of Steamed Meals Based on Composite Flour (Taro, Banana, Green Bean) and Its Predicted Shelf Life},
journal={IOP Conference Series: Earth and Environmental Science},
year={2018},
volume={147},
number={1},
doi={10.1088/1755-1315/147/1/012012},
art_number={012012},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048410191&doi=10.1088%2f1755-1315%2f147%2f1%2f012012&partnerID=40&md5=a0ae9fceb8cb954720dbf92c7c9ca5e5},
affiliation={Indonesian Center for Agriculture Postharvest Research and Development, Indonesian Agency for Agriculture Research and Development, Ministry of Agriculture, Indonesia},
abstract={Ongol-ongol is for food diversification by mixing composite flour of taro, banana and mung bean, then was steamed by hot air. The purpose of this study was to find out the optimum way to produce 'ongol-ongol' from composite flour and to know the storage life by prediction method. The research consisted of two stages, namely the determination of the optimum formula of 'ongol-ongol' with Design Expert DX 8.1.6 software and the estimation of product shelf life of the optimum formula by ASLT (Accelerated Shelf Life Test) method. The optimum formula of the steamed meal was produced from composite flour and arenga flour with ratio of 50: 50 and flour to water ratio of 1: 1. The proximate content of steamed meal of optimum formula is 36.53% moisture content, ash content of 1,36%, fat content of 14.48%, protein level of 28.5%, and carbohydrate of 44.77% (w/w). Energy Value obtained from 100 g of 'ongol-ongol' was 320.8 Kcal. Recommended for steamed meal storage life is 12.54 days at ambient temperature. © Published under licence by IOP Publishing Ltd.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nelson20186,
author={Nelson, R.},
title={Key trends include migration, modularity, interoperability, and long-term support},
journal={EE: Evaluation Engineering},
year={2018},
volume={57},
number={5},
pages={6-9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046409908&partnerID=40&md5=bead51b73802232788e3a14bacd2c51d},
abstract={Military and aerospace applications present significant test challenges, which companies are addressing with products including a wide variety of software tools, instruments, modules, and systems. The specific application areas are diverse, ranging from shipboard power to rocket telemetry and from spectrum management to smart weapons. The performance and capabilities of today's FPGA devices are key enablers for instrument suppliers and test system users alike. Bob Stasonis, director of sales and marketing at Pickering Interfaces, sees various trends, including VXI to PXI migration, with users updating old VXI systems to new PXI versions with the same functionality, with long-term support.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Byrd2018255,
author={Byrd, K.B. and Ballanti, L. and Thomas, N. and Nguyen, D. and Holmquist, J.R. and Simard, M. and Windham-Myers, L.},
title={A remote sensing-based model of tidal marsh aboveground carbon stocks for the conterminous United States},
journal={ISPRS Journal of Photogrammetry and Remote Sensing},
year={2018},
volume={139},
pages={255-271},
doi={10.1016/j.isprsjprs.2018.03.019},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044580018&doi=10.1016%2fj.isprsjprs.2018.03.019&partnerID=40&md5=36f9a1cc1a1ab8ddbcc0e984bf7926c6},
affiliation={U.S. Geological Survey, Western Geographic Science Center, 345 Middlefield Road, MS-531, Menlo Park, CA, United States; Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Dr., 300-319D, Pasadena, CA  91109, United States; Smithsonian Environmental Research Center, 647 Contees Wharf Rd, Edgewater, MD  21037, United States; U.S. Geological Survey, National Research Program, 345 Middlefield Road, MS-480, Menlo Park, CA, United States},
abstract={Remote sensing based maps of tidal marshes, both of their extents and carbon stocks, have the potential to play a key role in conducting greenhouse gas inventories and implementing climate mitigation policies. Our objective was to generate a single remote sensing model of tidal marsh aboveground biomass and carbon that represents nationally diverse tidal marshes within the conterminous United States (CONUS). We developed the first calibration-grade, national-scale dataset of aboveground tidal marsh biomass, species composition, and aboveground plant carbon content (%C) from six CONUS regions: Cape Cod, MA, Chesapeake Bay, MD, Everglades, FL, Mississippi Delta, LA, San Francisco Bay, CA, and Puget Sound, WA. Using the random forest machine learning algorithm, we tested whether imagery from multiple sensors, Sentinel-1 C-band synthetic aperture radar, Landsat, and the National Agriculture Imagery Program (NAIP), can improve model performance. The final model, driven by six Landsat vegetation indices and with the soil adjusted vegetation index as the most important (n = 409, RMSE = 310 g/m2, 10.3% normalized RMSE), successfully predicted biomass for a range of marsh plant functional types defined by height, leaf angle and growth form. Model results were improved by scaling field-measured biomass calibration data by NAIP-derived 30 m fraction green vegetation. With a mean plant carbon content of 44.1% (n = 1384, 95% C.I. = 43.99%–44.37%), we generated regional 30 m aboveground carbon density maps for estuarine and palustrine emergent tidal marshes as indicated by a modified NOAA Coastal Change Analysis Program map. We applied a multivariate delta method to calculate uncertainties in regional carbon densities and stocks that considered standard error in map area, mean biomass and mean %C. Louisiana palustrine emergent marshes had the highest C density (2.67 ± 0.004 Mg/ha) of all regions, while San Francisco Bay brackish/saline marshes had the highest C density of all estuarine emergent marshes (2.03 ± 0.004 Mg/ha). Estimated C stocks for predefined jurisdictional areas ranged from 1023 ± 39 Mg in the Nisqually National Wildlife Refuge in Washington to 507,761 ± 14,822 Mg in the Terrebonne and St. Mary Parishes in Louisiana. This modeling and data synthesis effort will allow for aboveground C stocks in tidal marshes to be included in the coastal wetland section of the U.S. National Greenhouse Gas Inventory. With the increased availability of free post-processed satellite data, we provide a tractable means of modeling tidal marsh aboveground biomass and carbon at the global extent as well. © 2018},
author_keywords={Aboveground carbon stocks;  C-band synthetic aperture radar;  Multispectral imagery;  National greenhouse gas inventory;  Plant functional type;  Tidal marsh biomass},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Portillo-Dominguez2018130,
author={Portillo-Dominguez, A.O. and Ayala-Rivera, V.},
title={Improving the testing of clustered systems through the effective usage of Java benchmarks},
journal={Proceedings - 2017 5th International Conference in Software Engineering Research and Innovation, CONISOFT 2017},
year={2018},
volume={2018-January},
pages={130-139},
doi={10.1109/CONISOFT.2017.00023},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050966550&doi=10.1109%2fCONISOFT.2017.00023&partnerID=40&md5=01aec6e7b898c1e42c034b6986cd2d3f},
affiliation={Lero UCD, School of Computer Science, University College Dublin, Dublin, Ireland},
abstract={Nowadays, cluster computing has become a cost-effective and powerful solution for enterprise-level applications. Nevertheless, the usage of this architecture model also increases the complexity of the applications, complicating all activities related to performance optimisation. Thus, many research works have pursued to develop advancements for improving the performance of clusters. Comprehensively evaluating such advancements is key to understand the conditions under which they can be more useful. However, the creation of an appropriate test environment, that is, one which offers different application behaviours (so that the obtained conclusions can be better generalised) is typically an effort-intensive task. To help tackle this problem, this paper presents a tool that helps to decrease the effort and expertise needed to build useful test environments to perform more robust cluster testing. This is achieved by enabling the effective usage of Java Benchmarks to easily create clustered test environments; hence, diversifying the application behaviours that can be evaluated. We also present the results of a practical validation of the proposed tool, where it has been successfully applied to the evaluation of two cluster-related advancements. Such results demonstrate the benefits that our tool can bring to the evaluation of cluster-related advancements. © 2017 IEEE.},
author_keywords={Clusters;  Evaluation;  Java;  Performance;  Software-Testing},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Favre2018192,
author={Favre, L.},
title={A framework for modernizing non-mobile software: A model-driven engineering approach},
journal={Protocols and Applications for the Industrial Internet of Things},
year={2018},
pages={192-224},
doi={10.4018/978-1-5225-3805-9.ch007},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049491578&doi=10.4018%2f978-1-5225-3805-9.ch007&partnerID=40&md5=d76c4617c79e9d45d15044a320959865},
affiliation={National University of Central Buenos Aires, Argentina},
abstract={New paradigms such as pervasive computing, cloud computing, and the internet of things (IoT) are transforming the software industry and the business world. Organizations need to redesign their models and processes to be sustainable. Smartphones are at the core of these paradigms, letting us locate and easily interact with the world around us. Frequently, the development of mobile software requires of the adaption of valuable and tested non-mobile software. Most challenges in this kind of software modernization are related to the diversity of platforms on the smartphones market and to the need of systematic and reusable processes with a high degree of automation that reduce time, cost, and risks. This chapter proposes a modernization framework based on model-driven engineering (MDE). It allows integrating legacy code with the native behaviors of the different mobile platform through cross-platform languages. Realizations of the framework for the migration of C/C++ or Java code to mobile platforms through the Haxe multiplatform language are described. © 2018, IGI Global.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Ognawala20181475,
author={Ognawala, S. and Hutzelmann, T. and Psallida, E. and Pretschner, A.},
title={Improving function coverage with munch: A hybrid fuzzing and directed symbolic execution approach},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2018},
pages={1475-1482},
doi={10.1145/3167132.3167289},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050514369&doi=10.1145%2f3167132.3167289&partnerID=40&md5=5c4405307caa55ff76572c27ff77d939},
affiliation={Technical University of Munich, Germany},
abstract={Fuzzing and symbolic execution are popular techniques for finding vulnerabilities and generating test-cases for programs. Fuzzing, a blackbox method that mutates seed input values, is generally incapable of generating diverse inputs that exercise all paths in the program. Due to the path-explosion problem and dependence on SMT solvers, symbolic execution may also not achieve high path coverage. A hybrid technique involving fuzzing and symbolic execution may achieve better function coverage than fuzzing or symbolic execution alone. In this paper, we present Munch, an open-source framework implementing two hybrid techniques based on fuzzing and symbolic execution. We empirically show using nine large open-source programs that overall, Munch achieves higher (in-depth) function coverage than symbolic execution or fuzzing alone. Using metrics based on total analyses time and number of queries issued to the SMT solver, we also show that Munch is more efficient at achieving better function coverage. © 2018 ACM.},
author_keywords={Compositional analysis;  Function coverage;  Fuzzing;  Software testing;  Symbolic execution},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Haraty2018197,
author={Haraty, R.A. and Mansour, N. and Zeitunlian, H.},
title={Metaheuristic Algorithm for State-Based Software Testing},
journal={Applied Artificial Intelligence},
year={2018},
volume={32},
number={2},
pages={197-213},
doi={10.1080/08839514.2018.1451222},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045296120&doi=10.1080%2f08839514.2018.1451222&partnerID=40&md5=24b50995e80ef5c0eb8c34dbc3bd25d9},
affiliation={Department of Computer Science and Mathematics, Lebanese American University, Beirut, Lebanon},
abstract={This article presents a metaheuristic algorithm for testing software, especially web applications, which can be modeled as a state transition diagram. We formulate the testing problem as an optimization problem and use a simulated annealing (SA) metaheuristic algorithm to generate test cases as sequences of events while keeping the test suite size reasonable. SA evolves a solution by minimizing an energy function that is based on testing objectives such as coverage, diversity, and continuity of events. The suggested method includes a “significance weight” assigned to events, which leads to important web pages and ensures coverage of relevant features by test cases. The experimental results demonstrate the effectiveness of simulated annealing and show that SA yields good results for testing web applications in comparison with other heuristics. © 2018 Taylor & Francis.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sander20181,
author={Sander, U. and Lubbe, N.},
title={The potential of clustering methods to define intersection test scenarios: Assessing real-life performance of AEB},
journal={Accident Analysis and Prevention},
year={2018},
volume={113},
pages={1-11},
doi={10.1016/j.aap.2018.01.010},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041483516&doi=10.1016%2fj.aap.2018.01.010&partnerID=40&md5=b8ded358c9fc9e71c70d4fdb37f12ae6},
affiliation={Autoliv Research, Wallentinsvägen 22, Vårgårda, 447 83, Sweden; Chalmers University of Technology, Gothenburg, 412 96, Sweden},
abstract={Intersection accidents are frequent and harmful. The accident types ‘straight crossing path’ (SCP), ‘left turn across path – oncoming direction’ (LTAP/OD), and ‘left-turn across path – lateral direction’ (LTAP/LD) represent around 95% of all intersection accidents and one-third of all police-reported car-to-car accidents in Germany. The European New Car Assessment Program (Euro NCAP) have announced that intersection scenarios will be included in their rating from 2020; however, how these scenarios are to be tested has not been defined. This study investigates whether clustering methods can be used to identify a small number of test scenarios sufficiently representative of the accident dataset to evaluate Intersection Automated Emergency Braking (AEB). Data from the German In-Depth Accident Study (GIDAS) and the GIDAS-based Pre-Crash Matrix (PCM) from 1999 to 2016, containing 784 SCP and 453 LTAP/OD accidents, were analyzed with principal component methods to identify variables that account for the relevant total variances of the sample. Three different methods for data clustering were applied to each of the accident types, two similarity-based approaches, namely Hierarchical Clustering (HC) and Partitioning Around Medoids (PAM), and the probability-based Latent Class Clustering (LCC). The optimum number of clusters was derived for HC and PAM with the silhouette method. The PAM algorithm was both initiated with random start medoid selection and medoids from HC. For LCC, the Bayesian Information Criterion (BIC) was used to determine the optimal number of clusters. Test scenarios were defined from optimal cluster medoids weighted by their real-life representation in GIDAS. The set of variables for clustering was further varied to investigate the influence of variable type and character. We quantified how accurately each cluster variation represents real-life AEB performance using pre-crash simulations with PCM data and a generic algorithm for AEB intervention. The usage of different sets of clustering variables resulted in substantially different numbers of clusters. The stability of the resulting clusters increased with prioritization of categorical over continuous variables. For each different set of cluster variables, a strong in-cluster variance of avoided versus non-avoided accidents for the specified Intersection AEB was present. The medoids did not predict the most common Intersection AEB behavior in each cluster. Despite thorough analysis using various cluster methods and variable sets, it was impossible to reduce the diversity of intersection accidents into a set of test scenarios without compromising the ability to predict real-life performance of Intersection AEB. Although this does not imply that other methods cannot succeed, it was observed that small changes in the definition of a scenario resulted in a different avoidance outcome. Therefore, we suggest using limited physical testing to validate more extensive virtual simulations to evaluate vehicle safety. © 2018 Elsevier Ltd},
author_keywords={AEB;  AEBS;  Clustering;  Intersection;  Junction;  Left turn across path;  LTAP/OD;  SCP;  Straight crossing path;  Test scenarios},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhao2018796,
author={Zhao, L. and Zhou, K. and Guo, J. and Wang, S. and Lin, T.},
title={A Universal String Matching Approach to Screen Content Coding},
journal={IEEE Transactions on Multimedia},
year={2018},
volume={20},
number={4},
pages={796-809},
doi={10.1109/TMM.2017.2758519},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030775419&doi=10.1109%2fTMM.2017.2758519&partnerID=40&md5=f74c07fef279bb9d71c097753f870004},
affiliation={Department of Computer Science and Engineering, Shaoxing University, Shaoxing, 312000, China; Institute of VLSI, College of Electronics and Information Engineering, Tongji University, Shanghai, 200092, China},
abstract={This paper proposes a universal string matching (USM) approach to screen content coding (SCC). USM uses a primary reference buffer and a secondary reference buffer for string matching and includes three modes: general string (GS) mode, constrained string 1 (CS1) mode, and constrained string 2 (CS2) mode. The CS1 mode and CS2 mode are constrained cases of the GS mode. Due to the diversity of the screen content, each of the three modes plays an indispensable role in coding some types of screen content. When using USM to code a coding unit (CU), one of the three modes is selected to code the CU. Compared with high-efficiency video coding (HEVC) SCC reference software HM-16.6 + SCM-5.2 of full frame search range for intrablock copy, USM achieves an average Y BD-rate of -28.4% for five text and graphics with motion (TGM) sequences from the audio video coding standard SCC common test condition (CTC) test suite and -5.8% for eight TGM test sequences from the HEVC SCC CTC test suite in all intraconfigurations, with a nearly 10% decrease in encoding runtime and almost the same decoding runtime. © 2017 IEEE.},
author_keywords={audio video coding standard (AVS);  hash table;  High efficiency video coding (HEVC);  screen content coding (SCC);  string matching},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Puskarczyk2018,
author={Puskarczyk, E.},
title={Applying of the Artificial Neural Networks (ANN) to Identify and Characterize Sweet Spots in Shale Gas Formations},
journal={E3S Web of Conferences},
year={2018},
volume={35},
doi={10.1051/e3sconf/20183503008},
art_number={03008},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045145892&doi=10.1051%2fe3sconf%2f20183503008&partnerID=40&md5=2c8a62498027aa00dcc493b9f1203259},
affiliation={AGH University of Science and Technology, Faculty of Geology, Geophysics and Environmental Protection, A. Mickiewicza Av. 30, Krakow, 30-059, Poland},
abstract={The main goal of the study was to enhance and improve information about the Ordovician and Silurian gas-saturated shale formations. Author focused on: firstly, identification of the shale gas formations, especially the sweet spots horizons, secondly, classification and thirdly, the accurate characterization of divisional intervals. Data set comprised of standard well logs from the selected well. Shale formations are represented mainly by claystones, siltstones, and mudstones. The formations are also partially rich in organic matter. During the calculations, information about lithology of stratigraphy weren't taken into account. In the analysis, selforganizing neural network - Kohonen Algorithm (ANN) was used for sweet spots identification. Different networks and different software were tested and the best network was used for application and interpretation. As a results of Kohonen networks, groups corresponding to the gas-bearing intervals were found. The analysis showed diversification between gas-bearing formations and surrounding beds. It is also shown that internal diversification in sweet spots is present. Kohonen algorithm was also used for geological interpretation of well log data and electrofacies prediction. Reliable characteristic into groups shows that Ja Mb and Sa Fm which are usually treated as potential sweet spots only partially have good reservoir conditions. It is concluded that ANN appears to be useful and quick tool for preliminary classification of members and sweet spots identification. © 2018 The Authors, published by EDP Sciences.},
author_keywords={Artificial Neural Networks;  Classification;  Shale gas;  Sweet spots},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kluge2018,
author={Kluge, M. and Friedel, C.C.},
title={Watchdog - a workflow management system for the distributed analysis of large-scale experimental data},
journal={BMC Bioinformatics},
year={2018},
volume={19},
number={1},
doi={10.1186/s12859-018-2107-4},
art_number={97},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043570775&doi=10.1186%2fs12859-018-2107-4&partnerID=40&md5=d1cceb6c182dc8311198790f13260332},
affiliation={Ludwig-Maximilians-Universität München, Institute for Informatics, Amalienstraße 17, München, 80333, Germany},
abstract={Background: The development of high-throughput experimental technologies, such as next-generation sequencing, have led to new challenges for handling, analyzing and integrating the resulting large and diverse datasets. Bioinformatical analysis of these data commonly requires a number of mutually dependent steps applied to numerous samples for multiple conditions and replicates. To support these analyses, a number of workflow management systems (WMSs) have been developed to allow automated execution of corresponding analysis workflows. Major advantages of WMSs are the easy reproducibility of results as well as the reusability of workflows or their components. Results: In this article, we present Watchdog, a WMS for the automated analysis of large-scale experimental data. Main features include straightforward processing of replicate data, support for distributed computer systems, customizable error detection and manual intervention into workflow execution. Watchdog is implemented in Java and thus platform-independent and allows easy sharing of workflows and corresponding program modules. It provides a graphical user interface (GUI) for workflow construction using pre-defined modules as well as a helper script for creating new module definitions. Execution of workflows is possible using either the GUI or a command-line interface and a web-interface is provided for monitoring the execution status and intervening in case of errors. To illustrate its potentials on a real-life example, a comprehensive workflow and modules for the analysis of RNA-seq experiments were implemented and are provided with the software in addition to simple test examples. Conclusions:Watchdog is a powerful and flexible WMS for the analysis of large-scale high-throughput experiments. We believe it will greatly benefit both users with and without programming skills who want to develop and apply bioinformatical workflows with reasonable overhead. The software, example workflows and a comprehensive documentation are freely available at www.bio.ifi.lmu.de/watchdog. © 2018 The Author(s).},
author_keywords={Automated execution;  Distributed analysis;  High-throughput experiments;  Large-scale datasets;  Reproducibility;  Reusability;  RNA-seq;  Workflow management system},
document_type={Article},
source={Scopus},
}

@ARTICLE{Davidson201812,
author={Davidson, P.C.},
title={Engaging students in life-changing experiences},
journal={Resource: Engineering and Technology for Sustainable World},
year={2018},
volume={25},
number={2},
pages={12-15},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043315110&partnerID=40&md5=3eef1c9d4cf947754dc4c3621f4c997a},
affiliation={Department of Agricultural and Biological Engineering, University of Illinois, Urbana, United States},
abstract={Agricultural and biological engineers were well equipped to solve the diverse challenges facing society. Experts debated whether students could be better prepared for an ever-changing, global economy by immersing them in life-changing study abroad programs. ASABE member, Alan Hansen strongly believed that a study abroad program could be one of the most critical learning environments for students when he initiated a project-based study abroad program at the University of Illinois in 2004. Alan envisioned a program that would combine cultural experience with technical experience through senior-level engineering projects in partnership with the University of KwaZulu-Natal (UKZN) in Pietermaritzburg, South Africa. The one-month program extends from mid-July to mid-August and was structured to allow U of I students to team up with UKZN students to complete the construction and testing phases of their projects.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mullins2018197,
author={Mullins, G.E. and Stankiewicz, P.G. and Hawthorne, R.C. and Gupta, S.K.},
title={Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles},
journal={Journal of Systems and Software},
year={2018},
volume={137},
pages={197-215},
doi={10.1016/j.jss.2017.10.031},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037521968&doi=10.1016%2fj.jss.2017.10.031&partnerID=40&md5=a4529ef4d3402e215bb9b0aad65e8e03},
affiliation={Department of Mechanical Engineering and Institute for Systems Research, University of Maryland, College Park, MD20742, United States; Johns Hopkins University Applied Physics Laboratory, 11100 Johns Hopkins Road, Laurel, Maryland, 20723, United States; Department of Aerospace and Mechanical Engineering and Center for Advanced Manufacturing, University of Southern California, Los Angeles, California90089, United States},
abstract={In this paper we propose a new method for generating test scenarios for black-box autonomous systems that demonstrate critical transitions in performance modes. This method provides a test engineer with key insights into the software's decision-making engine and how those decisions affect transitions between performance modes. We achieve this via adaptive, simulation-based testing of the autonomous system where each sample represents a simulated scenario. The test scenario, i.e the system input, represents a given configuration of environmental or mission parameters and the resulting outputs are the system's performance based on high-level success criteria. For realistic testing scenarios, the dimensionality of the configuration space and the computational expense of high-fidelity simulations precludes exhaustive or uniform sampling. Thus, we have developed specialized adaptive search algorithms designed to discover performance boundaries of the autonomy using a minimal number of samples. Further, unsupervised clustering techniques are presented that can group test scenarios by the resulting performance modes and sort them by those which are most effective at diagnosing changes in the autonomous system's behavior. The result is a testing framework that gives the test engineer a set of diverse scenarios that exercises the decision boundaries of the autonomous system under test. © 2017 Elsevier Inc.},
author_keywords={Autonomous vehicles;  Optimization;  Simulation based testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{LazzariniLemos2018497,
author={Lazzarini Lemos, O.A. and Fagundes Silveira, F. and Cutigi Ferrari, F. and Garcia, A.},
title={The impact of Software Testing education on code reliability: An empirical assessment},
journal={Journal of Systems and Software},
year={2018},
volume={137},
pages={497-511},
doi={10.1016/j.jss.2017.02.042},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015018996&doi=10.1016%2fj.jss.2017.02.042&partnerID=40&md5=174249705427381a7449cb4f277c2d45},
affiliation={Science and Technology Department, Federal University of São Paulo at S. J. dos Campos, Brazil; Computing Department, Federal University of São Carlos, Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro, Brazil},
abstract={Software Testing (ST) is an indispensable part of software development. Proper testing education is thus of paramount importance. Indeed, the mere exposition to ST knowledge might have an impact on programming skills. In particular, it can encourage the production of more correct - and thus reliable - code. Although this is intuitive, to the best of our knowledge, there are no studies about such effects. Concerned with this, we have conducted two investigations related to ST education: (1) a large experiment with students to evaluate the possible impact of ST knowledge on the production of reliable code; and (2) a survey with professors that teach introductory programming courses to evaluate their level of ST knowledge. Our study involved 60 senior-level computer science students, 8 auxiliary functions with 92 test cases, a total of 248 implementations, and 53 professors of diverse subfields that completed our survey. The investigation with students shows that ST knowledge can improve code reliability in terms of correctness in as much as 20%, on average. On the other hand, the survey with professors reveals that, in general, university instructors tend to lack the same knowledge that would help students increase their programming skills toward more reliable code. © 2017 Elsevier Inc.},
author_keywords={Computer science education;  Software Testing;  Student experiments},
document_type={Article},
source={Scopus},
}

@ARTICLE{Li201848,
author={Li, M. and Li, T. and Guan, X. and Zhao, G. and Zhou, F.},
title={Design and Experiment of Rotary Hole Seeder for Dryland},
journal={Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery},
year={2018},
volume={49},
number={2},
pages={48-57},
doi={10.6041/j.issn.1000-1298.2018.02.007},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048254913&doi=10.6041%2fj.issn.1000-1298.2018.02.007&partnerID=40&md5=eae18e23ce1e681c9f80fbcf60943c83},
affiliation={College of Engineering, Northeast Agricultural University, Harbin, 150030, China},
abstract={In order to further improve the quality of hole metering, aiming at hilling and blocking problems of touching parts soil sowing in the process, combined with the requirements of precision seeding technology and the use of four bar kinematic principle, a kind of dry type rotary pricking hole seeding method was put forward, its variety can be applied to different working environments with diverse mode. Selecting the core motion parameters by establishing the mathematical model and the structure of tie points VB visual programming platform and the design of key components of the concave wheel and the return spring was optimized. The principal axis speed and the spring wire diameter were selected as experiment factors, with qualified index and coefficient of variation of test indexes as two orthogonal revolving combination test, the data processing was done by using Design-Expert 10.0.3 software and selecting the optimal working parameters. Through field verification test, under the condition that the spindle speed was 27 r/min and the line diameter of the spring was 1.5 mm, the operation qualification index of the seeder was 92.1%, the variation coefficient was 2.5%, and the blockage rate was 0. The results showed that the mechanism can meet the requirements of operation technology of precision seeding at the same time, and further improve the reliability and uniformity of soil particle spacing. The optimization of pricking hole seeder for dry rotary provided a theoretical basis and technical reference for improvement. © 2018, Chinese Society of Agricultural Machinery. All right reserved.},
author_keywords={Design;  Experiment;  Optimization;  Rotary pricking hole type;  Seeder},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tiwari2018,
author={Tiwari, S. and Saini, V. and Singh, P. and Sureka, A.},
title={A case study on the application of case-based learning in software testing},
journal={ACM International Conference Proceeding Series},
year={2018},
doi={10.1145/3172871.3172881},
art_number={a11},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044357533&doi=10.1145%2f3172871.3172881&partnerID=40&md5=6e510d7f6b981e4f9d7689c854b9b086},
affiliation={DAIICT, Gandhinagar, India; NIT, Jalandhar, India; Ashoka University, India},
abstract={Software testing is a popular mean of examining the adequacy of a developed product. However, in academic institutions more emphasis is given to software development than ensuring its quality. In order to address the gaps between existing university-level software testing education and the training standards used in industry, we experiment with employing a popular teaching method Case-Based Learning (CBL) for the first time to facilitate the training of selected software testing concepts at tertiary-level. The CBL exercise is conducted for undergraduate students of DAIICT, Gandhinagar (India) to cultivate the decision making skills in a self-learning environment. After the CBL execution we collect students' responses through a short survey and perform an empirical analysis on the survey results. The outcome of this CBL practice is positive as a majority of students are able to achieve the five stated objectives of CBL. We examine that there is a statistically significant difference between students' responses based on gender diversity. We also investigate the difference in students' feedback to the two different CBL cases that we use for practicing some aspects of software testing. Moreover, we draw useful inferences from the opinions of TAs (Teaching Assistants) about the CBL sessions. © 2018 ACM.},
author_keywords={Case-based learning;  Software engineering education;  Software testing;  Teaching methodology},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sullivan2018237,
author={Sullivan, K. and Thomas, S. and Rosano, M.},
title={Using industrial ecology and strategic management concepts to pursue the Sustainable Development Goals},
journal={Journal of Cleaner Production},
year={2018},
volume={174},
pages={237-246},
doi={10.1016/j.jclepro.2017.10.201},
note={cited By 90},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038881789&doi=10.1016%2fj.jclepro.2017.10.201&partnerID=40&md5=8281f8400894660eee1b6ecd904c6567},
affiliation={Sustainability Science Lab, University of Melbourne, Australia; School of Ecosystem and Forest Sciences, University of Melbourne, Australia; Australian-German Climate and Energy College, University of Melbourne, Australia; Sustainable Engineering Group, Curtin University, Australia},
abstract={The subjectivity, complexity, and often competing interests of sustainable development have limited the effectiveness of integrating these important ideas into mainstream business strategy. With the adoption of the UN Sustainable Development Goals (SDGs) in January 2016, there are now global sustainability benchmarks that apply across diverse sectors and national contexts, allowing public and private organizations to orient and evaluate their activities, strategies, and business outcomes. However, it is not directly apparent where the advantage for business lies in pursuit of these actions within the prevailing economic paradigm, highlighting the need for new analytical frameworks and tools. Industrial ecology (IE) has been successfully used in engineering practice for decades and has been suggested as a method that can provide the concepts and methods necessary to bridge the gap between traditional business practice and sustainable development. To test this, literature bridging the fields of industrial ecology, business strategy, and sustainable development was collected and analyzed using the textual analysis software Leximancer™. The analysis showed that while the SDGs are primarily aimed at the national level, they also hold relevance for business through innovation, partnerships, and strategic positioning, inter alia. The analysis found that the integration of IE and business strategy is highly relevant for three of the SDGs, but captures elements of all 17 to varying degrees. IE has a strong focus on innovation and its potential in new markets, products, and business models. IE is also consciously aimed at the efficient use of energy and resources, ideas that are relevant to mitigating, adapting, and building resilience in a changing future, but are also relevant to traditional concepts of business strategy and competitive advantage. This paper shows that through the combination of IE and strategic management theory, commercial organizations can positively contribute to the Sustainable Development Goals while building competitive advantage. © 2017 Elsevier Ltd},
author_keywords={Business;  Competitive advantage;  Global goals;  Leximancer;  Text mining},
document_type={Article},
source={Scopus},
}

@ARTICLE{Faisal20181,
author={Faisal, M. and Nazir, S. and Babar, M.},
title={Notational Modeling for Model Driven Cloud Computing using Unified Modeling Language},
journal={EAI Endorsed Transactions on Scalable Information Systems},
year={2018},
volume={5},
number={16},
pages={1-7},
doi={10.4108/eai.13-4-2018.154475},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122846829&doi=10.4108%2feai.13-4-2018.154475&partnerID=40&md5=4f62188bb151011c2214f96dbfc3c1b7},
affiliation={Iqra University, Islamabad, Pakistan},
abstract={Cloud Computing and Model-Driven Engineering (MDE) are two of the most dominant paradigms nowadays. Models are considered as major parts in the MDE, and concentrates on the formation and then conversion of models to the implementation. Cloud Computing is also getting reputation as the standard approach for designing and organizing software applications over the internet, especially for distributed and e-commerce applications. In recent times, Cloud Computing has become known as a latest opportunity that how software and other resources can be provided to the consumers as a service. The cloud aspects need to be well engineered for the software engineering methodologies particularly modeling the cloud aspects to provide logical tested solution prior to implementation. On the other side, general-purpose language UML, provides modeling and designing notions to symbolize software, platforms and architectural artifacts from diverse viewpoints of object-oriented paradigm. UML can also be extended to model and visualize the non objectoriented systems. Lately, few cloud modeling methodologies have emerged, however, useful support for designing cloud application is still missing. As a result, we propose UML-based framework using UML extension mechanism for modeling cloud computing paradigm aspects. © 2018 Muhammad Faisal et al., licensed to EAI. This is an open access article distributed under the terms ofthe Creative Commons Attribution licence (http://creativecommons.org/licenses/by/3.0/), which permits unlimited use, distribution and reproduction in any medium so long as the original work is properly cited.},
author_keywords={Cloud Computing;  Model-Driven Engineering;  Unified Modeling Language},
document_type={Article},
source={Scopus},
}

@BOOK{Zamli2018639,
author={Zamli, K.Z. and Ahmed, B.S. and Mahmoud, T. and Afzal, W.},
title={Fuzzy adaptive tuning of a particle swarm optimization algorithm for variable-strength combinatorial test suite generation},
journal={Swarm Intelligence - Volume 3: Applications},
year={2018},
pages={639-662},
doi={10.1049/PBCE119H_ch22},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114032957&doi=10.1049%2fPBCE119H_ch22&partnerID=40&md5=9e78cd4faeb090b35c08d3d53c4dcbd7},
affiliation={Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, Malaysia; Department of Computer Science, Czech Technical University, Czech Republic; Centre for Communications Engineering Research, Edith Cowan University, Australia; Division of Networked and Embedded Systems, Mälardalen University, Sweden},
abstract={Combinatorial interaction testing (CIT) is an important software testing technique that has seen lots of recent interest. It can reduce the number of test cases needed by considering interactions between combinations of input parameters. Empirical evidence shows that it effectively detects faults, in particular, for highly configurable software systems. In real-world software testing, the input variables may vary in how strongly they interact; variable strength CIT (VS-CIT) can exploit this for higher effectiveness. The generation of variable strength test suites is a non-deterministic polynomialtime (NP) hard computational problem. Research has shown that stochastic population-based algorithms such as particle swarm optimization (PSO) can be efficient compared to alternatives for VS-CIT problems. Nevertheless, they require detailed control for the exploitation and exploration trade-off to avoid premature convergence (i.e., being trapped in local optima) as well as to enhance the solution diversity. Here, we present a new variant of PSO based on Mamdani fuzzy inference system (FIS), to permit adaptive selection of its global and local search operations. We detail the design of this combined algorithm and evaluate it through experiments on multiple synthetic and benchmark problems. We conclude that fuzzy adaptive selection of global and local search operations is, at least, feasible as it performs only second-best to a discrete variant of PSO, called discrete PSO (DPSO). Concerning obtaining the best mean test suite size, the fuzzy adaptation even outperforms DPSO occasionally.We discuss the reasons behind this performance and outline relevant areas of future work. © The Institution of Engineering and Technology 2018.},
author_keywords={Combinatorial mathematics;  Computational complexity;  Evolutionary computation;  Fuzzy reasoning;  Particle swarm optimisation;  Program testing;  Search problems},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Wäschebach2018153,
author={Wäschebach, T.},
title={Coordinate measuring machines can do more! efficient handling of variant diversity in the worker's self-check of a large industrial series manufacturer [Koordinatenmessgeräte können mehr! effizientes handling von variantenvielfalt in der werkerselbst-prüfung eines industriellen großserienherstellers]},
journal={VDI Berichte},
year={2018},
volume={2018},
number={2326},
pages={153-155},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106171311&partnerID=40&md5=8c3a5b4ea877be55452a959b77c2c16a},
affiliation={Paul Hettich GmbH & Co. KG, Kirchlengern, Germany},
abstract={Hettich is one of the world's largest manufacturers of furniture fittings (2017: 6,600 employees, 975 million euros in sales). One focus of production at the headquarters in Kirchlengern are drawer runners. Every day, thousands of these runners are produced in various variations. In addition to inline measurement technology, multi-sensor coordinate measurement technology is used in a worker's self-checking concept to monitor the manufacturing processes. This enables a highly accurate and human-independent evaluation of the status of products and processes. For efficient management of the associated measuring programs, these are intelligently pa-rameterized so that one measuring program covers hundreds of variants. All coordinate measuring machines write their measurement results directly to SAP. In selected areas, the necessary SAP test plans and measurement program parameter files are generated completely from a common data basis in advance. All coordinate measuring machines and the associated offline programming stations are also connected via a measuring program server for administrative purposes.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Aghli2018330,
author={Aghli, N. and Carvalho, M.},
title={A reinforcement learning approach to autonomous speed control in robotic systems},
journal={Proceedings of the 31st International Florida Artificial Intelligence Research Society Conference, FLAIRS 2018},
year={2018},
pages={330-335},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071948017&partnerID=40&md5=05e5a6b725131498773423e8e0fadaec},
affiliation={Harris Institute for Assured Information, School of Computing, Florida, Institute of Technology, 150 W. University Blvd, Melbourne, FL  32901, United States},
abstract={Model-free reinforcement learning techniques have been successfully used in diverse robotic applications. In this paper, we design and implement the Q-learning algorithm, a widely used model-free algorithm to find the optimal speed control function for a fast moving train on a fixed track. The goal is to allow for the train to learn the fastest speed profile it may use on a track, without derailment. We contrast the performance of the learning algorithm with the performance of the human controlling trying to perform the same task. In order the test the proposed algorithm, a complete hardware and software testbed has been designed and implemented, allowing for the evaluation of the learning models over a physical environment. We conclude that in simple tasks, the performance on humans in speed control is similar to the performance of the reinforcement learning algorithm, but when a more complex track is considered, the proposed reinforcement learning learning models outperforms the humans. Copyright ©2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pailoor2018729,
author={Pailoor, S. and Aday, A. and Jana, S.},
title={MoonShine: Optimizing OS fuzzer seed selection with trace distillation},
journal={Proceedings of the 27th USENIX Security Symposium},
year={2018},
pages={729-743},
note={cited By 44},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071913575&partnerID=40&md5=7fa1af99d5f21e5c1edba45d8a605b72},
affiliation={Columbia University, United States},
abstract={OS fuzzers primarily test the system-call interface between the OS kernel and user-level applications for security vulnerabilities. The effectiveness of all existing evolutionary OS fuzzers depends heavily on the quality and diversity of their seed system call sequences. However, generating good seeds for OS fuzzing is a hard problem as the behavior of each system call depends heavily on the OS kernel state created by the previously executed system calls. Therefore, popular evolutionary OS fuzzers often rely on hand-coded rules for generating valid seed sequences of system calls that can bootstrap the fuzzing process. Unfortunately, this approach severely restricts the diversity of the seed system call sequences and therefore limits the effectiveness of the fuzzers. In this paper, we develop MoonShine, a novel strategy for distilling seeds for OS fuzzers from system call traces of real-world programs while still preserving the dependencies across the system calls. MoonShine leverages light-weight static analysis for efficiently detecting dependencies across different system calls. We designed and implemented MoonShine as an extension to Syzkaller, a state-of-the-art evolutionary fuzzer for the Linux kernel. Starting from traces containing 2.8 million system calls gathered from 3,220 real-world programs, MoonShine distilled down to just over 14,000 calls while preserving 86% of the original code coverage. Using these distilled seed system call sequences, MoonShine was able to improve Syzkaller's achieved code coverage for the Linux kernel by 13% on average. MoonShine also found 17 new vulnerabilities in the Linux kernel that were not found by Syzkaller. © 2018 Proceedings of the 27th USENIX Security Symposium. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feng2018,
author={Feng, C. and Liu, T. and Mao, W. and Wang, W.},
title={Centralized real time monitoring system based on multilayer network architecture for flight test telemetry},
journal={Proceedings of the International Telemetering Conference},
year={2018},
volume={2018-November},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062733629&partnerID=40&md5=7352c331b2e335975a2a5409f806307a},
affiliation={Instrumentation Department Flight Test Center of the COMAC Shang Hai200232, China},
abstract={The flight test telemetry real-time monitoring system is an indispensable part of civil aircraft flight test. With the current trend of network system, the traditional real-time monitoring model has difficulties in satisfying the requirements of increasing number of parameters, diversified types, large-scale system and high concurrency data streams. In response to the above issues, this paper proposes a monitoring system based on a three-tier architecture (data layer, business logic layer and presentation layer). The system uses TMoIP technology and Best Data Engine (BDE) to complete the selection of the best data source of multi-site flight test data streams. At the same time, the use of portability and rapid integration enables hundreds of terminals to work simultaneously. The system has been used successfully in China’s developing large civil aircraft C919 flight test program. The preparation time of the system has been greatly reduced, and the system performs stably. © held by the author; distribution rights International Foundation for Telemetering.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Manakhayev2018,
author={Manakhayev, R. and Abilgaziyeva, N. and Bopiyev, C. and Saduakassov, B. and Abdrazakov, D. and Stepanov, V. and Se, Y. and Clarke, J. and Kamispaev, A. and Tyre, R. and Nurmanov, S. and Ismailov, B. and Argynov, D.},
title={Staged acid stimulation initiative in giant carbonate reservoir},
journal={Society of Petroleum Engineers - SPE Annual Caspian Technical Conference and Exhibition 2018, CTCE 2018},
year={2018},
doi={10.2118/192565-MS},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060035318&doi=10.2118%2f192565-MS&partnerID=40&md5=f8fb1e3f23e8e997f12f8bc44cbe8717},
affiliation={Tengizchevroil, Kazakhstan; Schlumberger, United States},
abstract={Tengiz field is a super-giant carbonate reservoir located in the Western Kazakhstan. The carbonate matrix consists of almost pure calcite, which makes it very attractive for acid stimulation. Over the years matrix acid stimulation has been successfully used in Tengiz to remove near wellbore damage and enhance well productivity. Despite successful production response from acid stimulation in the past, it was noticed from poststimulation surveillance analysis that tighter and less depleted intervals were often left untreated. This observation suggested that chemical diverting agents were not effective for long completion intervals and further opportunity for improvement was identified. In addition, existing two phase retarded acid had limitation due to high viscosity and had known issues during plant flowback. New acid stimulation design has been proposed and successfully executed in three newly drilled wells. This acid treatment utilized staged stimulation concept, where diversion was ensured by mechanical isolation of lower intervals with inflatable packer. New single phase retarded acid system was introduced to address operational and plant processing challenges with two phase retarded acid. Core flow tests were performed prior stimulation to evaluate effectiveness of different acid systems. Results of core flow tests and wireline log data were used during simulations on new generation of matrix acidizing modeling software to determine parameters for optimal wormhole creation. Proper planning and thorough technical assessment enabled execution of staged acid stimulation with new acid system incident free with less than 10% incremental cost in all three wells. Post-job surveillance program is in place to evaluate acid diversion and production contribution from tighter and less depleted intervals. © Copyright 2018, Society of Petroleum Engineers},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Grove2018,
author={Grove, B. and Manning, D.},
title={Shaped charge perforation depth at full downhole conditions: New understandings},
journal={Proceedings - SPE Annual Technical Conference and Exhibition},
year={2018},
volume={2018-September},
doi={10.2118/191526-ms},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059737936&doi=10.2118%2f191526-ms&partnerID=40&md5=9c830e106b1c67cbdd2d34912d85f139},
affiliation={Halliburton Jet Research Center, United States},
abstract={Explosive perforating has been the dominant method of establishing communication between the reservoir and cased wellbore for more than 70 years. Effective perforations, which provide an unimpeded flowpath, are critical to deliver the well performance required to justify overall project investment. To reliably estimate or predict well flow performance, it is essential to have an accurate understanding of critical perforation parameters that exist downhole, including tunnel penetration depth into the formation, cleanness of the tunnels, and hole diameter through the casing. Consequently, the industry has focused significant attention toward developing this understanding in recent decades. This is particularly true today, as downhole environments are becoming more extreme. Project investment decisions require increasingly accurate well performance estimates, both initially and over the life of a development. This current state of affairs has motivated a recent and ongoing effort to better understand perforator performance at full downhole conditions, up to and exceeding 30,000 psi. A large program is underway to investigate the penetration and hole size performance of several charges across a range of rocks and pressure conditions. The goal of this program is to obtain fundamental insights into the effects of extreme values of certain downhole conditions on perforator performance. The current test program follows the recently revised API-RP 19B Section II protocol and includes high-pressure variations of the standard test configuration. One area of key findings thus far is in the context of recent industry frameworks for analyzing laboratory penetration data, including ballistic stress and the ballistic indicator function. These are found to be useful tools that simplify analysis and provide insight and guidance. These frameworks make it possible to collapse multiple diverse penetration datasets, from across a range of test conditions, toward a single performance curve. This curve can be used to enable ballpark estimates of the performance of a given charge in a specific rock strength and stress regime. It provides the potential to identify a penetration asymptote (assumed to be a fundamental charge property that would be observed in very strong and/or highly-stressed rocks). It is also a useful framework to quickly visualize a vast spectrum of reservoir conditions, and to identify where a specific reservoir may fit in the broader context. Of particular interest to the perforating testing community is the relatively narrow range of values encompassed by the newly-revised API-RP 19B Section II standard test conditions. To extend this framework to predictive models of charge penetration over a broad range of downhole conditions, however, study results indicate that more work is needed. It will be necessary, for example, to account for charge-dependent wellbore effects to move closer to a predictive capability that exhibits the level of quantitative accuracy required for many applications. Other fundamental findings involve wellbore pressure influence on perforator performance. For one charge studied somewhat extensively, wellbore pressure was observed to exhibit an interesting non-monotonic influence on penetration. Moderate wellbore pressures increased penetration depth; higher wellbore pressures decreased penetration depth. Wellbore fluid pressure was also found to exhibit a charge-dependent influence on casing hole size performance; increasing wellbore pressure tended to reduce the hole size slightly for one charge tested, but had no effect for two other charges tested. © Copyright 2018, Society of Petroleum Engineers},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2018137,
author={Wang, Y. and Meng, N. and Zhong, H.},
title={CMSuggester: Method Change Suggestion to Complement Multi-entity Edits},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11293 LNCS},
pages={137-153},
doi={10.1007/978-3-030-04272-1_9},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057504932&doi=10.1007%2f978-3-030-04272-1_9&partnerID=40&md5=99ed5873fd4e160cd5bb523e88d2f47d},
affiliation={Virginia Tech, Blacksburg, VA  24061, United States; Shanghai Jiao Tong University, Shanghai, 200240, China},
abstract={Developers spend significant time and effort in maintaining software. In a maintenance task, developers sometimes have to simultaneously modify multiple program entities (i.e., classes, methods, and fields). We refer to such complex changes as multi-entity edits. It is challenging for developers to apply multi-entity edits consistently and completely. Existing tools provide limited support for such edits, mainly because the co-changed entities usually contain diverse program contexts and experience different changes. This paper introduces CMSuggester, an automatic approach that suggests complementary changes for multi-entity edits. Given a multi-entity edit that adds a field and modifies one or more methods to access the field, CMSuggester suggests other methods to co-change for the new field access. CMSuggester is inspired by our previous empirical study, which reveals that the methods co-changed to access a new field usually commonly access the same set of fields declared in the same class. By extracting the fields accessed by the given changed method(s), CMSuggester identifies and recommends any unchanged method that also accesses those fields. Our evaluation shows that CMSuggester recommends changes for 279 out of 408 suggestion tasks. With the recommended methods, CMSuggester achieves 73% F-score on average, while the widely used tool ROSE achieves 48% F-score. In most cases, as shown in our evaluation results, CMSuggester are useful for developers, since it recommend complete and correct multi-entity edits. © 2018, Springer Nature Switzerland AG.},
author_keywords={Change suggestion;  Common field access;  Multi-entity edit},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang201833,
author={Yang, D. and Qi, Y. and Mao, X.},
title={Evaluating the Strategies of Statement Selection in Automated Program Repair},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11293 LNCS},
pages={33-48},
doi={10.1007/978-3-030-04272-1_3},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057485772&doi=10.1007%2f978-3-030-04272-1_3&partnerID=40&md5=caa5fb3347661ec803563fea21d623a4},
affiliation={National University of Defense Technology, Changsha, 410073, China; Beijing Institute of Tracking and Communication Technology, Beijing, 100094, China; Laboratory of Software Engineering for Complex Systems, National University of Defense Technology, Changsha, 410073, China},
abstract={Automated program repair has drawn significant attention in recent years for its potential to alleviate the heavy burden of debugging activities. Fault localization, which generally provides a rank list of suspicious statements, is necessary for automated repair tools to identify the fault. With such rank list, existing repair tools have two statement selecting strategies for statement modification: suspiciousness-first algorithm (SFA) based on the suspiciousness of statements and rank-first algorithm (RFA) relying on the rank of statements. However, despite the extensive application of SFA and RFA in repair tools and different selecting methods between both strategies, little is known about the performance of the two strategies in automated program repair. In this paper we conduct an empirical study to compare the effectiveness of SFA and RFA in automated program repair. We implement SFA and RFA as well as 6 popular spectrum-based fault localization techniques into 4 automated program repair tools, and then use these tools to perform repair experiments on 44 real-world bugs from Defects4J to compare SFA against RFA in automated program repair. The results suggest that: (1) RFA performs statistically significantly better than SFA in 64.76% cases according to the number of candidate patches generated before a valid patch is found (NCP), (2) SFA achieves better performance in parallel repair and patch diversity, (3) The performance of SFA can be improved by increasing the suspiciousness accuracy of fault localization techniques. These observations provide directions for future research on the usage of statement selecting strategies and a new avenue for improving the effectiveness of fault localization techniques in automated program repair. © 2018, Springer Nature Switzerland AG.},
author_keywords={Automated program repair;  Empirical study;  Fault localization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lu201851,
author={Lu, B. and Dong, W. and Yin, L. and Zhang, L.},
title={Evaluating and Integrating Diverse Bug Finders for Effective Program Analysis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11293 LNCS},
pages={51-67},
doi={10.1007/978-3-030-04272-1_4},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057448793&doi=10.1007%2f978-3-030-04272-1_4&partnerID=40&md5=538ba7b341801f7a2ba8dec9a69c2c19},
affiliation={National University of Defense Technology, Changsha, China; Meituan Corporation, Beijing, China},
abstract={Many static analysis methods and tools have been developed for program bug detection. They are based on diverse theoretical principles, such as pattern matching, abstract interpretation, model checking and symbolic execution. Unfortunately, none of them can meet most requirements for bug finding. Individual tool always faces high false negatives and/or false positives, which is the main obstacle for using them in practice. A direct and promising way to improve the capability of static analysis is to integrate diverse bug finders. In this paper, we first selected five state-of-the-art C/C++ static analysis tools implemented with different theories. We then evaluated them over different defect types and code structures in detail. To increase the precision and recall for tool integration, we studied how to properly employ machine learning algorithms based on features of programs and tools. Evaluation results show that: (1) the abilities of diverse tools are quite different for defect types and code structures, and their overlaps are quite small; (2) the integration based on machine learning can obviously improve the overall performance of static analysis. Finally, we investigated the defect types and code structures which are still challenging for existing tools. They should be addressed in future research on static analysis. © 2018, Springer Nature Switzerland AG.},
author_keywords={Machine learning;  Static analysis;  Tool integration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Haghighatkhah2018243,
author={Haghighatkhah, A. and Mäntylä, M. and Oivo, M. and Kuvaja, P.},
title={Test case prioritization using test similarities},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11271 LNCS},
pages={243-259},
doi={10.1007/978-3-030-03673-7_18},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057232137&doi=10.1007%2f978-3-030-03673-7_18&partnerID=40&md5=d83661c1001ed2c52f70f175aed8020b},
affiliation={M3S Research Unit, University of Oulu, P.O. Box 3000, Oulu, 90014, Finland},
abstract={A classical heuristic in software testing is to reward diversity, which implies that a higher priority must be assigned to test cases that differ the most from those already prioritized. This approach is commonly known as similarity-based test prioritization (SBTP) and can be realized using a variety of techniques. The objective of our study is to investigate whether SBTP is more effective at finding defects than random permutation, as well as determine which SBTP implementations lead to better results. To achieve our objective, we implemented five different techniques from the literature and conducted an experiment using the defects4j dataset, which contains 395 real faults from six real-world open-source Java programs. Findings indicate that running the most dissimilar test cases early in the process is largely more effective than random permutation (Vargha–Delaney A [VDA]: 0.76–0.99 observed using normalized compression distance). No technique was found to be superior with respect to the effectiveness. Locality-sensitive hashing was, to a small extent, less effective than other SBTP techniques (VDA: 0.38 observed in comparison to normalized compression distance), but its speed largely outperformed the other techniques (i.e., it was approximately 5–111 times faster). Our results bring to mind the well-known adage, “don’t put all your eggs in one basket”. To effectively consume a limited testing budget, one should spread it evenly across different parts of the system by running the most dissimilar test cases early in the testing process. © Springer Nature Switzerland AG 2018.},
author_keywords={Regression testing;  Test case prioritization;  Test diversity;  Test similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Korpalski2018,
author={Korpalski, M. and Sosnowski, J.},
title={Correlating software metrics with software defects},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2018},
volume={10808},
doi={10.1117/12.2501150},
art_number={108081P},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056252712&doi=10.1117%2f12.2501150&partnerID=40&md5=d316101f5b5497564e80dcf755c1f7a2},
affiliation={Warsaw University of Technology, Institute of Computer Science, Nowowiejska 15/19, Warsaw, 00-665, Poland},
abstract={In software development and testing an interesting issue is checking correlations of observed software defects with various product and process metrics. Such analysis is helpful in predicting potential defects and optimization of testing processes. In the paper we present results of deeper studies in this area, they involve many metrics and various prediction schemes taking into account diverse correlation parameters. Special attention is paid to the problem of selecting most significant metrics. In the prediction schemes we consider modified and non modified program objects. The presented analysis methods have been verified in an experimental investigation covering twelve open source projects, for some of them several subsequent versions have been examined. This is followed by result discussion. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author_keywords={Defect prediction;  Empirical study;  Software metrics;  Software reliability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lee201864556,
author={Lee, S.J. and Lee, S.H. and Chu, T.-L. and Varuttamaseni, A. and Yue, M. and Li, M. and Cho, J. and Kang, H.G.},
title={Bayesian belief network model quantification using distribution-based node probability and experienced data updates for software reliability assessment},
journal={IEEE Access},
year={2018},
volume={6},
pages={64556-64568},
doi={10.1109/ACCESS.2018.2878376},
art_number={8510804},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055717082&doi=10.1109%2fACCESS.2018.2878376&partnerID=40&md5=027df350360f501b268c2da0083c0aac},
affiliation={Ulsan National Institute of Science and Technology, School of Mechanical, Aerospace and Nuclear Engineering, Ulsan, 44919, South Korea; Department of Mechanical, Rensselaer Polytechnic Institute, Aerospace, Nuclear Engineering, Troy, NY  12180, United States; Brookhaven National Laboratory, Upton, NY  11973, United States; U.S. Nuclear Regulatory Commission, Washington, DC  20555, United States; Korea Atomic Energy Research Institute, Daejeon, 34057, South Korea},
abstract={Since digital instrumentation and control systems are expected to play an essential role in safety systems in nuclear power plants (NPPs), the need to incorporate software failures into NPP probabilistic risk assessment has arisen. Based on a Bayesian belief network (BBN) model developed to estimate the number of software faults considering the software development lifecycle, we performed a pilot study of software reliability quantification using the BBN model by aggregating different experts' opinions. In this paper, we suggest the distribution-based node probability table (D-NPT) development method which can efficiently represent diverse expert elicitation in the form of statistical distributions and provides mathematical quantification scheme. Besides, the handbook data on U.S. software development and VV and testing results for two nuclear safety software were used for a Bayesian update of the D-NPTs in order to reduce the BBN parameter uncertainty due to experts' different background or levels of experience. To analyze the effect of diverse expert opinions on the BBN parameter uncertainties, the sensitivity studies were conducted by eliminating the significantly different NPT estimates among expert opinions. The proposed approach demonstrates a framework that can effectively and systematically integrate different kinds of available source information to quantify BBN NPTs for NPP software reliability assessment. © 2013 IEEE.},
author_keywords={Bayesian belief network;  Nuclear power plant;  Probabilistic risk assessment;  Software reliability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ilak2018,
author={Ilak, P. and Rajšl, I. and Dakovic, J. and Delimar, M.},
title={Duality based risk mitigation method for construction of joint hydro-wind coordination short-run marginal cost curves},
journal={Energies},
year={2018},
volume={11},
number={5},
doi={10.3390/en11051254},
art_number={en11051254},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054982487&doi=10.3390%2fen11051254&partnerID=40&md5=fb307416192b1ecb6960d8329c5b6117},
affiliation={Department of Energy and Power Systems, University of Zagreb, Faculty of Electrical Engineering and Computing, Unska 3, Zagreb, HR-10000, Croatia},
abstract={This study analyzes the short-run hydro generation scheduling for the wind power differences from the contracted schedule. The approach for construction of the joint short-run marginal cost curve for the hydro-wind coordinated generation is proposed and applied on the real example. This joint short-run marginal cost curve is important for its participation in the energy markets and for economic feasibility assessment of such coordination. The approach credibly describes the short-run marginal costs which this coordination bears in “real life”. The approach is based on the duality framework of a convex programming and as a novelty combines the shadow price of risk mitigation, which quantifies the hourly cost of mitigating risk, and the water shadow price, which quantifies the marginal cost of electricity production. The proposed approach is formulated as a stochastic linear program and tested on the case of the Vinodol hydropower system and the wind farm Vrataruša in Croatia. The result of the case study is a family of 24 joint short-run marginal cost curves. The proposed method is expected to be of great interest to investors as it enables risk mitigation for investors with diverse risk preferences, from risk-averse to risk-seeking. © 2018 by the authors.},
author_keywords={Convex programming;  CVaR;  Hydropower;  Risk mitigation;  Short-run marginal cost curve;  Wind power},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Faltot2018,
author={Faltot, P.-J. and Pitel Welnitz, D. and Vertenoeuil, P. and Vlach, T. and Lombardi, L. and D'Ercole, M.},
title={ANALYSIS AND TESTING OF AEROBATIC TURBOPROP AIRCRAFT INLET},
journal={Proceedings of the ASME Turbo Expo},
year={2018},
volume={1},
doi={10.1115/GT2018-76398},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053629568&doi=10.1115%2fGT2018-76398&partnerID=40&md5=b432faa07cdb26f7f0585e6453f686ac},
affiliation={GE Aviation Aerodynamics and Instrumentation Engineer, Prague, Czech Republic; GE Aviation Aerodynamics Engineer, Prague, Czech Republic; GE Aviation Engineering Program Manager, Prague, Czech Republic; GE Aviation Aerodynamics Intern, Prague, Czech Republic; GE Aviation Aero/Perfo/Thermals Sub-Section Manager, Prague, Czech Republic; GE Aviation Engineering Leader, Prague, Czech Republic},
abstract={Aerobatic aircraft have become popular for the training of military pilots, and nowadays an increasing number of such airframes are being developed. Modern turboprop engines provide high performance allowing the pilots to get similar handling characteristics to military jet aircraft engines. Prior to the availability of high performance turboprops, the basic pilot training was conducted using jet aircraft. Furthermore, the introduction of electronic control systems on last-generation turboprop engines enables single lever control, making it an ideal candidate for this type of aerobatic and training airframes. This new type of engine operation is however accompanied by several challenges from the point of view of the engine design and installation aerodynamics. GEAC has gone through a complex design process, in cooperation with the airframer, to validate the design of a new aerobatic aircraft inlet in the context of developing an aerobatic version of the H80 engine. In order to ensure a) surge-free operation, b) optimal engine performance and c) effective ice/FOD separation in inclement weather conditions and during any kind of aerobatic maneuver, the team has done extensive CFD predictions of the flow behaviour, performance/operability studies and finally a ground test campaign. First, a back-to-back comparison of the aerobatic inlet geometry versus a reference commuter inlet geometry was conducted. Then, flight conditions were simulated in calm and crosswind environments. Distortion patterns were examined using in-house developed tools and the diverse sources of distortion were identified. One of the results is the introduction of geometry improvements to guarantee improved performance and extended engine operability range. Advanced propeller modeling techniques were introduced and benchmarked in order to have the most exact representation of the propeller aerodynamic effect on inlet flow. Finally, a test campaign was conducted for validation purposes. An exhaustive instrumentation, data acquisition system and detailed test program were developed to validate CFD methods and assumptions made during the design phase, and to raise our confidence in the flight conditions simulation results. Copyright © 2018 ASME.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Soltani2018325,
author={Soltani, M. and Derakhshanfar, P. and Panichella, A. and Devroey, X. and Zaidman, A. and van Deursen, A.},
title={Single-objective versus multi-objectivized optimization for evolutionary crash reproduction},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={11036 LNCS},
pages={325-340},
doi={10.1007/978-3-319-99241-9_18},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053140680&doi=10.1007%2f978-3-319-99241-9_18&partnerID=40&md5=3d5491984774a6dcd1b1e29e4a146752},
affiliation={Delft University of Technology, Delft, Netherlands},
abstract={EvoCrash is a recent search-based approach to generate a test case that reproduces reported crashes. The search is guided by a fitness function that uses a weighted sum scalarization to combine three different heuristics: (i) code coverage, (ii) crash coverage and (iii) stack trace similarity. In this study, we propose and investigate two alternatives to the weighted sum scalarization: (i) the simple sum scalarization and (ii) the multi-objectivization, which decomposes the fitness function into several optimization objectives as an attempt to increase test case diversity. We implemented the three alternative optimizations as an extension of EvoSuite, a popular search-based unit test generator, and applied them on 33 real-world crashes. Our results indicate that for complex crashes the weighted sum reduces the test case generation time, compared to the simple sum, while for simpler crashes the effect is the opposite. Similarly, for complex crashes, multi-objectivization reduces test generation time compared to optimizing with the weighted sum; we also observe one crash that can be replicated only by multi-objectivization. Through our manual analysis, we found out that when optimizing the original weighted function gets trapped in local optima, optimization for decomposed objectives improves the search for crash reproduction. Generally, while multi-objectivization is under-explored, our results are promising and encourage further investigations of the approach. © Springer Nature Switzerland AG 2018.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{PereiradeSousa2018,
author={Pereira de Sousa, B.F.},
title={Engaging students in the evaluation process using co-creation and technology enhanced learning (CC-TEL)},
journal={CEUR Workshop Proceedings},
year={2018},
volume={2190},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053126466&partnerID=40&md5=b8c3efd6f3d9b4f0e701a8e209bd6c3f},
affiliation={ESPAMOL, Rua Carlos Boto, Lagoa, 8400999, Portugal},
abstract={Evaluating is a complex procedure, especially in education. The main aspect of assessment, according to some studies and Portuguese law, should be formative assessment, helping the student and the teacher evaluate and adapt the process to promote educative success. Classic methods have the teacher as creator of the quizzes and tests that the students resolve along the school year, and use those to ascertain the knowledge attained. That distances students from the evaluation process and from becoming engaged in their own success. This case study used co-creation of the quizzes and tests using learning enhancing software like Kahoot! and self-evaluation using shared e-docs, to bring the student to the center stage of the assessment process and giving the teacher another set of tools to assess the knowledge obtained by the student, as well as providing him new data and feedback to evaluate his teaching practice. The gamification of part of the education process using technology as the focus and enhancer, engaged the diverse partners in a more regulated assessment procedure, leaning to more self-conscious and self-critic students, involved in several parts of their own evaluation. © 2018 CEUR-WS. All Rights Reserved.},
author_keywords={Co-creation;  E-docs;  Evaluation;  Kahoot!;  Self-evaluation;  Test},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yan2018600,
author={Yan, Y. and Huang, S.-Y.},
title={Protein-protein docking with improved shape complementarity},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10954 LNCS},
pages={600-605},
doi={10.1007/978-3-319-95930-6_60},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051871306&doi=10.1007%2f978-3-319-95930-6_60&partnerID=40&md5=7e1ef1c46574f37daa639118c3747862},
affiliation={Institute of Biophysics, School of Physics, Huazhong University of Science and Technology, Wuhan, Hubei  430074, China},
abstract={Protein-protein docking is a useful computational tool for predicting the complex structure and interaction between proteins. As the most basic ingredient of scoring functions, shape complementarity plays a critical role in protein-protein docking. In this study, we have presented a new pairwise scoring function to consider long-range interactions in shape complementarity (LSC) for protein-protein docking. Our docking program with LSC was tested on the protein docking benchmark 4.0 of 176 diverse protein-protein complexes, and compared with four other shape-based docking approaches, ZDOCK2.1, MolFit/G, GRAMM, and FTDock/G. It was shown that our LSC significantly improved the docking performance in binding mode predictions in both success rate and number of hits per complex, compared to the other four approaches. The software is freely available as part of our HDOCK web server at http://hdock.phys.hust.edu.cn/. © 2018, Springer International Publishing AG, part of Springer Nature.},
author_keywords={Fast-fourier transformation;  Protein- protein interactions;  Protein-protein docking;  Scoring function;  Shape complementarity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20181,
author={Zhang, T. and Wang, Y. and Yang, T. and Lu, J. and Liu, B.},
title={Design and implementation of an evaluation platform for NDN name lookup algorithms},
journal={Qinghua Daxue Xuebao/Journal of Tsinghua University},
year={2018},
volume={58},
number={1},
pages={1-7},
doi={10.16511/j.cnki.qhdxxb.2018.22.001},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049858286&doi=10.16511%2fj.cnki.qhdxxb.2018.22.001&partnerID=40&md5=da2ffd11df93367922ae7d0195a3d1e3},
affiliation={Department of Computer Science and Technology, Tsinghua University, Beijing, 100084, China; Huawei Future Network Theory Laboratory, Hong Kong, 999072, Hong Kong; School of Electronics Engineering and Computer Science, Peking University, Beijing, 100871, China},
abstract={Many name lookup algorithms have been proposed for named data networking (NDN). These algorithms need to be evaluated based on their reachable speed, scalability, and update performance. However, NDN is still in the research stage so there are no large NDN networks and no large real name routing tables or NDN traffic. This paper presents a software test platform, NDNBench, to evaluate, compare and test different name lookup algorithms. NDNBench consists of a seed forwarding information base (FIB) analyzer, an FIB generator, a name trace generator and an update generator. Tests show that the name table and traffic characteristics greatly influence the NDN name lookup algorithm performance. The platform extracts these features, forms quantifiable parameters and provides them to the user. The parameters of NDNBench can be adjusted to obtain various FIBs and traces with structure and size diversity to test the lookup algorithms. This paper also evaluates some existing name lookup algorithms. © 2018, Tsinghua University Press. All right reserved.},
author_keywords={Name lookup;  Named data networking (NDN);  Performance evaluation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cappelen2018479,
author={Cappelen, B. and Andersson, A.-P.},
title={The health promoting potential of interactive art},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10897 LNCS},
pages={479-483},
doi={10.1007/978-3-319-94274-2_70},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049807430&doi=10.1007%2f978-3-319-94274-2_70&partnerID=40&md5=5a3ba288b8b2ac66e26487bdcf5ab64f},
affiliation={Oslo School of Architecture and Design, Oslo, Norway; Norwegian University of Science and Technology, Gjøvik, Norway},
abstract={In this paper, we argue for the value of participatory and interactive art, to increase the quality of health and health promoting technology, for children with special needs. UN states through several conventions that everyone has a right to take part in art and cultural experiences, also children and people with disabilities, because art is an important value in our society. With technology, we can make art accessible to people with special needs in completely new ways. By building on the participatory art tradition, in combination with new technology, we can develop new forms of expression and groundbreaking experiences. By incorporating knowledge about health promotion and universal design, we can create new health promoting technology and artistic empowering experiences, by making them more engaging, inspiring and participating for children with special needs. This opportunity has in too little extent, been recognized within Assistive Technology. The paper is based on our research and experience from testing an interactive art installation, with children with special needs at six schools within the Norwegian national school art program. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={Art;  Design for diversity;  Health promoting technology},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jianqi2018355,
author={Jianqi, S. and Yanhong, H. and Ang, L. and Fangda, C.},
title={An optimal solution for software testing case generation based on particle swarm optimization},
journal={Open Physics},
year={2018},
volume={16},
number={1},
pages={355-363},
doi={10.1515/phys-2018-0048},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049182557&doi=10.1515%2fphys-2018-0048&partnerID=40&md5=2e283e969dbbc4b0e025bd3b9707a218},
affiliation={National Trusted Embedded Software Engineering Technology Research Center, East China Normal University, Shanghai, 200062, China},
abstract={Searching based testing case generation technology converts the problem of testing case generation to function optimizations, through a fitness function, which is usually optimized using heuristic search algorithms. The particle swarm optimization (PSO) optimized testing case generation algorithm tends to lose population diversity of locally optimal solutions with low accuracy of local search. To overcome the above defects, a self-adaptive PSO based software testing case optimization algorithm is proposed. It adjusts the inertia weight dynamically according to the current iteration and average relative speed, to improve the performance of standard PSO. An improved alternating variable method is put forward to accelerate local search speed, which can coordinate both global and local search ability thereby improving the overall generation efficiency of testing cases. The experimental results demonstrate that the approach outlined here keeps higher testing case generation efficiency, and it shows certain advantages in coverage, evolution generation amount and running time when compared to standard PSO and GA-PSO. © 2018 Shi Jianqi et al.},
author_keywords={inertia;  instrumentation;  local search;  PSO;  testing case},
document_type={Article},
source={Scopus},
}

@ARTICLE{Asoudeh2018199,
author={Asoudeh, N. and Labiche, Y.},
title={Life sciences-inspired test case similarity measures for search-based, FSM-based software testing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10890 LNCS},
pages={199-215},
doi={10.1007/978-3-319-92997-2_13},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048889966&doi=10.1007%2f978-3-319-92997-2_13&partnerID=40&md5=a9b54fe13851b85f24908d3ef904ec51},
affiliation={Carleton University, Ottawa, ON  K1S5B6, Canada},
abstract={Researchers and practitioners alike have the intuition that test cases diversity is positively correlated to fault detection. Empirical results already show that some measurement of diversity within a pre-existing state-based test suite (i.e., a test suite not necessarily created to have diverse tests in the first place) indeed relates to fault detection. In this paper we show how our procedure, based on a genetic algorithm, to construct an entire (all-transition) adequate test suite with as diverse tests as possible fares in terms of fault detection. We experimentally compare on a case study nine different ways of computing test suite diversity, including measures already used by others in software testing as well as measures inspired by the notion of diversity in the life sciences. Although our results confirm a positive correlation between diversity and fault detection, we believe our results raise more questions than they answer about the notion and measurement of test suite diversity, which leads us to argue that more work needs to be dedicated to this topic. © 2018, Springer International Publishing AG, part of Springer Nature.},
author_keywords={Fault detection;  FSM;  State-based testing;  Test suite diversity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nurmuradov2018459,
author={Nurmuradov, D. and Bryce, R. and Piparia, S. and Bryant, B.},
title={Clustering and Combinatorial Methods for Test Suite Prioritization of GUI and Web Applications},
journal={Advances in Intelligent Systems and Computing},
year={2018},
volume={738},
pages={459-466},
doi={10.1007/978-3-319-77028-4_60},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045847744&doi=10.1007%2f978-3-319-77028-4_60&partnerID=40&md5=db606f4db52dc9864d7e9759f614e2ad},
affiliation={University of North Texas, Denton, TX, United States},
abstract={This work introduces a novel test case prioritization method that combines clustering methods, dimensionality reduction techniques (DRTs), and combinatorial-based two-way prioritization for GUI and web applications. The use of clustering with interleaved cluster prioritization increases the diversity of the earliest selected test cases. The study applies four DRTs, four clustering algorithms, and three inter-cluster ranking methods to three GUI and one web applications in order to determine the best combination of methods. We compare the proposed clustering and dimensionality reduction approaches to random and two-way inter-window prioritization techniques. The outcome of the study indicates that the Principal Component Analysis (PCA) dimensionality reduction technique and Mean Shift clustering method outperform other techniques. There is no statistical difference between the three inter-cluster ranking criteria. In comparison to two-way inter-window prioritization, the Mean Shift clustering algorithm with PCA or Independent Component Analysis (FICA) generally produces faster rates of fault detection in the studies. © 2018, Springer International Publishing AG, part of Springer Nature.},
author_keywords={Cluster prioritization method;  Dimensionality reduction approach;  Graphical user interface;  Inter-cluster ranking method;  Test suite prioritization;  User session-based test},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Semeráth2018227,
author={Semeráth, O. and Varró, D.},
title={Iterative generation of diverse models for testing specifications of DSL tools},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2018},
volume={10802 LNCS},
pages={227-245},
doi={10.1007/978-3-319-89363-1_13},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045672500&doi=10.1007%2f978-3-319-89363-1_13&partnerID=40&md5=b52fbec6466d41d83abf7865c090d7b4},
affiliation={MTA-BME Lendület Cyber-Physical Systems Research Group, Budapest, Hungary; Department of Measurement and Information Systems, Budapest University of Technology and Economics, Budapest, Hungary; Department of Electrical and Computer Engineering, McGill University, Montreal, Canada},
abstract={The validation of modeling tools of custom domain-specific languages (DSLs) frequently relies upon an automatically generated set of models as a test suite. While many software testing approaches recommend that this test suite should be diverse, model diversity has not been studied systematically for graph models. In the paper, we propose diversity metrics for models by exploiting neighborhood shapes as abstraction. Furthermore, we propose an iterative model generation technique to synthesize a diverse set of models where each model is taken from a different equivalence class as defined by neighborhood shapes. We evaluate our diversity metrics in the context of mutation testing for an industrial DSL and compare our model generation technique with the popular model generator Alloy. © The Author(s) 2018.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shih2018232,
author={Shih, H.-Y. and Lu, H.-L. and Yeh, C.-C. and Hsiao, H.-C. and Huang, S.-K.},
title={A generic web application testing and attack data generation method},
journal={Advances in Intelligent Systems and Computing},
year={2018},
volume={733},
pages={232-247},
doi={10.1007/978-3-319-76451-1_22},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045339050&doi=10.1007%2f978-3-319-76451-1_22&partnerID=40&md5=197c6c40697456aca9cdf74c1439a649},
affiliation={Department of Computer Science, National Chiao Tung University, Hsinchu, 300, Taiwan; Information Technology Service Center, National Chiao Tung University, Hsinchu, 300, Taiwan; Computational Intelligence Technology Center, Industrial Technology Research Institute, Hsinchu, 300, Taiwan; Department of Computer Science and Information Engineering, National Taiwan University, Taipei, Taiwan},
abstract={With the advances of diversified online services, there is an increasing demand for web applications. However, most web applications contain critical bugs affecting their security, allowing unauthorized access and remote code execution. It is challenging for programmers to identify potential vulnerabilities in their applications before releasing the service due to the lack of resources and security knowledge, and thus such hidden defects may remain unnoticed for a long time until being reported by users or third-party risk exposure. In this paper, we develop an automated detection method to support timely and flexible discovery of a wide variety of vulnerability types in web applications. The key insight of our work is adding a lightweight detecting sensor that differentiates attack types before performing symbolic execution. Based on the technique of symbolic execution, our work generates testing and attack data by tracking the address of program instruction and checking the arguments of dangerous functions. Compared to prior analysis tools that also use symbolic execution, our work flexibly supports the detection of more types of web attacks and improve system flexibility for users thanks to the detecting sensor. We have evaluated our solution by applying this detecting process to several known vulnerabilities on open-source web applications and CTF (Capture The Flag) problems, and detected various types of web attacks successfully. © Springer International Publishing AG, part of Springer Nature 2018.},
author_keywords={Capture The Flag;  Software vulnerability;  Symbolic execution;  Web application testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Akbulut2018677,
author={Akbulut, A. and Karataş, G.},
title={Automated testing for distributed databases with fuzzy fragment reallocation},
journal={Turkish Journal of Electrical Engineering and Computer Sciences},
year={2018},
volume={26},
number={2},
pages={677-692},
doi={10.3906/elk-1701-135},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045011397&doi=10.3906%2felk-1701-135&partnerID=40&md5=e528a6b16b557d629a6e427d5fe2024e},
affiliation={Department of Computer Engineering, Faculty of Engineering, Istanbul Kültür University, Istanbul, Turkey; Department of Mathematics and Computer Science, Faculty of Science and Letters, Istanbul Kültür University, Istanbul, Turkey},
abstract={As the Internet and big data become more widespread, relational database management systems (RDBMSs) become increasingly inadequate for web applications. To provide what RDBMSs cannot, during the past 15 years distributed database systems (DDBSs) have thus emerged. However, given the complicated structure of these systems, the methods used to validate the efficiency of databases, all towards ensuring quality and security, have become diversified. In response, this paper demonstrates a system for performing automated testing with DDBSs, given that testing is significant in software verification and that accredited systems are more productive in business environments. The proposed system applies several tests to MongoDB-based NoSQL databases to evaluate their instantaneous conditions, such as average query response times and fragment utilizations, and, if necessary, suggest improvements for indexes and fragment deployment. Within this context, autogenerated data, replica, meta, system, fragment, and index tests are applied. Clearly, the system's most important feature is its fuzzy logic-enabled fragment reallocation module, which allows the creation and application of reallocation strategies that account for data changes in query frequency. © TUBITAK.},
author_keywords={Distributed database testing;  Empirical software validation;  Fragment reallocation;  Fuzzy logic;  MongoDB;  NoSQL testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2018107,
author={Chen, J. and Zhu, L. and Chen, T.Y. and Towey, D. and Kuo, F.-C. and Huang, R. and Guo, Y.},
title={Test case prioritization for object-oriented software: An adaptive random sequence approach based on clustering},
journal={Journal of Systems and Software},
year={2018},
volume={135},
pages={107-125},
doi={10.1016/j.jss.2017.09.031},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032007990&doi=10.1016%2fj.jss.2017.09.031&partnerID=40&md5=b657182eefc5bdd268044b5beca3b75f},
affiliation={School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, 202000, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, 3122, Australia; School of Computer Science, University of Nottingham Ningbo China, Ningbo, 315100, China},
abstract={Test case prioritization (TCP) attempts to improve fault detection effectiveness by scheduling the important test cases to be executed earlier, where the importance is determined by some criteria or strategies. Adaptive random sequences (ARSs) can be used to improve the effectiveness of TCP based on white-box information (such as code coverage information) or black-box information (such as test input information). To improve the testing effectiveness for object-oriented software in regression testing, in this paper, we present an ARS approach based on clustering techniques using black-box information. We use two clustering methods: (1) clustering test cases according to the number of objects and methods, using the K-means and K-medoids clustering algorithms; and (2) clustered based on an object and method invocation sequence similarity metric using the K-medoids clustering algorithm. Our approach can construct ARSs that attempt to make their neighboring test cases as diverse as possible. Experimental studies were also conducted to verify the proposed approach, with the results showing both enhanced probability of earlier fault detection, and higher effectiveness than random prioritization and method coverage TCP technique. © 2017 Elsevier Inc.},
author_keywords={Adaptive random sequence;  Cluster analysis;  Object-oriented software;  Test cases prioritization;  Test cases selection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gruzenkin2018293,
author={Gruzenkin, D.V. and Chernigovskiy, A.S. and Tsarev, R.Y.},
title={N-version Software Module Requirements to Grant the Software Execution Fault-Tolerance},
journal={Advances in Intelligent Systems and Computing},
year={2018},
volume={661},
pages={293-303},
doi={10.1007/978-3-319-67618-0_27},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029597236&doi=10.1007%2f978-3-319-67618-0_27&partnerID=40&md5=89a5b71bd1cc4d4ec8f42e5e968e2da2},
affiliation={Department of Informatics, Siberian Federal University, Kirenskogo. 26B, Krasnoyarsk, 660074, Russian Federation},
abstract={N-version programming is one of the approach ensuring high reliability and fault-tolerance of software on the basis of program redundancy and diversity. This approach ensures that faults of one of the versions of an N-version software module will not result in malfunction of the module operation process. N-version software realization, as a rule, depends upon capacities and preferences of the teams of designers and developers. This work is an attempt to denote basic requirements, which should be met at the design of N-version software to minimize the occurrence of possible program faults and influence of the modules versions on one another. The requirements to versions (program modules) of N-version software allow to ensure high-level reliability and fault-tolerance due to the elimination of the possible influence of separate versions on each other. A special attention has been paid to their interaction, which should not have any impact on the operation of the other components. For realization and research of N-version software developed taking into account the defined requirements an N-version software execution environment has been developed. Testing of the N-version software execution environment has demonstrated expediency of a component architecture application and high efficiency of N-version programming as a method of fault-tolerant software development. © 2018, Springer International Publishing AG.},
author_keywords={Execution environment;  N-version software;  Requirements;  Software reliability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{SáSousa2018143,
author={Sá Sousa, J. and Vilela, J.P.},
title={Uncoordinated Frequency Hopping for Wireless Secrecy Against Non-Degraded Eavesdroppers},
journal={IEEE Transactions on Information Forensics and Security},
year={2018},
volume={13},
number={1},
pages={143-155},
doi={10.1109/TIFS.2017.2737963},
art_number={8006315},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028988726&doi=10.1109%2fTIFS.2017.2737963&partnerID=40&md5=88a3e9a1e69570233b5c3d356fcd917a},
affiliation={CISUC, Department of Informatics Engineering, University of Coimbra, Coimbra, Portugal; LCA1 Laboratory, École Polytechnique Fédérale de Lausanne, Lausanne, 1015, Switzerland},
abstract={Current physical-layer security techniques typically rely on a degraded eavesdropper, thus warranting some sort of advantage that can be relied upon to achieve higher levels of security. We consider instead non-degraded eavesdroppers that possess equal or better capabilities than legitimate receivers. Under this challenging setup, most of the current physical-layer security techniques become hard to administer and new dimensions to establish advantageous periods of communication are needed. For that, we consider employing a spread spectrum uncoordinated frequency hopping (UFH) scheme aided by friendly jammers for improved secrecy. We characterize the secrecy level of this spread spectrum scheme, by devising a stochastic geometry mathematical model to assess the secure packet throughput (probability of secure communication) of devices operating under UFH that accommodates the impact of friendly jammers. We further implement and evaluate these techniques in a real-world test-bed of software-defined radios. Results show that although UFH with jamming leads to low secure packet throughput values, by exploiting frequency diversity, these methods may be used for establishing secret keys. We propose a method for secret-key establishment that builds on the advantage provided by UFH and jamming to establish secret keys, notably against non-degraded adversary eavesdroppers that may appear in advantageous situations. © 2005-2012 IEEE.},
author_keywords={jamming;  non-degraded eavesdroppers;  Physical layer security;  secret-key agreement;  uncoordinated frequency hopping},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Morgan20171,
author={Morgan, J. and Lekivetz, R. and Donnelly, T.},
title={Covering arrays: Evaluating coverage and diversity in the presence of disallowed combinations},
journal={2017 IEEE 28th Annual Software Technology Conference, STC 2017},
year={2017},
volume={2017-January},
pages={1-4},
doi={10.1109/STC.2017.8234455},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049246636&doi=10.1109%2fSTC.2017.8234455&partnerID=40&md5=65a011816743804ad8f69323fd683b11},
affiliation={JMP Division, SAS Institute Incorporated, Cary, NC, United States},
abstract={Test engineers are often faced with the challenge of selecting test cases that maximize the chance of discovering faults while working with a limited budget. Combinatorial testing is an effective test case selection strategy to address this challenge. The basic idea is to select test cases that ensure that all possible combinations of settings from two (or more) inputs are accounted for, regardless of which subset of two (or more) inputs are selected. Currently, combinatorial testing usually implies a covering array as the underlying mathematical construct. Yet, despite their demonstrated utility, practitioners sometimes encounter challenges that impede their use. For example, given a covering array with constraints on allowed combinations of settings for some subset of inputs, it is often unclear how to assess the coverage and diversity [2] properties of the resulting covering array. © 2017 IEEE.},
author_keywords={covering array;  disallowed combinations;  forbidden configurations;  software testing;  systems testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pastore20171,
author={Pastore, T. and Galdorisi, G. and Jones, A.},
title={Command and Control (C2) to enable multi-domain teaming of unmanned vehicles (UxVs)},
journal={OCEANS 2017 - Anchorage},
year={2017},
volume={2017-January},
pages={1-7},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048184683&partnerID=40&md5=bd409c132b856787385a8a3dc605adfc},
affiliation={Intelligence, Surveillance, Reconnassaince Dept, SPAWAR Systems Center Pacific, San Diego, United States; Strategic and Business Planning Dept, SPAWAR Systems Center Pacific, San Diego, United States},
abstract={In his best-selling book, War Made New, military historian Max Boot supports his thesis with historical examples to show how technological-driven 'Revolutions in Military Affairs' have transformed warfare and altered the course of history. The U.S. military has embraced a wave of technological change that has constituted a true transformation of the way that military forces will fight in the 21st Century. One of the most transformational technologies adapted for military use is unmanned and autonomous systems. The expanding use of these systems for civilian and military applications has increased dramatically over the past decade. This should come as no surprise, as these systems represent one of the most rapidly growing areas of innovative technology adoption. In the military trade space, the use of military unmanned systems (UxS) is already creating strategic, operational, and tactical possibilities that did not exist a decade ago. These systems are not only changing the face of modern warfare, but are also altering the process of decision-making in combat operations. Indeed, it has been argued that the rise in drone warfare is changing the way we conceive of and define 'warfare' itself. However, while these unmanned systems are of enormous value today and are evolving to deliver better capabilities to the warfighter, it is their promise for the future that causes the most excitement. Indeed, these systems have created substantial buzz in policy, military, industry and academic circles. One of the most cutting-edge and challenging aspects of autonomous systems is enabling systems that operate in different domains - Unmanned Aerial Vehicles (UAVs), Unmanned Surface Vehicles (USVs), and Unmanned Underwater Vehicles (UUVs) - work together as a heterogeneous whole. As autonomous systems become more important to military operators, and especially as they are used for more diverse and complex missions, the issue of command and control (C2) of cross-domain unmanned vehicles (UxVs) will become more important. Today, while the performance of UxV in all domains has improved dramatically, the C2 issues of controlling UxVs in multiple domains simultaneously remains an area requiring additional research, modeling and simulation, and operational testing. This paper will present the results of operational testing of cross-domain UxVs in The Technical Cooperation Program (five-eyes) Hell Bay 4 experiment during international exercise Unmanned Warrior 2016 at the British Underwater Test & Evaluation Centre (BUTEC), in the Kyle of Lochalsh, Scotland, United Kingdom. The primary objectives of this experiment focused on cooperative teaming of a UAV with UUVs to demonstrate extended range C2 of remotely deployed UUVs in a contested littoral environment. One of the most promising results of this experiment was the ability of the test UAV to travel for one hour at twenty-five miles per hour while carrying a ten-pound payload. This UAV (a United States Vapor 55) was thus able to transit from one ship to the other, autonomously take off and land, and recharge when necessary. This capability demonstrates the potential for these UAVs to operate from ships in international waters (outside a nation's twelve mile territorial sea) and have enough endurance to conduct missions ashore. The undersea portion of the experiment was equally promising, with the operational team able to effectively control Iver2 and Iver3 UUVs - as well as the UAV - via the ONR CaSHMI control station. Of note, we were able to successfully relay data between UUVs and the UAV at several miles separation. Finally, this paper will present the details of proposed future experimentation and provide on-ramps for industry, academia, military and allied partners to participate in future events. The presenting author will provide appropriate points of contact at the various organizations that participated in The Technical Cooperation Program Hell Bay 4 experiment during international exercise Unmanned Warrior 2016, and will suggest ways of other parties can join future experiments. © 2017 Marine Technology Society.},
author_keywords={autonomy;  command and control;  cross-domain;  multi domain;  unmanned systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Moharikar201745,
author={Moharikar, P. and Guddeti, J.},
title={Automated test generation for post silicon microcontroller validation},
journal={2017 IEEE International High Level Design Validation and Test Workshop, HLDVT 2017},
year={2017},
volume={2017-January},
pages={45-52},
doi={10.1109/HLDVT.2017.8167462},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043468463&doi=10.1109%2fHLDVT.2017.8167462&partnerID=40&md5=dc0b20ce38f1a7f1d9e0f7cf0d1169e8},
affiliation={Automotive Microcontroller, Infineon Technologies, Bangalore, India},
abstract={Post Silicon Validation is critical step in order to deliver quality microcontroller chips to customers but is increasingly becoming complex and time consuming process as the design size is increasing. Due to increased number & diversity of design intellectual property, microcontroller post silicon validation has moved towards customized validation concept and hardware setup for individual design block. Two major challenges faced by validation team are to generate sufficient number of system level test-cases that can cover wide range of configuration and second to ensure quality & stability of these test-cases throughout validation cycle. This paper describes challenges in test-case generation and factors that impact post silicon test content stability. This paper presents a method to auto generate system level test-cases and to improve test content stability. The proposed technique was deployed in validating advanced driver assistance systems in next generation of automotive microcontroller. © 2017 IEEE.},
author_keywords={ADAS;  Automation;  Coverage;  Test content;  Validatio},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Temizkan2017828,
author={Temizkan, O. and Park, S. and Saydam, C.},
title={Software diversity for improved network security: Optimal distribution of software-based shared vulnerabilities},
journal={Information Systems Research},
year={2017},
volume={28},
number={4},
pages={828-849},
doi={10.1287/isre.2017.0722},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038097761&doi=10.1287%2fisre.2017.0722&partnerID=40&md5=064f9bd3dbcefb1f5c822c1410eaceac},
affiliation={Faculty of Business, Ozyegin University, Cekmekoy, Istanbul, 34794, Turkey; Belk College of Business, University of North Carolina at Charlotte, Charlotte, NC  28223, United States},
abstract={Firms, and other agencies, tend to adopt widely used software to gain economic benefits of scale, which can lead to a software monoculture. This can, in turn, involve the risk of correlated computer systems failure as all systems on the network are exposed to the same software-based vulnerabilities. Software diversity has been introduced as a strategy for disrupting such a monoculture and ultimately decreasing the risk of correlated failure. Nevertheless, common vulnerabilities can be shared by different software products. We thus expand software diversity research here and consider shared vulnerabilities between different software alternatives. We develop a combinatorial optimization model of software diversity on a network in an effort to identify the optimal software distribution that best improves network security. We also develop a simulation model of virus propagation based on the susceptible-infected-susceptible model. This model allows calculation of the epidemic threshold, a measure of network resilience to virus propagation. We then test the effectiveness of the proposed software diversity strategies against the spreading of viruses through a series of experiments. © © 2017 INFORMS.},
author_keywords={Combinatorial optimization;  Epidemic spreading;  Epidemic threshold;  Network security;  Shared vulnerabilities;  Simulation;  Software diversity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20171193,
author={Zhang, G. and Su, Z. and Li, M. and Yue, F. and Jiang, J. and Yao, X.},
title={Constraint handling in NSGA-II for solving optimal testing resource allocation problems},
journal={IEEE Transactions on Reliability},
year={2017},
volume={66},
number={4},
pages={1193-1212},
doi={10.1109/TR.2017.2738660},
art_number={8023833},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029154192&doi=10.1109%2fTR.2017.2738660&partnerID=40&md5=cb0c02be944b5177e110134080dde0de},
affiliation={School of Computer and Information, Hefei University of Technology, Hefei, 230009, China; CERCIA, School of Computer Science, University of Birmingham, Birmingham, B15 2TT, United Kingdom; Shenzhen Key Lab of Computational Intelligence, Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, 518055, China},
abstract={In software testing, optimal testing resource allocation problems (OTRAPs) are important when seeking a good tradeoff between reliability, cost, and time with limited resources. There have been intensive studies of OTRAPs using multiobjective evolutionary algorithms (MOEAs), but little attention has been paid to the constraint handling. This paper comprehensively investigates the effect of the constraint handling on the performance of nondominated sorting genetic algorithm II (NSGA-II) for solving OTRAPs, from both theoretical and empirical perspectives. The heuristics for individual repairs are first proposed to handle constraint violations in NSGA-II, based on which several properties are derived. Additionally, the Z-score based Euclidean distance is adopted to estimate the difference between solutions. Finally, the above methods are evaluated and the experiments show several results. 1) The developed heuristics for constraint handling are better than the Existing Strategy in terms of the capacity and coverage values. 2) The Z-score operation obtains better diversity values and reduces repeated solutions. 3) The modified NSGA-II for OTRAPs (called NSGA-II-TRA) performs significantly better than the existing MOEAs in terms of capacity and coverage values, which suggests that NSGA-II-TRA could obtain more and higher quality testing-time-allocation schemes, especially for large, complex datasets. 4) NSGA-II-TRA is robust according to the sensitivity analysis results. © 2017 IEEE.},
author_keywords={Constraint handling;  Heuristics;  Multiobjective optimization;  Software reliability;  Testing-resource allocation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Barazzetta20172029,
author={Barazzetta, M. and Micheli, D. and Bastianelli, L. and Diamanti, R. and Totta, M. and Obino, P. and Lattanzi, R. and Moglie, F. and Primiani, V.M.},
title={A Comparison between Different Reception Diversity Schemes of a 4G-LTE Base Station in Reverberation Chamber: A Deployment in a Live Cellular Network},
journal={IEEE Transactions on Electromagnetic Compatibility},
year={2017},
volume={59},
number={6},
pages={2029-2037},
doi={10.1109/TEMC.2017.2657122},
art_number={7847417},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012226149&doi=10.1109%2fTEMC.2017.2657122&partnerID=40&md5=c0dce7407cdf1eaa9ebe2574f66bee85},
affiliation={Nokia Networks Italia, Vimercate, 20871, Italy; Telecom Italia Mobile S.p.A., Rome, 00148, Italy; Dipartimento di Ingegneria dell'Informazione, Università Politecnica Delle Marche, Ancona, 60131, Italy; Telecom Italia Mobile S.p.A., Ancona, 60131, Italy},
abstract={The uplink performance of a real fourth-generation long-term-evolution (LTE) frequency-division multiplexing base station was observed by adopting a reverberation chamber as propagating environment. In the downlink direction, the reception of the LTE signal is limited to mobile station receivers. On the other hand, different reception schemes could be implemented by the base station in the uplink direction. Interference rejection and coordinated multipoint reception criteria are based on spatial diversity and are analyzed in a rich multipath environment. These options could be susceptible to impairments due to the presence of Gaussian noise, also including the more realistic case of a discontinuous noise source. The testing session was carried out under a collaboration program between TIM S.p.A., Nokia, and Università Politecnica delle Marche, and it ended up by introducing envisioned reception solutions of the base station in a live customer's cellular network. © 1964-2012 IEEE.},
author_keywords={Base station (BS);  block error rate (BLER);  cooperative multiple input multiple output;  coordinated multipoint (CoMP);  fourth-generation long-term evolution (4G LTE);  key performance indicators (KPIs);  live radio access network;  modulation and coding scheme (MCS);  reverberation chambers (RCs);  signal-to-interference-plus-noise ratio (SINR);  throughput (TP);  transport format indication (TFI)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Aparna2017299,
author={Aparna, M. and Vineetha, P.V. and Kirthiga, S.},
title={Channel modeling and estimation of polarized MIMO for land mobile satellite systems},
journal={2017 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2017},
year={2017},
volume={2017-January},
pages={299-304},
doi={10.1109/ICACCI.2017.8125857},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042793483&doi=10.1109%2fICACCI.2017.8125857&partnerID=40&md5=63d5b0a7d90fdba0dcb7eb2246616ce8},
affiliation={Department of Electronics and Communication Engineering, Amrita School of Engineering, Coimbatore, India; Amrita Vishwa Vidyapeetham, Amrita University641 112, India},
abstract={Spectral efficiency and capacity of Land Mobile Satellite (LMS) systems can be enhanced by using MIMO (Multiple Input Multiple Output). Polarization diversity is an efficient technique to realize MIMO in LMS communications. Polarization diversity employs different polarization to realize multiple independent propagation paths. Dual polarized antennas offer a space and cost effective alternative compared to spatially separated antennas with different polarization. Design of communication systems can be facilitated using channel modeling. Dual polarized MIMO-LMS channel is complex with many factors influencing the channel. The ionospheric, tropospheric effects along with mobility of the user equipment are some of the parameters that make the LMS channel model unique.Widely used dual polarized LMS MIMO channel model Loo is analyzed. Loo is a statistical model which can analyze statistical parameters of the channel. Thus the models have best captured the ionospheric, tropospheric and fading effects for a LMS system. Critical channel parameters like Scatter plot, Auto correlation function (ACF), Doppler spectrum, Correlation coefficient and Outage capacity has been analyzed and a coding scheme that jointly utilizes space, time and polarization diversities is considered for better reliability. LS (Least Squares) and MMSE (Minimum Mean Squared Error) techniques are used to perform channel estimation and the hardware testing has been done using GNU Radio and USRP (Universal Software Radio Peripheral). © 2017 IEEE.},
author_keywords={Loo;  Polarization Diversity;  Polarized MIMO;  USRP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hu201754,
author={Hu, J. and Chen, Z. and Cai, H. and Liu, X. and Fei, X. and Jiang, L.},
title={Semantic Annotation and Retrieval Approach for Historical Testcases},
journal={Proceedings - 14th IEEE International Conference on E-Business Engineering, ICEBE 2017 - Including 13th Workshop on Service-Oriented Applications, Integration and Collaboration, SOAIC 207},
year={2017},
pages={54-61},
doi={10.1109/ICEBE.2017.18},
art_number={8119130},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041669827&doi=10.1109%2fICEBE.2017.18&partnerID=40&md5=5c21659fb945c3d9f243955588bcc62d},
affiliation={School of Software, Shanghai Jiao Tong University, Shanghai, China; UM-SJTU Joint Institute, Shanghai Jiao Tong University, Shanghai, China; Jiangsu Hoperun Software Co., Ltd, Jiangsu, China; Faculty of Engineering and Computing, Conventry University, Conventry, United Kingdom},
abstract={Reusing Historical testcases play a crucial role in ensuring software testing quality. However, the diversity of historical testcases limits their potential uses. As a result, large amounts of human effort is required to write testcases for complex functional testings. In this paper, an effective framework is proposed to integrate and retrieve historical testcase bases with semantic analysis technologies. Firstly, semantic similarity is calculated to integrate the metadata of the inputted semi-structured testcases. Then, testcases are clustered by using similarity measures to eliminate heterogeneity existed in the contents of the testcases. The clustering results are added to the testcases as semantic annotations for the later semantic query. Using the semantic query interface, testers can easily obtain useful testcases without ambiguity. Finally, a case study demonstrates the effectiveness and scalability of this method for testcases retrieval for bank information systems testing. © 2017 IEEE.},
author_keywords={information retrieval;  semantic query;  testcases reuse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lin2017973,
author={Lin, J. and Liu, Y. and Guo, J. and Cleland-Huang, J. and Goss, W. and Liu, W. and Lohar, S. and Monaikul, N. and Rasin, A.},
title={TiQi: A natural language interface for querying software project data},
journal={ASE 2017 - Proceedings of the 32nd IEEE/ACM International Conference on Automated Software Engineering},
year={2017},
pages={973-977},
doi={10.1109/ASE.2017.8115714},
art_number={8115714},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041449550&doi=10.1109%2fASE.2017.8115714&partnerID=40&md5=f4b493528f46f84eb9d5abfa40ec6550},
affiliation={University of Notre Dame, South Bend, IN, United States; School of Computing, DePaul University, Chicago, United States},
abstract={Software projects produce large quantities of data such as feature requests, requirements, design artifacts, source code, tests, safety cases, release plans, and bug reports. If leveraged effectively, this data can be used to provide project intelligence that supports diverse software engineering activities such as release planning, impact analysis, and software analytics. However, project stakeholders often lack skills to formulate complex queries needed to retrieve, manipulate, and display the data in meaningful ways. To address these challenges we introduce TiQi, a natural language interface, which allows users to express software-related queries verbally or written in natural language. TiQi is a web-based tool. It visualizes available project data as a prompt to the user, accepts Natural Language (NL) queries, transforms those queries into SQL, and then executes the queries against a centralized or distributed database. Raw data is stored either directly in the database or retrieved dynamically at runtime from case tools and repositories such as Github and Jira. The transformed query is visualized back to the user as SQL and augmented UML, and raw data results are returned. Our tool demo can be found on YouTube at the following link:http://tinyurl.com/TIQIDemo. © 2017 IEEE.},
author_keywords={Natural Language Interface;  Project Data;  Query},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Verdugo201710,
author={Verdugo, P. and Salvachiua, J. and Huecas, G.},
title={An agile container-based approach to TaaS},
journal={2017 56th FITCE Congress, FITCE 2017},
year={2017},
pages={10-15},
doi={10.1109/FITCE.2017.8093000},
art_number={8093000},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041205089&doi=10.1109%2fFITCE.2017.8093000&partnerID=40&md5=e2ea01a0fb31d519f8881cd281638742},
affiliation={Grupo de Internet de Nueva Generación, DIT, ETSIT, UPM, Madrid, Spain},
abstract={Current cloud deployment scenarios imply a need for fast testing of user oriented software in diverse, heterogeneous and often unknown hardware and network environments, making it difficult to ensure optimal or reproducible in-site testing. The current paper proposes the use of container based lightweight virtualization with a ready-to-run, just-intime deployment strategy in order to minimize time and resources needed for streamlined multicomponent prototyping in PaaS systems. To that end, we will study a specific case of use consisting of providing end users with pre-tested custom prepackaged and preconfigured software, guaranteeing the viability of the aforementioned custom software, the syntactical integrity of the provided deployment system, the availability of needed dependencies as well as the sanity check of the already deployed and running software. From an architectural standpoint, by using standard, common use deployment packages as Chef or Puppet hosted in parallellizable workloads over ready-to-run Docker images, we can minimize the time required for full-deployment multicomponent systems testing and validation, as well as wrap the commonly provided features via a user-accessible RESTful API. The proposed infrastructure is currently available and freely accessible as part of the FIWARE EU initiative, and is open to third party collaboration and extension from a FOSS perspective. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Su20172447,
author={Su, S. and You, Y. and Lv, D. and Wang, H. and Qian, Y.},
title={Study on distributed measurement and control system for 12000kN marine component mechanical properties test platform},
journal={Proceedings of 2017 IEEE 2nd Advanced Information Technology, Electronic and Automation Control Conference, IAEAC 2017},
year={2017},
pages={2447-2451},
doi={10.1109/IAEAC.2017.8054463},
art_number={8054463},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034579032&doi=10.1109%2fIAEAC.2017.8054463&partnerID=40&md5=43946d7de6cc0c4f3461d6eaf42b6838},
affiliation={Nanjing University of Aeronautics and Astronautics, Nanjing, China; 28th Research Institute of China Electronics Technology Group Corporation, Nanjing, China; Zhoushan Institute of Calibration and Testing for Quality and Technology Supervision, Zhoushan, China; Jiangsu University of Science and Technology, Zhenjiang, China},
abstract={Mechanical property test of marine components has the characteristics of complicated structure, diverse testing resources, varied testing environment, high testing cost and long cycle. Firstly, the transfer function of the servo control system is established on the basis of considering the non-linear factors of the test platform. Secondly, a measurement and control system framework combining the centralized control and the distributed control is proposed. Thirdly, hardware and software design of the distributed measurement and control system for the test platform is designed. Finally, a measurement and control system of the test platform is developed, and a 2000kN tow cable tensile experiment was carried out. The results show that the force measurement error of the system is less than ± 0.5%, and the displacement measurement error is less than ± 0.1%. © 2017 IEEE.},
author_keywords={Marine components;  Measurement and control system;  Mechanical properties test;  Servo control system},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chung201765,
author={Chung, S.},
title={Object-oriented programming with DevOps},
journal={SIGITE 2017 - Proceedings of the 18th Annual Conference on Information Technology Education},
year={2017},
pages={65},
doi={10.1145/3125659.3125670},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037140186&doi=10.1145%2f3125659.3125670&partnerID=40&md5=133dce9276e66b32308455d97f459359},
affiliation={Southern Illinois University, 1365 Douglas Dr. Mailcode 6614, Carbondale, IL  62901, United States},
abstract={DevOps is an emerging culture that emphasizes continuous collaboration between software developers and IT operators through continuous standard process with automated tools for continuous delivery. DevOps participants take diverse roles to support its values - continuous collaboration, continuous process, and continuous delivery. A development team needs to be familiar with user cases, Object-Oriented Analysis (OOA), Object-Oriented Design (OOD), Object-Oriented Programming (OOP), and software testing. A quality assurance team must know use cases, abuse cases, software testing, and penetration testing. An operation team requires understanding deployment of Application Programming Interface (API) documents and executable components, and monitoring them and sharing their monitoring outcomes with both development and quality assurance teams. © 2017 Copyright is held by the owner/author(s).},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Du2017,
author={Du, Q. and Yin, K. and Qiu, J. and Li, H. and Shi, K. and Tian, Y. and Ma, A.},
title={Test case design method targeting environmental fault tolerance for high availability clusters},
journal={Proceedings of 2016 11th International Conference on Reliability, Maintainability and Safety: Integrating Big Data, Improving Reliability and Serving Personalization, ICRMS 2016},
year={2017},
doi={10.1109/ICRMS.2016.8050033},
art_number={8050033},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032796919&doi=10.1109%2fICRMS.2016.8050033&partnerID=40&md5=a8a5e501ba79542b6109a4488adafa2a},
affiliation={School of Software Engineering, Tongji University, Shanghai, 201804, China},
abstract={With increasing critical business organizations focusing on the quality of service in cloud applications, high availability (HA) has become critical for the product level cluster of cloud applications. Existing availability evaluation methods or testing projects for cloud platforms, however, cannot sufficiently verify cluster tolerance to various reasonable environmental faults. This paper proposes a test case design method targeting environmental fault tolerance for HA clusters. Test cases are designed considering fault modes and components of a cluster application in diverse scenarios. As a result, the fault tolerance of different components to various reasonable faults can be verified via the proposed method. A case study is conducted on Openstack, a widely used open source software platform for cloud environments, to show the feasibility of using this method. © 2016 IEEE.},
author_keywords={Fault Tolerance;  High availability;  Reliability;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cui201775,
author={Cui, Q. and Wang, J. and Yang, G. and Xie, M. and Wang, Q. and Li, M.},
title={Who Should Be Selected to Perform a Task in Crowdsourced Testing?},
journal={Proceedings - International Computer Software and Applications Conference},
year={2017},
volume={1},
pages={75-84},
doi={10.1109/COMPSAC.2017.265},
art_number={8029593},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031939914&doi=10.1109%2fCOMPSAC.2017.265&partnerID=40&md5=fe2e104e62566631db90e732cb36085e},
affiliation={Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, China; Department of Computer Science, Texas State University, United States; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China; University of Chinese, Academy of Sciences, China},
abstract={Crowdsourced testing is an emerging trend in software testing, which relies on crowd workers to accomplish test tasks. Due to the cost constraint, a test task usually involves a limited number of crowd workers. Furthermore, more workers does not necessarily result in detecting more bugs. Different workers, who may have different testing experience and expertise, may make much differences in the test outcomes. For example, some inappropriate workers may miss true bug, introduce false bugs or report duplicated bugs, which decreases the test quality. In current practice, a test task is usually dispatched in a random manner, and the quality of testing cannot be guaranteed. Therefore, it is important to select an appropriate subset of workers to perform a test task to ensure high bug detection rate. This paper introduces ExReDiv, a novel hybrid approach to select a set of workers for a test task. It consists of three key strategies: The experience strategy selects experienced workers, the relevance strategy selects workers with expertise relevant to the given test task, the diversity strategy selects diverse workers to avoid detecting duplicated bugs. We evaluate ExReDiv based on 42 test tasks from one of the largest crowdsourced testing platforms in China, and the experimental results show its effectiveness. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hou20171750,
author={Hou, S. and Yu, L. and Li, Z. and Zhang, X.},
title={Based on the Random Vector Mirror Method Improve the ART Algorithm},
journal={Jisuanji Fuzhu Sheji Yu Tuxingxue Xuebao/Journal of Computer-Aided Design and Computer Graphics},
year={2017},
volume={29},
number={9},
pages={1750-1758},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032273924&partnerID=40&md5=c2b23de27d6fea7d3d3e52bd24916bd4},
affiliation={PLA Information Engineering University, Zhengzhou, 450001, China; State Key Laboratory of Mathematical Engineering and Advanced Computing, Zhengzhou, 450001, China},
abstract={To cope with problem that the low randomness of test cases, generated by the mirror function of existing mirroring adaptive random testing (MART), dynamic mirror adaptive random testing (DMART) algorithms and so on, make the effectiveness of the algorithms so declined obviously in varying degrees. A mirror method based on random vector is proposed to improve the adaptive random testing (ART) algorithms. Firstly, the traditional mirror function is improved by introducing the random vector to enlarge the diversity between the mirror test cases. And then, the random vector mirror function is applied to the mirror method to improve the ART algorithms. The experimental results show that, use the random vector mirror method can improve the effectiveness of mirror algorithms visibly, and this algorithm enhances prominently in comparison with the original algorithm in efficiency. © 2017, Beijing China Science Journal Publishing Co. Ltd. All right reserved.},
author_keywords={Adaptive random testing;  Divide and conquer;  Mapping;  Mirror;  Ramdom vector;  Random testing;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ikhsan20175276,
author={Ikhsan, A. and Yulherniwati and Muchtiar, Y.},
title={Decision support system for forecasting production time (case study: Fiberglass industry)},
journal={ARPN Journal of Engineering and Applied Sciences},
year={2017},
volume={12},
number={18},
pages={5276-5280},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030092768&partnerID=40&md5=c6b92249e1c826193579ede6c2fadf7b},
affiliation={Industrial Engineering Department, Bung Hatta University Kampus III UBH, Indonesia; Information Technology Department, Polytechnic of Padang, Indonesia},
abstract={Fiber glass industry is an industry that uses polyester reinforced with fiber glass as the main raw material. Various products can be made by using fiber glass. The diversity of products in the fiber glass industry often pose a problem, that is difficulty in determining when the work can be completed. Timeliness is critical to customer satisfaction. By utilizing visual feature recognition approach of the product to be produced and Artificial Neural Network, the production time based on feature standard time-making is predicted. This study consisted of modelling decision support system using artificial intelligence algorithms, testing alternative models, developing decision support system software, software testing to confirm the results with the real in field situation. With this system, fiber glass industry is expected to be able to maintain customer satisfaction by completing the work on time. By using a neural network, the timing of the completion can be done more accurately and the latest production data became the basis for considering the production time for the next orders. © 2006-2017 Asian Research Publishing Network (ARPN).},
author_keywords={Artificial neural network;  Decision support system;  Feature based;  Fiber glass;  Production time},
document_type={Article},
source={Scopus},
}

@ARTICLE{Matalonga20171,
author={Matalonga, S. and Rodrigues, F. and Travassos, G.H.},
title={Characterizing testing methods for context-aware software systems: Results from a quasi-systematic literature review},
journal={Journal of Systems and Software},
year={2017},
volume={131},
pages={1-21},
doi={10.1016/j.jss.2017.05.048},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019735323&doi=10.1016%2fj.jss.2017.05.048&partnerID=40&md5=b1d16f8c40086e8fa83de190455e9de4},
affiliation={Universidad ORT Uruguay Montevideo, Uruguay; Universidade Federal do Rio de Janeiro PESC/COPPE Rio de Janeiro, Brazil},
abstract={Context-Aware Software Systems (CASS) use environmental information to provide better service to the systems’ actors to fulfill their goals. Testing of ubiquitous software systems can be challenging since it is unlikely that, while designing the test cases, the tester can identify all possible context variations. A quasi-Systematic Literature Review has been undertaken to characterize the methods usually used for testing CASS. The analysis and generation of knowledge in this work rely on classifying the extracted information. Established taxonomies of software testing and context-aware were used to characterize and interpret the findings. The results show that, although it is possible to observe the utilization of some software testing methods, few empirical studies are evaluating such methods when testing CASS. The selected technical literature conveys a lack of consensus on the understanding of context and CASS, and on the meaning of software testing. Furthermore, context variation in CASS has only been partially addressed by the identified approaches. They either rely on simulating context or in fixing the values of context variables during testing. We argue that the tests of context-aware software systems need to deal with the diversity of context instead of mitigating their effects. © 2017 Elsevier Inc.},
author_keywords={Context-aware;  Software testing;  Systematic literature review;  Test case design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mao2017,
author={Mao, C. and Chen, T.Y. and Kuo, F.-C.},
title={Out of sight, out of mind: a distance-aware forgetting strategy for adaptive random testing},
journal={Science China Information Sciences},
year={2017},
volume={60},
number={9},
doi={10.1007/s11432-016-0087-0},
art_number={092106},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018767570&doi=10.1007%2fs11432-016-0087-0&partnerID=40&md5=825cbfefd854b321667f54b1075cd1e7},
affiliation={School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, 330013, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Melbourne, 3122, Australia},
abstract={Adaptive random testing (ART) achieves better failure-detection effectiveness than random testing by increasing the diversity of test cases. However, the intention of ensuring even spread of test cases inevitably causes an overhead problem. Although two basic forgetting strategies (i.e. random forgetting and consecutive retention) were proposed to reduce the computation cost of ART, they only considered the temporal distribution of test cases. In the paper, we presented a distance-aware forgetting strategy for the fixed size candidate set version of ART (DF-FSCS), in which the spatial distribution of test cases is taken into consideration. For a given candidate, the test cases out of its “sight” are ignored to reduce the distance computation cost. At the same time, the dynamic adjustment for partitioning and the second-round forgetting are adopted to ensure the linear complexity of DF-FSCS algorithm. Both simulation analysis and empirical study are employed to investigate the efficiency and effectiveness of DF-FSCS. The experimental results show that DF-FSCS significantly outperforms the classical ART algorithm FSCS-ART in efficiency, and has comparable failure-detection effectiveness. Com-pared with two basic forgetting methods, DF-FSCS is better in both efficiency and effectiveness. In contrast with a typical linear-time ART algorithm RBCVT-Fast, our algorithm requires less computational overhead and exhibits the similar failure-detection capability. In addition, DF-FSCS has more reliable performance than RBCVT-Fast in detecting failures for the programs with high-dimensional input domain. © 2017, Science China Press and Springer-Verlag Berlin Heidelberg.},
author_keywords={adaptive random testing;  computational overhead;  diversity;  software testing;  test cases},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gopinath2017871,
author={Gopinath, R. and Ahmed, I. and Alipour, M.A. and Jensen, C. and Groce, A.},
title={Does choice of mutation tool matter?},
journal={Software Quality Journal},
year={2017},
volume={25},
number={3},
pages={871-920},
doi={10.1007/s11219-016-9317-7},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966667381&doi=10.1007%2fs11219-016-9317-7&partnerID=40&md5=b0e1bc973b3ce995d298e1c11603f2ba},
affiliation={EECS Department, Oregon State University, Corvallis, OR, United States; Oregon State University, Corvallis, OR, United States},
abstract={Though mutation analysis is the primary means of evaluating the quality of test suites, it suffers from inadequate standardization. Mutation analysis tools vary based on language, when mutants are generated (phase of compilation), and target audience. Mutation tools rarely implement the complete set of operators proposed in the literature and mostly implement at least a few domain-specific mutation operators. Thus different tools may not always agree on the mutant kills of a test suite. Few criteria exist to guide a practitioner in choosing the right tool for either evaluating effectiveness of a test suite or for comparing different testing techniques. We investigate an ensemble of measures for evaluating efficacy of mutants produced by different tools. These include the traditional difficulty of detection, strength of minimal sets, and the diversity of mutants, as well as the information carried by the mutants produced. We find that mutation tools rarely agree. The disagreement between scores can be large, and the variation due to characteristics of the project—even after accounting for difference due to test suites—is a significant factor. However, the mean difference between tools is very small, indicating that no single tool consistently skews mutation scores high or low for all projects. These results suggest that experiments yielding small differences in mutation score, especially using a single tool, or a small number of projects may not be reliable. There is a clear need for greater standardization of mutation analysis. We propose one approach for such a standardization. © 2016, Springer Science+Business Media New York.},
author_keywords={Empirical analysis;  Mutation analysis;  Software testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sotiropoulos2017150,
author={Sotiropoulos, T. and Waeselynck, H. and Guiochet, J. and Ingrand, F.},
title={Can robot navigation bugs be found in simulation? An exploratory study},
journal={Proceedings - 2017 IEEE International Conference on Software Quality, Reliability and Security, QRS 2017},
year={2017},
pages={150-159},
doi={10.1109/QRS.2017.25},
art_number={8009918},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029452401&doi=10.1109%2fQRS.2017.25&partnerID=40&md5=2cee58b5212cac2aa979e58cfeace43c},
affiliation={LAAS-CNRS, Universite de Toulouse, CNRS, UPS, Toulouse, France},
abstract={The ability to navigate in diverse and previously unknown environments is a critical service of autonomous robots. The validation of the navigation software typically involves test campaigns in the field, which are costly and potentially risky for the robot itself or its environment. An alternative approach is to perform simulation-based testing, by immersing the software in virtual worlds. A question is then whether the bugs revealed in real worlds can also be found in simulation. The paper reports on an exploratory study of bugs in an academic software for outdoor robots navigation. The detailed analysis of the triggers and effects of these bugs shows that most of them can be revealed in low-fidelity simulation. It also provides insights into interesting navigation scenarios to test as well as into how to address the test oracle problem. © 2017 IEEE.},
author_keywords={Autonomous systems;  Domain specific defects;  Exploratory study;  Safety;  Simulation-based testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Georgiadis2017,
author={Georgiadis, L. and Giannis, K. and Karanasiou, A. and Laura, L.},
title={Incremental low-high orders of directed graphs and applications},
journal={Leibniz International Proceedings in Informatics, LIPIcs},
year={2017},
volume={75},
doi={10.4230/LIPIcs.SEA.2017.27},
art_number={27},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028747892&doi=10.4230%2fLIPIcs.SEA.2017.27&partnerID=40&md5=f41a7c0a0765950ada427bdb776db58c},
affiliation={Department of Computer Science and Engineering, University of Ioannina, Ioannina, Greece; Universita di Roma Tor Vergata, Rome, Italy; Sapienza Universita di Roma, Rome, Italy},
abstract={A flow graph G = (V, E, s) is a directed graph with a distinguished start vertex s. The dominator tree D of G is a tree rooted at s, such that a vertex v is an ancestor of a vertex w if and only if all paths from s to w include v. The dominator tree is a central tool in program optimization and code generation, and has many applications in other diverse areas including constraint programming, circuit testing, biology, and in algorithms for graph connectivity problems. A low-high order of G is a preorder δ of D that certifies the correctness of D, and has further applications in connectivity and path-determination problems. In this paper we consider how to maintain efficiently a low-high order of a flow graph incrementally under edge insertions. We present algorithms that run in 0(mn) total time for a sequence of edge insertions in a flow graph with n vertices, where m is the total number of edges after all insertions. These immediately provide the first incremental certifying algorithms for maintaining the dominator tree in O(mn) total time, and also imply incremental algorithms for other problems. Hence, we provide a substantial improvement over the 0(m2) straightforward algorithms, which recompute the solution from scratch after each edge insertion. Furthermore, we provide efficient implementations of our algorithms and conduct an extensive experimental study on real-world graphs taken from a variety of application areas. The experimental results show that our algorithms perform very well in practice. © Loukas Georgiadis, Konstantinos Giannis, Aikaterini Karanasiou, and Luigi Laura.},
author_keywords={Connectivity;  Directed graphs;  Dominators;  Dynamic algorithms},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Santos20171,
author={Santos, I.D.S. and Andrade, R.M.D.C. and Rocha, L.S. and Matalonga, S. and de Oliveira, K.M. and Travassos, G.H.},
title={Test case design for context-aware applications: Are we there yet?},
journal={Information and Software Technology},
year={2017},
volume={88},
pages={1-16},
doi={10.1016/j.infsof.2017.03.008},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016195256&doi=10.1016%2fj.infsof.2017.03.008&partnerID=40&md5=6b01703003d6dc57c8c7ca35a6377e8b},
affiliation={Federal University of Ceará, Ceará, Brazil; Universidad ORT Uruguay, Montevideo, Uruguay; University of Valenciennes, LAMIH, UMR CNRS 8201, Valenciennes, France; Federal University of Rio de Janeiro, Rio de Janeiro, Brazil},
abstract={Context Current software systems have increasingly implemented context-aware adaptations to handle the diversity of conditions of their surrounding environment. Therefore, people are becoming used to a variety of context-aware software systems (CASS). This context-awareness brings challenges to the software construction and testing because the context is unpredictable and may change at any time. Therefore, software engineers need to consider the dynamic context changes while testing CASS. Different test case design techniques (TCDT) have been proposed to support the testing of CASS. However, to the best of our knowledge, there is no analysis of these proposals on the advantages, limitations and their effective support to context variation during testing. Objective To gather empirical evidence on TCDT concerned with CASS by identifying, evaluating and synthesizing knowledge available in the literature. Method To undertake a secondary study (quasi-Systematic Literature Review) on TCDT for CASS regarding their assessed quality characteristics, used coverage criteria, test type, and test technique. Results From 833 primary studies published between 2004 and 2014, just 17 studies regard the design of test cases for CASS. Most of them focus on functional suitability. Furthermore, some of them take into account the changes in the context by providing specific test cases for each context configuration (static perspective) during the test execution. These 17 studies revealed five challenges affecting the design of test cases and 20 challenges regarding the testing of CASS. Besides, seven TCDT are not empirically evaluated. Conclusion A few TCDT partially support the testing of CASS. However, it has not been observed evidence on any TCDT supporting the truly context-aware testing, which that can adapt the expected output based on the context variation (dynamic perspective) during the test execution. It is an open issue deserving greater attention from researchers to increase the testing coverage and ensure users confidence in CASS. © 2017},
author_keywords={Context aware application;  Software testing;  Systematic review},
document_type={Review},
source={Scopus},
}

@CONFERENCE{Wood2017,
author={Wood, B.J. and Drain, T.D. and Mainzer, A. and Valerino, P. and Schroeder, A. and Walker, M. and LeDoux, C. and Barron, C. and Piepol, D.},
title={The academy's science and technology council presents hidden figures in collaboration with NASA},
journal={ACM SIGGRAPH 2017 Panels, SIGGRAPH 2017},
year={2017},
doi={10.1145/3084847.3110024},
art_number={2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033383719&doi=10.1145%2f3084847.3110024&partnerID=40&md5=aed90b7cd9f7ce7aa1d97b0b1bb17c3d},
affiliation={Academy Sci-Tech Council, United States; NASA Jet Propulsion Laboratory, United States; Hidden Figures, Crafty Apes, United States},
abstract={America's Mercury Space Program mastered orbital flight after a series of daunting challenges, including test rockets that exploded on launch pads. Astronauts riding on Redstone and Atlas rockets were risking their lives before all the unknowns could be answered.},
author_keywords={Animation;  Jet propulsion lab;  JPL;  NASA;  National aeronautics and space administration;  Space;  Space exploration;  Space navigation;  STEM;  Visual effects;  Workplace diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Styugin2017,
author={Styugin, M. and Zolotarev, V. and Prokhorov, A. and Gorbil, R.},
title={New approach to software code diversification in interpreted languages based on the moving target technology},
journal={Application of Information and Communication Technologies, AICT 2016 - Conference Proceedings},
year={2017},
doi={10.1109/ICAICT.2016.7991694},
art_number={7991694},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034227923&doi=10.1109%2fICAICT.2016.7991694&partnerID=40&md5=94d2c14fadc1967c60106b525f930577},
affiliation={Department of Information Security, SibSAU, Krasnoyarsk, Russian Federation; Department of Applied Mathematics and Information Security, SFU, Krasnoyarsk, Russian Federation},
abstract={The paper presents a method based on moving target technology for protecting software components of distributed systems. Practical implementation of this method is a code diversifier, which adds intermediary functions and inserts transformation of key variables into the program code. Theoretical analysis of the method presented here demonstrated its effectiveness even when an adversary had access to a application's program code. The diversifier presented below can also function in a mode of program code obfuscation, which was tested and demonstrated by the example of interpreted programming language functions. Metrics obtained from the program code after obfuscation, showed sufficient level of code modification for independent use in obfuscation mode. © 2016 IEEE.},
author_keywords={diversification;  moving target defense;  obfuscation;  software security},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Erkkila2017,
author={Erkkila, J. and Koskela, M. and Heikkila, J. and Kupiainen, T. and Heikkila, M. and Kippola, T. and Nykanen, A. and Saukkonen, R.},
title={Antenna configuration comparison in challenging NLOS locations},
journal={EuCNC 2017 - European Conference on Networks and Communications},
year={2017},
doi={10.1109/EuCNC.2017.7980710},
art_number={7980710},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035809916&doi=10.1109%2fEuCNC.2017.7980710&partnerID=40&md5=feccfc1ab311ebb88e9be9512fe22f8d},
affiliation={Centria University of Applied Sciences, Ylivieska, Finland; Nokia Mobile Networks, Oulu, Finland},
abstract={The objective of this paper is to compare uplink (UL) performance with different antenna technologies when receiving Non-Line of Sight (NLOS) signal in field tests. The antenna configurations in use were a passive antenna system and an active antenna system (AAS). The radiation pattern can be controlled horizontally by changing its azimuth angle and vertically by changing the tilt angle of the antenna. AAS includes a flexible configuration that consists of diversity beams and other features for beam controlling. The field trial benefitted 2-way and 4-way receive (RX) diversity in both antenna systems. The field trial environment consists of three macro-cellular Long Term Evolution (LTE) evolved Node B (eNB) operating in 2.1 GHz band. This trial environment has two AAS and one passive system in use for the measurements. The environment can encompass one macro cell and with vertical controlling, it is possible to add an additional beam and with horizontal controlling it is possible to steer the main beam towards the user equipment (UE). Field trial used drive test software to evaluate the test network performance. Mobile network user had UE in drive testing to evaluate cellular network quality from the mobile device's point of view. The field trial results indicate that AAS beam controlling can achieve remarkable capacity gain in uplink direction when UE does not have a line of sight to the eNB. © 2017 IEEE.},
author_keywords={2 and 4-way RX diversity;  AAS;  FDD LTE;  Field trial;  Horizontal beamforming;  Non-line of Sight Environment;  Uplink capacity improvement;  Vertical beamforming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cohen201747,
author={Cohen, M.B.},
title={The evolutionary landscape of SBST: A 10 year perspective},
journal={Proceedings - 2017 IEEE/ACM 10th International Workshop on Search-Based Software Testing, SBST 2017},
year={2017},
pages={47-48},
doi={10.1109/SBST.2017.2},
art_number={7967961},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027465493&doi=10.1109%2fSBST.2017.2&partnerID=40&md5=b9be833d1168c13c57b31ebc70db555c},
affiliation={Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE, United States},
abstract={A key indicator of the health and quality of any evolutionary algorithm is the landscape of its search. By analyzing the landscape one can determine the peaks (local maxima) where significant solutions exist. In this paper we examine the landscape for the history of the International Workshop on Search-Based Software Testing (SBST) within the context of the broader field of search-based software testing. We study the evolution of the field, highlighting key advances during three phases of its ten year history. In 2008 the focus of SBST was inner looking, with advances in existing search techniques, improvements to individual generation techniques, and methods to transform the problem space for search effectiveness. However, diverse seeds of new ideas (such as automated program repair) were already being injected into the population. A few SBST tools existed, but the engineer still required skill and expertise to effectively apply search based approaches. During the middle years, open source tools were created and released, whole test suite generation appeared, and searches hybridized. Tool competitions began and industry started to play a stronger role. As we move to the most recent workshop years and look towards the future, more sophisticated techniques such as those that incorporate hyper-heuristics via learning, and/or balance multiple objectives at once are now common. SBST has become a mainstream topic in the testing community, tools are being commercialized and these tools often hide their inner workings, leading to a future that is optimized towards SBST for all. © 2017 IEEE.},
author_keywords={SBST;  Search-based software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yohanes201715,
author={Yohanes, B.W. and Rusli, S.Y. and Wardana, H.K.},
title={Location prediction model using Naïve Bayes algorithm in a half-open building},
journal={Proceedings - 2017 4th International Conference on Information Technology, Computer, and Electrical Engineering, ICITACEE 2017},
year={2017},
volume={2018-January},
pages={15-19},
doi={10.1109/ICITACEE.2017.8257667},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050381049&doi=10.1109%2fICITACEE.2017.8257667&partnerID=40&md5=c989a0146b3794b2d831a6eb0c5a94df},
affiliation={Faculty of Electronic and Computer Engineering, Universitas Kristen Satya Wacana, Salatiga, Indonesia},
abstract={Currently, received signal strength (RSS) becomes an interesting method for indoor location prediction. Some advantages using RSS compared to other methods are it does not require transmitter and receiver synchronization and it allows construction without any additional devices. There are two phases to make a location prediction, offline and online phase. Offline phase uses training data to build a fingerprint database. Then online phase is used to test the fingerprint database that has been built in the offline phase. After finishing both phases, average error between the actual distances in real coordinate points and the coordinates predicted is calculated. By performing several stages of testing, the result shows that the best prediction obtained by combining data from fingerprint database, Netsurveyor's log, and software implementation. In this way, the average error distance can be reduced up to 3 m. This is caused by lacking of variety of data retrieval with the diverse conditions. © 2017 IEEE.},
author_keywords={Location Prediction;  Naïve Bayes;  RSS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sugave2017701,
author={Sugave, S.R. and Patil, S.H. and Reddy, B.E.},
title={DDF: Diversity Dragonfly Algorithm for cost-aware test suite minimization approach for software testing},
journal={Proceedings of the 2017 International Conference on Intelligent Computing and Control Systems, ICICCS 2017},
year={2017},
volume={2018-January},
pages={701-707},
doi={10.1109/ICCONS.2017.8250554},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047623567&doi=10.1109%2fICCONS.2017.8250554&partnerID=40&md5=1fa44a02de08f03a45d4617fb75adce9},
affiliation={MIT College of Engineering, Pune, MH, India; Bharati Vidyapeeth University College of Engineering, Pune, MH, India; JNTUA College of Engineering, Kalikiri, Chittor District, AP, India},
abstract={The test suite minimization approach is a major research topic that it requires huge attention from the researchers as the traditional methods used for performing the test suite minimization is concentrated on the cost of regression testing but the requirements were not satisfied. To solve the problem of satisfying requirements, researchers proposed greedy algorithms, optimization algorithms, and so on. In this paper, a novel optimization algorithm is proposed termed as the Diversity Dragonfly Algorithm (DDF) algorithm that concentrates on the cost and quality of the test suite. The diversification included in the standard Dragonfly algorithm forms the DDF that uses three bitwise operators for diversification. The DDF algorithm determines the best suite based on the hunting mechanism of the dragonfly using a minimum objective function such that the selected test suite satisfies all the requirements. The experiment is carried out using five subject programs and the performance analysis of the proposed DDF is carried out and compared with the existing methods. It is found that the reduction capability of the DDF is better than existing methods and the cost of the proposed DDF is low ensuring a quality test suite reduction. © 2017 IEEE.},
author_keywords={Cost constraint;  DDF optimization algorithm;  Diversity function;  Software testing;  Test suite reduction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang20171555,
author={Yang, S.-H. and Vo, P.B.},
title={Adaptive Bit Allocation for Consistent Video Quality in Scalable High Efficiency Video Coding},
journal={IEEE Transactions on Circuits and Systems for Video Technology},
year={2017},
volume={27},
number={7},
pages={1555-1567},
doi={10.1109/TCSVT.2016.2539639},
art_number={7428907},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022330373&doi=10.1109%2fTCSVT.2016.2539639&partnerID=40&md5=45d55597f9df0cc3e464c3db44252f61},
affiliation={National Taipei University of Technology, Taipei, 10608, Taiwan; Dalat University, Da Lat, 61000, Viet Nam},
abstract={Scalable video coding (SVC) is a coding paradigm that allows once-encoded video content to be used in diverse scenarios. SVC-coded videos can be transmitted and rendered at specified bitrates according to network bandwidth and end device requirements. In this paper, an adaptive bit allocation algorithm is proposed for the emerging scalable High Efficiency Video Coding (SHVC) standard. The bit budget at the group-of-pictures level is allocated according to buffer occupancy. Picture complexity, measured using the predicted mean absolute difference (MAD), buffer occupancy, and hierarchical level, is proposed for regulating the bitrate at the picture level. The MAD of the current picture is predicted using a novel mean prediction error (MPE) model, which is obtained from the advanced motion vector prediction, and the test zone search specified in SHVC and the associated reference software of SHVC. Moreover, MPE is used to determine the number of assigned bits at the coding-tree-unit level. The bit budget of each level is incorporated with the R-λ model for computing the required quantization parameter. Experimental results reveal that the proposed method achieves accurate bitrates with enhanced and consistent visual quality and more satisfactorily controls buffer occupancy compared with the state-of-the-art approaches reported in the literature. © 1991-2012 IEEE.},
author_keywords={Bit allocation;  mean absolute difference (MAD);  mean prediction error (MPE);  rate control (RC);  scalable High Efficiency Video Coding (SHVC);  scalable video coding (SVC)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Curro201743,
author={Curro, D. and Derpanis, K.G. and Miranskyy, A.V.},
title={Building usage profiles using deep neural nets},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering: New Ideas and Emerging Results Track, ICSE-NIER 2017},
year={2017},
pages={43-46},
doi={10.1109/ICSE-NIER.2017.12},
art_number={7966877},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026789360&doi=10.1109%2fICSE-NIER.2017.12&partnerID=40&md5=0ec632ed6285b84c8496e5d52e56d4c1},
affiliation={Department of Computer Science, Ryerson University, Toronto, Canada},
abstract={To improve software quality, one needs to build test scenarios resembling the usage of a software product in the field. This task is rendered challenging when a product's customer base is large and diverse. In this scenario, existing profiling approaches, such as operational profiling, are difficult to apply. In this work, we considerpublicly available video tutorials of a product to profile usage. Our goal is to construct an automatic approach to extract information about user actions from instructional videos. To achieve this goal, we use a Deep Convolutional Neural Network (DCNN) to recognize user actions. Our pilot study shows that a DCNN trained to recognize user actions in video can classify five different actions in a collection of 236 publicly available Microsoft Word tutorial videos (published on YouTube). In our empirical evaluation we report a mean average precision of 94.42% across all actions. This study demonstrates the efficacy of DCNN-based methods for extracting software usage information from videos. Moreover, this approach may aid in other software engineering activities that require information about customer usage of a product. © 2017 IEEE.},
author_keywords={Deep Neural Net;  Usage Profile;  Video},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gazzola2017429,
author={Gazzola, L.},
title={Field testing of software applications},
journal={Proceedings - 2017 IEEE/ACM 39th International Conference on Software Engineering Companion, ICSE-C 2017},
year={2017},
pages={429-432},
doi={10.1109/ICSE-C.2017.30},
art_number={7965376},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026786868&doi=10.1109%2fICSE-C.2017.30&partnerID=40&md5=0d384da99cafdd39757a5055d73c551e},
affiliation={Universita' degli Studi di Milano-Bicocca Milano, Italy},
abstract={When interacting with their software systems, users may have to deal with problems like crashes, failures, and program instability. Faulty software running in the field is not only the consequence of ineffective in-house verification and validation techniques, but it is also due to the complexity and diversity of the interactions between an application and its environment. Many of these interactions can be hardly predicted at testing time, and even when they could be predicted, often there are so many cases to be tested that they cannot be all feasibly addressed before the software is released. This Ph.D. thesis investigates the idea of addressing the faults that cannot be effectively addressed in house directly in the field, exploiting the field itself as testbed for running the test cases. An enormous number of diverse environments would then be available for testing, giving the possibility to run many test cases in many different situations, timely revealing the many failures that would be hard to detect otherwise. © 2017 IEEE.},
author_keywords={Field failures;  Field testing;  Isolation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2017347,
author={Zhang, Q. and Sun, C. and Su, Z.},
title={Skeletal program enumeration for rigorous compiler testing},
journal={ACM SIGPLAN Notices},
year={2017},
volume={52},
number={6},
pages={347-361},
doi={10.1145/3062341.3062379},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084483865&doi=10.1145%2f3062341.3062379&partnerID=40&md5=7b7ba6151956b7c16e4511743452dd3a},
affiliation={University of California, Davis, United States},
abstract={A program can be viewed as a syntactic structure P (syntactic skeleton) parameterized by a collection of identifiers V (variable names). This paper introduces the skeletal program enumeration (SPE) problem: Given a syntactic skeleton P and a set of variables V , enumerate a set of programs P exhibiting all possible variable usage patterns within P. It proposes an effective realization of SPE for systematic, rigorous compiler testing by leveraging three important observations: (1) Programs with different variable usage patterns exhibit diverse control- and data-dependence, and help exploit different compiler optimizations; (2) most real compiler bugs were revealed by small tests (i.e., small-sized P) - this "small-scope" observation opens up SPE for practical compiler validation; and (3) SPE is exhaustive w.r.t. a given syntactic skeleton and variable set, offering a level of guarantee absent from all existing compiler testing techniques. The key challenge of SPE is how to eliminate the enormous amount of equivalent programs w.r.t. α-conversion. Our main technical contribution is a novel algorithm for computing the canonical (and smallest) set of all non-α-equivalent programs. To demonstrate its practical utility, we have applied the SPE technique to test C/C++ compilers using syntactic skeletons derived from their own regression test-suites. Our evaluation results are extremely encouraging. In less than six months, our approach has led to 217 confirmed GCC/Clang bug reports, 119 of which have already been fixed, and the majority are long latent despite extensive prior testing efforts. Our SPE algorithm also provides six orders of magnitude reduction. Moreover, in three weeks, our technique has found 29 CompCert crashing bugs and 42 bugs in two Scala optimizing compilers. These results demonstrate our SPE technique's generality and further illustrate its effectiveness. © 2017 ACM.},
author_keywords={compiler testing;  Program enumeration},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2017347,
author={Zhang, Q. and Sun, C. and Su, Z.},
title={Skeletal program enumeration for rigorous compiler testing},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2017},
volume={Part F128414},
pages={347-361},
doi={10.1145/3062341.3062379},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025123212&doi=10.1145%2f3062341.3062379&partnerID=40&md5=8118b0815956dba272b5361ed30ef603},
affiliation={University of California, Davis, CA, United States},
abstract={A program can be viewed as a syntactic structure P (syntactic skeleton) parameterized by a collection of identifiers V (variable names). This paper introduces the skeletal program enumeration (SPE) problem: Given a syntactic skeleton P and a set of variables V , enumerate a set of programs P exhibiting all possible variable usage patterns within P. It proposes an effective realization of SPE for systematic, rigorous compiler testing by leveraging three important observations: (1) Programs with different variable usage patterns exhibit diverse control- and data-dependence, and help exploit different compiler optimizations; (2) most real compiler bugs were revealed by small tests (i.e., small-sized P)-this "small-scope" observation opens up SPE for practical compiler validation; and (3) SPE is exhaustive w.r.t. a given syntactic skeleton and variable set, offering a level of guarantee absent from all existing compiler testing techniques. The key challenge of SPE is how to eliminate the enormous amount of equivalent programs w.r.t. conversion. Our main technical contribution is a novel algorithm for computing the canonical (and smallest) set of all non-equivalent programs. To demonstrate its practical utility, we have applied the SPE technique to test C/C++ compilers using syntactic skeletons derived from their own regression test-suites. Our evaluation results are extremely encouraging. In less than six months, our approach has led to 217 confirmed GCC/Clang bug reports, 119 of which have already been fixed, and the majority are long latent despite extensive prior testing efforts. Our SPE algorithm also provides six orders of magnitude reduction. Moreover, in three weeks, our technique has found 29 CompCert crashing bugs and 42 bugs in two Scala optimizing compilers. These results demonstrate our SPE technique's generality and further illustrate its effectiveness. © 2017 ACM.},
author_keywords={Compiler testing;  Program enumeration},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ahmed201720,
author={Ahmed, B.S. and Gambardella, L.M. and Afzal, W. and Zamli, K.Z.},
title={Handling constraints in combinatorial interaction testing in the presence of multi objective particle swarm and multithreading},
journal={Information and Software Technology},
year={2017},
volume={86},
pages={20-36},
doi={10.1016/j.infsof.2017.02.004},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028241787&doi=10.1016%2fj.infsof.2017.02.004&partnerID=40&md5=99eaa7384dec7d8f2cf00cf5f7c227e3},
affiliation={Istituto Dalle Molle di Studi sullIntelligenza Artificiale (IDSIA), CH-6928 Manno-Lugano, Switzerland; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University, Karlovo nam. 13, 121 35 Praha 2, Czech Republic; School of Innovation, Design and Engineering, Mlardalen University, Sweden; Faculty of Computer Systems and Software Engineering, University Malaysia Pahang, Gambang, Malaysia},
abstract={Context Combinatorial testing strategies have lately received a lot of attention as a result of their diverse applications. In its simple form, a combinatorial strategy can reduce several input parameters (configurations) of a system into a small set based on their interaction (or combination). In practice, the input configurations of software systems are subjected to constraints, especially in case of highly configurable systems. To implement this feature within a strategy, many difficulties arise for construction. While there are many combinatorial interaction testing strategies nowadays, few of them support constraints. Objective This paper presents a new strategy, to construct combinatorial interaction test suites in the presence of constraints. Method The design and algorithms are provided in detail. To overcome the multi-judgement criteria for an optimal solution, the multi-objective particle swarm optimisation and multithreading are used. The strategy and its associated algorithms are evaluated extensively using different benchmarks and comparisons. Results Our results are promising as the evaluation results showed the efficiency and performance of each algorithm in the strategy. The benchmarking results also showed that the strategy can generate constrained test suites efficiently as compared to state-of-the-art strategies. Conclusion The proposed strategy can form a new way for constructing of constrained combinatorial interaction test suites. The strategy can form a new and effective base for future implementations. © 2017 Elsevier B.V.},
author_keywords={Constrained combinatorial interaction;  Multi-objective particle swarm optimisation;  Search-based software engineering;  Test case design techniques;  Test generation tools},
document_type={Article},
source={Scopus},
}

@BOOK{Vallacher20171,
author={Vallacher, R.R. and Nowak, A. and Read, S.J.},
title={Computational social psychology},
journal={Computational Social Psychology},
year={2017},
pages={1-381},
doi={10.4324/9781315173726},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021109828&doi=10.4324%2f9781315173726&partnerID=40&md5=76fb4c262a76c0782c867f9f7729c7cc},
affiliation={Florida Atlantic UniversityFL, United States; Warsaw School of Social Sciences and Humanities, Warsaw University, Poland; University of Southern California, United States; Center for Complex Systems, University of Warsaw, Poland; Columbia University, United States},
abstract={Computational Social Psychology showcases a new approach to social psychology that enables theorists and researchers to specify social psychological processes in terms of formal rules that can be implemented and tested using the power of high speed computing technology and sophisticated software. This approach allows for previously infeasible investigations of the multi-dimensional nature of human experience as it unfolds in accordance with different temporal patterns on different timescales. In effect, the computational approach represents a rediscovery of the themes and ambitions that launched the field over a century ago. The book brings together social psychologists with varying topical interests who are taking the lead in this redirection of the field. Many present formal models that are implemented in computer simulations to test basic assumptions and investigate the emergence of higher-order properties; others develop models to fit the real-time evolution of people's inner states, overt behavior, and social interactions. Collectively, the contributions illustrate how the methods and tools of the computational approach can investigate, and transform, the diverse landscape of social psychology. © 2017 Taylor & Francis. All rights reserved.},
document_type={Book},
source={Scopus},
}

@ARTICLE{Huson201745,
author={Huson, D.H. and Steel, M. and El-Hadidi, M. and Mitra, S. and Peter, S. and Willmann, M.},
title={A simple statistical test of taxonomic or functional homogeneity using replicated microbiome sequencing samples},
journal={Journal of Biotechnology},
year={2017},
volume={250},
pages={45-50},
doi={10.1016/j.jbiotec.2016.10.020},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006469943&doi=10.1016%2fj.jbiotec.2016.10.020&partnerID=40&md5=cb578838173cb5aedcca265987691e6e},
affiliation={Center for Bioinformatics, University of Tübingen, Germany; Biomathematics Research Centre, University of Canterbury, Christchurch, New Zealand; Faculty of Medicine and Health – University of Leeds, Old Medical School, Leeds General Infirmary, Leeds, United Kingdom; Institute of Medical Microbiology and Hygiene, University of Tübingen, Germany; German Center for Infection Research (DZIF), Partner Site Tübingen, Germany},
abstract={One important question in microbiome analysis is how to assess the homogeneity of the microbial composition in a given environment, with respect to a given analysis method. Do different microbial samples taken from the same environment follow the same taxonomic distribution of organisms, or the same distribution of functions? Here we provide a non-parametric statistical “triangulation test” to address this type of question. The test requires that multiple replicates are available for each of the biological samples, and it is based on three-way computational comparisons of samples. To illustrate the application of the test, we collected three biological samples taken from different locations in one piece of human stool, each represented by three replicates, and analyzed them using MEGAN. (Despite its name, the triangulation test does not require that the number of biological samples or replicates be three.) The triangulation test rejects the null hypothesis that the three biological samples exhibit the same distribution of taxa or function (error probability ≤0.05), indicating that the microbial composition of the investigated human stool is not homogenous on a macroscopic scale, suggesting that pooling material from multiple locations is a reasonable practice. We provide an implementation of the test in our open source program MEGAN Community Edition. © 2016 The Author(s)},
author_keywords={Environmental inhomogeneity;  Functional diversity;  Metagenomics;  Statistical testing;  Taxonomic composition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Taylor2017,
author={Taylor, B.K. and Johnsen, S. and Lohmann, K.J.},
title={Detection of magnetic field properties using distributed sensing: A computational neuroscience approach},
journal={Bioinspiration and Biomimetics},
year={2017},
volume={12},
number={3},
doi={10.1088/1748-3190/aa6ccd},
art_number={036013},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020424026&doi=10.1088%2f1748-3190%2faa6ccd&partnerID=40&md5=95ce32523457d10528ac0847dcefda7a},
affiliation={Integrated Sensing and Processing Sciences, Air Force Research Laboratory - Munitions Directorate, Eglin Air Force Base, FL, United States; Department of Biology, Duke University, Durham, NC, United States; Department of Biology, University of North Carolina at Chapel Hill, Chapel Hill, NC, United States},
abstract={Diverse taxa use Earth's magnetic field to aid both short- and long-distance navigation. Study of these behaviors has led to a variety of postulated sensory and processing mechanisms that remain unconfirmed. Although several models have been proposed to explain and understand these mechanisms' underpinnings, they have not necessarily connected a putative sensory signal to the nervous system. Using mathematical software simulation, hardware testing and the computational neuroscience tool of dynamic neural fields, the present work implements a previously developed conceptual model for processing magnetite-based magnetosensory data. Results show that the conceptual model, originally constructed to stimulate thought and generate insights into future physiological experiments, may provide a valid approach to encoding magnetic field information. Specifically, magnetoreceptors that are each individually capable of sensing directional information can, as a population, encode magnetic intensity and direction. The findings hold promise both as a biological magnetoreception concept and for generating engineering innovations in sensing and processing. © 2017 IOP Publishing Ltd.},
author_keywords={animal magnetic reception;  computational neuroscience;  distributed sensing;  dynamic neural fields;  magnetoreception;  magnetosensing;  navigation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bentley2017,
author={Bentley, M.J. and Lin, A.C. and Hodson, D.D.},
title={Overcoming challenges to air force satellite ground control automation},
journal={2017 IEEE Conference on Cognitive and Computational Aspects of Situation Management, CogSIMA 2017},
year={2017},
doi={10.1109/COGSIMA.2017.7929585},
art_number={7929585},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021406416&doi=10.1109%2fCOGSIMA.2017.7929585&partnerID=40&md5=e43eb6a5aa6490ef79253f0ae9a835f3},
affiliation={Air Force Institute of Technology, 2950 Hobson Way, Wright-Patterson AFB, OH  45433, United States},
abstract={US Air Force satellite ground stations require significant manpower to operate. To improve operating efficiencies, the Air Force seeks to incorporate more automation into routine satellite operations. Interaction with autonomous systems includes not only daily operations, but also the development, maintainability, and the extensibility of such systems. This paper presents challenges to Air Force satellite automation: 1) existing architecture of legacy systems, 2) space segment diversity, and 3) unclear definition and scoping of the term 'automation.' Using a qualitative case study approach, we survey comparable non-satellite operation domains (Industrial Control Automation and Software Testing) that have successfully integrated automation, and other satellite operation enterprises (NASA Goddard, Naval Research Laboratory, European Ground Station National Institute for Space Research in Brazil) to identify common themes and best practices. From this insight, we recommend that future satellite operation ground stations encourage the use of layered architectures, abstract satellite operation processes, and integrate simulators in future systems as concrete implementations of this common operating platform. © 2017 IEEE.},
author_keywords={automation;  case study;  ground control;  process mining;  satellite;  simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2017299,
author={Wang, C. and Pastore, F. and Briand, L.},
title={System Testing of Timing Requirements Based on Use Cases and Timed Automata},
journal={Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017},
year={2017},
pages={299-309},
doi={10.1109/ICST.2017.34},
art_number={7927984},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020727173&doi=10.1109%2fICST.2017.34&partnerID=40&md5=e9cb73d41dcd3de289d60dc39771fc8f},
affiliation={SNT - University of Luxembourg, Luxembourg},
abstract={In the context of use-case centric development and requirements-driven testing, this paper addresses the problem of automatically deriving system test cases to verify timing requirements. Inspired by engineering practice in an automotive software development context, we rely on an analyzable form of use case specifications and augment such functional descriptions with timed automata, capturing timing requirements, following a methodology aiming at minimizing modeling overhead. We automate the generation of executable test cases using a test strategy based on maximizing test suite diversity and building over the UPPAAL model checker. Initial empirical results based on an industrial case study provide evidence of the effectiveness of the approach. © 2017 IEEE.},
author_keywords={System Testing;  Timed Automata;  Timing Requirements;  Use cases},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Poulding2017333,
author={Poulding, S. and Feldt, R.},
title={Automated Random Testing in Multiple Dispatch Languages},
journal={Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation, ICST 2017},
year={2017},
pages={333-344},
doi={10.1109/ICST.2017.37},
art_number={7927987},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020711461&doi=10.1109%2fICST.2017.37&partnerID=40&md5=f8fe9fffe8f0aff8fc216b69fe750d6d},
affiliation={Software Engineering Research Lab, Blekinge Institute of Technology, Karlskrona, 37179, Sweden},
abstract={In programming languages that use multiple dispatch, a single function can have multiple implementations, each of which may specialise the function's operation. Which one of these implementations to execute is determined by the data types of all the arguments to the function. Effective testing of functions that use multiple dispatch therefore requires diverse test inputs in terms of the data types of the input's arguments as well as their values. In this paper we describe an approach for generating test inputs where both the values and types are chosen probabilistically. The approach uses reflection to automatically determine how to create inputs with the desired types, and dynamically updates the probability distribution from which types are sampled in order to improve both the test efficiency and efficacy. We evaluate the technique on 247 methods across 9 built-in functions of Julia, a technical computing language that applies multiple dispatch at runtime. In the process, we identify three real faults in these widely-used functions. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chatzidimitriou2017,
author={Chatzidimitriou, A. and Kaliorakis, M. and Tselonis, S. and Gizopoulos, D.},
title={Performance-Aware reliability assessment of heterogeneous chips},
journal={Proceedings of the IEEE VLSI Test Symposium},
year={2017},
doi={10.1109/VTS.2017.7928940},
art_number={7928940},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020192881&doi=10.1109%2fVTS.2017.7928940&partnerID=40&md5=5d4e975cf78209eec5f56dec40eefef6},
affiliation={Department of Informatics and Telecommunications, University of Athens, Greece},
abstract={Technology evolution has raised serious reliability considerations, as transistor dimensions shrink and modern microprocessors become denser and more vulnerable to faults. Reliability studies have proposed a plethora of methodologies for assessing system vulnerability which, however, highly rely on traditional reliability metrics that solely express failure rate over time. Although Failures In Time (FIT) is a very strong and representative reliability metric, it may fail to offer an objective comparison of highly diverse systems, such as CPUs against GPUS or other accelerators that are often employed to execute the same algorithms implemented for these platforms. In this paper, we propose a reliability evaluation methodology that takes into account the probability of a workload execution failure in order to compare heterogeneous systems, while we also capture the differences in the performance of these systems. We demonstrate the usefulness of the methodology with a test case scenario that compares the reliability and performance of three different commercial CPUs (different ISAs and microarchitectures) and one GPU. We use statistical fault injection to assess the vulnerability of the register file for the four computing systems of our study. The evaluation was performed using a comprehensive set of benchmarks with the same algorithms implemented for each individual system (serial code for the CPUs and parallel code for the GPU). Our findings show that, even though the GPU proves to be three orders of magnitude more vulnerable than CPUs using traditional reliability metrics, our performance-Aware evaluation methodology shrinks this gap by 1-2 orders of magnitude providing more informative and realistic measurements to guide designers or programmers decisions. © 2017 IEEE.},
author_keywords={CPU;  Fault injection;  GPU;  Microarchitectural;  Performance;  Reliability;  Simulators;  Vulnerability evaluation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wei2017646,
author={Wei, L. and Ning, L. and Lian, Z. and Wang, L. and Hou, Z.},
title={Software testing method for Logistic growth model based on neural network},
journal={Harbin Gongcheng Daxue Xuebao/Journal of Harbin Engineering University},
year={2017},
volume={38},
number={4},
pages={646-651},
doi={10.11990/jheu.201605108},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021298666&doi=10.11990%2fjheu.201605108&partnerID=40&md5=31400dcfe5fc76572fcf4f689da4aa48},
affiliation={School of Information Science and Technology, Gansu Agriculture University, Lanzhou, 730070, China; School of Biological Sciences, Nanyang Technological University, Singapore City, 639798, Singapore; School of Computer Science and Engineering, Nanjing university of science and technology, Nanjing, 210094, China; School of Information Science and Engineering, Lanzhou University, Lanzhou, 730000, China},
abstract={Evaluation of software reliability performance directly affects the course of software testing. In this paper, we investigate fault detection in software testing to improve its performance. A software testing methods based on neural network of the logistic growth was proposed. Considering the diversity of software engineering, the proposed method using the logistic growth curve to construct the neural network model in order to complete fault detection. The proposed method combines the exponential distribution correction time in order to complete the fault correction process. Through the test of two sets of real failure data sets (Ohba and Wood), the proposed method is compared with the existing software reliability growth model (software reliability growth model, SRGM). The results confirm that the model fitting effect of the proposed logistic growth neural network model is optimal, demonstrating the adaption and better performance of the software reliability assessment model. © 2017, Editorial Department of Journal of HEU. All right reserved.},
author_keywords={Logistic curve;  Neural network;  Ohba data set;  Reliability evaluation;  Software reliability growth model;  Software testing;  Wood data set},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Wang2017385,
author={Wang, J. and Bai, X. and Ma, H. and Li, L. and Ji, Z.},
title={Cloud API Testing},
journal={Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2017},
year={2017},
pages={385-386},
doi={10.1109/ICSTW.2017.71},
art_number={7899088},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018419950&doi=10.1109%2fICSTW.2017.71&partnerID=40&md5=1f5100ff245ffcfa197fddb5079e1c74},
affiliation={Department of Computer Science and Technology, Tsinghua University, Beijing, China},
abstract={Following the Service-Oriented Architecture, Cloud services are exposed as Web APIs (Application Program Interface), which serve as the contracts between the service providers and service consumers. With increasing massive and broad applications of Cloud-based development, a large number of diversified APIs are emerging. Due to their wide impacts, any flaw in the cloud APIs may lead to serious consequences. API testing is thus necessary to ensure the availability, reliability, and stability of cloud services. The research proposed an approach to automating API testing following the model-driven architecture, so that services can be continuously fetched, analyzed and validated. A prototype system ATcloud was constructed to illustrate the process of API understanding, test scenario modeling using directed graph annotated with transfer probabilities between operations, cloud-based test resources management, distributed workload simulation, and performance monitoring. © 2017 IEEE.},
author_keywords={API testing;  Cloud computing;  Test automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Deng2017262,
author={Deng, X. and Wu, T. and Yan, J. and Zhang, J.},
title={Combinatorial Testing on Implementations of HTML5 Support},
journal={Proceedings - 10th IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2017},
year={2017},
pages={262-271},
doi={10.1109/ICSTW.2017.47},
art_number={7899064},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018376276&doi=10.1109%2fICSTW.2017.47&partnerID=40&md5=93b32ca080f849970725ba605bb939ed},
affiliation={Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China; University of Chinese Academy of Sciences, China},
abstract={The fifth version of HTML standard, which is widely accepted in the diverse landscape of browser vendors and their continuously upgrading releases, requires web browsers to support playback of multimedia natively, instead of by third-party plugins. Works on testing browsers' implementations of the HTML5 support, however, are not efficient enough till now. Regarding a browser's processing on HTML5 element tags of web pages and attributes of multimedia files, this paper treats the browser as a parameterized black-box and employs the combinatorial testing technique to design web pages to test its system behavior. Experiments are conducted on two sets of browsers. The first set includes nine popular ones in both desktop and mobile for discovering the distribution of multimedia related errors, and the second set contains five versions of the same browser for revealing the evolution of such errors. The experimental results indicate that the proposed approach is promising as it can reveal errors in browsers by various vendors and in various versions, and that the upgrades of the browser will not necessarily fix the existing bugs, and even introduce new ones, due to inefficient testing. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ferri20172981,
author={Ferri, P. and Ramil, M. and Rodríguez, I. and Bergamasco, R. and Vieira, A.M.S. and Cela, R.},
title={Assessment of quinoxyfen phototransformation pathways by liquid chromatography coupled to accurate mass spectrometry},
journal={Analytical and Bioanalytical Chemistry},
year={2017},
volume={409},
number={11},
pages={2981-2991},
doi={10.1007/s00216-017-0241-x},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013128183&doi=10.1007%2fs00216-017-0241-x&partnerID=40&md5=c1ec50ac28b685092749eb7a7d2ab13f},
affiliation={Departamento de Química Analítica, Nutrición y Bromatología, Instituto de Investigación y Análisis Alimentario (IIAA), Universidad de Santiago de Compostela, Santiago de Compostela, 15782, Spain; Postgraduate Program in Chemical Engineering, Universidade Estadual de Maringá, Maringá, Paraná  87020-900, Brazil; Postgraduate Program in Food Science, Universidade Estadual de Maringá, Maringá, Paraná  87020-900, Brazil},
abstract={Quinoxyfen has been recently identified as a priority hazardous substance in the field of the European water policy. In this work, its fate in aqueous samples and solid supports under UV and solar radiation is investigated. Diverse degradation experiments were carried out, at lab scale, using spiked aliquots of different aqueous matrices (ultrapure, treated wastewater and river water) irradiated at different wavelengths (λ = 254 nm, λ = 365 nm and solar light). Half-lives of quinoxyfen (2–26 min) depended on the wavelength and the intensity of radiation whilst the nature of the aqueous matrix did not play an important role in degradation kinetics. Moreover, experiments under solar radiation of doped silicone tubes were performed to simulate degradation when quinoxyfen is adsorbed on plant leaves or soil. As the compound is not completely mineralized, the identification of quinoxyfen transformation products (TPs) was performed by liquid chromatography quadrupole time-of-flight mass spectrometry (LC-QTOF-MS) injection of different irradiated time aliquots. The full-fragment ion spectra, at different collision energies, allowed the elucidation of the chemical structure of TPs formed by hydroxylation, cyclization or cleavage reactions. Five out of seven identified TPs have not been reported previously. The ecotoxicity simulation by software (TEST and ECOSAR) for TPs revealed that some of them could cause harmful effects to organisms such as Daphnia magna or Fathead minnow in a similar extent to the precursor; moreover, the time course profiles of major TPs (TP1 and TP2) revealed a much higher resistance to further photodegradation than quinoxyfen. [Figure not available: see fulltext.] © 2017, Springer-Verlag Berlin Heidelberg.},
author_keywords={Liquid chromatography;  Photodegradation;  Quinoxyfen;  Time-of-flight mass spectrometry;  Transformation products},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Silva2017250,
author={Silva, T.R. and Hak, J.-L. and Winckler, M.},
title={A Behavior-Based Ontology for Supporting Automated Assessment of Interactive Systems},
journal={Proceedings - IEEE 11th International Conference on Semantic Computing, ICSC 2017},
year={2017},
pages={250-257},
doi={10.1109/ICSC.2017.73},
art_number={7889535},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018318192&doi=10.1109%2fICSC.2017.73&partnerID=40&md5=5faa67a486097b57c355c7afefc2b344},
affiliation={ICS-IRIT, Université Paul Sabatier, Toulouse, France},
abstract={Nowadays many software development frameworks implement Behavior-Driven Development (BDD) as a mean of automating the test of interactive systems under construction. Automated testing helps to simulate user's action on the User Interface and therefore check if the system behaves properly and in accordance to Scenarios that describe functional requirements. However, most of tools supporting BDD requires that tests should be written using low-level events and components that only exist when the system is already implemented. As a consequence of such low-level of abstraction, BDD tests can hardly be reused with diverse artifacts and with versions of the system. To address this problem, this paper proposes to raise the abstraction level by the means of a behavior-based ontology that is aimed at supporting test automation. The paper presents an ontology and an ontology-based approach for automating the test of functional requirements of interactive systems. With the help of a case study for the flight tickets e-commerce domain, we demonstrate how tests written using our ontology can be used to assess functional requirements using different artifacts, from low-fidelity to full-fledged UI Prototypes. © 2017 IEEE.},
author_keywords={Automated Requirements Assessment;  Behavior-Driven Development;  Ontological Modeling;  Testing of Interactive Systems},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Santos2017,
author={Santos, M.M. and Mendes, C. and Banik, T. and Franco, F. and Neme, J. and Prado, W. and Cerri, F. and Nunes, L.},
title={New Approach of Tools Application for Systems Engineering in Automotive Software Development},
journal={SAE Technical Papers},
year={2017},
volume={2017-March},
number={March},
doi={10.4271/2017-01-1601},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018430190&doi=10.4271%2f2017-01-1601&partnerID=40&md5=6dcdb962f3d89b42ceb2acd5ee9ffa00},
affiliation={UTFPR-PG, Brazil; OpenCadd, Brazil; Marcopolo, Brazil},
abstract={This paper outlines the modeling process in SysML (Systems Modeling Language) in context of MBSE (Model Based Software Engineering) as well as the MBD (Model-Based Design) in Simulink and we compare the models to get useful information into software. For this goal, we propose the use of an RM/SM tool (Requirements Management and Systems Modeling) (3SL Cradle) and Matlab/Simulink to model the system, do the system validations, and finally embed the generated code. For automotive systems, the development process is visualized through the V-Model, which leads to the right choice of components, the integration of the system and the project realization. The first step in V-Model handles the requirements management for the development, i.e., the requirements for a project will be collected in respect to the stakeholder's needs and system limitations. Then, the next steps consist of modeling the system based on its requirements, going through simulation, system validation through Model-In-the-Loop (MIL), Software-In-the-Loop (SIL), Processor-In-the-Loop (PIL), and Hardware-In-the-Loop (HIL) tests. For this paper, the chosen modeling language was SysML for the MBSE point of view because it aims to standardize Modeling Design, by unifying diverse modeling languages used by engineers. This language also supports specification, analysis, design, verification, and validation of systems. To get executable models, we use Matlab/Simulink models that are largely used by the Original Equipment Manufacturers (OEMs) to develop new products. Our approach addresses the V-Model through SysML and MBD in Matlab/Simulink towards software validation. To achieve that, we use the commercial RM/SM tool that is used to collect stakeholder's and system requirements. It provides a SysML design section as well where SysML models can be developed according to project requirements. One of the objectives in using the commercial tool is that it will be possible to analyze the transition from models in RM/SM tools to models for simulation, such as Simulink and offer a new possibility for OEM's and suppliers to abstract system models into executable models. The main contribution of this paper is that the automotive software development process is showed from its concept to its realization in real systems. © 2017 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu2017359,
author={Liu, B. and Lucia and Nejati, S. and Briand, L.C.},
title={Improving fault localization for Simulink models using search-based testing and prediction models},
journal={SANER 2017 - 24th IEEE International Conference on Software Analysis, Evolution, and Reengineering},
year={2017},
pages={359-370},
doi={10.1109/SANER.2017.7884636},
art_number={7884636},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018455025&doi=10.1109%2fSANER.2017.7884636&partnerID=40&md5=0fbeef0acd8861b0d44726257f31e0ac},
affiliation={SnT Centre, University of Luxembourg, Luxembourg},
abstract={One promising way to improve the accuracy of fault localization based on statistical debugging is to increase diversity among test cases in the underlying test suite. In many practical situations, adding test cases is not a cost-free option because test oracles are developed manually or running test cases is expensive. Hence, we require to have test suites that are both diverse and small to improve debugging. In this paper, we focus on improving fault localization of Simulink models by generating test cases. We identify three test objectives that aim to increase test suite diversity. We use these objectives in a search-based algorithm to generate diversified but small test suites. To further minimize test suite sizes, we develop a prediction model to stop test generation when adding test cases is unlikely to improve fault localization. We evaluate our approach using three industrial subjects. Our results show (1) the three selected test objectives are able to significantly improve the accuracy of fault localization for small test suite sizes, and (2) our prediction model is able to maintain almost the same fault localization accuracy while reducing the average number of newly generated test cases by more than half. © 2017 IEEE.},
author_keywords={Fault localization;  search-based testing;  Simulink models;  supervised learning;  test suite diversity},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Pandey2017429,
author={Pandey, A. and Banerjee, S.},
title={Bio-inspired computational intelligence and its application to software testing},
journal={Handbook of Research on Soft Computing and Nature-Inspired Algorithms},
year={2017},
pages={429-444},
doi={10.4018/978-1-5225-2128-0.ch014},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027521126&doi=10.4018%2f978-1-5225-2128-0.ch014&partnerID=40&md5=2695e9014e737eb6d483cccd0c0dff8e},
affiliation={UPES Dehradun, India; BIT Mesra, India},
abstract={Bio inspired algorithms are computational procedure inspired by the evolutionary process of nature and swarm intelligence to solve complex engineering problems. In the recent times it has gained much popularity in terms of applications to diverse engineering disciplines. Now a days bio inspired algorithms are also applied to optimize the software testing process. In this chapter authors will discuss some of the popular bio inspired algorithms and also gives the framework of application of these algorithms for software testing problems such as test case generation, test case selection, test case prioritization, test case minimization. Bio inspired computational algorithms includes genetic algorithm (GA), genetic programming (GP), evolutionary strategies (ES), evolutionary programming (EP) and differential evolution(DE) in the evolutionary algorithms category and Ant colony optimization(ACO), Particle swarm optimization(PSO), Artificial Bee Colony(ABC), Firefly algorithm(FA), Cuckoo search(CS), Bat algorithm(BA) etc. In the Swarm Intelligence category(SI). © 2017, IGI Global. All rights reserved.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Szala2017303,
author={Szala, J. and Kuc, D.},
title={Determination of pearlite morphology in high-carbon hot rolled steel},
journal={Archives of Metallurgy and Materials},
year={2017},
volume={62},
number={1},
pages={303-308},
doi={10.1515/amm-2017-0045},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017317139&doi=10.1515%2famm-2017-0045&partnerID=40&md5=b26ba05188280f1ab3f7c13d250a615b},
affiliation={Silesian University of Technology, 8 Krasińskiego Str., Katowice, Poland},
abstract={The article presents the results of tests of influence of the thermo-mechanical treatment parameters on the mechanical properties and microstructure of steel C70D for wire rod. The methodology of quantitative description of pearlite morphology in steels with the use of the method on which a new computer program PILS - Pearlite Inter-Lamellar Spacing is based was presented. In order to verify the method, some quantitative tests of microstructure in samples after physical simulation of heat-plastic treatment were conducted on a deformation dilatometer device with diverse cooling rate for steel C70D. The process of rolling was conducted in simulation in continuous finishing train arrangement. Elaborated program and conducted tests will be used during preparations of modified technologies of wire rod rolling to prepare products made of steel, the microstructure of which is characterised with smaller interlamellar spacing. © 2017 Polish Academy of Sciences.},
author_keywords={interlamellar spacing;  microstructure;  physical simulation;  quantitative metallography},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kudaravalli201743,
author={Kudaravalli, S. and Faraj, S. and Johnson, S.L.},
title={A configural approach to coordinating expertise in software development teams},
journal={MIS Quarterly: Management Information Systems},
year={2017},
volume={41},
number={1},
pages={43-64},
doi={10.25300/MISQ/2017/41.1.03},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016300216&doi=10.25300%2fMISQ%2f2017%2f41.1.03&partnerID=40&md5=9a3093d9b42cdb97fcc9493dd0bd89e6},
affiliation={HEC Paris, 1 rue de la Libération, Joun-en-Josas Cedex, 78351, France; Desautels Faculty of Management, McGill University, Montreal, QC  H3A 1G5, Canada; McIntire School of Commerce, University of Virginia, Rouss and Robertson Halls, East Lawn, Charlottesville, VA  22904, United States},
abstract={Despite the recognition of how important expertise coordination is to the performance of software development teams, understanding of how expertise is coordinated in practice is limited. We adopt a configural approach to develop a theoretical model of expertise coordination that differentiates between design collaboration and technical collaboration. We propose that neither a strictly centralized, top-down model nor a largely decentralized approach is superior. Our model is tested in afield study of 71 software development teams. We conclude that because design work addresses ill-structured problems with diverse potential solutions, decentralization of design collaboration can lead to greater coordination success and reduced team conflict. Conversely, technical work benefits from centralized collaboration. We find that task knowledge tacitness strengthenstheserelationshipsbetweencollaborationconfigurationandcoordinationoutcomesandthatteam conflict mediates the relationships. Our findings underline the need to differentiate between technical and design collaboration and point to the importance of certain configurations in reducing team conflict and increasing coordination success in software development teams. This paper opens up new research avenues to explore the collaborative mechanisms underlying knowledge team performance.},
author_keywords={Centralization;  Configuration;  Coordination success;  Expertise coordination;  Knowledge tacitness;  Software development;  Software teams;  Team conflict},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zamli201735,
author={Zamli, K.Z. and Din, F. and Baharom, S. and Ahmed, B.S.},
title={Fuzzy adaptive teaching learning-based optimization strategy for the problem of generating mixed strength t-way test suites},
journal={Engineering Applications of Artificial Intelligence},
year={2017},
volume={59},
pages={35-50},
doi={10.1016/j.engappai.2016.12.014},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007109693&doi=10.1016%2fj.engappai.2016.12.014&partnerID=40&md5=6f19ccc228498f475ab874163f1ab865},
affiliation={IBM Centre of Excellence, Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, Lebuhraya Tun Razak, Kuantan, Pahang Darul Makmur  26300, Malaysia; Faculty of Computer Science and Information Technology, Universiti Putra Malaysia, Malaysia; Department of Computer Science, Faculty of Electrical Engineering, Czech Technical University, Karlovo n´am. 13, Praha 2, 121 35, Czech Republic},
abstract={The teaching learning-based optimization (TLBO) algorithm has shown competitive performance in solving numerous real-world optimization problems. Nevertheless, this algorithm requires better control for exploitation and exploration to prevent premature convergence (i.e., trapped in local optima), as well as enhance solution diversity. Thus, this paper proposes a new TLBO variant based on Mamdani fuzzy inference system, called ATLBO, to permit adaptive selection of its global and local search operations. In order to assess its performances, we adopt ATLBO for the mixed strength t-way test generation problem. Experimental results reveal that ATLBO exhibits competitive performances against the original TLBO and other meta-heuristic counterparts. © 2016 Elsevier Ltd},
author_keywords={Mamdani fuzzy inference system;  Software testing;  t-way testing;  Teaching learning-based optimization algorithm},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chiu2017422,
author={Chiu, C.-C. and Weng, M.-C. and Huang, T.-H.},
title={Characterization of clastic rock using a biconcave bond model of DEM},
journal={International Journal for Numerical and Analytical Methods in Geomechanics},
year={2017},
volume={41},
number={3},
pages={422-441},
doi={10.1002/nag.2568},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983436857&doi=10.1002%2fnag.2568&partnerID=40&md5=bc5430585fc8fec21ae50da640e792ad},
affiliation={Department of Civil Engineering, National Taiwan University, Taipei, Taiwan; Department of Civil and Environmental Engineering, National University of Kaohsiung, Kaohsiung, Taiwan},
abstract={This paper presents a biconcave bond model to investigate the effect of the cementation between grains on the mechanical behavior of rock. The proposed model considers the shape of the bonds among particles that have a biconcave cement form, based on observations of microscopic rock images. The general equations of the proposed model are based on Dvorkin theory. The accuracy and efficiency of the bond model is improved in three ways. After the biconcave bond model is implemented in the discrete element method software Particle Flow Code in 2 Dimensions, a series of numerical uniaxial compression tests were performed to investigate the relationships between the micro- to macro-parameters. The simulations revealed that the biconcave bond model reflects the effect of micro-parameters, such as the elastic modulus and Poisson's ratio of the cement, on the macroscopic deformation of cemented granular material. Variations in the bond geometry caused extremely diverse macro-mechanical behaviors. Experimental results concerning rock demonstrate that the biconcave bond model accurately captures the mechanical behavior of intact rock and supports an innovative method for investigating the relationships between the micro- and macro-parameters of cemented granular material. Copyright © 2016 John Wiley & Sons, Ltd. Copyright © 2016 John Wiley & Sons, Ltd.},
author_keywords={Bond;  cementation;  discrete element method (DEM);  granular material;  rock},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nielson2017255,
author={Nielson, S.J.},
title={PLAYGROUND: preparing students for the cyber battleground},
journal={Computer Science Education},
year={2017},
volume={26},
number={4},
pages={255-276},
doi={10.1080/08993408.2016.1271526},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009291602&doi=10.1080%2f08993408.2016.1271526&partnerID=40&md5=e6f40fb6e0b7eb649734efe88e6d04da},
affiliation={Department of Computer Science, Johns Hopkins University, Baltimore, MD, United States},
abstract={Attempting to educate practitioners of computer security can be difficult if for no other reason than the breadth of knowledge required today. The security profession includes widely diverse subfields including cryptography, network architectures, programming, programming languages, design, coding practices, software testing, pattern recognition, economic analysis, and even human psychology. While an individual may choose to specialize in one of these more narrow elements, there is a pressing need for practitioners that have a solid understanding of the unifying principles of the whole. We created the Playground network simulation tool and used it in the instruction of a network security course to graduate students. This tool was created for three specific purposes. First, it provides simulation sufficiently powerful to permit rigorous study of desired principles while simultaneously reducing or eliminating unnecessary and distracting complexities. Second, it permitted the students to rapidly prototype a suite of security protocols and mechanisms. Finally, with equal rapidity, the students were able to develop attacks against the protocols that they themselves had created. Based on our own observations and student reviews, we believe that these three features combine to create a powerful pedagogical tool that provides students with a significant amount of breadth and intense emotional connection to computer security in a single semester. © 2017 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={lab work;  Security;  simulation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mammo201745,
author={Mammo, B. and Lee, D. and Davis, H. and Hou, Y. and Bertacco, V.},
title={AGARSoC: Automated test and coverage-model generation for verification of accelerator-rich SoCs},
journal={Proceedings of the Asia and South Pacific Design Automation Conference, ASP-DAC},
year={2017},
pages={45-50},
doi={10.1109/ASPDAC.2017.7858294},
art_number={7858294},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015282345&doi=10.1109%2fASPDAC.2017.7858294&partnerID=40&md5=e30441dca73cff1b3f1589a4e6860b48},
affiliation={Electrical Engineering and Computer Science, University of Michigan, Ann Arbor, MI  48105, United States},
abstract={SoC design trends show increasing integration of special-purpose, third-party hardware blocks to accelerate diverse types of computation. These accelerator blocks interact with each other in unexpected ways when integrated into a complex, accelerator-rich SoC. In this work we propose a novel solution that guides verification engineers to the high-priority accelerator interaction scenarios during RTL verification. We observe that interaction scenarios frequently exercised by software for the SoC, which is typically developed alongside the RTL, should be the highest priority targets for verification. To this end we analyze the behavior of software executed on high-level simulation models to identify commonly occurring accelerator interaction scenarios. We encapsulate scenarios observed from diverse software executions into an abstract representation that can then be used to extract coverage models and generate test programs. Our experiments show that our solution is able to identify frequently exercised scenarios, extract coverage models, and generate compact, high-quality tests for two completely different SoC designs. © 2017 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hong201780,
author={Hong, S. and Kwak, T. and Lee, B. and Jeon, Y. and Ko, B. and Kim, Y. and Kim, M.},
title={MUSEUM: Debugging real-world multilingual programs using mutation analysis},
journal={Information and Software Technology},
year={2017},
volume={82},
pages={80-95},
doi={10.1016/j.infsof.2016.10.002},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992146806&doi=10.1016%2fj.infsof.2016.10.002&partnerID=40&md5=3bcc6e927079eea04256c3c2191a6cb6},
affiliation={Handong Global University, 558 Handong-ro, Buk-gu, Pohang, Gyeongbuk  37554, South Korea; Korea Institute of Science and Technology, 291 Daehak-ro, Yuseong-guDaejeon  34141, South Korea; Gwangju Institute of Science and Technology, 123 Cheomdangwagi-ro, Buk-guGwangju  61005, South Korea},
abstract={Context: The programming language ecosystem has diversified over the last few decades. Non-trivial programs are likely to be written in more than a single language to take advantage of various control/data abstractions and legacy libraries. Objective: Debugging multilingual bugs is challenging because language interfaces are difficult to use correctly and the scope of fault localization goes beyond language boundaries. To locate the causes of real-world multilingual bugs, this article proposes a mutation-based fault localization technique (MUSEUM). Method: MUSEUM modifies a buggy program systematically with our new mutation operators as well as conventional mutation operators, observes the dynamic behavioral changes in a test suite, and reports suspicious statements. To reduce the analysis cost, MUSEUM selects a subset of mutated programs and test cases. Results: Our empirical evaluation shows that MUSEUM is (i) effective: it identifies the buggy statements as the most suspicious statements for both resolved and unresolved non-trivial bugs in real-world multilingual programming projects; and (ii) efficient: it locates the buggy statements in modest amount of time using multiple machines in parallel. Also, by applying selective mutation analysis (i.e., selecting subsets of mutants and test cases to use), MUSEUM achieves significant speedup with marginal accuracy loss compared to the full mutation analysis. Conclusion: It is concluded that MUSEUM locates real-world multilingual bugs accurately. This result shows that mutation analysis can provide an effective, efficient, and language semantics agnostic analysis on multilingual code. Our light-weight analysis approach would play important roles as programmers write and debug large and complex programs in diverse programming languages. © 2016 The Authors},
author_keywords={Debugging;  Foreign function interface;  Language interoperability;  mutation analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hashem2017,
author={Hashem, R. and Xu, W. and Stommel, M. and Cheng, L.},
title={Conceptualisation and specification of a biologically-inspired, soft-bodied gastric robot},
journal={M2VIP 2016 - Proceedings of 23rd International Conference on Mechatronics and Machine Vision in Practice},
year={2017},
doi={10.1109/M2VIP.2016.7827316},
art_number={7827316},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015146220&doi=10.1109%2fM2VIP.2016.7827316&partnerID=40&md5=7da9fbbb797219cee576d2376afc73c5},
affiliation={Department of Mechanical Engineering, University of Auckland, Auckland, New Zealand; Department of Electrical and Electronic Engineering, Auckland University of Technology, Auckland, New Zealand; Auckland Bioengineering Institute, University of Auckland, Auckland, New Zealand},
abstract={Digestion of food is a regular activity in many species to maintain nourishment and growth. In humans, a typical digestion depends on the process of a churning action to break down the food in a stomach. Medical and biomedical experts stated that this physical motility frequency and deformation significantly affect the digestion process. Antral contraction wave (ACW) is the mechanical peristalsis action occurs in the smooth muscles of a stomach. Researchers from diverse disciplines are seeking more understanding of the ACW mechanism. The current state of physical stomach replicants is usually rigid models. The motility of these robots is different compared to the observed biological organ. In response to this gap, the interest of the proposed robot is to mimic the ACW physical deformation more similar to the gastric motility. It is assumed that the biologically-inspired robot will facilitate the investigation of ACW by physical peristaltic actuation model. The conceptualisation and specification of the soft gastric actuator discussed in this article. The primary robot conduit had been designed with geometry similar to that in the biological gastric. The proposed actuation method was tested by finite element analysis (FEA) software preceding the final conceptual gastric-soft robot design. © 2016 IEEE.},
author_keywords={Biologically inspired robot;  Gastric soft robot;  Peristaltic motility;  Soft robotics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li201730,
author={Li, Y. and Du, J. and Hu, Q. and Liu, X.},
title={A method for structure-oriented regression test path generation},
journal={Proceedings - 2016 International Symposium on System and Software Reliability, ISSSR 2016},
year={2017},
pages={30-36},
doi={10.1109/ISSSR.2016.015},
art_number={7810794},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014088122&doi=10.1109%2fISSSR.2016.015&partnerID=40&md5=23acdae7086be76ae5a0fe45967c1814},
affiliation={School of Information Science and Technology, Qingdao University of Science and Technology, Qingdao, 266061, China},
abstract={With the spread of Internet application, the software products need update quickly to adapt the dynamic application environments and diverse business requirement. How to efficiently select the regression test cases based on existing test cases set directly affect the evolution quality of software products. Basic paths coverage is a popular structure-oriented test way in academia and industry. A method to analyze the possible execution path of software components affected by update version based on program slicing method is proposed in this paper. Some matching pattern criterions are put forward and they can incrementally extend the test cases that fail to match. The structure-oriented regression test paths can be easily obtained from these matching pattern criterions. Moreover, the automatic generation of regression test path can be achieved in the process of software product evolution. An example is also presented to show the generation of regression test path in the level of source codes. © 2016 IEEE.},
author_keywords={Matching pattern;  Program slicing;  Regression test;  Test path},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lorang2017,
author={Lorang, T. and Carr, J.},
title={C-17A globemaster III aft fuselage drag reduction flight demonstrations},
journal={AIAA Flight Testing Conference, 2017},
year={2017},
page_count={7},
doi={10.2514/6.2017-3653},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085406114&doi=10.2514%2f6.2017-3653&partnerID=40&md5=ea6367bad32db7fd10f3f555dd278612},
affiliation={Boeing Defense, Space & Security, BDS Flight Aerodynamics, Mail Stop H013-C336, Huntington Beach, CA  92647, United States; Boeing Defense, Space & Security, BDS Global Support and Sustainment, Mail Stop D800-0034, Long Beach, CA  90808, United States},
abstract={Three concepts with potential to reduce fuel burn and harmful engine emissions during long range cruise flight were evaluated on a C-17 A aircraft during a one-year flight test demonstration program at Edwards Air Force Base, California. The effort was led by the Air Force Research Laboratory (AFRL) and utilized the combined resources of a diverse array of organizations and contractors. The concepts focused on using passive devices designed to reduce aircraft drag by controlling vortex shedding on the aft fuselage. The devices were installed and multiple flights flown to collect data for the evaluation and comparison of the fuel-efficiency benefits and operational risks of the concepts. This paper will discuss the concepts evaluated, test objectives and execution, and results of the evaluation. © 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Choudhary2017899,
author={Choudhary, K. and Nahata, A. and Shilpa},
title={An implication of multi-objective optimization in test case generation},
journal={Communication and Computing Systems - Proceedings of the International Conference on Communication and Computing Systems, ICCCS 2016},
year={2017},
pages={899-904},
doi={10.1201/9781315364094-162},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058073209&doi=10.1201%2f9781315364094-162&partnerID=40&md5=5186134b013abdc2c3f4d49655a7f499},
affiliation={JK Lakshmipat University, Jaipur, India; ITM University, Gurgaon, Haryana, India},
abstract={Test case generation is a priority task in software development methodology. Current scenario of testing field is diversified toward automation. The prime objective of paper is automated generation of Boolean-specific test cases; and another objective is optimization of generated test cases using multi-objective genetic algorithm approach for Boolean operator based conditions. © 2017 Taylor & Francis Group, London.},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Goel20171,
author={Goel, S.K. and Chakrabarty, K.},
title={Testing for small-delay defects in nanoscale CMOS integrated circuits},
journal={Testing for Small-Delay Defects in Nanoscale CMOS Integrated Circuits},
year={2017},
pages={1-247},
doi={10.1201/b15549},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051832756&doi=10.1201%2fb15549&partnerID=40&md5=adfc6cef5970186a8fe8c5fd52c9dd67},
affiliation={Taiwan Semiconductor Manufacturing Co. (TSMC), San Jose, CA, United States; Duke University, Durham, NC, United States; Tsinghua University, Beijing, China; National Cheng Kung University, Taiwan; University of Bremen, Germany},
abstract={Advances in design methods and process technologies have resulted in a continuous increase in the complexity of integrated circuits (ICs). However, the increased complexity and nanometer-size features of modern ICs make them susceptible to manufacturing defects, as well as performance and quality issues. Testing for Small-Delay Defects in Nanoscale CMOS Integrated Circuits covers common problems in areas such as process variations, power supply noise, crosstalk, resistive opens/bridges, and design-for-manufacturing (DfM)-related rule violations. The book also addresses testing for small-delay defects (SDDs), which can cause immediate timing failures on both critical and non-critical paths in the circuit. • Overviews semiconductor industry test challenges and the need for SDD testing, including basic concepts and introductory material • Describes algorithmic solutions incorporated in commercial tools from Mentor Graphics • Reviews SDD testing based on “alternative methods” that explores new metrics, top-off ATPG, and circuit topology-based solutions • Highlights the advantages and disadvantages of a diverse set of metrics, and identifies scope for improvement Written from the triple viewpoint of university researchers, EDA tool developers, and chip designers and tool users, this book is the first of its kind to address all aspects of SDD testing from such a diverse perspective. The book is designed as a one-stop reference for current industrial practices, research challenges in the domain of SDD testing, and recent developments in SDD solutions. © 2014 by Taylor & Francis Group, LLC.},
document_type={Book},
source={Scopus},
}

@CONFERENCE{Wilks201792,
author={Wilks, R. and Scholes, E.},
title={DNA sequencing of pulp and paper wastewater treatment systems to inform process analysis},
journal={Fibre Value Chain 2017 Conference},
year={2017},
volume={2017-November},
pages={92-99},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051786726&partnerID=40&md5=5df88c6b157d1ae0391483233c5afa5e},
affiliation={Australian Paper, Mary vale, VIC  3869, Australia; Monash University, Australia},
abstract={Australian Paper is located 150km east of Melbourne and produces a range of packaging and printing papers. This site is an integrated pulp and paper mill with a wastewater treatment system on site. The wastewater treatment system prepares water for final discharge under EPA licence. An analytical program was launched in December 2016 which utilises DNA sequencing, online parameters and chemical testing to supply information for correlation and multivariate studies. The focus of the study was key microbiological processes within the wastewater treatment plant in which microorganisms are used to treat and purify the wastewater. Analysis to date is preliminary, however some trends are readily apparent. The influent of the primary clarifiers was found to be the least diverse (white paper less than brown paper). The primary treatment area was heavily dominated by the Genera Cloacibacterium and Hydrogenophaga suggesting these are major populations in the paper making processes. In the aerated system, the diversity markedly increases with major populations of Microbacterium, Magnetospirrilum and Asticcacaulis. DNA sequencing results show that the phylum Cyanobacteria (algae) increased throughout the system and averaged of 18.8% of sequences detected in the discharge. Correlation analysis shows that the TSS concentration at the river discharge is affected by the temperature for the 28d period prior to discharge. This combined evidence supports the hypothesis that algal growth is an important contributor to TSS in the river discharge. Biomass removal from the secondary clarifier has in inverse relationship with BOD in the river discharge. In light of this finding, maintenance on equipment involved in biomass removal should be high priority to ensure maximum biomass removal. These preliminary results give insight into the complex nature of the Australian Paper treatment system and will inform operational action to minimise environmental impact. The results also show that tracking Bacterial populations using Next Generation sequencing is a viable undertaking and is a valuable addition to the conventional monitoring methods. © 2018 Appita Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Birkeland201711125,
author={Birkeland, R. and Gjersvik, A. and Grande, J.},
title={The benefit of project based courses as a ⇜first contact” between students and space industry},
journal={Proceedings of the International Astronautical Congress, IAC},
year={2017},
volume={17},
pages={11125-11130},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051444605&partnerID=40&md5=5d4e664aed26e18d5eef0f8c54580751},
affiliation={Department of Electronic Systems, Norwegian University of Science, Technology NTNU, Trondheim, Norway; Department of Electronic Systems, NTNU, Norway},
abstract={Space technology plays an ever larger role in our society, even though most people are unaware of this fact. Luckily, an increasing interest in space technology among students has been observed over the last years. At some universities, space related courses are available as part of a physics degree or an engineering degree with a special focus on e.g. satellite communications or aerospace engineering. In Norway, there exists no specialized aerospace education resulting in a Bachelor or Master's degree. Introductory courses and project work, both within the curriculum and as volunteer activities, are used as a “first contact” between students, space technology and space industry. At NTNU project classes such as the multidisciplinary group work course Experts in Teamwork (EiT) and more long term group projects, such as the NUTS project are examples of this. Many students are very fond of space and space technology and it is often a motivational factor for STEM-studies in general. Unfortunately, only a few students get the chance to directly work with space technology during their studies. Space related project work will therefore further nourish the space interest. Even if the space industry in Norway is quite substantial with a turnover of around 640 M EUR / year, it is fairly unknown both to most students as well as to the general public. As a consequence, space related job opportunities (both nationally and internationally) are not well known. An improved connection between students and the industry will hopefully lead to the most motivated students getting relevant industry jobs after graduation. The industry plays an important role in making relevant jobs both known and available. This year, the Kongsberg Group launched the Starburst summer intern program in close cooperation with NAROM. Even without any aerospace program, several space related projects are available at NTNU. One example is NTNU Test Satellite (NUTS) where students are designing and prototyping hardware and software for a CubeSat. EiT offers a wide range of topics. Projects are diverse and spans from creating a “Mars lab” on Earth, working as part of an international project investigating how to better track satellites during the initial launch phase or building new payloads for a student rocket in close cooperation with NAROM. As space technology is international and multidisciplinary, it is important to allow students to both share and gain experiences by participating at relevant international conferences and workshops. © 2018 International Astronautical Federation IAF. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gunawan20171,
author={Gunawan, S. and Fowler, D.A. and Mack, A.},
title={An advanced, balanced, and high-performance displacement spacer system for deepwater formations},
journal={Proceedings - SPE International Symposium on Oilfield Chemistry},
year={2017},
volume={2017-April},
pages={1-7},
doi={10.2118/184516-ms},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050462024&doi=10.2118%2f184516-ms&partnerID=40&md5=8f4ff570241d4eea15fd781d661bd243},
affiliation={TETRA Technologies, Inc., United States},
abstract={A successful wellbore displacement and clean-up is a critical step during well completion operations. A poor cleaning performance can lead to increased cost of completion operations, potential formation damage, and thus reduced oil and gas productions. Failure to achieve a balanced displacement system can lead to negative pressures on cement jobs and tie-backs, wellbore control issues, and wellbore instability. Deepwater wells require a crucial and complete evaluation of a displacement system which will provide a balanced, effective, and efficient cleaning spacer system. Traditional displacement spacer systems typically consist of a solvent spacer and a surfactant spacer. A solvent spacer consisting of d-limonene is effective for removing oil-based drilling fluids from the wellbore; however, this is incompatible in water-based spacer systems and results in an underbalanced displacement system. On the other hand, a surfactant spacer is ineffective in displacing ester/isomerized olefin (IO) based drilling fluids. As the chemistry of drilling fluid additives become more diversified to accommodate various base oils, such as the blend of ester/IO used here, so too must the chemistry of cleaner/displacement additives to ensure appropriate compatibility. On many occasions, barite residue from the weighted spacer was found to adhere strongly to pipe surfaces and be nearly impossible to remove chemically upon contact with the surfactant spacer. This issue can cause incomplete zonal isolation. A new, balanced, and high-performance displacement system (ADS-RD) was developed and optimized for its ability to effectively remove oil-based drilling fluids, barite residue, and water wet pipe surfaces. Lab test results showed that ADS-RD was fully compatible, consistently achieved an average of 98% cleaning efficiency, and yielded water wet pipe surfaces against the ester/IO based invert emulsion drilling fluids. ADS-RD also proved to be an effective and efficient solution to other oil-based invert emulsion drilling fluids. This paper presents the testing program and lab test results for ADS-RD against a traditional displacement system with various oil-based invert emulsion drilling fluid systems with densities up to 14.8 lb/gal. This paper will also include cost savings and recommendations for field applications. Copyright © 2017, SPE International Conference on Oilfield Chemistry.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Behounek2017399,
author={Behounek, M. and Hofer, E. and Thetford, T. and White, M. and Yang, L. and Llc, T.},
title={Taking a different approach to drilling data aggregation to improve drilling performance},
journal={SPE/IADC Drilling Conference, Proceedings},
year={2017},
volume={2017-March},
pages={399-411},
doi={10.2118/184741-ms},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048852860&doi=10.2118%2f184741-ms&partnerID=40&md5=5b7fc63284d2424a3078f48420fe2dd2},
affiliation={Apache Corp, United States; Marcos Taccolini, United States},
abstract={Currently, there is a multitude of commercially available real-time drilling data aggregation and distribution systems, yet the industry remains plagued with issues that limit the usability and effectiveness of data before, during, and after a well is drilled. There are challenges with moving, merging, analyzing, qualifying, and formatting data as well as having access to like-data in sufficient quantity and on a reliable data frequency. This paper discusses a novel, adaptable, and low cost approach to building a system to drive drilling performance and set the stage for future automation. The Operator embarked on a project to develop a powerful, low cost system in order to leverage both high and low frequency data to gain value from real-time data models and algorithms at the rig site. High frequency data is defined as 1 to 100 Hertz data frequency. Low frequency data is defined as longer than once per hour or asynchronous, and is usually contextual - BHA information, mud reports, rig state, etc. Existing commercial systems fail to meet the requirements due to multiple factors. These include an inability to handle and process high frequency data, communicate with different protocols, and work across different proprietary systems. The result leads to higher costs, extra human resources and efforts, and a lack of consistency across a diverse rig fleet. Druing this process severe data quality issues were discovered at the rig site and needed the flexibility to modify, replace, or add sensors and data streams to remedy the problem. After evaluating more than thirty potential process controls and other industry applications, a software solution was selected, prototyped, tested and deployed to seven North American land rigs within a ten month period. This effort employed the agile development methodology which is an incremental, iterative work cadence using empirical feedback for rapid deployment of updated versions. The system was designed to take in all forms of data, file types, and communication protocols for seamless integration. The system includes rig state determination, data quality verification, a real time Bayesian model for analytics and smart alarms, integration to the Daily Drilling Report (DDR) database, real-time visualizations, and an open application layer with a Human Machine Interface (HMI) - all at the rig site. Ultimately the platform can also be used as a building block to assist automated drilling due to it being a Supervisory Control Advisory and Data Acquisition (SCADA) system although this is not the goal for this project. Copyright © 2017, SPE/IADC Drilling Conference and Exhibition.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2017551,
author={Li, M. and Kang, H.G. and Lee, S.H. and Lee, S.J. and Chu, T.-L. and Varuttamaseni, A. and Yue, M. and Cho, J.},
title={Treatment of expert opinion diversity in Bayesian belief network model for nuclear digital I&C safety software reliability assessment},
journal={International Topical Meeting on Probabilistic Safety Assessment and Analysis, PSA 2017},
year={2017},
volume={1},
pages={551-558},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047835161&partnerID=40&md5=f8180126acd7510a81cea5e353e20b9c},
affiliation={U.S. Nuclear Regulatory Commission, Washington, DC, United States; Department of Mechanical, Aerospace, and Nuclear Engineering, Rensselaer Polytechnic Institute, 110 8th St, Troy, NY, United States; School of Mechanical and Nuclear Engineering, Ulsan National Institute of Science and Technology, 50, UNIST-gil, Ulsan, South Korea; Brookhaven National Laboratory, Brookhaven Avenue, Upton, NY, United States; Korea Atomic Energy Research Institute, 111, Daedeok-daero, Daejeon, South Korea},
abstract={Since digital instrumentation and control systems are expected to play an important role in safety systems in nuclear power plants (NPPs), the need to incorporate software failures into NPP probabilistic risk assessments has arisen. In order to estimate the failure probability of safety software in NPP and incorporate it into a PRA model, a Bayesian belief network (BBN) model was developed which estimates the number of defects in the software considering the software development life cycle (SDLC) characteristics. In the model, due to a lack of sufficient safety software operation experience data, expert opinion was instead used to quantify the distributed node probability tables (NPTs) that are tables of random variables whose probabilistic distributions were aggregated from experts' elicitation. In addition, handbook data on U.S. software developments and V&V as well as the testing results of two example nuclear safety software were used to Bayesian update the BBN distributed NPTs in order to reduce the BBN parameter uncertainty from the diverse expert opinion. Based on the estimated NPTs, the number of defects at each SDLC phase is evaluated for the typical digital protection software (50 function points and Medium development, V&V quality). This study is expected to provide insight on several aspects of BBN model quantification for nuclear safety-related software reliability assessment, including the expert opinion elicitation and aggregation, the representation of the node probabilities using probability distribution, and the Bayesian updating of the NPTs using available software development data. � 2018 American Nuclear Society - International Topical Meeting on Probabilistic Safety Assessment and Analysis, PSA 2007. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Arndt20172031,
author={Arndt, S.A. and Alvarado, R. and Dittman, B. and Mott, K. and Wood, R.},
title={NRC technical basis for evaluation of its position on protection against common cause failure in digital systems used in nuclear power plants},
journal={10th International Topical Meeting on Nuclear Plant Instrumentation, Control, and Human-Machine Interface Technologies, NPIC and HMIT 2017},
year={2017},
volume={3},
pages={2031-2045},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047823681&partnerID=40&md5=d9019b3ad0e22173b4b8d2aec35a2e07},
affiliation={U.S. Nuclear Regulatory Commission, Washington, DC  20555, United States; Department of Nuclear Engineering, University of Tennessee, Knoxville, TN  37996, United States},
abstract={Digital technology has advantages over analog systems, including automated monitoring and alerts for standby safety functions, and predictive algorithms to maintain critical safety systems. Additionally, digital technology generally has higher reliability and can be designed to reduce single point vulnerabilities. For these reasons many nuclear plants have applied digital technology to safety and non-safety related applications, including reactor protection system, feedwater and turbine controls, etc. with a corresponding significant improvement in trip reduction. Nonetheless, digital instrumentation and control (I&C) systems also present potential new vulnerabilities that need to be assessed, including potential failures due to increased complexity of digital systems, the introduction of unique failure modes due to software (including software common cause failure (CCF)), and limited operating history of digital systems in nuclear safety related applications compared to analog systems. The fact that software is intangible means that common methods, such as analysis or testing, used for detecting CCF may not be effective when applied to software. Consequently, digital technology is perceived to pose a potential risk from the introduction of undetected systematic faults that could result in CCF. Despite the I&C system upgrades and modifications performed to date, the U.S. Nuclear Regulatory Commission (NRC) and industry stakeholders have identified the need to modernize the regulatory infrastructure to efficiently address risks associated with the use of digital technology for nuclear safety applications and address regulatory uncertainties. The NRC’s current position on CCF is guided by the staff requirements memorandum (SRM) on SECY 93-087. The SRM provides specific acceptance criteria for the evaluation of CCF, which the staff implemented in the Branch Technical Position (BTP) 7-19. However, industry stakeholders have proposed using methods to characterize the likelihood of software CCF and eliminate it from further consideration in a defense-in-depth and diversity analysis. The NRC’s current position does not consider these alternatives, and thus corresponding acceptance criteria is not currently available. The work discussed in this paper assesses the underlying technical basis associated with CCF, provides technical support for updating the NRC position and considers proposed methods for addressing potential CCF in digital systems while enhancing efficiency, clarity, and confidence. Copyright © (2017) by American Nuclear Society. All rights reserved.},
author_keywords={Common cause failure;  Control system;  Defense-in-depth;  Digital technology;  Diversity;  Instrumentation;  Software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Won20172223,
author={Won, J.-Y. and Cotton, B.},
title={Stress path triaxial tests for a deep open cut excavation},
journal={ICSMGE 2017 - 19th International Conference on Soil Mechanics and Geotechnical Engineering},
year={2017},
volume={2017-September},
pages={2223-2226},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045435768&partnerID=40&md5=4bd2d43108dd9e85c69b6be0304aa167},
affiliation={Barr Engineering Co., United States; GeoEngineers, United States},
abstract={A deep open cut excavation in Pleistocene overconsolidated clay was considered for a river diversion channel. A stress path testing program was carried out to investigate effects of the stress changes on deformation and shear strength of the clay. Total and effective stress changes in vertical and horizontal directions were replicated by actively controlled axial loads, cell pressures and back pressures. Undrained and drained shear strengths from the stress path testing were significantly different from the conventional Ko-consolidated triaxial tests. Complex directional deformations were observed during the stress paths. The stress path approach is an attractive method to investigate soil behaviors for slope stability problems in clay. © 2017 19th ICSMGE Secretariat. All rights reserved.},
author_keywords={Clay;  Excavation;  Shear strength;  Slope stability;  Stress-path;  Triaxial test},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Allanic2017,
author={Allanic, C. and Trallero, J.-L. and Vincent, A.},
title={TOTAL new production data management system: From the concept to deployment},
journal={Society of Petroleum Engineers - SPE Abu Dhabi International Petroleum Exhibition and Conference 2017},
year={2017},
volume={2017-January},
doi={10.2118/188766-ms},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044524400&doi=10.2118%2f188766-ms&partnerID=40&md5=243a89175a4f160c69054d3d7a57f6ae},
affiliation={TOTAL E and P, France},
abstract={Actual context of low barrel prices is driving all field development cost optimizations across the oil industry. This environment is also an opportunity to extend these policies to field production operations. Developing a process oriented Production Data Management System (N-PDMS) grounded on fundamentals, industry best practices and lessons learnt from legacy systems has been the contribution of production departments within TOTAL E&P. The approach should be good enough for handling Total's operations diversity efficiently, with full transparency and traceability. Project's main objective is to continuously improve data quality from sites to offices (affiliates and headquarter teams), partners and authorities in the host countries. A commercial product was selected to develop the N-PDMS template embedding key production processes from Well Test to Reporting passing through Well allocation, Shortfalls, Material Balance, Forecasts, Lifting Planning, Lifting Reporting and Gas Production Program. Both Production and Injection are covered. New functionalities like production forecast, automated data capture based on standard asset models and automated historical data migrations were developed along the way. N-PDMS template is now in the deployment phase in all operated affiliates of TOTAL E&P. This paper describes how the N-PDMS was built as a process driven project; from concepts to their deliverables in a software solution. © 2017, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Blanc2017,
author={Blanc, P. and Ducastel, B. and Cazin, J. and Al Blooshi, M. and Al Dhaheri, S.S. and Al Marzooqi, M.H.A. and Maneux, E. and Ciret, P. and Sow, M. and Massabuau, J.-C.},
title={First-Time implementation of innovative in situ biotechnology on an offshore platform in arabian gulf for continuous water quality monitoring and early leak detection},
journal={Society of Petroleum Engineers - SPE Abu Dhabi International Petroleum Exhibition and Conference 2017},
year={2017},
volume={2017-January},
doi={10.2118/188821-ms},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044508172&doi=10.2118%2f188821-ms&partnerID=40&md5=50a15a053a9020ccbec6ce2248a2cc0f},
affiliation={TOTAL SA, France; TOTAL ABK, Abu Dhabi, United Arab Emirates; Environment Agency, Abu Dhabi, United Arab Emirates; GEO-Transfert, ADERA and University of Bordeaux, France; University of Bordeaux and ADERA, CNRS, France},
abstract={We present first-Time ever implementation of an innovative biomonitoring technology (High-Frequency Non-Invasive Valvometry) on an offshore oil producing platform located in the Arabian Gulf (TOTAL Abu Al Bukhoosh - UAE). This technology is unique; it is a tool allowing continuous in situ monitoring of water quality and detection of hydrocarbons at extremely low levels. This constitutes a key step in responsible management to minimize environmental footprint through the development of an advanced technology for operational use. This technology, developed by CNRS and University of Bordeaux (France), previously tested in a Research & Development program by TOTAL in laboratory, artificial outdoor river streams, and coastal sites, uses bivalves equipped with micro-electrodes recording their opening and closing activity. The latter is interpreted in terms of natural behavior and reaction to stressors. For offshore implementation, a Technical Committee with contributors from all technical departments was established, in order to identify and resolve all operational constraints and issues. The emblematic local pearl oyster Pinctada radiata was selected, and an agreement with the Environment Agency Abu Dhabi enabled appropriate local species collection and preparation. Effective leadership and strong involvement and collaboration of all technical departments in Abu Dhabi and France were key component in achieving success. All technical issues were assessed: HSE, technological safety, logistics, construction, IT, maintenance, metocean data.... Pearl oysters were collected and equipped with electromagnets at the Al Mirfa oyster farm with the support of the Environment Agency of Abu Dhabi. The installation solution adopted consists of valvometers (with multiparameter water probes to measure water temperature, oxygen content, and chlorophyll a), positioned along a suspension cable under the platform and securely moored at seafloor and platform structure. This solution avoids the use of divers (hence reducing both costs and safety risks), and allows easy equipment removal if needed. Crane and 1.3T deadweight were used to install the equipment and ensure mooring. Installation of two valvometers and two water probes was successfully completed in April 2017, before seawater reached a high temperature, to avoid mortality of oysters when manipulated. Valvometers and water probes are installed close to the seabed (-26m water depth) and in the water column (-10m water depth). The system is planned to run until March 2018. Preliminary results are shown and demonstrate that this technology is operationally viable for in-situ continuous water quality monitoring in a real industrial context such as an offshore oil producing platform. Natural biological rhythms of pearl oysters Pinctata radiata are well reconstructed from HFNI Valvometry data (circadian and circatidal cycles), while changes in water quality can be observed from valvometry bar codes data logs and animals' growth rate indices. Yearround data acquisition and capture will data to be generated to give a detailed characterization of oysters' behavior for biomonitoring purpose. As a result, a step change in environmental monitoring solutions for O&G industry is expected to come from this new technology and approach. © 2017, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Niewielski2017490,
author={Niewielski, G. and Kuc, D. and Hadasik, E. and Bednarczyk, I.},
title={Influence of hot working and cooling conditons on the microstructure and properties of C70D steel for wire rod},
journal={METAL 2017 - 26th International Conference on Metallurgy and Materials, Conference Proceedings},
year={2017},
volume={2017-January},
pages={490-495},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043365737&partnerID=40&md5=2425b11b04f5cdef7cc379cd49e9d215},
affiliation={Silesian University of Technology, Katowice, Poland},
abstract={The article presents the results of tests of influence of the thermo-mechanical treatment parameters on the mechanical properties and microstructure of steel C70D for wire rod. The methodology of quantitative description of pearlite morphology in steels with the use of the method on which a new computer program "PILS" - Pearlite Inter-Lamellar Spacing is based was presented. In order to verify the method, some quantitative tests of microstructure in samples after physical simulation of heat-plastic treatment were conducted on a deformation dilatometer device with diverse cooling rate for steel C70D. The process of rolling was conducted in simulation in continuous finishing train arrangement. Elaborated program and conducted tests will be used during preparations of modified technologies of wire rod rolling to prepare products made of steel, the microstructure of which is characterized with smaller interlamellar spacing. © 2017 TANGER Ltd., Ostrava.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2017,
author={Li, Y. and Chen, L. and Men, T. and Yang, Q. and Li, N. and Nie, T.},
title={Power amplifier automatic test system based on LXI bus technology},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2017},
volume={10458},
doi={10.1117/12.2285346},
art_number={104581D},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040514377&doi=10.1117%2f12.2285346&partnerID=40&md5=a60a29021ea5275308adc6a199fb2c4d},
affiliation={Beijing Institute of Radio Measurement, Beijing, 100854, China},
abstract={The power amplifier is an important part of the high power digital transceiver module, because of its great demand and diverse measurement indicators, an automatic test system is designed to meet the production requirements of the power amplifiers as the manual test cannot meet the demand of consistency. This paper puts forward the plan of the automatic test system based on LXI bus technology, introduces the hardware and software architecture of the system. The test system has been used for debugging and testing the power amplifiers stably and efficiently, which greatly saves work force and effectively improves productivity. © 2017 SPIE.},
author_keywords={Automatic test system;  LXI bus;  Power amplifier},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chumakov2017288,
author={Chumakov, P.V. and Egoro, D.S. and Nagdasev, R.V. and Shutov, V.B.},
title={Multi-Purpose Detector (MPD) Slow control system, historical background, status and plans},
journal={CEUR Workshop Proceedings},
year={2017},
volume={2023},
pages={288-292},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040364504&partnerID=40&md5=bd217ced5a5eb301cf6db87326615f25},
affiliation={Veksler and Baldin Laboratory of High Energy Physics, Joint Institute for Nuclear Research, 6 Joliot-Curie, Dubna, Moscow region, 141980, Russian Federation},
abstract={The Multi-Purpose Detector (MPD) is a 4π spectrometer to detect charged hadrons, electrons and photons in heavy-ion collisions at high luminosity in the energy range of the NICA collider. Among many important tasks necessary for successful operation of such a complex apparatus there is one to provide adequate monitoring of operational parameters and convenient control of various equipment used in the experiment. The report presented approaches and basic principles of development of the Slow Control system for the MPD. Tango Control System based approach allows one to unify representation and storage of Slow Control data from many diverse data sources. Presently running BMatN experiment serves as a perfect test-bench for the software. Special attention was paid to integrity of Slow Control data and operation stability. The status and plans of developing Slow Control system design for the MPD is also presented. © 2017 Petr V. Chumakov, Dmitry S. Egorov, Roman V. Nagdasev, Vitaly B. Shutov.},
author_keywords={Detector;  MPD;  NICA;  Slow Control},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sassi2017741,
author={Sassi, J. and Vuorinen, T. and Rytkönen, J.},
title={Material selection study for the brush type arctic skimmers},
journal={40th AMOP Technical Seminar on Environmental Contamination and Response},
year={2017},
pages={741-753},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040075992&partnerID=40&md5=59259a0052a6280af21f62394bdfbee6},
affiliation={VTT Technical Research Centre of Finland Ltd, Espoo, Finland; Finnish Environment Institute (SYKE), Helsinki, Finland},
abstract={The objective of the study was to test several bristle materials, which are designed for arctic brush skimmers, and find out the possible differences in their performance with two different oil type and in three temperatures. Additional objective was to clarify the possible correlation between the results from Wilhelmy plate experiments and macro-scale test results. The tests were performed in the collaboration with VTT and SYKE. Two test drums from both bristle materials were manufactured for the macro scale tests, one for marine diesel oil and one for heavy fuel oil. The design of the drums and bristles were predefined to enable the comparison of the results. The test program was two-fold; in the first phase totally 11 different materials were tested with the Wilhelmy plate method in order to determine the surface tension of the particular test oil with the bristle materials. According to the Wilhelmy plate measurements, two materials were selected for the macro-scale tests. The tests were carried out according to a specific test program particularly prepared for the macro-scale tests. All the tests were performed without water or ice, and depending on the oil type and temperature, each test was repeated 5 or 10 times. The results indicated that the differences in the oil pick-up rates were smaller with heavy fuel oil than with marine diesel oil, and different materials had diverse trends in the variation of the pick-up rates between the test runs. In addition, the correlation between the Wilhelmy plate experiments and the macro scale tests could be observed.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gunabalan2017295,
author={Gunabalan, R. and Binu Ben Jose, D.R. and Sanjeevikumar, P.},
title={Buck-Boost LED driver with dimming characteristics},
journal={Lecture Notes in Electrical Engineering},
year={2017},
volume={436},
pages={295-305},
doi={10.1007/978-981-10-4394-9_30},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038884360&doi=10.1007%2f978-981-10-4394-9_30&partnerID=40&md5=b3af57a0c9f5a9a21aaa88aa61d89fd4},
affiliation={School of Electrical Engineering, Vellore Institute of Technology (VIT) University, Chennai, India; Department of Electrical and Electronics Engineering, University of Johannesburg, Auckland Park, Johannesburg, South Africa},
abstract={Light Emitting Diode (LED) lighting plays a major role nowadays in industry and commercial applications. An efficient control technique is introduced for low power LED lighting with dimming characteristics. The driver circuit consists of a simple buck-boost converter with dc input voltage. The dimming characteristics are achieved by relating a low and high frequency control signal in order to eliminate flickering and colour shift. Software implementation is performed in MATLAB-simulink for a power rating of 10 W and tested under diverse brightness conditions. The simulation results demonstrate that dimming can be achieved with high efficiency. © Springer Nature Singapore Pte Ltd. 2018.},
author_keywords={Buck-boost converter;  LED dimming;  LED lighting;  Pulse width modulation},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{Azouaou201724,
author={Azouaou, L. and Slimani, N. and Chadlia, A.},
title={Development issues of alternative energy in Algeria},
journal={Defect and Diffusion Forum},
year={2017},
volume={379},
pages={24-30},
doi={10.4028/www.scientific.net/DDF.379.24},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85036667665&doi=10.4028%2fwww.scientific.net%2fDDF.379.24&partnerID=40&md5=a1fb475ee1192e32c28be44c2f80a89e},
affiliation={Superior Business School, Kolea, Algeria; Graduate School of Management and International Trade, Kolea, Algeria},
abstract={Nowadays, global warming is central to all discussions, as governments are trying to make decisions in order to slow the greenhouse effect. This lead to the creation of "sustainable development", which relies on renewable energies in order to preserve the environment. The development issues of renewable energy in Algeria drove the Algerian State to integrate it in the national energy mix. It is a major challenge regarding the preservation of fossil fuels, diversification of electricity production chains and contribution to sustainable development. Algeria launched a renewable energy development program going from 2011 to 2030 in order to revive economic growth in this country. The program recently concluded its first phase dedicated to pilot projects and testing various technologies available. The Renewable Energies and Energy Efficiency Development Program (REEEDP), in the revised version by the services of the Department of Energy, has just been adopted with the conclusions highlighted by its first phase. © 2017 Trans Tech Publications, Switzerland.},
author_keywords={Algeria;  Economic growth;  Renewable energies;  Sustainable development;  Technologies},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shastry201726,
author={Shastry, B. and Leutner, M. and Fiebig, T. and Thimmaraju, K. and Yamaguchi, F. and Rieck, K. and Schmid, S. and Seifert, J.-P. and Feldmann, A.},
title={Static Program Analysis as a Fuzzing Aid},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10453 LNCS},
pages={26-47},
doi={10.1007/978-3-319-66332-6_2},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032868095&doi=10.1007%2f978-3-319-66332-6_2&partnerID=40&md5=e03776e682c9c04e4e04875ac88ab361},
affiliation={TU Berlin, Berlin, Germany; TU Braunschweig, Braunschweig, Germany; Aalborg University, Aalborg, Denmark},
abstract={Fuzz testing is an effective and scalable technique to perform software security assessments. Yet, contemporary fuzzers fall short of thoroughly testing applications with a high degree of control-flow diversity, such as firewalls and network packet analyzers. In this paper, we demonstrate how static program analysis can guide fuzzing by augmenting existing program models maintained by the fuzzer. Based on the insight that code patterns reflect the data format of inputs processed by a program, we automatically construct an input dictionary by statically analyzing program control and data flow. Our analysis is performed before fuzzing commences, and the input dictionary is supplied to an off-the-shelf fuzzer to influence input generation. Evaluations show that our technique not only increases test coverage by 10–15% over baseline fuzzers such as afl but also reduces the time required to expose vulnerabilities by up, to an order of magnitude. As a case study, we have evaluated our approach on two classes of network applications: nDPI, a deep packet inspection library, and tcpdump, a network packet analyzer. Using our approach, we have uncovered 15 zero-day vulnerabilities in the evaluated software that were not found by stand-alone fuzzers. Our work not only provides a practical method to conduct security evaluations more effectively but also demonstrates that the synergy between program analysis and testing can be exploited for a better outcome. © 2017, Springer International Publishing AG.},
author_keywords={Fuzzing;  Program analysis;  Protocol parsers},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Manteghi2017,
author={Manteghi, S. and Gibson, D. and Johnston, C.},
title={Fatigue performance of friction welds manufactured both in air and underwater},
journal={Proceedings of the International Conference on Offshore Mechanics and Arctic Engineering - OMAE},
year={2017},
volume={4},
doi={10.1115/OMAE201762495},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031908928&doi=10.1115%2fOMAE201762495&partnerID=40&md5=8b14fb1f423c0a861c0efcc4105015d8},
affiliation={BP Exploration Operating Company Ltd, Sunbury, United Kingdom; Proserv, Westhill, Aberdeenshire, United Kingdom; TWI Ltd, Great Abington, Cambridge, United Kingdom},
abstract={Friction welding is being performed offshore in environments where arc welding may be difficult and where fatigue performance is critical. Friction welding underwater with Remotely Operated Vehicles (ROVs) can greatly reduce the cost of a project compared with using divers and arc welding because the support vessel, which is the major cost component in such an operation, is smaller. This paper describes two different programs of experimental work in which the fatigue endurance of friction welds were found to be better than that which could be expected from arc welded joints of similar geometry. The first program involved experimental work done with 25mm diameter steel bars. It found that, in the as-welded condition, friction welds have high fatigue strength. Residual stress measurements showed that this was due to a beneficial residual stress distribution in which compressive stresses are present at the surface adjacent to the failure site. Further evidence of this was obtained by subjecting some specimens to thermal stress relief. The fatigue strength of the stress relieved specimens was reduced compared with the aswelded joints but nevertheless the fatigue strength of these specimens was still high. The second program involved fatigue tests on friction stud welds in which the friction welding equipment was deployed offshore by divers or ROVs. The test specimens were made up of 19mm diameter studs friction welded onto structural steel plate. As with the first program, the specimens showed high fatigue endurance with results approximating to a DNV Class C1 curve. In some of the tests, the studs were preloaded in tension and results from specimens that were preloaded to the correct value specified for the joint were all stopped as run-outs, with specimens remaining unbroken. Copyright © 2017 ASME.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kongot2017189,
author={Kongot, A. and Pattanaik, M.},
title={Empowering project managers in enterprises - A design thinking approach to manage commercial projects},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10516 LNCS},
pages={189-197},
doi={10.1007/978-3-319-68059-0_12},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030835054&doi=10.1007%2f978-3-319-68059-0_12&partnerID=40&md5=8d78dcc89fbf693e247d487ab0385f8f},
affiliation={SAP Labs India Private Limited, Bangalore, India},
abstract={Lack of insights into potential issues in enterprise projects is a major problem that leads to cost and revenue targets being missed. This not only lowers project margins but also adversely affects customer relationship and future businesses for the organization. Many oversights occur during the project execution phase, especially for large scale enterprise projects, where the project manager spends a lot of time in proactively ensuring various deadlines are met. Commercial Project Management is an Enterprise solution from SAP that provides Project Managers an overview of the potential issues along with insights into their impact on margin, statuses and several key performance indicators. This allows Project Managers to ensure smooth delivery while meeting project targets, thereby gaining time to hone skills needed increase profit for their organizations. This paper explains how SAP adopted the Design Thinking methodology to build this product that addresses their varied customer’s needs from diverse industries. © IFIP International Federation for Information Processing 2017.},
author_keywords={Design thinking;  Enterprise software;  Formative usability testing;  Project management;  SAP fiori user experience;  User experience},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rodosta20175933,
author={Rodosta, T. and Bromhal, G. and Damiani, D.},
title={U.S. DOE/NETL Carbon Storage Program: Advancing Science and Technology to Support Commercial Deployment},
journal={Energy Procedia},
year={2017},
volume={114},
pages={5933-5947},
doi={10.1016/j.egypro.2017.03.1730},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029652674&doi=10.1016%2fj.egypro.2017.03.1730&partnerID=40&md5=9463a799f739123b7e1c2cffb4ae6983},
affiliation={U.S. Department of Energy, National Energy Technology Laboratory, 3610 Collins Ferry Road, P.O. Box 880, Morgantown, WV  26507, United States; U.S. Department of Energy, Office of Fossil Energy, 1000 Independence Avenue, SW, Washington, DC  20585, United States},
abstract={Since its inception in 1997, the U.S. Department of Energy's (DOE) Carbon Storage Program, managed by the National Energy Technology Laboratory (NETL), has significantly advanced geologic storage science and technology through a diverse portfolio of applied research projects. The Program is focused on developing and advancing technologies that address the overarching technical challenges of geologic storage, with the goal to achieve technology readiness for widespread commercial deployment in the 2025-2035 timeframe. The Program approaches these challenges through integration of the technologies developed in the "Advanced Storage" component of the Program and field tested in the "Storage Infrastructure" component. The Carbon Storage Program is now well positioned to begin feasibility projects on commercial-scale saline storage complexes, building upon almost two decades of knowledge and experience gained from Storage Infrastructure field projects. An early key milestone was the implementation of the Regional Carbon Sequestration Partnership (RCSP) Initiative. Experience and knowledge gained from these field projects provide a firm foundation for future larger-scale field projects, either onshore or offshore. Perhaps most importantly, it is only by performing these field projects that the knowledge needed to identify additional subsurface reservoir and operational issues still requiring further research can be acquired. © 2017 The Authors.},
author_keywords={accomplishments;  carbon capture and storage (CCS) research;  field projects;  geologic storage technologies;  National Energy Technology Laboratory (NETL);  Regional Carbon Sequestration Partnerships (RCSP);  U.S. DOE Carbon Storage Program},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tao2017260,
author={Tao, C. and Gao, J. and Wang, T.},
title={An approach to mobile application testing based on natural language scripting},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2017},
pages={260-265},
doi={10.18293/SEKE2017-170},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029519968&doi=10.18293%2fSEKE2017-170&partnerID=40&md5=8ce5dcbdd8a883b5630fb89c335cba04},
affiliation={College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Engineering, San Jose State University, United States; Taiyuan University of Technology, Taiyuan, China},
abstract={With the rapid advance of mobile computing technology and wireless networking, there is a significant increase of mobile subscriptions. This brings new business requirements and demands in mobile software testing, and causes new issues and challenges in mobile testing and automation. As there are multiple platforms for diverse devices, engineers suffer from the different scripting languages to write platform-specific test scripts. In addition, a unified automation infrastructure is not offered with the existing test platform. This paper proposes a novel approach to mobile application testing based on natural language scripting. A Java-based test script generation approach is developed to support executable test script generation based on the given natural language-based mobile app test operation scripts. A prototype tool is implemented based on some open sources. Finally, the paper reports empirical studies to indicate the feasibility and effectiveness of the proposed approach.},
author_keywords={Behavior-based testing;  Mobile application testing;  Scenario-based testing;  Test automation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ros2017190,
author={Ros, R. and Bjarnason, E. and Runeson, P.},
title={Automated controlled experimentation on software by evolutionary bandit optimization},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10452 LNCS},
pages={190-196},
doi={10.1007/978-3-319-66299-2_18},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029371185&doi=10.1007%2f978-3-319-66299-2_18&partnerID=40&md5=25519f74d6d5e9c8a38c0db00e19f21d},
affiliation={Department of Computer Science, Lund University, Lund, Sweden},
abstract={Controlled experiments, also called A/B tests or split tests, are used in software engineering to improve products by evaluating variants with user data. By parameterizing software systems, multivariate experiments can be performed automatically and in large scale, in this way, controlled experimentation is formulated as an optimization problem. Using genetic algorithms for automated experimentation requires repetitions to evaluate a variant, since the fitness function is noisy. We propose to combine genetic algorithms with bandit optimization to optimize where repetitions are evaluated, instead of uniform sampling. We setup a simulation environment that allows us to evaluate the solution, and see that it leads to increased fitness, population diversity, and rewards, compared to only genetic algorithms. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Albunian2017183,
author={Albunian, N.M.},
title={Diversity in search-based unit test suite generation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10452 LNCS},
pages={183-189},
doi={10.1007/978-3-319-66299-2_17},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029366292&doi=10.1007%2f978-3-319-66299-2_17&partnerID=40&md5=89b6a8caad33a53758c13fee44bb2a59},
affiliation={The University of Sheffield, Sheffield, United Kingdom},
abstract={Search-based unit test generation is often based on evolutionary algorithms. Lack of diversity in the population of an evolutionary algorithm may lead to premature convergence at local optima, which would negatively affect the code coverage in test suite generation. While methods to improve population diversity are well-studied in the literature on genetic algorithms (GAs), little attention has been paid to diversity in search-based unit test generation so far. The aim of our research is to study the effects of population diversity on search-based unit test generation by applying different diversity maintenance and control techniques. As a first step towards understanding the influence of population diversity on the test generation, we adapt diversity measurements based on phenotypic and genotypic representation to the search space of unit test suites. © Springer International Publishing AG 2017.},
author_keywords={Genetic algorithm;  Population diversity;  Search-based test generation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lei2017696,
author={Lei, T.},
title={Study on identity authentication system design based on intelligent image technology},
journal={Boletin Tecnico/Technical Bulletin},
year={2017},
volume={55},
number={6},
pages={696-703},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028778965&partnerID=40&md5=1ab7c8734422f9fa29c987dc0c833eb1},
affiliation={South China Normal University, Guangzhou city, Guangdong province, 510631, China},
abstract={In order to enhance the identification ability of legitimate digital identity of computer network operation, the design of identity authentication system, the electronic signature digital encryption authentication, prone to forgery and tampering, identity authentication to be copied, the effect is not good. This paper proposes a method of identity authentication technology based on intelligent image sensor, first through the scanner or extraction of biological image information reflecting the identity, and the acquisition of human retinal iris image, image segmentation and location according to the prior knowledge of the iris, provided from inner and outer edge of the iris, according to the active template matching extraction physical and behavioural characteristics reflect the identity information, and then using neural network classifier for identity classification to achieve recognition, identity recognition. The final authentication in Linux embedded development environment The system software design, test results show that using the method of identity authentication, through the body of the eyelids, pupil, iris, retina identification of characteristics such as diversity, EER is reduced by 23.8%, identity authentication accuracy and average frame tracking shorter time.},
author_keywords={Biological characteristics;  Identity authentication;  Intelligent image;  Iris;  Retina},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zima2017672,
author={Zima, M.},
title={Coincer: Decentralised trustless platform for exchanging decentralised cryptocurrencies},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10394 LNCS},
pages={672-682},
doi={10.1007/978-3-319-64701-2_53},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028453276&doi=10.1007%2f978-3-319-64701-2_53&partnerID=40&md5=176f807176e945482149b5c2d8069fbc},
affiliation={Faculty of Informatics, Masaryk University, Brno, Czech Republic},
abstract={We address the problem of a trustless decentralised exchange of cryptocurrencies. Centralised exchanges are neither trustworthy nor secure. As of 2017, there has been more than 25 million US dollars’ worth of cryptocurrencies stolen from (or by) centralised exchanges. With Coincer we allow any two users to exchange their diverse cryptocurrencies directly between them, yet with no need to trust each other. Former approaches either do not do without a server or rely on a trusted issuer of exchangeable tokens. Our approach is to fully eliminate any elements susceptible to becoming a single point of failure. Coincer therefore leverages an efficient anonymous P2P overlay and an atomic protocol for exchanging money across different cryptocurrencies. It is implemented as free software and has been successfully tested with Bitcoin and Litecoin. © Springer International Publishing AG 2017.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Guo2017,
author={Guo, C. and Xiong, H. and Huang, X. and Li, D.},
title={Design and Development Framework of Safety-Critical Software in HTR-PM},
journal={Science and Technology of Nuclear Installations},
year={2017},
volume={2017},
doi={10.1155/2017/2981943},
art_number={2981943},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027009457&doi=10.1155%2f2017%2f2981943&partnerID=40&md5=8a1c2be32bc15a42a76ab3cf2699976f},
affiliation={Institute of Nuclear and New Energy Technology, Collaborative Innovation Center of Advanced Nuclear Energy Technology, Key Laboratory of Advanced Reactor Engineering and Safety, Ministry of Education, Tsinghua University, Beijing, 100084, China},
abstract={With the development of information technology, the instrumentation and control system of nuclear power plant nowadays rely heavily on the massive and complex software to ensure the safe and efficient operation of the power plant. The improvement of the software design and development for the safety systems has been a research focus for its decisive impact on the nuclear safety. The framework of the software design and development for reactor protection system in High Temperature Gas-Cooled Reactor-Pebble bed Module was introduced in this paper. Firstly, during the design period, in addition to multichannel redundancy, grouping of protection variables and diverse 2-out-of-4 logics were adopted by different subsystems of each channel in case of common cause failure. Then a series of development characteristics together with strict software verification and validation were performed. Thirdly, during the software test period, an improved software reliability growth model based on the Goel-Okumoto model according to the analysis of fault severity was proposed to help in estimating the reliability of the software product and identifying the software release time. © 2017 Chao Guo et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Luo2017247,
author={Luo, T. and Lee, G.-L. and Molina, C.},
title={Incorporating istation into early childhood classrooms to improve reading comprehension},
journal={Journal of Information Technology Education: Research},
year={2017},
volume={16},
number={1},
pages={247-266},
doi={10.28945/3788},
art_number={3788},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026211502&doi=10.28945%2f3788&partnerID=40&md5=dcab999f569905712e83ef135c4c40c7},
affiliation={Old Dominion University, Norfolk, VA, United States},
abstract={Aim/Purpose IStation is an adaptive computer-based reading program that adapts to the learner's academic needs. This study investigates if the Istation computer-based reading program promotes reading improvement scores as shown on the STAR Reading test and the IStation test scaled scores for elementary school third-grade learners on different reading levels. Background Prior literature provided a limited evidence base for incorporating computer-adaptive learning technologies to improve reading comprehension in the con-text of early childhood education. Methodology Using a mixed-method case study research approach, this study purports to in-vestigate the effects of IStation and examine the perspectives of teachers and students. Supported by survey and interview data, this case study employed a sample of 98 public school third-grade students in an urban elementary school in the southeastern United States as well as the three classroom teachers. Findings The results of this study show a strong correlation between the usage of ISta-tion and the rise of STAR reading scores during the time IStation was inte-grated. There were differing opinions regarding the effectiveness of IStation be-tween students and teachers, as well as between low and high achieving stu-dents. Teachers recognized that intervening variables of teachers' whole and small group lessons individualized for each class, as well as students' practice sessions both at home and at school, could have also resulted in improved STAR reading scores. Recommendations for Practitioners There is no one-size-fits-all solution when implementing such technology to a diverse array of learners on different reading levels, such as Tier 1 (high reader), Tier 2 (medium average benchmark reader), and Tier 3 (low reader). It is essen-tial to provide professional development and training opportunities for teachers. Teachers can also train and elevate the higher achieving students with using IStation to monitor their own progress as well as set their own individual learn-ing goals. Recommendation for Researchers We recommend studies with a larger sample size that would likely yield more definitive and generalizable results, studies using a randomized control group that would have teased out extraneous factors and truly measuring the effects of IStation alone on STAR, as well as longitudinal studies examining the long-term effects of IStation. Contribution This study has provided a) additional data to show evidence for the effective-ness of a computer-based reading program, IStation, by using the students' and teachers' viewpoints as well as reading comprehension test scores data; and b) recommendations for practitioners and researchers regarding professional de-velopment for IStation implementation.},
author_keywords={Computer-adaptive learning technologies;  Early childhood education;  IStation;  Reading intervention},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alzahmi201721,
author={Alzahmi, S. and Shakya, S. and Boyd, I.},
title={Transforming telecommunications service execution},
journal={Journal of the Institute of Telecommunications Professionals},
year={2017},
volume={11},
number={2},
pages={21-26},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025710977&partnerID=40&md5=fe0d2eb9ccfacb6e13030b7a1c604eed},
affiliation={EBTIC, Khalifa University, Abu Dhabi, United Arab Emirates},
abstract={A recent estimate suggests 60% of global software spend (approximately $50Bn) is on software maintenance. Thirty percent of this is associated with software comprehension dealing with the inherent complexity of software. Other factors include insufficient governance of the software development process and a lack of comprehensive documentation of the final software product. Often the only comprehensive documentation of software, essential to plan new versions and undertake maintenance, is the software source code itself. The latest developments in the software industry are aimed at improving software governance and the automation of documentation creation. One such technology is Software Product Line (SPL). SPL is an innovative software governance and reuse approach. It introduces a robust technique which enables the planning and creation of reusable software units at the start of the software development. It is distinguished from most other software development methodologies by the comprehensive scope of reuse - not just source code but all the types of software assets involved in the development lifecycle, such as user requirements, architectures and test plans. SPL is defined by two development processes, 1) domain engineering and 2) application engineering. Domain engineering creates a reusable software platform whereas application engineering derives different versions of the product from this platform. The reusable software platform consists of different software assets such as requirements models, architectural models, software components, test plans, and test designs. A group of software assets defining a specific capability or functionality are grouped as a feature. Each new software product created by SPL is defined by a unique combination of features. The relationship between features are captured in a feature model which defines the product line. The feature model and the reusable software assets are fed to the application engineering process. Here, different features are selected to determine which software assets will be included and configured in a new product. The output of this process is a collection of configured software assets, typically a new (version of) product or software, ready to be deployed. Many organisations are using SPL to achieve extraordinary gains in productivity, time-to-market, and product quality. A good application area of the use of SPL in the telecommunications industry is in Network Function Virtualisation (NFV). NFV aims to address cost reduction and flexibility in network operations - network functions are implemented as software running over a virtualised infrastructure and provisioned on a service-by-service basis. NFV can benefit from SPL by adopting a systematic method for customising network services to accommodate diverse requirements. Not only does this enable the proper planning of NFV platform software artefacts, but it also enables a robust customisation of the chain of service functions that can adapt rapidly to situations, such as fluctuations in the network execution environment, and service failures. The challenges of SPL methodology include managing the variability information; lack of suitable tools to fully support the methodology; legacy systems where knowledge of the architecture and software components is not available. To address this, BT and Etisalat commissioned research by EBTIC (an ICT research and innovation centre established by Etisalat, BT, and Khalifa University and supported by the United Arab Emirates ICT fund) into the development of the EBTIC-SPL tool. The tool is currently being trialled and has demonstrated the potential for improved software comprehension and governance. As software increasingly underpins the services that telcos deliver, the importance of understanding and applying the best software engineering practices is critically important to service quality, speed of new service delivery and service flexibility. SPL is one of the most important approaches that telcos should be considering.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lechniak2017,
author={Lechniak, J.A. and Melton, J.E.},
title={Manned versus unmanned risk and complexity considerations for future midsized X-planes},
journal={AIAA Flight Testing Conference, 2017},
year={2017},
page_count={14},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023612497&partnerID=40&md5=df8b23f6fb91ed4d808cfe5d188f9f70},
affiliation={NASA Armstrong Flight Research Center, Aerodynamics Branch, P.O. Box 273/MS 2228, Edwards, CA  93523, United States; NASA Ames Research Center, Systems Analysis Office, MS 258-1, Moffet Field, CA  94035, United States},
abstract={The objective of this work was to identify and estimate complexity and risks associated with the development and testing of new low-cost medium-scale X-plane aircraft primarily focused on air transport operations. Piloting modes that were evaluated for this task were manned, remotely piloted, and unmanned flight research programs. This analysis was conducted early in the data collection period for X-plane concept vehicles before preliminary designs were complete. Over 50 different aircraft and system topics were used to evaluate the three piloting control modes. Expert group evaluations from a diverse set of pilots, engineers, and other experts at Aeronautics Research Mission Directorate centers within the National Aeronautics and Space Administration provided qualitative reasoning on the many issues surrounding the decisions regarding piloting modes. The group evaluations were numerically rated to evaluate each topic quantitatively and were used to provide independent criteria for vehicle complexity and risk. An Edwards Air Force Base instruction document was identified that emerged as a source of the effects found in our qualitative and quantitative data. The study showed that a manned aircraft was the best choice to align with test activities for transport aircraft flight research from a low-complexity and low-risk perspective. The study concluded that a manned aircraft option would minimize the risk and complexity to improve flight-test efficiency and bound the cost of the flight-test portion of the program. Several key findings and discriminators between the three modes are discussed in detail. © 2017, American Institute of Aeronautics and Astronautics Inc, AIAA. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{SanchezMartinez2017994,
author={Sanchez Martinez, H. and Hernandez Rodriguez, M.D.L. and Lopez Dominguez, E.},
title={Tuúm: Test Model for Native Mobile Applications},
journal={IEEE Latin America Transactions},
year={2017},
volume={15},
number={5},
pages={994-1000},
doi={10.1109/TLA.2017.7912598},
art_number={7912598},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018881652&doi=10.1109%2fTLA.2017.7912598&partnerID=40&md5=c8454098a65d073d2bb75a23027ab062},
affiliation={Laboratorio Nacional de Informática Avanzada A. C., Veracruz, Mexico},
abstract={Nowadays the testing area in mobile application is strongly studied due to the increasing use of mobile devices and the development of mobile applications with different approaches, such as multimedia, games, information, among others. Mobile applications must consider several quality aspects such as optimizing resource use and usability to help meet targets. To accomplish this, several authors have proposed testing models. However, these models do not consider important aspects such as diversity of devices and do not cover the entire application developing process. This article describes a testing model for native mobile applications, characterized by specific tools and steps that contribute to measure aspects of internal and external quality such as failures or code optimization and usability recommendations. In addition, our model is attached to the software development process, from requirements to system testing, which facilitates its implementation and helps timely detection and correction of errors before continuing with the software development cycle. © 2017 IEEE.},
author_keywords={native mobile applications;  study case;  Test model;  testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gainer-Dewar201763,
author={Gainer-Dewar, A. and Vera-Licona, P.},
title={The minimal hitting set generation problem: Algorithms and computation},
journal={SIAM Journal on Discrete Mathematics},
year={2017},
volume={31},
number={1},
pages={63-100},
doi={10.1137/15M1055024},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018710582&doi=10.1137%2f15M1055024&partnerID=40&md5=de62c87ff70d0e422099f617b8e499be},
affiliation={Center for Quantitative Medicine, UConn Health, Farmington, CT, United States},
abstract={Finding inclusion-minimal hitting sets (MHSs) for a given family of sets is a fundamental combinatorial problem with applications in domains as diverse as Boolean algebra, computational biology, and data mining. Although many algorithms are available in the literature to generate these MHSs, application papers typically consider only a few before selecting one (or introducing a novel algorithm), suggesting the need for a comprehensive survey and performance comparison. We introduce several of these applications, discussing how MHS generation is applied in each domain and which algorithms have been used, providing a unified view of these applications for researchers from diverse areas. We survey twenty-one algorithms for MHS generation from across a variety of domains, considering their history, classification, and useful features. We provide the results of a comprehensive suite of benchmarks of public software implementations of seventeen of these algorithms, including six we implemented ourselves in C++, emphasizing problem instances taken from real applications in the literature. We find that the fastest algorithms in practice are not those with the tightest complexity bounds or those most commonly used in applications, suggesting that, for a given application, benchmarking from across the broad span of available algorithms will enable a better choice. Finally, we provide a public repository of these software implementations as ready-to-use, platform-agnostic Docker containers based on the AlgoRun framework, so interested computational scientists can easily perform similar tests with inputs from their own research areas, either on their own computers or through a convenient Web interface or deploy the algorithms in their own analysis pipelines. © 2017 Society for Industrial and Applied Mathematics.},
author_keywords={Boolean dualization;  Combinatorial algorithms;  Hypergraph transversal;  Minimal hitting set;  Set cover problem},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khatri2017118,
author={Khatri, A.R. and Hayek, A. and Börcsök, J.},
title={Hardness analysis and instrumentation of verilog gate level code for FPGA-based designs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2017},
volume={10216 LNCS},
pages={118-128},
doi={10.1007/978-3-319-56258-2_11},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017553291&doi=10.1007%2f978-3-319-56258-2_11&partnerID=40&md5=f170db11bff3762e1298489fe272198d},
affiliation={Department of Computer Architecture and System, University of Kassel, Kassel, Germany},
abstract={Dependability analysis and test approaches are key steps in order to test and verify system robustness and fault-tolerance capabilities. Owing to the shrinking size of components, it is very difficult to guarantee an acceptable degree of reliability. With the growing computational power of FPGAs and other diverse advantages, they have become indispensable solutions for embedded applications. However, these systems are also prone to faults and errors. Therefore, the testability and the dependability analysis are necessary. Both methods require the deliberate introduction of faults in the SUT. In this paper, a fault injection algorithm is proposed for Verilog gate level code, which injects faults in the design. Also, the method is proposed for finding sensitive locations of SUT. These methods are developed under a fault injection tool, with a GUI, for the ease of use, and it is named RASP-FIT tool. Benchmark circuits from ISCAS’85 and ISCAS’89 are considered to validate the both proposed methods. © Springer International Publishing AG 2017.},
author_keywords={Dependability analysis;  Fault injection;  Fault tolerance;  FPGA;  Instrumentation;  Verilog HDL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kuo2017249,
author={Kuo, T.-Y. and Wang, W.-C.},
title={Experimental and numerical investigation of effects of fiber orientation of wood stiffness},
journal={Conference Proceedings of the Society for Experimental Mechanics Series},
year={2017},
volume={2017-January},
pages={249-254},
doi={10.1007/978-3-319-28513-9_35},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005963629&doi=10.1007%2f978-3-319-28513-9_35&partnerID=40&md5=2d621542c0fe10435b0bee93d9bd1fb4},
affiliation={Department of Power Mechanical Engineering, National Tsing Hua University, Hsinchu, Taiwan},
abstract={Wood is one of the most useful and important natural materials with diverse applications in civil, architectural and constructional engineering. The stiffness of the wood depends on the fiber orientation, distribution of knot and percentage of latewood, etc. Japanese cedar (Cryptomeria japonica) was used to prepare the tensile test specimen in this paper to investigate the effects of fiber orientation on wood stiffness. Before performing the tensile test, surface image of the test specimen was captured and the image was analyzed by least squares method and digital image processing software of MATLAB to obtain the fiber orientation. Based on the obtained fiber orientation, finite element method (FEM) software package ANSYS was employed to calculate the strain distribution of the test specimen. Three-dimensional digital image correlation (3D-DIC) method was also used to verify the FEM results. The DIC software, VIC-3D, was used to analyze the surface deformation of the test specimen under tension. Strain distribution differences between the earlywood and latewood were investigated. With the integration of the digital image analysis technique, FEM and 3D-DIC method, the effective stiffness of the wood can be predicted and the reliability and safety of wood construction can be ensured. © Springer International Publishing Switzerland 2017.},
author_keywords={Digital image correlation (DIC);  Fiber orientation;  Finite element method (FEM);  Japanese cedar;  Wood stiffness},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chilenski2017155,
author={Chilenski, M.A. and Faust, I.C. and Walk, J.R.},
title={eqtools: Modular, extensible, open-source, cross-machine Python tools for working with magnetic equilibria},
journal={Computer Physics Communications},
year={2017},
volume={210},
pages={155-162},
doi={10.1016/j.cpc.2016.09.011},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991819431&doi=10.1016%2fj.cpc.2016.09.011&partnerID=40&md5=174ffa4973f38e3f1daede02c3c40ef3},
affiliation={Plasma Science and Fusion Center, Massachusetts Institute of Technology, 77 Massachusetts Avenue, Cambridge, MA  02139, United States; Cinch Financial, 24 School St, Boston, MA  02108, United States},
abstract={As plasma physics research for fusion energy transitions to an increasing emphasis on cross-machine collaboration and numerical simulation, it becomes increasingly important that portable tools be developed to enable data from diverse sources to be analyzed in a consistent manner. This paper presents eqtools, a modular, extensible, open-source toolkit implemented in the Python programming language for handling magnetic equilibria and associated data from tokamaks. eqtools provides a single interface for working with magnetic equilibrium data, both for handling derived quantities and mapping between coordinate systems, extensible to function with data from different experiments, data formats, and magnetic reconstruction codes, replacing the diverse, non-portable solutions currently in use. Moreover, while the open-source Python programming language offers a number of advantages as a scripting language for research purposes, the lack of basic tokamak-specific functionality has impeded the adoption of the language for regular use. Implementing equilibrium-mapping tools in Python removes a substantial barrier to new development in and porting legacy code into Python. In this paper, we introduce the design of the eqtools package and detail the workflow for usage and expansion to additional devices. The implementation of a novel three-dimensional spline solution (in two spatial dimensions and in time) is also detailed. Finally, verification and benchmarking for accuracy and speed against existing tools are detailed. Wider deployment of these tools will enable efficient sharing of data and software between institutions and machines as well as self-consistent analysis of the shared data. Program summary Program title: eqtools Catalogue identifier: AFBK_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFBK_v1_0.html Program obtainable from: CPC Program Library, Queen's University, Belfast, N. Ireland Licensing provisions: GNU GPL v3 No. of lines in distributed program, including test data, etc.: 27204 No. of bytes in distributed program, including test data, etc.: 1217844 Distribution format: tar.gz Programming language: Python, C. Computer: PCs. Operating system: Linux, Macintosh OS X, Microsoft Windows. RAM: Several megabytes, depends on resolution of data Classification: 19.4. External routines: F2PY [1], matplotlib [2], MDSplus [3], NumPy [4], SciPy [5] Nature of problem: Access to results from magnetic equilibrium reconstruction code, conversion between various coordinate systems tied to the magnetic equilibrium. Solution method: Data are stored in an object-oriented data structure with human-readable getter methods. Coordinates are converted using bivariate or trivariate splines. Running time: Coordinate transformations on a 66x66 point spatial grid take between 1 and 5ï¿½ms per time slice, depending on the transformation used and how many intermediate results have been stored. References: [1] P. Peterson, F2PY: a tool for connecting Fortran and Python programs, International Journal of Computational Science and Engineering 4 (4) (2009) 296–305.[2] J. D. Hunter, Matplotlib: A 2D graphics environment, Computing in Science and Engineering, 9 (3) (2007) 90–95.[3] J. A. Stillerman, T. W. Fredian, K. A. Klare, G. Manduchi, MDSplus data acquisition system, Review of Scientific Instruments 68 (1) (1997) 939–942.[4] S. van der Walt, S. C. Colbert and G. Varoquaux, The NumPy array: a structure for efficient numerical computation, Computing in Science and Engineering 13 (2) (2011) 22–30.[5] E. Jones, T. Oliphant, P Peterson, etï¿½al., SciPy: Open source scientific tools for Python (2001-). ï¿½ 2016 Elsevier B.V.},
author_keywords={Data analysis;  Magnetic equilibrium reconstruction;  Plasma physics;  Tokamaks},
document_type={Article},
source={Scopus},
}

@ARTICLE{Patrick201736,
author={Patrick, M. and Jia, Y.},
title={KD-ART: Should we intensify or diversify tests to kill mutants?},
journal={Information and Software Technology},
year={2017},
volume={81},
pages={36-51},
doi={10.1016/j.infsof.2016.04.009},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964802330&doi=10.1016%2fj.infsof.2016.04.009&partnerID=40&md5=dd8c91ff137b9b98e7f52743dccef295},
affiliation={Epidemiology and Modelling Group, Department of Plant Sciences, University of Cambridge, CB2 3EA, United Kingdom; CREST, Department of Computer Science, University College London, WC1E 6BT, United Kingdom},
abstract={Context: Adaptive Random Testing (ART) spreads test cases evenly over the input domain. Yet once a fault is found, decisions must be made to diversify or intensify subsequent inputs. Diversification employs a wide range of tests to increase the chances of finding new faults. Intensification selects test inputs similar to those previously shown to be successful. Objective: Explore the trade-off between diversification and intensification to kill mutants. Method: We augment Adaptive Random Testing (ART) to estimate the Kernel Density (KD–ART) of input values found to kill mutants. KD–ART was first proposed at the 10th International Workshop on Mutation Analysis. We now extend this work to handle real world non numeric applications. Specifically we incorporate a technique to support programs with input parameters that have composite data types (such as arrays and structs). Results: Intensification is the most effective strategy for the numerical programs (it achieves 8.5% higher mutation score than ART). By contrast, diversification seems more effective for programs with composite inputs. KD–ART kills mutants 15.4 times faster than ART. Conclusion: Intensify tests for numerical types, but diversify them for composite types. © 2016 Elsevier B.V.},
author_keywords={Adaptive random testing;  Intensification and diversification;  Mutation analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sun201765,
author={Sun, C.-A. and Xue, F. and Liu, H. and Zhang, X.},
title={A path-aware approach to mutant reduction in mutation testing},
journal={Information and Software Technology},
year={2017},
volume={81},
pages={65-81},
doi={10.1016/j.infsof.2016.02.006},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959911592&doi=10.1016%2fj.infsof.2016.02.006&partnerID=40&md5=274eb4b263686558fc6746aeb394393a},
affiliation={School of Computer and Communication Engineering, University of Science and Technology Beijing, China; Australia-India Research Centre for Automation Software Engineering, RMIT University, Melbourne, Australia; Department of Computer Science, Purdue University, West Lafayette, IN, United States},
abstract={Context: Mutation testing, which systematically generates a set of mutants by seeding various faults into the base program under test, is a popular technique for evaluating the effectiveness of a testing method. However, it normally requires the execution of a large amount of mutants and thus incurs a high cost. Objective: A common way to decrease the cost of mutation testing is mutant reduction, which selects a subset of representative mutants. In this paper, we propose a new mutant reduction approach from the perspective of program structure. Method: Our approach attempts to explore path information of the program under test, and select mutants that are as diverse as possible with respect to the paths they cover. We define two path-aware heuristic rules, namely module-depth and loop-depth rules, and combine them with statement- and operator-based mutation selection to develop four mutant reduction strategies. Results: We evaluated the cost-effectiveness of our mutant reduction strategies against random mutant selection on 11 real-life C programs with varying sizes and sampling ratios. Our empirical studies show that two of our mutant reduction strategies, which primarily rely on the path-aware heuristic rules, are more effective and systematic than pure random mutant selection strategy in terms of selecting more representative mutants. In addition, among all four strategies, the one giving loop-depth the highest priority has the highest effectiveness. Conclusion: In general, our path-aware approach can reduce the number of mutants without jeopardizing its effectiveness, and thus significantly enhance the overall cost-effectiveness of mutation testing. Our approach is particularly useful for the mutation testing on large-scale complex programs that normally involve a huge amount of mutants with diverse fault characteristics. © 2016},
author_keywords={Control flow;  Mutation testing;  Path depth;  Selective mutation testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Atwal2016164,
author={Atwal, K.S. and Bassiouni, M.},
title={A Novel Approach for Simulation and Analysis of Cloud Data Center Applications},
journal={Proceedings - 2016 IEEE International Conference on Smart Cloud, SmartCloud 2016},
year={2016},
pages={164-169},
doi={10.1109/SmartCloud.2016.16},
art_number={7796168},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011041192&doi=10.1109%2fSmartCloud.2016.16&partnerID=40&md5=8f70d5af3b5915c4b1046d5be319c7d4},
affiliation={Department of Computer Science, University of Central Florida, Orlando, FL, United States},
abstract={Considering the unprecedented surge in demands and expectations of smart cloud-based services and applications, there is an increasing demand of scalable tools that can effectively evaluate the performance of these services and provide insights for improving their design and implementation. A realistic and accurate emulation platform is a prerequisite for the efficient assessment of smart cloud data centers, and for improving the capability and flexibility of utilizing the vast resources available to smart cloud applications. The platform needs to be scalable and diverse enough to handle out of the box experiments, as well as simple enough for ease of use and management. The over heads incurred to meet these goals directly hamper the performance of a framework. Therefore, mitigation of the overheads adds to the salient features. In this paper, we present a reference model that strives to meet such requirements while addressing overheads. We demonstrate proof of the concept using off-the-shelf software components and present some test cases of the performance results obtained by the implementation of our platform. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bakhshi2016289,
author={Bakhshi, T. and Ghita, B.},
title={User-Centric Network Provisioning in Software Defined Data Center Environment},
journal={Proceedings - Conference on Local Computer Networks, LCN},
year={2016},
pages={289-297},
doi={10.1109/LCN.2016.57},
art_number={7796801},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010041843&doi=10.1109%2fLCN.2016.57&partnerID=40&md5=139000036d40b39d7a93ba2634292ffc},
affiliation={Center for Security, Communications and Network Research, University of Plymouth, Plymouth, United Kingdom},
abstract={Present data center (DC) network provisioning schemes primarily utilize conventional load-balancing technologies, offering individual application performance improvement. Diversity in application usage however, makes isolated application prioritization a performance caveat for users with varying application trends. The present paper proposes a user profiling approach to capture application trends based on generic flow measurements (NetFlow) and employs the extracted profiles to create DC traffic forwarding policies. The scheme allows operators to define a global profile and application hierarchy based on extracted profiles to prioritize traffic for individual user classes. The proposed design was tested by extracting user profiles from a realistic enterprise network, and further simulated to dynamically manage DC traffic using the software defined networking paradigm. Compared to conventional traffic management schemes, the frame delivery ratio and effective throughput of our design was significantly higher for high priority north-south user traffic as well as the inter-server east-west application traffic. © 2016 IEEE.},
author_keywords={Data center networking;  software defined networking;  user traffic profiling},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lux2016,
author={Lux, M. and Krüger, J. and Rinke, C. and Maus, I. and Schlüter, A. and Woyke, T. and Sczyrba, A. and Hammer, B.},
title={acdc - Automated Contamination Detection and Confidence estimation for single-cell genome data},
journal={BMC Bioinformatics},
year={2016},
volume={17},
number={1},
doi={10.1186/s12859-016-1397-7},
art_number={543},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006814675&doi=10.1186%2fs12859-016-1397-7&partnerID=40&md5=79c17c599520f704159013e1d48743a1},
affiliation={Bielefeld University, Computational Methods for the Analysis of the Diversity and Dynamics of Genomes, Universitätsstr. 25, Bielefeld, 33615, Germany; Bielefeld University, Center for Biotechnology - CeBiTec, Universitätsstr. 27, Bielefeld, 33615, Germany; 2800 Mitchell Drive, Walnut Creek, CA  94598, United States; Bielefeld University, CITEC centre of excellence, Inspiration 1, Bielefeld, 33619, Germany; University of Queensland, Australian Centre for Ecogenomics, ST LUCIA, Brisbane, QLD  4072, Australia},
abstract={Background: A major obstacle in single-cell sequencing is sample contamination with foreign DNA. To guarantee clean genome assemblies and to prevent the introduction of contamination into public databases, considerable quality control efforts are put into post-sequencing analysis. Contamination screening generally relies on reference-based methods such as database alignment or marker gene search, which limits the set of detectable contaminants to organisms with closely related reference species. As genomic coverage in the tree of life is highly fragmented, there is an urgent need for a reference-free methodology for contaminant identification in sequence data. Results: We present acdc, a tool specifically developed to aid the quality control process of genomic sequence data. By combining supervised and unsupervised methods, it reliably detects both known and de novo contaminants. First, 16S rRNA gene prediction and the inclusion of ultrafast exact alignment techniques allow sequence classification using existing knowledge from databases. Second, reference-free inspection is enabled by the use of state-of-the-art machine learning techniques that include fast, non-linear dimensionality reduction of oligonucleotide signatures and subsequent clustering algorithms that automatically estimate the number of clusters. The latter also enables the removal of any contaminant, yielding a clean sample. Furthermore, given the data complexity and the ill-posedness of clustering, acdc employs bootstrapping techniques to provide statistically profound confidence values. Tested on a large number of samples from diverse sequencing projects, our software is able to quickly and accurately identify contamination. Results are displayed in an interactive user interface. Acdc can be run from the web as well as a dedicated command line application, which allows easy integration into large sequencing project analysis workflows. Conclusions: Acdc can reliably detect contamination in single-cell genome data. In addition to database-driven detection, it complements existing tools by its unsupervised techniques, which allow for the detection of de novo contaminants. Our contribution has the potential to drastically reduce the amount of resources put into these processes, particularly in the context of limited availability of reference species. As single-cell genome data continues to grow rapidly, acdc adds to the toolkit of crucial quality assurance tools. © 2016 The Author(s).},
author_keywords={Binning;  Clustering;  Contamination detection;  Machine learning;  Quality control;  Single-cell sequencing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zekany2016,
author={Zekany, S. and Rings, D. and Harada, N. and Laurenzano, M.A. and Tang, L. and Mars, J.},
title={CrystalBall: Statically analyzing runtime behavior via deep sequence learning},
journal={Proceedings of the Annual International Symposium on Microarchitecture, MICRO},
year={2016},
volume={2016-December},
doi={10.1109/MICRO.2016.7783727},
art_number={7783727},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009347627&doi=10.1109%2fMICRO.2016.7783727&partnerID=40&md5=cb567a43d294746f7e38400b0e945801},
affiliation={University of Michigan, Ann Arbor, MI, United States; Clinc, Inc., Ann Arbor, MI, United States},
abstract={Understanding dynamic program behavior is critical in many stages of the software development lifecycle, for purposes as diverse as optimization, debugging, testing, and security. This paper focuses on the problem of predicting dynamic program behavior statically. We introduce a novel technique to statically identify hot paths that leverages emerging deep learning techniques to take advantage of their ability to learn subtle, complex relationships between sequences of inputs. This approach maps well to the problem of identifying the behavior of sequences of basic blocks in program execution. Our technique is also designed to operate on the compiler's intermediate representation (IR), as opposed to the approaches taken by prior techniques that have focused primarily on source code, giving our approach language-independence. We describe the pitfalls of conventional metrics used for hot path prediction such as accuracy, and motivate the use of Area Under the Receiver Operating Characteristic curve (AUROC). Through a thorough evaluation of our technique on complex applications that include the SPEC CPU2006 benchmarks, we show that our approach achieves an AUROC of 0.85. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rios2016,
author={Rios, J. and Mulfinger, D. and Homola, J. and Venkatesan, P.},
title={NASA UAS traffic management national campaign: Operations across Six UAS Test Sites},
journal={AIAA/IEEE Digital Avionics Systems Conference - Proceedings},
year={2016},
volume={2016-December},
doi={10.1109/DASC.2016.7778080},
art_number={7778080},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85009509148&doi=10.1109%2fDASC.2016.7778080&partnerID=40&md5=1a0bf3c7772817b36114f80267dcb7d5},
affiliation={NASA Ames Research Center, Moffett Field, CA, United States; ASRC Federal, Moffett Field, CA, United States},
abstract={NASA's Unmanned Aircraft Systems Traffic Management research aims to develop policies, procedures, requirements, and other artifacts to inform the implementation of a future system that enables small drones to access the low altitude airspace. In this endeavor, NASA conducted a geographically diverse flight test in conjunction with the FAA's six unmanned aircraft systems Test Sites. A control center at NASA Ames Research Center autonomously managed the airspace for all participants in eight states as they flew operations (both real and simulated). The system allowed for common situational awareness across all stakeholders, kept traffic procedurally separated, offered messages to inform the participants of activity relevant to their operations. Over the 3-hour test, 102 flight operations connected to the central research platform with 17 different vehicle types and 8 distinct software client implementations while seamlessly interacting with simulated traffic. © 2016 IEEE.},
author_keywords={air traffic management;  UAS;  UTM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dang2016,
author={Dang, H.V. and Zivanovic, S.},
title={Influence of Low-Frequency Vertical Vibration on Walking Locomotion},
journal={Journal of Structural Engineering (United States)},
year={2016},
volume={142},
number={12},
doi={10.1061/(ASCE)ST.1943-541X.0001599},
art_number={04016120},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996538329&doi=10.1061%2f%28ASCE%29ST.1943-541X.0001599&partnerID=40&md5=c8e42778f16c4e80495f3085cb9bb526},
affiliation={TSP, Alpha Tower, Suffolk St., Birmingham, B1 1TT, United Kingdom; Univ. of Warwick, School of Engineering, Coventry, CV4 7AL, United Kingdom},
abstract={Walking locomotion has been a subject of studies in diverse research fields, such as computer, medical, and sport sciences, biomechanics, and robotics, resulting in improved understanding of underlying body motion and gait efficiency and pathology (when present). Only recently, a detailed understanding of kinematics and kinetics of the walking locomotion has become an important requirement in structural engineering applications due to an increasing sensitivity of modern, lightweight, low-frequency, and lightly damped footbridges to pedestrian-induced dynamic excitation. To facilitate development, calibration and verification of pedestrian models requires experimental characterization of walking gait parameters and understanding whether and how these parameters are influenced by the structural vibration. This study investigates whether low-frequency vibrations in the vertical direction affect seven walking locomotion parameters: pacing frequency, step length, step width, angle of attack, end-of-step angle, trunk angle, and amplitude of the first forcing harmonic. Three participants took part in a testing program consisting of walking on a treadmill placed on both stationary and vibrating supporting surfaces. The collected data suggest that an increasing level of vibration results in an increase in step-by-step variability for the majority of parameters. Furthermore, the existence of the self-excited force, previously observed only in numerical simulations of walking on pre-excited bridge decks, was confirmed. In addition, the deck vibration tended to have a beneficial effect of reducing the net force induced into the structure when walking at a pacing rate close to the vibration frequency. Finally, it was found that the vibration level perceptible by a pedestrian is one to two orders of magnitude larger than that typical of a standing person, and that the sensitivity to vibration decreases as the speed of walking increases. © 2016 This work is made available under the terms of the Creative Commons Attribution 4.0 International license,.},
author_keywords={Pedestrian-structure interaction;  Self-excited force;  Shock and vibratory effects;  Vertical vibration;  Vibration perception;  Walking locomotion parameters},
document_type={Article},
source={Scopus},
}

@ARTICLE{Pereira2016201,
author={Pereira, J.C.R. and de Jesus, A.M.P. and Fernandes, A.A.},
title={A new ultra-low cycle fatigue model applied to the X60 piping steel},
journal={International Journal of Fatigue},
year={2016},
volume={93},
pages={201-213},
doi={10.1016/j.ijfatigue.2016.08.017},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986253887&doi=10.1016%2fj.ijfatigue.2016.08.017&partnerID=40&md5=1e590d47ea7e3ef39e3bd9e4b0caab48},
affiliation={Faculty of Engineering, University of Porto, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal; INEGI, Rua Dr. Roberto Frias, Porto, 4200-465, Portugal},
abstract={Fatigue damage under extreme cyclic plastic loading conditions (ultra-low-cycle fatigue (ULCF)) has been increasingly investigated motivated by several applications involving structures and mechanical components, such as pipelines, that may be, during operation, likely exposed to severe cyclic loading regimes (e.g. accidental loads, earthquakes, pipeline reeling). ULCF corresponds to a transition damage mechanism between the monotonic ductile damage and the low-cycle-fatigue (LCF), both widely investigated in the literature, using independent approaches. Investigation in this transition damage mechanism is still scarce covering a reduced number of materials and few models are available. The main goal of this paper is to investigate the cyclic behavior of the X60 piping steel under cyclic extreme loading conditions, also covering the respective monotonic ductile and low-cycle fatigue behaviors. A unified model to describe the three damage regimes will be also proposed. This investigation is supported by an experimental program covering tests of smooth and notched specimens to derive monotonic and elastoplastic cyclic/fatigue data under a diversity of multiaxial stress conditions. Data reduction schemes based on each individual test simulation, by non-linear elastoplastic finite element models, is performed. In detail, the monotonic fracture strain, the average stress triaxiality and the average Lode angle parameters were obtained and considered for the calibration of 3D ductile fracture locus in accordance with Bai and Wierzbicki formulation, a satisfactory agreement being found. Afterwards, cyclic test data is used to calibrate a modified Xue model that is made explicitly sensitive to the stress triaxiality and Lode angle parameters. This model relates the equivalent plastic strain range, normalized by the fracture strain with the number of cycles to failure. Aiming the determination of the strain fracture of each specimen, two distinct methods are proposed. The first one consists of a direct method, based on the simulation of each ULCF specimen under monotonic conditions and in the other one is based on the use of the previously generated 3D monotonic ductile fracture locus. © 2016},
author_keywords={3D fracture surface;  Lode angle parameter;  Monotonic ductile fracture;  Triaxiality;  ULCF;  Xue model},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Koutsoubelias2016115,
author={Koutsoubelias, M. and Lalis, S.},
title={TeCoLa: A programming framework for dynamic and heterogeneous robotic teams},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={28-November-2016},
pages={115-124},
doi={10.1145/2994374.2994397},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007613594&doi=10.1145%2f2994374.2994397&partnerID=40&md5=b620bc2d3d23d2dc0774479cad0ba29e},
affiliation={University of Thessaly, CERTH, Volos, Greece},
abstract={Many pervasive computing applications can benefit from the advanced operational capability and diversity of sensing and actuation resources of modern robotic platforms. Even more powerful functionality can be achieved by combining robots with different mobility capabilities, some of which may already be deployed in the area of interest. However, the current programming frameworks make such flexible resource utilization a daunting task, as the application developer is responsible for the laborious and awkward management of the heterogeneity and transient availability of resources and services, which is done in a manual way. TeCoLa is a programming framework addressing the above issues. It supports structured services as first-class entities, which can appear/disappear dynamically, and are accessed in a transparent way. In addition, to ease the task of multi-robot programming, TeCoLa supports the creation and management of teams based on the dynamic service capability and availability of individual robots. Teams are maintained behind the scenes, without any effort from the application programmer. Furthermore, they can be controlled through team-level service interfaces which are instantiated in an automatic way, based on the services of the robots that are members of the team. We present a first implementation of TeCoLa for a Python environment, and discuss how we test the functionality of our prototype using a software-in-theloop approach. © 2016 ACM.},
author_keywords={High-level coordination;  Mobile robots;  Pervasive computing;  Resource dynamics and heterogeneity;  Team programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Siddappa20161920,
author={Siddappa, M. and Prakash, G.C.B. and Sridhar, H.S.},
title={Agent based communication architecture for smart grid},
journal={International Conference on Electrical, Electronics, and Optimization Techniques, ICEEOT 2016},
year={2016},
pages={1920-1925},
doi={10.1109/ICEEOT.2016.7755022},
art_number={7755022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006717903&doi=10.1109%2fICEEOT.2016.7755022&partnerID=40&md5=bea1907dabf1008a6665a80fa2e77442},
affiliation={HOD Dept. of Computer Science and Engg, Sri Siddhartha Institute of Technology, Tumkur, India; Dept. of Computer Science and Engg, Sir M. Visvesvaraya Institute of Technology, Bangalore, India; Dept. of Electrical and Electronics, Engg Siddaganga Institute of Technology, Tumkur, India},
abstract={Integration of renewable energy sources, mission critical power demands and diversion of energy sources are challenges that the electrical grid operators face today. These challenges are mainly generated by new operational situations for the power grid that was not visualized when it was developed decades ago, to beat these challenges, it's widely accepted that intelligent management of the electrical grid, also called the smart Grid, are going to be a part of the solution. However, since current electricity storage technologies aren't economical viable enough to store the desired quantity of energy, so the electricity should be used more intelligently. The present realization of the smart grid integrates information and Communication Technology (ICT) into the power grid and facilitates intelligent automation of power systems. Design of the smart grid architecture intends to find new ways of connecting appliances, while the software design tests the flexibility, compatibility and auto-configuration capabilities of innovative software ideas. In this paper architecture for the smart Grid is arranged. It will be shown that the architecture's standard and agent primarily based design will offer a good advantage for the combination of appliances into a smart grid and the creation of agents controlling them. The introduction of the publish/subscribe paradigm in conjunction with auto configuration of agents demonstrated a good answer for service data exchange within a smart grid. With a useful validation and also the use of unit testing it had been shown that the agents work reliably. © 2016 IEEE.},
author_keywords={Communication Architecture;  Smart Agent;  Smart grid},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khan2016,
author={Khan, A.I. and Al-Khanjari, Z. and Sarrab, M.},
title={Crowd sourced testing through end users for Mobile Learning application in the context of Bring Your Own Device},
journal={7th IEEE Annual Information Technology, Electronics and Mobile Communication Conference, IEEE IEMCON 2016},
year={2016},
doi={10.1109/IEMCON.2016.7746256},
art_number={7746256},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85005993334&doi=10.1109%2fIEMCON.2016.7746256&partnerID=40&md5=8e79cbe9294437ed3b87c934ea0c377d},
affiliation={Dept. Computer Science, Communication and Information Research Center, Sultan Qaboos University, Muscat, 123, Oman; Dept. Computer Science, Sultan Qaboos University, Muscat, 123, Oman; Communication and Information Research Center, Sultan Qaboos University, Muscat, 123, Oman},
abstract={Mobile Learning (M-Learning) has gained considerable popularity among the learners. Accessibility and compatibility of M-Learning application is no more restricted to limited number of mobile devices. Now M-Learning providers have given freedom of learning on any mobile device owned by the user in the environment called Bring Your Own Device (BYOD). However, the major problem exists when it comes to development and testing of such kind of application, which would run on variety of mobile platforms and under the geographically diverse environment. The testing process carried out by the development team in such a diverse environment and on variety of mobile platforms is not economical both in terms of cost and time. Hence this article proposes a solution in the form of Crowd sourced testing to overcome the M-Learning application testing problem. Crowd sourced testing would be conducted by the end users while using the application on their own devices. The users would report issues related to installation, downloading and updating, GUI etc. to a feedback repository. A Software test engineer analyzes the issues for relevancy and forward to Software developer if found appropriate. The Software developer would solve the issue by either customizing the application according to the particular device and platforms or make necessary changes to suite all the available devices and platforms This approach is limited to the basic functionality and usage problem which could be easily identified by the application users. Testing of M-Learning application related to performance, load and stress etc. which needs technical resources and expertise would be conducted using approaches such as Cloud based testing, testing on Emulators etc. by Software test engineer himself. The proposed approach still needs validation in real user's environment. © 2016 IEEE.},
author_keywords={Bring Your Own Device;  Crowd Sourced Mobile Learning Application Testing;  Crowd Sourced Testing;  End Users Testing;  M-Learning Application Testing;  Mobile Learning Application},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mehra2016504,
author={Mehra, A. and Verma, V. and Rajput, S.K. and Tyagi, D.},
title={Series computation using Vedic mathematics},
journal={Conference on Advances in Signal Processing, CASP 2016},
year={2016},
pages={504-506},
doi={10.1109/CASP.2016.7746224},
art_number={7746224},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85004147170&doi=10.1109%2fCASP.2016.7746224&partnerID=40&md5=4bd82d2f8cd94e6dd1676894def6d88a},
affiliation={ECE Dept., ASET, Amity University Uttar Pradesh, Noida, India; India IBM India Pvt. Ltd., Bangalore, India},
abstract={This paper emphasize on the Taylor's series expansion of sinusoidal, cosine, tangent and exponential functions. The strategy of these series is based on the Vedic mathematics. The impression of designing multiplier based series is embraced from the ancient Indian mathematics found in Wisdom Books Vedas. Diverse constraints (static and dynamic power, noise margin, delay, LUT's) of three unlike expansions are equated and the results are acquired. VHDL codes are developed and the functionality of these series was tested and executed by using Xilinx Vivado Software (Version: 2014.2). The algorithm used for this is 'Urdhva-Tiryagbhyam' one of the sutra of Vedic multiplication. The algorithm was used because it is most competent amid all the sutras. © 2016 IEEE.},
author_keywords={Maclaurin series;  Taylor serie;  Urdhva-tiryagbhyam;  Vedic mathematics},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lesi2016,
author={Lesi, V. and Jakovljevic, Z. and Pajic, M.},
title={Towards Plug-n-Play numerical control for Reconfigurable Manufacturing Systems},
journal={IEEE International Conference on Emerging Technologies and Factory Automation, ETFA},
year={2016},
volume={2016-November},
doi={10.1109/ETFA.2016.7733524},
art_number={7733524},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996598396&doi=10.1109%2fETFA.2016.7733524&partnerID=40&md5=78c56318fa95d424df61733bb72027e3},
affiliation={Dept. of Electrical and Comp. Eng., Duke University, United States; Faculty of Mechanical Engineering, University of Belgrade, Serbia},
abstract={Modern manufacturing systems require fast and effective adaptation to fluctuating market conditions and product diversification. This high level adaptability can be achieved through the utilization of Reconfigurable Manufacturing Systems (RMS), which should be based on modular equipment that is easily integrated, scalable, convertible in terms of functionality, and self diagnosable. RMS also necessitate the use of a dynamic controller architecture that is distributed, fully modular, and self configurable. In this paper, we present a control system design approach for reconfigurable machine tools through the use of modularized and decentralized CNC control. Specifically, we investigate design challenges for Plug-n-Play automation systems, where new system functionalities, such as adding new axes in existing CNC units, can be introduced without significant reconfiguration efforts and downtime costs. We propose a fully decentralized motion control architecture realized through a network of individual axis control modules. Reconfiguration of motion control systems based on this architecture can be achieved by only presenting the controller on each axis with information about machine configuration and the type of axis. This effectively enables modularity, reconfigurability, and interoperability of the machine control system. Finally, we present an implementation of the decentralized architecture based on the use of a real-time operating system, wireless networking, and low-cost ARM Cortex-M3 MCUs; we illustrate its effectiveness by considering machining of a standard test part defined in ISO 10791-7 using a software-in-the-loop testbed. © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhong2016981,
author={Zhong, H. and Zhang, L. and Khurshid, S.},
title={Combinatorial generation of structurally complex test inputs for commercial software applications},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2016},
volume={13-18-November-2016},
pages={981-986},
doi={10.1145/2950290.2983959},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997503528&doi=10.1145%2f2950290.2983959&partnerID=40&md5=a420b442996b63bb153a6e9a0f23bda4},
affiliation={Google Inc94043, United States; University of Texas at Austin78712, United States; University of Texas at Dallas75080, United States},
abstract={Despite recent progress in automated test generation research, significant challenges remain for applying these techniques on large-scale software systems. Large-scale software systems under test often require structurally complex test inputs within a large input domain. It is challenging to automatically generate a reasonable number of tests that are both legal and behaviorally-diverse to exercise these systems. Constraint-based test generation is an effective approach for generating structurally complex inputs for systematic testing. While this approach can typically generate large numbers of tests, it has limited scalability-tests generated by this approach are usually only up to a small bound on input size. Combinatorial test generation, e.g., pair-wise testing, is a more scalable approach but is challenging to apply on commercial software systems that require complex input structures that cannot be formed by using arbitrary combinations. This paper introduces comKorat, which unifies constraint-based generation of structurally complex tests with combinatorial test generation methods. Specifi-cally, comKorat integrates Korat and ACTS test generators to generate test suites for large-scale software systems with structurally complex test inputs. We have successfully applied comKorat on four large-scale software applications developed at eBay and Yahoo!. The experimental results show that comKorat outperforms existing solutions in execution time and test coverage. Furthermore, comKorat found a total of 59 previously unknown bugs in the above four applications.},
author_keywords={ACTS;  Combinatorial test generation;  Constraint-based test generation;  Korat},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang20162307,
author={Zhang, X.-F. and Zhang, Z.-Z. and Xie, X.-Y. and Zhou, Y.-C.},
title={An approach of iterative partition testing based on priority sampling},
journal={Jisuanji Xuebao/Chinese Journal of Computers},
year={2016},
volume={39},
number={11},
pages={2307-2323},
doi={10.11897/SP.J.1016.2016.02307},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996940103&doi=10.11897%2fSP.J.1016.2016.02307&partnerID=40&md5=76862914258e5504b9f2d63ff546c1f0},
affiliation={School of Computer Science and Technology, Soochow University, Suzhou, Jiangsu  215006, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, 210023, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, 430072, China},
abstract={Random testing and partition-based testing are two important test case generation methods. Comparisons on the efficiency and effectiveness between these two methods have been a popular research area. As an enhanced version of random testing, Adaptive Random Testing (ART) aims to evenly spread test cases within the input domain, in order to achieve better failure detection effectiveness. This paper follows the intuition of ART that high diversity is helpful in detecting failures, and proposes a novel algorithm, named Iterative Partition Testing based on Priority Sampling (IPT-PS). IPT-PS iteratively divides the input domain into grids, and selects the center point of each grid as test case. Priority-based execution strategy is then applied on newly generated test cases in each round of iteration. Iterative partition and fixed-center-point sampling only require information of the input domain, while priority-based execution considers different spatial characteristics of test cases. All these three steps need trivial time cost, and help to sample test cases much more even than traditional fixed-size partition testing, such that better failure detection effectiveness can be achieved. We theoretically prove the upper bound of the effectiveness for our method, and conduct comprehensive empirical analysis. Our experimental results show that IPT-PS can achieve effectiveness as high as ART, but with time cost as low as random testing. © 2016, Science Press. All right reserved.},
author_keywords={Adaptive random testing;  F-measure;  Failure rate;  Partition testing;  Random testing;  Software testing;  Test case generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yousefnezhad2016260,
author={Yousefnezhad, M. and Reihanian, A. and Zhang, D. and Minaei-Bidgoli, B.},
title={A new selection strategy for selective cluster ensemble based on Diversity and Independency},
journal={Engineering Applications of Artificial Intelligence},
year={2016},
volume={56},
pages={260-272},
doi={10.1016/j.engappai.2016.10.005},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993960697&doi=10.1016%2fj.engappai.2016.10.005&partnerID=40&md5=38d312138bb5ba9a91f0b8a17b889ab3},
affiliation={Department of Computer Science, Nanjing University of Aeronautics and Astronautics, China; Department of Computer Engineering, University of Tabriz, Iran; Department of Computer Engineering, Iran University of Science and Technology, Iran},
abstract={This research introduces a new strategy in cluster ensemble selection by using Independency and Diversity metrics. In recent years, Diversity and Quality, which are two metrics in evaluation procedure, have been used for selecting basic clustering results in the cluster ensemble selection. Although quality can improve the final results in cluster ensemble, it cannot control the procedures of generating basic results, which causes a gap in prediction of the generated basic results’ accuracy. Instead of quality, this paper introduces Independency as a supplementary method to be used in conjunction with Diversity. Therefore, this paper uses a heuristic metric, which is based on the procedure of converting code to graph in Software Testing, in order to calculate the Independency of two basic clustering algorithms. Moreover, a new modeling language, which we called as “Clustering Algorithms Independency Language” (CAIL), is introduced in order to generate graphs which depict Independency of algorithms. Also, Uniformity, which is a new similarity metric, has been introduced for evaluating the diversity of basic results. As a credential, our experimental results on varied different standard data sets show that the proposed framework improves the accuracy of final results dramatically in comparison with other cluster ensemble methods. © 2016 Elsevier Ltd},
author_keywords={Algorithm's graph;  Diversity of primary results;  Independency of algorithms;  Selective cluster ensemble},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reed2016367,
author={Reed, K. and Gillies, D.},
title={Automatic derivation of design schemata and subsequent generation of designs},
journal={Artificial Intelligence for Engineering Design, Analysis and Manufacturing: AIEDAM},
year={2016},
volume={30},
number={4},
pages={367-378},
doi={10.1017/S0890060416000354},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989924130&doi=10.1017%2fS0890060416000354&partnerID=40&md5=17d7c0eb9b32fe7131285971ca5fb5b2},
affiliation={Department of Computing, Imperial College London, London, SW7 2AZ, United Kingdom},
abstract={This paper presents a method for automatically generating new designs from a set of existing objects of the same class using machine learning. In this particular work, we use a custom parametric chair design program to produce a large set of chairs that are tested for their physical properties using ergonomic simulations. Design schemata are found from this set of chairs and used to generate new designs by placing constraints on the generating parameters used in the program. The schemata are found by training decision trees on the chair data sets. These are automatically reverse engineered by examining the structure of the trees and creating a schema for each positive leaf. By finding a range of schemata, rather than a single solution, we maintain a diverse design space. This paper also describes how schemata for different properties can be combined to generate new designs that possess all properties required in a design brief. The method is shown to consistently produce viable designs, covering a large range of our design space, and demonstrates a significant time saving over generate and test using the same program and simulations. © 2016 Cambridge University Press.},
author_keywords={Automatic Derivation;  Design Schemata;  Generation of Designs;  Machine Learning},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wu2016487,
author={Wu, Z. and Gao, X.},
title={An Efficient Practical MIMO–OFDM Scheme with Coded Modulation Diversity for Future Wireless Communications},
journal={Wireless Personal Communications},
year={2016},
volume={91},
number={1},
pages={487-508},
doi={10.1007/s11277-016-3472-9},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982124682&doi=10.1007%2fs11277-016-3472-9&partnerID=40&md5=5b4a574acfad2b178489f0b51d4c8ec4},
affiliation={School of Information and Communication Engineering, Beijing University of Posts and Telecommunications, Beijing, China},
abstract={An efficient coded modulation diversity scheme is proposed to improve the performance of multiple-input-multiple-output orthogonal-frequency-division-multiplexing (MIMO–OFDM) systems in frequency-selective time-varying fading channels. By introducing the rotated modulation and space-time-frequency component interleavers, the proposed scheme globally optimizes powerful forward error correction codes, rotated quadrature amplitude modulation, linear precoding MIMO, and OFDM, thus can achieve modulation diversity and space-time-frequency diversities as much as possible. A simple efficient optimal precoder selection criterion is put forward for this scheme. Based on the analysis of average mutual information, optimal rotation angles are given for different modulation and coding schemes, and the superiority of the proposed scheme is proved. Both software simulation and hardware prototype test results also verify that this new scheme can significantly outperform the conventional bit-interleaved coded modulation scheme under the non-ideal channel estimation, which is up to 4.5 dB signal-to-noise-ratio gain. In a word, the proposed scheme is a simple, efficient and robust transmission technique for the future MIMO–OFDM wireless systems. © 2016, Springer Science+Business Media New York.},
author_keywords={Coded modulation;  LDPC codes;  MIMO;  Modulation diversity;  OFDM;  Rotated modulation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Kaur20161057,
author={Kaur, K. and Kaur, A.},
title={Cloud era in mobile application testing},
journal={Proceedings of the 10th INDIACom; 2016 3rd International Conference on Computing for Sustainable Global Development, INDIACom 2016},
year={2016},
pages={1057-1060},
art_number={7724423},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84997483356&partnerID=40&md5=8811aa6ecd8e35d82f39de9936479502},
affiliation={School of IT, Apeejay Institute of Management Technical Campus, Jalandhar, India; I.K.G Punjab Technical University, Kapurthala, India},
abstract={Mobile technology usage is exploding across the world. There are lakhs of mobile applications available which run on various mobile platforms such as Android, RIM, windows Mobile, Apple iOS and all. With this incredible growth of mobile applications, evidently developers are expected to deliver high quality, on time and within budget applications. Mobile application testing is considered as a challenging task because of various reasons such as ever-changing mobile devices, limited computational power, limited storage, limited energy, diversity of devices etc. There are various mobile application testing approaches such as testing on real devices, simulators, emulators and testing on cloud. As with advent of cloud computing technology the entire IT enterprises are providing services on cloud. Cloud testing is a software testing using cloud computing offering various benefits as compared to traditional testing strategies. The focus of this paper is to provide baselines for testing the mobile app on cloud which is currently an emerging field in testing. This paper discusses cloud computing, trend of testing on cloud along with various advantages and challenges confronted in testing of mobile applications on cloud. © 2016 IEEE.},
author_keywords={Cloud;  Cloud computing;  Cloud services;  Cloud testing Challenges;  Mobile Applications},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2016374,
author={Zhang, X. and Xie, X. and Chen, T.Y.},
title={Test Case Prioritization Using Adaptive Random Sequence with Category-Partition-Based Distance},
journal={Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security, QRS 2016},
year={2016},
pages={374-385},
doi={10.1109/QRS.2016.49},
art_number={7589817},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995532835&doi=10.1109%2fQRS.2016.49&partnerID=40&md5=026288148d5a6f7ccb72c7e6845bf4dc},
affiliation={School of Computer Science and Technology, Soochow University, Suzhou, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; Department of Computer Science and Software Engineering, Swinburne University of Technology, Hawthorn, Australia},
abstract={Test case prioritization schedules test cases in a certain order aiming to improve the effectiveness of regression testing. Random sequence is a basic and simple prioritization technique, while Adaptive Random Sequence (ARS) makes use of extra information to improve the diversity of random sequence. Some researchers have proposed prioritization techniques using ARS with white-box information, such as code coverage information, or with black-box information, such as string distances of the input data. In this paper, we propose new black-box test case prioritization techniques using ARS, and the diversity of test cases is assessed by category-partition-based distance. Our experimental studies show that these new techniques deliver higher fault-detection effectiveness than random prioritization, especially in the case of smaller ratio of failed test cases. In addition, in the comparison of different distance metrics, techniques with category-partition-based distance generally deliver better fault-detection effectiveness and efficiency, meanwhile in the comparison of different ordering algorithms, our ARS-based ordering algorithms usually have comparable fault-detection effectiveness but much lower computation overhead, and thus are much more cost-effective. © 2016 IEEE.},
author_keywords={adaptive random sequence;  catergory partition;  random sequence;  string distance;  test case prioritization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{McGoldrick2016,
author={McGoldrick, M.},
title={Logging: Gaining access to the inner workings of your TPS},
journal={AUTOTESTCON (Proceedings)},
year={2016},
volume={2016-October},
doi={10.1109/AUTEST.2016.7589567},
art_number={7589567},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994201450&doi=10.1109%2fAUTEST.2016.7589567&partnerID=40&md5=3f1b653fec50d56e0023faedaf094d1a},
affiliation={Defense and Aerospace Division, Teradyne, North Reading, MA, United States},
abstract={This paper describes an enhanced test system software architecture that includes a general use facility for passing test program data from an executing test program set (TPS) to one or more support applications running in parallel with the TPS. These support applications may include tools to assist the TPS developer in debugging the TPS, characterizing the unit under test, archiving test results data, and other applications that a test organization may find relevant and helpful. The same test system software architecture can also be used by instrument vendors in the design and implementation of the debugging tools that accompany their instrument drivers and other software. The paper also includes a description of a possible implementation of such an architecture, and demonstrates how it can be used to simplify the development of multiple and diverse types of support applications with little to no impact on the development of the TPS itself. © 2016 IEEE.},
author_keywords={Architecture;  Debugging;  Test System;  Tools;  TPS;  UUT},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Senapati2016,
author={Senapati, P.R.R. and Kumar, J.S.J. and Juliet, A.V.},
title={LabVIEW based automatic speed assistance system in uncertain road conditions},
journal={IEEE WCTFTR 2016 - Proceedings of 2016 World Conference on Futuristic Trends in Research and Innovation for Social Welfare},
year={2016},
doi={10.1109/STARTUP.2016.7583940},
art_number={7583940},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994173284&doi=10.1109%2fSTARTUP.2016.7583940&partnerID=40&md5=313cc88ecae5feb86dc63579df7e5397},
affiliation={Dept. of Electronics and Control, SRM University, Chennai, India; Dept. of Electronics and Instrumentation, SRM University, Chennai, India},
abstract={Advanced driver assistance system includes various factors and components for passengers safety and riding comfort. Research on this driver assistance system is recently becoming one of the most important and crucial theme in the matter of intelligent transportation system (ITS). In this context, the proposed paper focuses on the design of automatic speed assistance system for diverse road conditions. For this purpose, the existing braking mechanism is considered for different applications. The proposed automatic speed assistance system is derived from antilock braking system (ABS) and automatic steering assistance system. Primarily the road condition introduces about the idea of road curvature model, altitude profile and the wheel slip-coefficient of friction characteristics. For designing, a brassy and trashy way is adapted to develop the speed assistance control system. The simulation model is formulated in measurement and automation giant, LabVIEW, which is a very easiest and modern approach to acquire, monitor and process the data. After all the practical application of this incredible software in the manufacturing field of automobiles yields a rapid control prototyping, real time simulation, in vehicle data logging and control and online noise, vibration and harshness testing. © 2016 IEEE.},
author_keywords={ABS;  Driver Assistance System;  LabVIEW;  Road condition;  Speed assistance system;  Steering assistance system},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Groen2016375,
author={Groen, D. and Bhati, A.P. and Suter, J. and Hetherington, J. and Zasada, S.J. and Coveney, P.V.},
title={FabSim: Facilitating computational research through automation on large-scale and distributed e-infrastructures},
journal={Computer Physics Communications},
year={2016},
volume={207},
pages={375-385},
doi={10.1016/j.cpc.2016.05.020},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991688360&doi=10.1016%2fj.cpc.2016.05.020&partnerID=40&md5=80d40634f4145867221e3558de751c4b},
affiliation={Centre for Computational Science, University College London, 20 Gordon street, London, WC1H 0AJ, United Kingdom; Department of Computer Science, Brunel University, St John's Building, Kingston Lane, Uxbridge, UB8 3PH, United Kingdom; Research Software Development Group, University College London, Podium Building, 1 Eversholt Street, London, NW1 2DN, United Kingdom},
abstract={We present FabSim, a toolkit developed to simplify a range of computational tasks for researchers in diverse disciplines. FabSim is flexible, adaptable, and allows users to perform a wide range of tasks with ease. It also provides a systematic way to automate the use of resources, including HPC and distributed machines, and to make tasks easier to repeat by recording contextual information. To demonstrate this, we present three use cases where FabSim has enhanced our research productivity. These include simulating cerebrovascular bloodflow, modelling clay-polymer nanocomposites across multiple scales, and calculating ligand–protein binding affinities. Program summary Program title: FabSim Catalogue identifier: AFAO_v1_0 Program summary URL:http://cpc.cs.qub.ac.uk/summaries/AFAO_v1_0.html Program obtainable from: CPC Programme Library, Queen's University, Belfast, N. Ireland Licensing provisions: BSD 3-Clause No. of lines in distributed program, including test data, etc.: 268282 No. of bytes in distributed program, including test data, etc.: 2791792 Distribution format: tar.gz Programming language: Python. Computer: PC or Mac. Operating system: Unix, OSX. RAM: 1 Gbytes Classification: 3, 4, 6.5. External routines: NumPy, SciPy, Fabric (1.5 or newer), PyYaml Nature of problem: Running advanced computations using remote resources is an activity that requires considerable time and human attention. These activities, such as organizing data, configuring software and setting up individual runs often vary slightly each time they are performed. To lighten this burden, we required an approach that introduced little burden of its own to set up and adapt, beyond which very substantial productivity ensues. Solution method: We present a toolkit which helps to simplify and automate the activities which surround computational science research. FabSim is aimed squarely at the experienced computational scientist, who can use the command line interface and a system of modifiable content to quickly automate sets of research tasks. Restrictions: FabSim relies on a command-line interface, and assumes some level of scripting knowledge from the user. Unusual features: FabSim has a proven track record of being easy to adapt. It has already been extensively adapted to facilitate leading research in the modelling of bloodflow, nanomaterials, and ligand–protein binding. Running time: FabSim can be used interactively, typically requiring a few seconds to perform a basic task. ï¿½ 2016 The Authors},
author_keywords={Automation;  Bloodflow modelling;  Clay-polymer nanocomposites;  Distributed computing;  Molecular dynamics;  Multiscale modelling;  Software;  Workflows},
document_type={Article},
source={Scopus},
}

@ARTICLE{Belkaid2016523,
author={Belkaid, A. and Colak, I. and Isik, O.},
title={Photovoltaic maximum power point tracking under fast varying of solar radiation},
journal={Applied Energy},
year={2016},
volume={179},
pages={523-530},
doi={10.1016/j.apenergy.2016.07.034},
note={cited By 82},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978842114&doi=10.1016%2fj.apenergy.2016.07.034&partnerID=40&md5=d3aa304d8f9d5aa437c9d6d10bf8afaa},
affiliation={Department of Electromechanics, University of Bordj BouArreridj, El-Anasser 34030, Algeria; Automatic Laboratory of Setif (LAS), University of Setif 1, El Maabouda, Street of Bejaia, Setif, 19000, Algeria; Engineering and Architecture Faculty of Istanbul Gelisim University, Turkey},
abstract={Perturb and Observe (P&O) and Incremental Conductance (INC) are widely used as Maximum Power Point Tracking (MPPT) techniques in Photovoltaic (PV) systems. But, they fail under rapidly varying of sunlight levels. This paper proposes a new MPPT technique, which can make a distinction between perturbation in the reference voltage and sudden-changing of sunlight and thus optimize the PV system efficiency. This method consists on a modified INC algorithm, which is used to fine-tune the duty cycle of the DC/DC converter in order to avoid divergences of the maximum power point (MPP) when using basic INC under fast varying of luminosity levels. The proposed PV-MPPT system, which is composed by a step-up converter as the interface to feed the load, is tested by simulation within the Matlab/Simulink software by taking into account the luminosity, the temperature and the load variation. The simulation results are satisfactory and demonstrate that the improved INC technique can track the PV maximum power at diverse operating conditions with the most excellent performance, the energy conversion efficiency is increased by approximately 5%. © 2016 Elsevier Ltd},
author_keywords={Incremental Conductance;  Maximum power point tracker;  Modeling;  Photovoltaic system;  Simulation;  Step-up converter},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mohammed2016363,
author={Mohammed, B. and Kiran, M. and Awan, I.-U. and Maiyama, K.M.},
title={Optimising fault tolerance in real-time cloud computing IaaS environment},
journal={Proceedings - 2016 IEEE 4th International Conference on Future Internet of Things and Cloud, FiCloud 2016},
year={2016},
pages={363-370},
doi={10.1109/FiCloud.2016.58},
art_number={7575886},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992124414&doi=10.1109%2fFiCloud.2016.58&partnerID=40&md5=babb3bca22eab88259f5d6cf094e6003},
affiliation={School of Electrical Engineering and Computer Science, University of Bradford, Bradford, BD7 1DP, United Kingdom},
abstract={Fault tolerance is the ability of a system to respond swiftly to an unexpected failure. Failures in a cloud computing environment are normal rather than exceptional, but fault detection and system recovery in a real time cloud system is a crucial issue. To deal with this problem and to minimize the risk of failure, an optimal fault tolerance mechanism was introduced where fault tolerance was achieved using the combination of the Cloud Master, Compute nodes, Cloud load balancer, Selection mechanism and Cloud Fault handler. In this paper, we proposed an optimized fault tolerance approach where a model is designed to tolerate faults based on the reliability of each compute node (virtual machine) and can be replaced if the performance is not optimal. Preliminary test of our algorithm indicates that the rate of increase in pass rate exceeds the decrease in failure rate and it also considers forward and backward recovery using diverse software tools. Our results obtained are demonstrated through experimental validation thereby laying a foundation for a fully fault tolerant IaaS Cloud environment, which suggests a good performance of our model compared to current existing approaches. © 2016 IEEE.},
author_keywords={Cloud computing;  Cloud fault handler;  Fail rate;  Fault tolerance;  Pass rate;  Virtual machine},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DaSilva201685,
author={Da Silva, O.J. and Crespo, A. and Jino, M.},
title={Experimental Comparison of Software Reliability Models Based on Code Coverage and on Time Domain},
journal={Proceedings - 2016 IEEE International Conference on Software Quality, Reliability and Security-Companion, QRS-C 2016},
year={2016},
pages={85-92},
doi={10.1109/QRS-C.2016.86},
art_number={7573728},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991807855&doi=10.1109%2fQRS-C.2016.86&partnerID=40&md5=4d2f70e7ad6962423041cc50eb568439},
affiliation={Faculdade Metrocamp, DeVry Brasil, Campinas-SP, Brazil; Centro de Pesquisas Renato, Archer-CTI, Campinas-SP, Brazil; Universidade de Campinas, UNICAMP, Campinas-SP, Brazil},
abstract={The study of software reliability has its origin in 1967 when the first experiments were performed. Since then, diverse models that use as parameter the time of execution of software or the time between failures have been considered. Another approach indicates that the use of code coverage, instead of execution time, yields better estimates of the reliability of the software. This paper presents the results observed in an experiment carried out to compare the software reliability models Binomial Model based on Coverage (BMBC) and Infinite Failure Software Reliability Model Based on Code Coverage (IFMBC) with the following models based on time domain: Geometric (GEO), Littlewood-Linear Verral (LAV-L), Littlewood-Quadratic Verral (LAVQ), Musa Basic (MS-B), Musa Logarithmic (MS-L) and Non-Homogeneous Poisson Model (NHP). These models, in addition to being much quoted in the literature, are implemented by SMERFS^3, used to calculate its parameters and to estimate reliability. As a result, it has been observed that traditional models do not provide robustness when the profile was changed. On the other hand, code coverage-based models have produced good results for all operational profiles. In addition to the good fit, coverage-based models have generated better estimates of software reliability, as shown by the Kolmogorov-Smirnov test. © 2016 IEEE.},
author_keywords={operational profile;  software reliability;  structural testing criteria;  test coverage.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Petrić2016,
author={Petrić, J. and Bowes, D. and Hall, T. and Christianson, B. and Baddoo, N.},
title={Building an Ensemble for Software Defect Prediction Based on Diversity Selection},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2016},
volume={08-09-September-2016},
doi={10.1145/2961111.2962610},
art_number={a46},
note={cited By 39},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991666877&doi=10.1145%2f2961111.2962610&partnerID=40&md5=e4a33467ee2c6c7c4fd2be149a4d5d70},
affiliation={Science and Technology Research Institute, University of Hertfordshire, Hatfield, Hertfordshire, United Kingdom; Department of Computer Science, Brunel University London, Uxbridge, Middlesex, United Kingdom},
abstract={Background: Ensemble techniques have gained attention in various scientific fields. Defect prediction researchers have investigated many state-of-the-art ensemble models and concluded that in many cases these outperform standard single classifier techniques. Almost all previous work using ensemble techniques in defect prediction rely on the majority voting scheme for combining prediction outputs, and on the implicit diversity among single classifiers. Aim: Investigate whether defect prediction can be improved using an explicit diversity technique with stacking ensemble, given the fact that different classifiers identify different sets of defects. Method: We used classifiers from four different families and the weighted accuracy diversity (WAD) technique to exploit diversity amongst classifiers. To combine individual predictions, we used the stacking ensemble technique. We used state-of-the-art knowledge in software defect prediction to build our ensemble models, and tested their prediction abilities against 8 publicly available data sets. Conclusion: The results show performance improvement using stacking ensembles compared to other defect prediction models. Diversity amongst classifiers used for building ensembles is essential to achieving these performance improvements. © 2016 ACM.},
author_keywords={diversity;  ensembles of learning machines;  Software defect prediction;  software faults;  stacking},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Landgraf201692,
author={Landgraf, K.M. and Kakkar, R. and Meigs, M. and Jankauskas, P.T. and Phan, T.T.H. and Nguyen, V.N. and Nguyen, D.T. and Duong, T.T. and Nguyen, T.H. and Bond, K.B.},
title={Open-source LIMS in Vietnam: The path toward sustainability and host country ownership},
journal={International Journal of Medical Informatics},
year={2016},
volume={93},
pages={92-102},
doi={10.1016/j.ijmedinf.2016.06.010},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978511520&doi=10.1016%2fj.ijmedinf.2016.06.010&partnerID=40&md5=60d01d9acfce9e895350ea9a2c7b4181},
affiliation={The QED Group LLC, assigned to the Centers for Disease Control and Prevention, Viet Nam; The Association of Public Health Laboratories, United States; The Vietnam Agency for HIV/Aids Control, Viet Nam; VAAC-US.CDC Project, Viet Nam; Ho Chi Minh City Aids Committee, Viet Nam; Centers for Disease Control and Prevention, United States},
abstract={Objective The objectives of this case report are as follows: to describe the process of establishing a national laboratory information management system (LIMS) program for clinical and public health laboratories in Vietnam; to evaluate the outcomes and lessons learned; and to present a model for sustainability based on the program outcomes that could be applied to diverse laboratory programs. Methods This case report comprises a review of program documentation and records, including planning and budgetary records of the donor, monthly reports from the implementer, direct observation, and ad-hoc field reports from technical advisors and governmental agencies. Additional data on program efficacy and user acceptance were collected from routine monitoring of laboratory policies and operational practices. Results LIMS software was implemented at 38 hospital, public health and HIV testing laboratories in Vietnam. This LIMS was accepted by users and program managers as a useful tool to support laboratory processes. Implementation cost per laboratory and average duration of deployment decreased over time, and project stakeholders initiated transition of financing (from the donor to local institutions) and of system maintenance functions (from the implementer to governmental and site-level staff). Collaboration between the implementer in Vietnam and the global LIMS user community was strongly established, and knowledge was successfully transferred to staff within Vietnam. Conclusion Implementing open-sourced LIMS with local development and support was a feasible approach towards establishing a sustainable laboratory informatics program that met the needs of health laboratories in Vietnam. Further effort to institutionalize IT support capacity within key government agencies is ongoing. © 2016 Elsevier Ireland Ltd},
author_keywords={Development;  FLOSS;  Health;  Information systems;  Laboratory;  LIMS;  LIS;  Medical informatics;  Open source;  OSS;  PEPFAR;  Resource-constrained;  Sustainability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen20161149,
author={Chen, L. and May, J.H.R.},
title={A Diversity Model Based on Failure Distribution and its Application in Safety Cases},
journal={IEEE Transactions on Reliability},
year={2016},
volume={65},
number={3},
pages={1149-1162},
doi={10.1109/TR.2015.2503335},
art_number={7358170},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949984736&doi=10.1109%2fTR.2015.2503335&partnerID=40&md5=283de8d4a7eb598a1bb9f37cf85c4d1f},
affiliation={Safety Systems Research Centre, University of Bristol, Bristol, BS81TR, United Kingdom},
abstract={This work develops a new basis for evaluating the reliability benefits of diverse software, based on fault injection testing. In particular, the work investigates new forms of argumentation that could in principle be used to justify diversity as a basis for the construction of safety claims. Failure distributions of two versions of diverse software under various fault conditions are revealed separately by fault injection methods, and then the common failure probability of the version-pair can be estimated. The approach is justified theoretically, and cross validated with other work. This method is also used to explain the fundamental influence of failure distributions on diversity. Furthermore, the unique capabilities of the method are demonstrated by implementation of the fault injection test on a program pair. © 2016 IEEE.},
author_keywords={Fault injection;  multi-version;  reliability;  safety case;  safety critical system;  Software diversity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ohmann201627,
author={Ohmann, P. and Brown, D.B. and Neelakandan, N. and Linderoth, J. and Liblit, B.},
title={Optimizing customized program coverage},
journal={ASE 2016 - Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
year={2016},
pages={27-38},
doi={10.1145/2970276.2970351},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989201927&doi=10.1145%2f2970276.2970351&partnerID=40&md5=89ba8600f5470bad2168db04a239b084},
affiliation={University of Wisconsin-Madison, Madison, WI, United States},
abstract={Program coverage is used across many stages of software development. While common during testing, program coverage has also found use outside the test lab, in production software. However, production software has stricter requirements on run-time overheads, and may limit possible program instrumentation. Thus, optimizing the placement of probes to gather program coverage is important. We introduce and study the problem of customized program coverage optimization. We generalize previous work that optimizes for complete coverage instrumentation with a system that adapts optimization to customizable program coverage requirements. Specifically, our system allows a user to specify desired coverage locations and to limit legal instrumentation locations. We prove that the problem of determining optimal coverage probes is NP-hard, and we present a solution based on mixed integer linear programming. Due to the computational complexity of the problem, we also provide two practical approximation approaches. We evaluate the effectiveness of our approximations across a diverse set of benchmarks, and show that our techniques can substantially reduce instrumentation while allowing the user immense freedom in defining coverage requirements. When naïve instrumentation is dense or expensive, our optimizations succeed in lowering execution time overheads. © 2016 ACM.},
author_keywords={Debugging;  Mixed integer linear optimization;  Program coverage},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Frank2016,
author={Frank, J.A. and Brill, A. and Kapila, V.},
title={Mounted smartphones as measurement and control platforms for motor-based laboratory test-beds},
journal={Sensors (Switzerland)},
year={2016},
volume={16},
number={8},
doi={10.3390/s16081331},
art_number={1331},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983489044&doi=10.3390%2fs16081331&partnerID=40&md5=61ca8a92e9da490b9d3eaa2415c018f0},
affiliation={Department of Mechanical and Aerospace Engineering, NYU Tandon School of Engineering, Mechatronics and Control Laboratory, Brooklyn, NY  11201, United States},
abstract={Laboratory education in science and engineering often entails the use of test-beds equipped with costly peripherals for sensing, acquisition, storage, processing, and control of physical behavior. However, costly peripherals are no longer necessary to obtain precise measurements and achieve stable feedback control of test-beds. With smartphones performing diverse sensing and processing tasks, this study examines the feasibility of mounting smartphones directly to test-beds to exploit their embedded hardware and software in the measurement and control of the test-beds. This approach is a first step towards replacing laboratory-grade peripherals with more compact and affordable smartphone-based platforms, whose interactive user interfaces can engender wider participation and engagement from learners. Demonstrative cases are presented in which the sensing, computation, control, and user interaction with three motor-based test-beds are handled by a mounted smartphone. Results of experiments and simulations are used to validate the feasibility of mounted smartphones as measurement and feedback control platforms for motor-based laboratory test-beds, report the measurement precision and closed-loop performance achieved with such platforms, and address challenges in the development of platforms to maintain system stability. © 2016 by the authors; licensee MDPI, Basel, Switzerland.},
author_keywords={Control;  Laboratory;  Mechatronics;  Sensing;  Smartphone},
document_type={Article},
source={Scopus},
}

@ARTICLE{Boomsma2016,
author={Boomsma, A. and Bhattacharya, S. and Troolin, D. and Pothos, S. and Vlachos, P.},
title={A comparative experimental evaluation of uncertainty estimation methods for two-component PIV},
journal={Measurement Science and Technology},
year={2016},
volume={27},
number={9},
doi={10.1088/0957-0233/27/9/094006},
art_number={094006},
note={cited By 40},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985903392&doi=10.1088%2f0957-0233%2f27%2f9%2f094006&partnerID=40&md5=3ac8a278d1095d1f342cba9fe8372271},
affiliation={Fluid Mechanics Research Instruments, TSI Incorporated, Shoreview, MN, United States; School of Mechanical Engineering, Purdue University, West Lafayatte, IN, United States},
abstract={Uncertainty quantification in planar particle image velocimetry (PIV) measurement is critical for proper assessment of the quality and significance of reported results. New uncertainty estimation methods have been recently introduced generating interest about their applicability and utility. The present study compares and contrasts current methods, across two separate experiments and three software packages in order to provide a diversified assessment of the methods. We evaluated the performance of four uncertainty estimation methods, primary peak ratio (PPR), mutual information (MI), image matching (IM) and correlation statistics (CS). The PPR method was implemented and tested in two processing codes, using in-house open source PIV processing software (PRANA, Purdue University) and Insight4G (TSI, Inc.). The MI method was evaluated in PRANA, as was the IM method. The CS method was evaluated using DaVis (LaVision, GmbH). Utilizing two PIV systems for high and low-resolution measurements and a laser doppler velocimetry (LDV) system, data were acquired in a total of three cases: a jet flow and a cylinder in cross flow at two Reynolds numbers. LDV measurements were used to establish a point validation against which the high-resolution PIV measurements were validated. Subsequently, the high-resolution PIV measurements were used as a reference against which the low-resolution PIV data were assessed for error and uncertainty. We compared error and uncertainty distributions, spatially varying RMS error and RMS uncertainty, and standard uncertainty coverages. We observed that qualitatively, each method responded to spatially varying error (i.e. higher error regions resulted in higher uncertainty predictions in that region). However, the PPR and MI methods demonstrated reduced uncertainty dynamic range response. In contrast, the IM and CS methods showed better response, but under-predicted the uncertainty ranges. The standard coverages (68% confidence interval) ranged from approximately 65%-77% for PPR and MI methods, 40%-50% for IM and near 50% for CS. These observations illustrate some of the strengths and weaknesses of the methods considered herein and identify future directions for development and improvement. © 2016 IOP Publishing Ltd.},
author_keywords={correlation statistics;  image matching;  mutual information;  particle image velocimetry;  peak ratio;  uncertainty quantification},
document_type={Article},
source={Scopus},
}

@CONFERENCE{CsÍkszentmihÁlyi2016,
author={CsÍkszentmihÁlyi, C. and Mukundane, J.},
title={RootIO: ICT + telephony for grassroots radio},
journal={2016 IST-Africa Conference, IST-Africa 2016},
year={2016},
doi={10.1109/ISTAFRICA.2016.7530700},
art_number={7530700},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986005441&doi=10.1109%2fISTAFRICA.2016.7530700&partnerID=40&md5=3d84fcd9fb25cad95a01c4c84dff7989},
affiliation={Madeira Interactive Technologies Institute, Portugal},
abstract={Communities across Africa depend on radio for access to information, often as their primary source of news and information. Given its importance, surprisingly little ICT work has focused on improving radio. Contemporary ICTs have the potential to catalyze radical changes in ownership models, facilitate greater linguistic and cultural diversity, increase inclusion, and allow for the types of major innovations we have seen in music, television, gaming, or other media in the last decade. The RootIO project set out to test some of these possibilities by building hyperlocal low-cost, low-power radio stations. This paper presents an argument for community media as a focus of ICT, a summary of our research and design process, a technical description of the free/open software and hardware platform we have built, and a description of some early findings from field testing the system. © 2016 IIMC.},
author_keywords={Community Media;  Radio;  Telephony and Internet for Radio},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2016,
title={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
year={2016},
page_count={333},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992187058&partnerID=40&md5=6d1aa11056defc7b3c20529f3c8fe170},
abstract={The proceedings contain 41 papers. The topics discussed include: automatic generation of UTP models from requirements in natural language; a study on the effectiveness of test-categories based test analysis; difference in quality of test architecture between service providers and subcontractors; combinatorial testing: from algorithms to applications; coverage, location, detection, and measurement; towards automatic cost model discovery for combinatorial interaction testing; pairwise coverage-based testing with selected elements in a query for database applications; an experimental evaluation of web mutation operators; on strong mutation and subsuming mutants; diversity-aware mutation adequacy criterion for improving fault detection capability; and measuring effectiveness of mutant sets.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Shin2016122,
author={Shin, D. and Yoo, S. and Bae, D.-H.},
title={Diversity-Aware Mutation Adequacy Criterion for Improving Fault Detection Capability},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
year={2016},
pages={122-131},
doi={10.1109/ICSTW.2016.37},
art_number={7528954},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992184224&doi=10.1109%2fICSTW.2016.37&partnerID=40&md5=69be5cfbeea460b1bd3ae46eb19e471b},
affiliation={School of Computing, KAIST, Daejeon, South Korea},
abstract={Many existing testing techniques adopt diversity as an important criterion for the selection and prioritization of tests. However, mutation adequacy has been content with simply maximizing the number of mutants that have been killed. We propose a novel mutation adequacy criterion that considers the diversity in the relationship between tests and mutants, as well as whether mutants are killed. Intuitively, the proposed criterion is based on the notion that mutants can be distinguished by the sets of tests that kill them. A test suite is deemed adequate by our criterion if the test suite distinguishes all mutants in terms of their kill patterns. Our hypothesis is that, simply by using a stronger adequacy criterion, it is possible to improve fault detection capabilities of mutation-adequate test suites. The empirical evaluation selects tests for real world applications using the proposed mutation adequacy criterion to test our hypothesis. The results show that, for real world faults, test suites adequate to our criterion can increase the fault detection success rate by up to 76.8 percentage points compared to test suites adequate to the traditional criterion. © 2016 IEEE.},
author_keywords={adequacy criteria;  diversity;  Mutation testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gopinath2016132,
author={Gopinath, R. and Alipour, A. and Ahmed, I. and Jensen, C. and Groce, A.},
title={Measuring Effectiveness of Mutant Sets},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2016},
year={2016},
pages={132-141},
doi={10.1109/ICSTW.2016.45},
art_number={7528955},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992184039&doi=10.1109%2fICSTW.2016.45&partnerID=40&md5=2c1eb395c30188d4cf542903420b6d9e},
affiliation={Department of EECS, Oregon State University, United States},
abstract={Redundancy in mutants, where multiple mutants end up producing the same semantic variant of a program, is a major problem in mutation analysis. Hence, a measure of effectiveness that accounts for redundancy is an essential tool for evaluating mutation tools, new operators, and reduction techniques. Previous research suggests using the size of the disjoint mutant set as an effectiveness measure. We start from a simple premise: test suites need to be judged on both the number of unique variations in specifications they detect (as a variation measure), and also on how good they are at detecting hard-to-find faults (as a measure of thoroughness). Hence, any set of mutants should be judged by how well it supports these measurements. We show that the disjoint mutant set has two major inadequacies - the single variant assumption and the large test suite assumption - when used as a measure of effectiveness in variation. These stem from its reliance on minimal test suites. We show that when used to emulate hard to find bugs (as a measure of thoroughness), disjoint mutant set discards useful mutants. We propose two alternatives: one measures variation and is not vulnerable to either the single variant assumption or the large test suite assumption, the other measures thoroughness. We provide a benchmark of these measures using diverse tools. © 2016 IEEE.},
author_keywords={Empirical Analysis;  Mutation Analysis;  Software Testing;  Theoretical Analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tao2016,
author={Tao, R. and Liu, G. and Shi, Y.},
title={One cloud computing node in one IP},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={Part F130520},
doi={10.1145/2925995.2926029},
art_number={a11},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030562684&doi=10.1145%2f2925995.2926029&partnerID=40&md5=ac82a23dbebc086d2f47f80f4d2c3708},
affiliation={School of Computer Science and Technology, Donghua University, Shanhai, China},
abstract={Cloud-based applications offers businesses the chance to deliver products that save time, money and makes it simple to access data from anywhere employees need to access it. This paper discusses a kind of Cloud Computing Node (CCN) from logical topology, physical topology and application based on virtualization and network engineering technologies, which can supplies IaaS, PaaS and SaaS services. This kind of CCN can be used as a container for web servers, software development and test environments, Hadoop related experiment environments, back-end management of Internet of things. All services supplied by this kind of CCN can be accessed through one IP Address by Network Address Translation, Domain Name and Remote Access technologies. It is safety and manageability. It's useful to organize the diverse phys-ical and virtual resources in organizations. It's helpful for educa-Tional institution and small enterprise building their own private cloud computing environments. © 2016 ACM.},
author_keywords={Cloud Computing;  Domain Name;  Network Address Translation;  Virtualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liskowski2016749,
author={Liskowski, P. and Krawiec, K.},
title={Non-negative matrix factorization for unsupervised derivation of search objectives in genetic programming},
journal={GECCO 2016 - Proceedings of the 2016 Genetic and Evolutionary Computation Conference},
year={2016},
pages={749-756},
doi={10.1145/2908812.2908888},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84985997793&doi=10.1145%2f2908812.2908888&partnerID=40&md5=6aa5b5ae510015a4d8994c995655eac8},
affiliation={Institute of Computing Science, Poznan University of Technology, Poznań, 60965, Poland},
abstract={In genetic programming (GP), the outcomes of the evaluation phase in an evolutionary loop can be represented as an interaction matrix, with rows corresponding to programs in a population, columns corresponding to tests that define a program synthesis task, and ones and zeroes signaling respectively passing a test and failing to do so. The conventional fitness, equivalent to a row sum in that matrix, only crudely reflects program's compliance with desired output, and recent contributions in semantic and behavioral GP point to alternative, multifaceted characterizations that facilitate navigation in the search space. In this paper, we propose DOF, a method that uses the popular machine learning technique of non-negative matrix factorization to heuristically derive a low number of underlying objectives from an interaction matrix. The resulting objectives redefine the original single-objective synthesis problem as a multiobjective optimization problem, and we posit that such characterization fosters diversification of search directions while maintaining useful search gradient. The comparative experiment conducted on 15 problems from discrete domains confirms this claim: DOF outperforms the conventional GP and GP equipped with an alternative method of derivation of search objectives on success rate and convergence speed. © 2016 ACM.},
author_keywords={Genetic programming;  Machine learning;  Multiobjective optimization;  Nonnegative matrix factorization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kochhar2016165,
author={Kochhar, P.S. and Xia, X. and Lo, D. and Li, S.},
title={Practitioners' expectations on automated fault localization},
journal={ISSTA 2016 - Proceedings of the 25th International Symposium on Software Testing and Analysis},
year={2016},
pages={165-176},
doi={10.1145/2931037.2931051},
note={cited By 134},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984863696&doi=10.1145%2f2931037.2931051&partnerID=40&md5=d1184a725c2251a2c6d021e8ee623d2b},
affiliation={School of Information Systems, Singapore Management University, Singapore, Singapore; College of Computer Science and Technology, Zhejiang University, China},
abstract={Software engineering practitioners often spend significant amount of time and effort to debug. To help practitioners perform this crucial task, hundreds of papers have proposed various fault localization techniques. Fault localization helps practitioners to find the location of a defect given its symptoms (e.g., program failures). These localization techniques have pinpointed the locations of bugs of various systems of diverse sizes, with varying degrees of success, and for various usage scenarios. Unfortunately, it is unclear whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by surveying 386 practitioners from more than 30 countries across 5 continents about their expectations of research in fault localization. In particular, we investigated a number of factors that impact practitioners' willingness to adopt a fault localization technique. We then compared what practitioners need and the current state-of-research by performing a literature review of papers on fault localization techniques published in ICSE, FSE, ESEC-FSE, ISSTA, TSE, and TOSEM in the last 5 years (2011-2015). From this comparison, we highlight the directions where researchers need to put effort to develop fault localization techniques that matter to practitioners. © 2016 ACM.},
author_keywords={Empirical study;  Fault localization;  Practitioners' expectations},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kowalczyk2016385,
author={Kowalczyk, E.},
title={Modeling App Behavior from Multiple Artifacts},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
year={2016},
pages={385-386},
doi={10.1109/ICST.2016.52},
art_number={7515493},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983339574&doi=10.1109%2fICST.2016.52&partnerID=40&md5=a2cd25cdfb11a5ca0e76aec0e70ca57d},
affiliation={Department of Computer Science, University of Maryland, College Park, College Park, MD, United States},
abstract={Understanding the behavior of a mobile application is typically obtained from static or dynamic artifacts including an app's market page. However, each of these artifacts have limitations which make the resulting models of behavior incomplete. In response, this work asks can a technique which combines a larger, more diverse set of artifacts extracted from an app's executable, runtime behavior and market page construct more accurate models of app behavior? And if so, which artifacts and features are most valuable when doing so? © 2016 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Marculescu201669,
author={Marculescu, B. and Feldt, R. and Torkar, R.},
title={Using Exploration Focused Techniques to Augment Search-Based Software Testing: An Experimental Evaluation},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
year={2016},
pages={69-79},
doi={10.1109/ICST.2016.26},
art_number={7515460},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983246859&doi=10.1109%2fICST.2016.26&partnerID=40&md5=aac4df4afeb2ca495e380f00da7e8c5f},
affiliation={Blekinge Institute of Technology, School of Computing, Karlskrona, Sweden; Chalmers and the University of Gothenburg, Dept. of Computer Science and Engineering, Gothenburg, Sweden},
abstract={Search-based software testing (SBST) often uses objective-based approaches to solve testing problems. There are, however, situations where the validity and completeness of objectives cannot be ascertained, or where there is insufficient information to define objectives at all. Incomplete or incorrect objectives may steer the search away from interesting behavior of the software under test (SUT) and from potentially useful test cases. This papers investigates the degree to which exploration-based algorithms can be used to complement an objective-based tool we have previously developed and evaluated in industry. In particular, we would like to assess how exploration-based algorithms perform in situations where little information on the behavior space is available a priori. We have conducted an experiment comparing the performance of an exploration-based algorithm with an objective-based one on a problem with a high-dimensional behavior space. In addition, we evaluate to what extent that performance degrades in situations where computational resources are limited. Our experiment shows that exploration-based algorithms are useful in covering a larger area of the behavior space and result in a more diverse solution population. Typically, of the candidate solutions that exploration-based algorithms propose, more than 80% were not covered by their objective-based counterpart. This increased diversity is present in the resulting population even when computational resources are limited. We conclude that exploration-focused algorithms are a useful means of investigating high-dimensional spaces, even in situations where limited information and limited resources are available. © 2016 IEEE.},
author_keywords={controlled experiment;  exploration-focused;  objective-based algorithms;  search-based software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feldt2016223,
author={Feldt, R. and Poulding, S. and Clark, D. and Yoo, S.},
title={Test Set Diameter: Quantifying the Diversity of Sets of Test Cases},
journal={Proceedings - 2016 IEEE International Conference on Software Testing, Verification and Validation, ICST 2016},
year={2016},
pages={223-233},
doi={10.1109/ICST.2016.33},
art_number={7515474},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983234033&doi=10.1109%2fICST.2016.33&partnerID=40&md5=447142dc84dd4b238577c1fb7e928d77},
affiliation={Software Engineering Research Lab, Blekinge Institute of Technology, Karlskrona, Sweden; Department of Computer Science, University College London, London, United Kingdom; School of Computing, KAIST, Daejeon, South Korea},
abstract={A common and natural intuition among software testers is that test cases need to differ if a software system is to be tested properly and its quality ensured. Consequently, much research has gone into formulating distance measures for how test cases, their inputs and/or their outputs differ. However, common to these proposals is that they are data type specific and/or calculate the diversity only between pairs of test inputs, traces or outputs. We propose a new metric to measure the diversity of sets of tests: the test set diameter (TSDm). It extends our earlier, pairwise test diversity metrics based on recent advances in information theory regarding the calculation of the normalized compression distance (NCD) for multisets. A key advantage is that TSDm is a universal measure of diversity and so can be applied to any test set regardless of data type of the test inputs (and, moreover, to other test-related data such as execution traces). But this universality comes at the cost of greater computational effort compared to competing approaches. Our experiments on four different systems show that the test set diameter can help select test sets with higher structural and fault coverage than random selection even when only applied to test inputs. This can enable early test design and selection, prior to even having a software system to test, and complement other types of test automation and analysis. We argue that this quantification of test set diversity creates a number of opportunities to better understand software quality and provides practical ways to increase it. © 2016 IEEE.},
author_keywords={Empirical study;  Information theory;  Software testing;  Test selection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kumar2016619,
author={Kumar, S. and Ranjan, P. and Rajesh, R.},
title={Modified ACO to maintain diversity in regression test optimization},
journal={2016 3rd International Conference on Recent Advances in Information Technology, RAIT 2016},
year={2016},
pages={619-625},
doi={10.1109/RAIT.2016.7507970},
art_number={7507970},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999053679&doi=10.1109%2fRAIT.2016.7507970&partnerID=40&md5=db9273f309fffeb683b1e7130c8db4fc},
affiliation={Department of Computer Science, Central University of South Bihar, India},
abstract={Regression testing is unavoidable maintenance activity that is performed several times in software development life cycle. Optimization of regression test case is required to minimize the test case (which will in-turn reduce the time and cost of testing) and to find the fault in early testing activity. The two widely used regression test case optimization techniques, namely, selection and prioritization are recently found to be integrated with different metaheuristic algorithms for fruitful regression test cases. Among the various meta-heuristic algorithms, Ant colony optimization (ACO) algorithm is most popularly used. ACO will try to find the smallest path out all the test cases and it is not sufficient because it will not cover all the test cases which are needed. In this paper we have proposed a modified ant colony optimization to solve test cases in huge search space. The modified algorithm selects the best test cases that find the maximum fault in minimum time. © 2016 IEEE.},
author_keywords={Multi-objective optimization;  Nature Based Optimization;  Regresssion testing;  Soft computing;  Test case optimization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brukiewa2016,
author={Brukiewa, T.F.},
title={First flight of a high power software defined electronic warfare system},
journal={IEEE International Symposium on Phased Array Systems and Technology},
year={2016},
volume={0},
doi={10.1109/ARRAY.2016.7832655},
art_number={7832655},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014098221&doi=10.1109%2fARRAY.2016.7832655&partnerID=40&md5=50acb9b47ad8ffec94f4aade6ceb6a0b},
affiliation={SAS, Systems Development Center, Electronic Warfare Systems Department, Raytheon, El Segundo, CA, United States},
abstract={A description and flight test results of the 'First Flight' of Raytheon's prototype next-generation, Pod based, airborne Electronic Warfare (EW) system is presented. The system is powered by the airstream flowing through a submerged turbine connected to a generator. The primary recipient of this prime power is a wideband, polarization diverse, Active Electronically Scanned Array (AESA) with Transmit/Receive (T/R) modules that contain efficient Gallium Nitride (GaN) amplifiers capable of operating between 0 to 100% duty cycle in either transmit or receive mode. Also featured is an all-digital, scalable, reprogrammable Software Defined Receiver/Exciter Unit that provides the technique generation. The airborne demonstration missions were flown on a Gulfstream III aircraft. They highlighted the jamming techniques that included Transmit and Receive waveforms, AESA beam agility and control, AESA transmit power, generator speed control/power management, and end-to-end system integration effectiveness against real target emitters. © 2016 IEEE.},
author_keywords={AESA;  electromagnetic spectrum;  EW;  Flight test;  GaN;  IADS;  MFIRES Software Defined Receiver Exciter},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ranaee20165341,
author={Ranaee, E. and Riva, M. and Porta, G.M. and Guadagnini, A.},
title={Comparative assessment of three-phase oil relative permeability models},
journal={Water Resources Research},
year={2016},
volume={52},
number={7},
pages={5341-5356},
doi={10.1002/2016WR018872},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978777027&doi=10.1002%2f2016WR018872&partnerID=40&md5=4db13c100d2e30263db65a183b000b4e},
affiliation={Dipartimento di Energia, Politecnico di Milano, Milano, Italy; Dipartimento di Ingegneria Civile e Ambientale, Politecnico di Milano, Milano, Italy},
abstract={We assess the ability of 11 models to reproduce three-phase oil relative permeability (kro) laboratory data obtained in a water-wet sandstone sample. We do so by considering model performance when (i) solely two-phase data are employed to render predictions of kro and (ii) two and three-phase data are jointly used for model calibration. In the latter case, a Maximum Likelihood (ML) approach is used to estimate model parameters. The tested models are selected among (i) classical models routinely employed in practical applications and implemented in commercial reservoir software and (ii) relatively recent models which are considered to allow overcoming some drawbacks of the classical formulations. Among others, the latter set of models includes the formulation recently proposed by Ranaee et al. (), which has been shown to embed the critical effects of hysteresis, including the reproduction of oil remobilization induced by gas injection in water-wet media. We employ formal model discrimination criteria to rank models according to their skill to reproduce the observed data and use ML Bayesian model averaging to provide model-averaged estimates (and associated uncertainty bounds) of kro by taking advantage of the diverse interpretive abilities of all models analyzed. The occurrence of elliptic regions is also analyzed for selected models in the framework of the classical fractional flow theory of displacement. Our study confirms that model outcomes based on channel flow theory and classical saturation-weighted interpolation models do not generally yield accurate reproduction of kro data, especially in the regime associated with low oil saturations, where water alternating gas injection (WAG) techniques are usually employed for enhanced oil recovery. This negative feature is not observed in the model of Ranaee et al. (2015) due to its ability to embed key effects of pore-scale phase distributions, such as hysteresis effects and cycle dependency, for modeling kro observed during WAG. © 2016. American Geophysical Union. All Rights Reserved.},
author_keywords={model comparison;  model discrimination criteria;  multimodel analysis;  parameter estimation;  three-phase oil relative permeability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zamli201657,
author={Zamli, K.Z. and Alkazemi, B.Y. and Kendall, G.},
title={A Tabu Search hyper-heuristic strategy for t-way test suite generation},
journal={Applied Soft Computing Journal},
year={2016},
volume={44},
pages={57-74},
doi={10.1016/j.asoc.2016.03.021},
note={cited By 88},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964058400&doi=10.1016%2fj.asoc.2016.03.021&partnerID=40&md5=8412796c5e7347744882b771bb6b7ad8},
affiliation={IBM Centre of Excellence, Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, Lebuhraya Tun Razak, Kuantan,Pahang Darul Makmur, 26300, Malaysia; College of Computer and Information Systems, Umm Al-Qura University, Saudi Arabia; School of Computer Science, University of Nottingham Malaysia Campus, Jalan Broga, Semenyih,Selangor Darul Ehsan, 43500, Malaysia},
abstract={This paper proposes a novel hybrid t-way test generation strategy (where t indicates interaction strength), called High Level Hyper-Heuristic (HHH). HHH adopts Tabu Search as its high level meta-heuristic and leverages on the strength of four low level meta-heuristics, comprising of Teaching Learning based Optimization, Global Neighborhood Algorithm, Particle Swarm Optimization, and Cuckoo Search Algorithm. HHH is able to capitalize on the strengths and limit the deficiencies of each individual algorithm in a collective and synergistic manner. Unlike existing hyper-heuristics, HHH relies on three defined operators, based on improvement, intensification and diversification, to adaptively select the most suitable meta-heuristic at any particular time. Our results are promising as HHH manages to outperform existing t-way strategies on many of the benchmarks. © 2016 Elsevier B.V. All rights reserved.},
author_keywords={Cuckoo Search Algorithm;  Global Neighborhood Algorithm;  Hyper-heuristic;  Particle Swarm Optimization;  Software testing;  t-way Testing;  Teaching Learning based Optimization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Lee2016479,
author={Lee, S.-C. and Oh, J.-H. and Cho, J.-Y.},
title={Fiber efficiency in SFRC members subjected to uniaxial tension},
journal={Construction and Building Materials},
year={2016},
volume={113},
pages={479-487},
doi={10.1016/j.conbuildmat.2016.03.076},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962271459&doi=10.1016%2fj.conbuildmat.2016.03.076&partnerID=40&md5=603d77fd3328461da2eeabe65daf532b},
affiliation={Department of NPP Engineering, KEPCO International Nuclear Graduate School, Seosaeng-myeon Uljugun, 658-91 Haemaji-ro, Ulsan, South Korea; Office of Offshore Wind Power, Korea Institute of Energy Technology Evaluation and Planning, Seoul, South Korea; Department of Civil and Environmental Engineering, Seoul National University, Seoul, South Korea},
abstract={This paper investigated the tensile behavior of steel fiber-reinforced concrete (SFRC) through an experimental program in which 64 specimens with end-hooked steel fibers were tested under uniaxial tension. The test variables were the concrete compressive strength, fiber volumetric ratio, and fiber aspect ratio (length to diameter). The test results showed that more ductile behavior could generally be achieved with a higher fiber volumetric ratio and a higher fiber aspect ratio; however, the tensile stress attained by the steel fibers did not increase when additional steel fibers were added because the fiber efficiency on the tensile behavior decreased. By comparing the test results and the predictions of the Diverse Embedment Model (DEM), a rational SFRC tension model, a coefficient to reflect the fiber efficiency was derived. The tensile behavior of SFRC members can be more accurately predicted with the proposed fiber efficiency factor, particularly for members with a high fiber volumetric ratio. © 2016 Elsevier Ltd. All rights reserved.},
author_keywords={Crack;  End-hooked fiber;  Fiber efficiency;  SFRC;  Tensile stress},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen201648,
author={Chen, T.Y. and Kuo, F.-C. and Ma, W. and Susilo, W. and Towey, D. and Voas, J. and Zhou, Z.Q.},
title={Metamorphic Testing for Cybersecurity},
journal={Computer},
year={2016},
volume={49},
number={6},
pages={48-55},
doi={10.1109/MC.2016.176},
art_number={7490306},
note={cited By 49},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976515867&doi=10.1109%2fMC.2016.176&partnerID=40&md5=100bd7550dd7f6e3f965e2e907a9933e},
affiliation={Swinburne University of Technology, Australia; University of Wollongong, Australia; University of Nottingham Ningbo China, China; US National Institute of Standards and Technology, United States},
abstract={Metamorphic testing (MT) can enhance security testing by providing an alternative to using a test oracle, which is often unavailable or impractical. The authors report how MT detected previously unknown bugs in real-world critical applications such as code obfuscators, giving evidence that software testing requires diverse perspectives to achieve greater cybersecurity. © 1970-2012 IEEE.},
author_keywords={cybersecurity;  cyberthreats;  fuzzing;  Heartbleed bug;  logic error;  metamorphic testing;  obfuscator testing;  oracle problem;  software testing;  software vulnerability;  Web testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Umut20161,
author={Umut, I.},
title={PSGMiner: A modular software for polysomnographic analysis},
journal={Computers in Biology and Medicine},
year={2016},
volume={73},
pages={1-9},
doi={10.1016/j.compbiomed.2016.03.023},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962745749&doi=10.1016%2fj.compbiomed.2016.03.023&partnerID=40&md5=29ee2d707b34d5f624335000818295e8},
affiliation={Department of Computer Engineering, Faculty of Engineering, Trakya University, Edirne, Turkey},
abstract={Purpose: Sleep disorders affect a great percentage of the population. The diagnosis of these disorders is usually made by polysomnography. This paper details the development of new software to carry out feature extraction in order to perform robust analysis and classification of sleep events using polysomnographic data. The software, called PSGMiner, is a tool, which visualizes, processes and classifies bioelectrical data. The purpose of this program is to provide researchers with a platform with which to test new hypotheses by creating tests to check for correlations that are not available in commercially available software. The software is freely available under the GPL3 License. Method: PSGMiner is composed of a number of diverse modules such as feature extraction, annotation, and machine learning modules, all of which are accessible from the main module. Using the software, it is possible to extract features of polysomnography using digital signal processing and statistical methods and to perform different analyses. The features can be classified through the use of five classification algorithms. PSGMiner offers an architecture designed for integrating new methods. Comparison with existing methods: Automatic scoring, which is available in almost all commercial PSG software, is not inherently available in this program, though it can be implemented by two different methodologies (machine learning and algorithms). While similar software focuses on a certain signal or event composed of a small number of modules with no expansion possibility, the software introduced here can handle all polysomnographic signals and events. Conclusions: The software simplifies the processing of polysomnographic signals for researchers and physicians that are not experts in computer programming. It can find correlations between different events which could help predict an oncoming event such as sleep apnea. The software could also be used for educational purposes. © 2016 Elsevier Ltd.},
author_keywords={Computer software;  Digital signal processing;  Machine learning;  Polysomnography},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tselonis201690,
author={Tselonis, S. and Gizopoulos, D.},
title={GUFI: A framework for GPUs reliability assessment},
journal={ISPASS 2016 - International Symposium on Performance Analysis of Systems and Software},
year={2016},
pages={90-100},
doi={10.1109/ISPASS.2016.7482077},
art_number={7482077},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978761937&doi=10.1109%2fISPASS.2016.7482077&partnerID=40&md5=16c57206752319941f5b03f0d2920b56},
affiliation={University of Athens, Department of Informatics and Telecommunications, Greece},
abstract={Modern many-core Graphics Processing Units (GPUs) are extensively employed in general purpose computing (GPGPU), offering a remarkable execution speedup to inherently data parallel workloads. Unlike graphics computing, GPGPU computing has more stringent reliability requirements. Thus, accurate reliability assessment of GPU hardware structures is important for making informed decisions for error protection. In this paper we focus on microarchitecture-level reliability assessment for GPU architectures. The paper makes the following contributions. First, it presents a comprehensive fault injection framework that targets key hardware structures of GPU architectures such as the register file, the shared memory, the SIMT stack and the instruction buffer, which altogether occupy large part of a modern GPU silicon area. Second, it reports our reliability assessment findings for the target structures, when the GPU executes a diverse set of twelve GPGPU applications. Third, it discusses remarkable differences in the results of fault injection when the applications are simulated in the virtual NVIDIA GPUs instruction set (ptx) vs. the actual instruction set (sass). Finally, it discusses how the framework can be employed either by architects in the early stages of design phase or by programmers for a GPU application's error resilience enhancement. © 2016 IEEE.},
author_keywords={fault injection;  GPGPU;  microarchitecture simulators;  reliability assessment},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Artho201629,
author={Artho, C. and Ma, L.},
title={Classification of randomly generated test cases},
journal={2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering, SANER 2016},
year={2016},
volume={2016-January},
pages={29-32},
doi={10.1109/SANER.2016.32},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115079019&doi=10.1109%2fSANER.2016.32&partnerID=40&md5=325b5762ae958d18e4ccca9b000c086e},
affiliation={AIST, Osaka, Japan; Chiba University, Chiba, Japan},
abstract={Random test case generation produces relatively diverse test sequences, but the validity of the test verdict is always uncertain. Because tests are generated without taking the specification and documentation into account, many tests are invalid. To understand the prevalent types of successful and invalid tests, we present a classification of 56 issues that were derived from 208 failed, randomly generated test cases. While the existing workflow successfully eliminated more than half of the tests as irrelevant, half of the remaining failed tests are false positives. We show that the new @NonNull annotation of Java 8 has the potential to eliminate most of the false positives, highlighting the importance of machine-readable documentation. © 2016 IEEE},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Staubitz2016314,
author={Staubitz, T. and Klement, H. and Teusner, R. and Renz, J. and Meinel, C.},
title={CodeOcean - A versatile platform for practical programming excercises in online environments},
journal={IEEE Global Engineering Education Conference, EDUCON},
year={2016},
volume={10-13-April-2016},
pages={314-323},
doi={10.1109/EDUCON.2016.7474573},
art_number={7474573},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994667280&doi=10.1109%2fEDUCON.2016.7474573&partnerID=40&md5=7878240ed52597dc317df220df01a0b9},
affiliation={Hasso Plattner Institute, University of Potsdam, Potsdam, Germany},
abstract={The paper at hand introduces CodeOcean, a web-based platform to provide practical programming exercises. CodeOcean is designed to be used in Massive Open Online Courses (MOOCs) to teach programming to beginners. Its concept and implementation are discussed with regard to tools provided to students and teachers, sandboxed and scalable code execution, scalable assessment, and interoperability. MOOCs bear a tremendous potential for teaching programming to a large and diverse audience. Learning to program, however, is a hands-on effort; watching videos and solving multiple choice tests will not be sufficient. A platform, such as CodeOcean, to work on practical programming exercises and to solve actual programming tasks is required. Due to the massiveness of the courses, teaching teams cannot check, give feedback, or assess the submissions of the participants manually. CodeOcean provides the participants with proper automated feedback in a timely manner and is able to assess the given programming tasks in an automated way. © 2016 IEEE.},
author_keywords={Automated Assessment;  E-Learning;  Hands-on Experience;  MOOC;  Online Assessment;  Programming;  Scalability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ali-Shahid201631,
author={Ali-Shahid, M.M. and Sulaiman, S.},
title={A case study on reliability and usability testing of a Web portal},
journal={2015 9th Malaysian Software Engineering Conference, MySEC 2015},
year={2016},
pages={31-36},
doi={10.1109/MySEC.2015.7475191},
art_number={7475191},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974623047&doi=10.1109%2fMySEC.2015.7475191&partnerID=40&md5=bb2dbb32641429bdf7f0c64fe5dc5e79},
affiliation={Faculty of Computing, Universiti Teknologi Malaysia, Skudai, Johor, 81310, Malaysia; Faculty of Computer Science, COMSATS Institute Oflnformation Technology Vehari, Pakistan},
abstract={Reliability testing of web portals is distinctive as compared to that of conventional software. It requires usability and reliability metrics besides functional testing to ensure diverse users are able to use such portals without any formal training and can rely on the portals or requirements quickly and efficiently. The study utilizes the measurable metrics for usability and reliability testing and establishes interpretation between both metrics by performing testing on a Web portal using online tools and analyze the results. The case study is based on a Web application that facilitates information and knowledge sharing among its online members. The outcome from the study provides an insight on how to measure usability and part of reliability of a Web portal based on ISO metrics using available testing tools. © 2015 IEEE.},
author_keywords={Reliability;  software testing;  usability;  Web portals},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Matinnejad2016585,
author={Matinnejad, R. and Nejati, S. and Briand, L.C. and Bruckmann, T.},
title={SimCoTest: A test suite generation tool for simulink/stateflow controllers},
journal={Proceedings - International Conference on Software Engineering},
year={2016},
pages={585-588},
doi={10.1145/2889160.2889162},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010983946&doi=10.1145%2f2889160.2889162&partnerID=40&md5=8c0d912863a1b129a5c826335bf48181},
affiliation={SnT Centre, University of Luxembourg, Luxembourg; Delphi Automotive Systems, Luxembourg},
abstract={We present SimCoTest, a tool to generate small test suites with high fault revealing ability for Simulink/Stateflow controllers. SimCoTest uses meta-heuristic search to (1) maximize the likelihood of presence of specific failure patterns in output signals (failure-based test generation), and to (2) maximize diversity of output signal shapes (output diversity test generation). SimCoTest has been evaluated on industrial Simulink models and has been systematically compared with Simuilnk Design Verifier (SLDV), an alternative commercial Simulink testing tool. Our results show that the fault revealing ability of SimCoTest outperforms that of SLDV. Further, in contrast to SLDV, SimCoTest is applicable to Simulink/Stateflow models in their entirety. © 2016 ACM.},
author_keywords={Failure-based test generation;  Output diversity;  Search-based software testing;  Simulink design verifier (SLDV);  Simulink/stateflow models;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gren2016121,
author={Gren, L. and Goldman, A.},
title={Useful statistical methods for human factors research in software engineering: A discussion on validation with quantitative data},
journal={Proceedings - 9th International Workshop on Cooperative and Human Aspects of Software Engineering, CHASE 2016},
year={2016},
pages={121-124},
doi={10.1145/2897586.2897588},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974555440&doi=10.1145%2f2897586.2897588&partnerID=40&md5=26d76ac010a8224735c1855c315896ab},
affiliation={Chalmers and University of Gothenburg, Gothenburg, 412-92, Sweden; University of São Paulo, São Paulo, 05508-090, Brazil},
abstract={In this paper we describe the usefulness of statistical validation techniques for human factors survey research. We need to investigate a diversity of validity aspects when creating metrics in human factors research, and we argue that the statistical tests used in other fields to get support for reliability and construct validity in surveys, should also be applied to human factors research in software engineering more often. We also show briey how such methods can be applied (Test-Retest, Cronbach's α and Exploratory Factor Analysis). © 2016 ACM.},
author_keywords={Human factors;  Psychology;  Quantitative data;  Statistical tests;  Validation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Matinnejad2016595,
author={Matinnejad, R. and Nejati, S. and Briand, L.C. and Bruckmann, T.},
title={Automated test suite generation for time-continuous simulink models},
journal={Proceedings - International Conference on Software Engineering},
year={2016},
volume={14-22-May-2016},
pages={595-606},
doi={10.1145/2884781.2884797},
note={cited By 52},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971426915&doi=10.1145%2f2884781.2884797&partnerID=40&md5=a3190453596d536ce7414ab814b32b64},
affiliation={SnT Centre, University of Luxembourg, Luxembourg, Luxembourg; Delphi Automotive Systems, Luxembourg, Luxembourg},
abstract={All engineering disciplines are founded and rely on models, although they may differ on purposes and usages of modeling. Interdisciplinary domains such as Cyber Physical Systems (CPSs) seek approaches that incorporate different modeling needs and usages. Specifically, the Simulink modeling platform greatly appeals to CPS engineers due to its seamless support for simulation and code generation. In this paper, we propose a test generation approach that is applicable to Simulink models built for both purposes of simulation and code generation. We define test inputs and outputs as signals that capture evolution of values over time. Our test generation approach is implemented as a meta-heuristic search algorithm and is guided to produce test outputs with diverse shapes according to our proposed notion of diversity. Our evaluation, performed on industrial and public domain models, demonstrates that: (1) In contrast to the existing tools for testing Simulink models that are only applicable to a subset of code generation models, our approach is applicable to both code generation and simulation Simulink models. (2) Our new notion of diversity for output signals outperforms random baseline testing and an existing notion of signal diversity in revealing faults in Simulink models. (3) The fault revealing ability of our test generation approach outperforms that of the Simulink Design Verifier, the only testing toolbox for Simulink. © 2016 ACM.},
author_keywords={Output diversity;  Search-based software testing;  Signal features;  Simulink Design Verifier (SLDV);  Simulink models;  Software testing;  Structural coverage;  Time-continuous behaviors},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Henard2016523,
author={Henard, C. and Papadakis, M. and Harman, M. and Jia, Y. and Traon, Y.L.},
title={Comparing white-box and black-box test prioritization},
journal={Proceedings - International Conference on Software Engineering},
year={2016},
volume={14-22-May-2016},
pages={523-534},
doi={10.1145/2884781.2884791},
note={cited By 101},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971425590&doi=10.1145%2f2884781.2884791&partnerID=40&md5=eabcf69f2468f612cf19ab5f30bb90d9},
affiliation={University of Luxembourg, Luxembourg; University College London, United Kingdom},
abstract={Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including wellestablished white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black-and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases. © 2016 ACM.},
author_keywords={Black-box;  Regression Testing;  White-box},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Al-Faifi201618,
author={Al-Faifi, S.A. and Migdadi, H.M. and Algamdi, S.S. and Khan, M.A. and Ammar, M.H. and Al-Obeed, R.S. and Al-Thamra, M.I. and El-Harty, E.H. and Jakse, J.},
title={Development, characterization and use of genomic SSR markers for assessment of genetic diversity in some Saudi date palm (Phoenix dactylifera L.) cultivars},
journal={Electronic Journal of Biotechnology},
year={2016},
volume={21},
pages={18-25},
doi={10.1016/j.ejbt.2016.01.006},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969568010&doi=10.1016%2fj.ejbt.2016.01.006&partnerID=40&md5=d3940ea3b5913074b614a030265befe7},
affiliation={Plant Production Department, College of Food and Agricultural Sciences, King Saud University, Riyadh, Saudi Arabia; Agronomy Department, Biotechnical Faculty, University of Ljubljana, Jamnikarjeva 101, Ljubljana, SI-1000, Slovenia},
abstract={Background: The present study was undertaken towards the development of SSR markers and assessing genetic relationships among 32 date palm (Phoenix dactylifera L.) representing common cultivars grown in different geographical regions in Saudi Arabia. Results: Ninety-three novel simple sequence repeat markers were developed and screened for their ability to detect polymorphism in date palm. Around 71% of genomic SSRs were dinucleotide, 25% tri, 3% tetra and 1% penta nucleotide motives. Twenty-two primers generated a total of 91 alleles with a mean of 4.14 alleles per locus and 100% polymorphism percentage. A 0.595 average polymorphic information content and 0.662 primer discrimination power values were recorded. The expected and observed heterozygosities were 0.676 and 0.763 respectively. Pair-wise similarity values ranged from 0.06 to 0.89 and the overall cultivars averaged 0.41. The UPGMA cluster analysis recovered by principal coordinate analysis illustrated that cultivars tend to group according to their class of maturity, region of cultivation, and fruit color. Analysis of molecular variations (AMOVA) revealed that genetic variation among and within cultivars were 27% and 73%, respectively according to geographical distribution of cultivars. Conclusions: The developed microsatellite markers are additional values to date palm characterization tools that can be used by researchers in population genetics, cultivar identification as well as genetic resource exploration and management. The tested cultivars exhibited a significant amount of genetic diversity and could be suitable for successful breeding program. Genomic sequences generated from this study are available at the National Center for Biotechnology Information (NCBI), Sequence Read Archive (Accession numbers. LIBGSS_039019). © 2016 Pontificia Universidad Católica de Valparaíso. Production and hosting by Elsevier B.V. All rights reserved.},
author_keywords={Expected and observed heterozygosity;  Microsatellite markers;  Polymorphic information content;  Polymorphism;  Primer discrimination power},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Hassan20161566,
author={Hassan, M.M. and Shah, S.M.A. and Afzal, W. and Andler, S.F. and Lindström, B. and Blom, M.},
title={Testability and software performance: A systematic mapping study},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2016},
volume={04-08-April-2016},
pages={1566-1569},
doi={10.1145/2851613.2851978},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975789870&doi=10.1145%2f2851613.2851978&partnerID=40&md5=3c57a28b398852c60b6cf15d7c9a3bfe},
affiliation={Karlstad University, Sweden; SICS, Sweden; Mälardalen Uni., Sweden; University of Skövde, Sweden},
abstract={In most of the research on software testability, functional correctness of the software has been the focus while the evidence regarding testability and non-functional properties such as performance is sporadic. The objective of this study is to present the current state-of-the-art related to issues of importance, types and domains of software under test, types of research, contribution types and design evaluation methods concerning testability and software performance. We find that observability, controllability and testing effort are the main testability issues while timeliness and response time (i.e., time constraints) are the main performance issues in focus. The primary studies in the area use diverse types of software under test within different domains, with realtime systems as being a dominant domain. The researchers have proposed many different methods in the area, however these methods lack implementation in practice. © 2016 ACM.},
author_keywords={Software performance;  Systematic mapping study;  Testability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shahbazi2016361,
author={Shahbazi, A. and Miller, J.},
title={Black-Box String Test Case Generation through a Multi-Objective Optimization},
journal={IEEE Transactions on Software Engineering},
year={2016},
volume={42},
number={4},
pages={361-378},
doi={10.1109/TSE.2015.2487958},
art_number={7293669},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968914428&doi=10.1109%2fTSE.2015.2487958&partnerID=40&md5=a60b531f47f6d07828f20a26fda93c01},
affiliation={Department of Electrical and Computer Engineering, University of Alberta, Edmonton, AB, Canada},
abstract={String test cases are required by many real-world applications to identify defects and security risks. Random Testing (RT) is a low cost and easy to implement testing approach to generate strings. However, its effectiveness is not satisfactory. In this research, black-box string test case generation methods are investigated. Two objective functions are introduced to produce effective test cases. The diversity of the test cases is the first objective, where it can be measured through string distance functions. The second objective is guiding the string length distribution into a Benford distribution based on the hypothesis that the population of strings is right-skewed within its range. When both objectives are applied via a multi-objective optimization algorithm, superior string test sets are produced. An empirical study is performed with several real-world programs indicating that the generated string test cases outperform test cases generated by other methods. © 2015 IEEE.},
author_keywords={Adaptive random testing;  automated test case generation;  black-box testing;  mutation;  random testing;  software testing;  string distance;  String test cases},
document_type={Article},
source={Scopus},
}

@ARTICLE{Botella2016113,
author={Botella, J. and Delahaye, J.-P. and Jaffuel, E. and Legeard, B. and Peureux, F.},
title={Achieving SCA Conformance Testing with Model-Based Testing},
journal={Journal of Signal Processing Systems},
year={2016},
volume={83},
number={1},
pages={113-128},
doi={10.1007/s11265-015-1089-y},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959459407&doi=10.1007%2fs11265-015-1089-y&partnerID=40&md5=fd049fae6746a935f7d0deefa5330be9},
affiliation={Smartesting R&D Center, Besançon, 25000, France; DGA/CELAR, French MoD, Bruz, 35170, France; eConsult, Cussey-sur-l’Ognon, 25870, France; Institut FEMTO-ST, UMR CNRS 6174, Besançon, 25030, France},
abstract={The Software Communications Architecture (SCA) is a software architecture provided and published by the Joint Tactical Networking Center (JTNC). Facing the multiplicity of the waveforms and the diversity of the platform architectures and form factors, the original aims of the SCA are to facilitate the waveform development in terms of portability and waveform deployments onto heterogeneous Software Defined Radio (SDR) platforms. In this paper, we present an approach using Model-Based Testing (MBT) to ensure the conformance of a software radio platform with SCA requirements. In this approach, an MBT model is developed on the basis of SCA specifications, and conformance tests and scripts are generated and then run on the targeted software radio platform. This approach has been developed within a French research project, called OSeP, with results regarding modeling for automated test generation for SCA conformance testing. The techniques involved in this project focus on functional requirements and automatically generate Java executable test scripts, which aim to evaluate the functional conformance of the software implementation with respect to their associated requirements. © 2015, Springer Science+Business Media New York.},
author_keywords={Conformance testing;  Dynamic testing;  Model-based testing (MBT);  Software communications architecture (SCA)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cai2016132,
author={Cai, J. and Zou, P. and Sheng, B. and He, J.},
title={Feedback fuzzing based on improved roulette wheel selection strategy},
journal={Sichuan Daxue Xuebao (Gongcheng Kexue Ban)/Journal of Sichuan University (Engineering Science Edition)},
year={2016},
volume={48},
number={2},
pages={132-138},
doi={10.15961/j.jsuese.2016.02.019},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962850063&doi=10.15961%2fj.jsuese.2016.02.019&partnerID=40&md5=cff77a91f1546ec7cca9542410b98e54},
affiliation={Sci. and Technol. on Complex Electronic System Simulation Lab., Academy of Equipment, Beijing, 101416, China; Dept. of Computer Sci. and Technol., Tsinghua Univ., Beijing, 100084, China},
abstract={In order to solve the problem of blindness and poor efficiency of traditional fuzzing, a feedback fuzzing method was proposed. This method can improve the test effect by optimizing the process of traditional fuzzing. Firstly, original sample files were collected via a web crawler to ensure the diversity of them. Then, the best seed file was selected from a large number of sample files, to avoid redundant testing while ensuring adequate testing space. Finally, test cases were generated by mutating the seed file, and the mutation parameters was continuously adjusted according to the testing result based on improved roulette wheel selection strategy, in order to find as many software failures as possible. A prototype system named OSSRWSFuzzer was implemented, which had found 56 failures of WPS Office for Linux in experiments, and its test effect is obviously better than that of the existing fuzzers. © 2016, Editorial Department of Journal of Sichuan University (Engineering Science Edition). All right reserved.},
author_keywords={Feedback fuzzing;  Optimizing seed selection;  Roulette wheel selection},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Deyati2016463,
author={Deyati, S. and Muldrey, B.J. and Chatterjee, A.},
title={TRAP: Test Generation Driven Classification of Analog/RF ICs Using Adaptive Probabilistic Clustering Algorithm},
journal={Proceedings of the IEEE International Conference on VLSI Design},
year={2016},
volume={2016-March},
pages={463-468},
doi={10.1109/VLSID.2016.118},
art_number={7434997},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964619902&doi=10.1109%2fVLSID.2016.118&partnerID=40&md5=9943b0dcd4e7117a09946f8292b4d70c},
affiliation={Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA  30332, United States},
abstract={In production testing of analog/RF ICs, application of standard specification-based tests for IC classification is not always an attractive option due to the high costs and test times involved. In this paper, a new test generation algorithm for IC classification is first developed that has the property that the corresponding DUT response signatures for devices from diverse process corners are maximally separable. In other words, the process space can be partitioned into a maximally large number of partitions and each tested IC can be uniquely diagnosed to lie in one such partition from its response to the applied test stimulus. Boolean zed representations of analog/RF circuits are used to facilitate rapid test stimulus generation. Next, the responses of ICs tested in production to the applied classification test are used to adaptively classify the ICs into clusters of «good», «bad» and «marginal» devices (TRAP). This is done through the use of probabilistic neural networks that do not require complete network retraining every time an IC with different input-output behavior is observed. The classification test applied helps in determining if a new device being tested is from a different process corner than encountered before, thereby aiding IC classification. Simulation experiments show that the learning process is rapid and minimizes device misclassification with significant savings in test time. © 2016 IEEE.},
author_keywords={Automatic test pattern generation;  Circuit testing;  Classification algorithms;  Integrated circuit modeling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Johri2016,
author={Johri, P. and Nasar, M. and Das, S. and Kumar, M.},
title={Open source software reliability growth models for distributed environment based on component-specific testing-efforts},
journal={ACM International Conference Proceeding Series},
year={2016},
volume={04-05-March-2016},
doi={10.1145/2905055.2905283},
art_number={a75},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988601023&doi=10.1145%2f2905055.2905283&partnerID=40&md5=d200e5220e17596a8bebf076ab311357},
affiliation={School of Computing Science and Engg, Galgotias University Gr. Noida, India},
abstract={Because of availability, redistributable, affordability, modifiability, of source code, free and no restriction in choice, open source is a favorite platform for lot of software industries and peoples, who consider using the power of extremely reliable and superior quality software. Numeouus SRGMs have been proposed to estimate the reliability of the software of OSSs; however, no one has proven to perform very well considering diverse project characteristics. In the models for OSSs, the error deletion experience for the reused and the newly developed components based on component-specific testing-effort is demonstrated. It is considered that there are several different types of faults for newly developed component and single type of faults for reused components for obtaining the unambiguous expressions for the mean number of individual types of errors. For OSSs system components testing-efforts have to be modeled separately for each and every component in the system. The total effort of the system is then calculated from the summation of component-specific testing-effort functions. We have employed MATLAB as implementation framework for performing all the estimations. Our approach partitions the testing effort with growth curves of varying nature among different components of the same OSS. To validate our analytical results, numerical illustrations have also been provided. © 2016 ACM.},
author_keywords={Distributed development environment (DDE);  Non Homogeneous Poisson Process (NHPP);  Open Source Software (OSS);  Software Reliability Growth Models (SRGMs).;  Testing-effort function (TEF)},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xia201643,
author={Xia, X. and Gong, L. and Le, T.-D.B. and Lo, D. and Jiang, L. and Zhang, H.},
title={Diversity maximization speedup for localizing faults in single-fault and multi-fault programs},
journal={Automated Software Engineering},
year={2016},
volume={23},
number={1},
pages={43-75},
doi={10.1007/s10515-014-0165-z},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955592581&doi=10.1007%2fs10515-014-0165-z&partnerID=40&md5=7c3fb6a18bbada7bb9e6126fae435f57},
affiliation={College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Electrical Engineering and Computer Sciences, University of California, Berkeley, CA, United States; School of Information Systems, Singapore Management University, Singapore, Singapore; Microsoft Research, Beijing, China},
abstract={Fault localization is useful for reducing debugging effort. Such techniques require test cases with oracles, which can determine whether a program behaves correctly for every test input. Although most fault localization techniques can localize faults relatively accurately even with a small number of test cases, choosing the right test cases and creating oracles for them are not easy. Test oracle creation is expensive because it can take much manual labeling effort (i.e., effort needed to decide whether the test cases pass or fail). Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (Dms). Dms orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. We evaluate the performance of Dms on 2 different types of programs, single-fault and multi-fault programs. Our experiments with 411 faults from the Software-artifact Infrastructure Repository show (1) that Dms can help existing fault localization techniques to achieve comparable accuracy with on average 67 and 6 % fewer labeled test cases than previously best test case prioritization techniques for single-fault and multi-fault programs, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), Dms can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant. © 2014, Springer Science+Business Media New York.},
author_keywords={Fault localization;  Multi-fault program;  Single-fault program;  Test case prioritization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Shi201619,
author={Shi, Q. and Chen, Z. and Fang, C. and Feng, Y. and Xu, B.},
title={Measuring the Diversity of a Test Set with Distance Entropy},
journal={IEEE Transactions on Reliability},
year={2016},
volume={65},
number={1},
pages={19-27},
doi={10.1109/TR.2015.2434953},
art_number={7116633},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930471834&doi=10.1109%2fTR.2015.2434953&partnerID=40&md5=e59eb84b35850dd75f0b934a858133ee},
affiliation={State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China},
abstract={Most existing metrics that we call white-box metrics, such as coverage metrics, require white-box information, like program structure information, and historical runtime information, to evaluate the fault detection capability of a test set. In practice, such white-box information is usually unavailable or difficult to obtain, which means they often cannot be used. In this paper, we propose a black-box metric, distance entropy, based on the diversification idea behind many published diversity-based techniques. Distance entropy provides a possible solution for test set evaluation when white-box information is not available. The empirical study illustrates that distance entropy can effectively evaluate test sets if the distance metric between tests is well defined. Meanwhile, distance entropy outperforms simple diversity metrics without increasing time complexity. © 2015 IEEE.},
author_keywords={diversity;  Fault detection capability;  metrics;  minimum spanning tree},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Shuai2016317,
author={Shuai, B. and Li, H. and Zhang, L. and Zhang, Q. and Tang, C.},
title={Software vulnerability detection based on code coverage and test cost},
journal={Proceedings - 2015 11th International Conference on Computational Intelligence and Security, CIS 2015},
year={2016},
pages={317-321},
doi={10.1109/CIS.2015.84},
art_number={7397098},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964413929&doi=10.1109%2fCIS.2015.84&partnerID=40&md5=1cbe30edc4e42a445e54b35c9b5467ec},
affiliation={School of Electronic Science and Engineering, National University of Defense Technology, Changsha, Hunan, 410073, China},
abstract={In order to solve the problems of traditional Fuzzing technique for software vulnerability detection, a novel method based on code coverage and test cost is proposed. Firstly, static analysis is applied to calculate the code coverage information, including basic block coverage and new block coverage. In addition, test path diversity information is introduced to elevate path coverage, which is achieved based on the sequence alignment algorithm. Secondly, test cost is analyzed respectively from running time and loop structure. The loop structure is simplified using finite expansion manner. Thirdly, the genetic algorithm fitness function is constructed based on the code coverage and test cost to guide the test case generation. Experiments on realistic binary software show that the method could obtain higher vulnerability detection accuracy and efficiency than the traditional Fuzzing technique. © 2015 IEEE.},
author_keywords={Code coverage;  Genetic algorithm;  Test cost;  Test path diversity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lin2016428,
author={Lin, Y.-D. and Lai, Y.-C. and Lu, C.-N. and Hung, J.-T. and Shao, C.-P.},
title={Traffic diversity and code coverage: A preliminary analysis},
journal={International Journal of Communication Systems},
year={2016},
volume={29},
number={3},
pages={428-440},
doi={10.1002/dac.2849},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954440238&doi=10.1002%2fdac.2849&partnerID=40&md5=edf1a80483d7862903c5991602d590a5},
affiliation={Department of Computer Science, National Chiao Tung University, Hsinchu, Taiwan; Department of Information Management, National Taiwan University of Science and Technology, Taipei, Taiwan},
abstract={Summary It is generally assumed that using more diverse traffic to test network devices could achieve larger code coverage. However, how to describe the diversity of traffic traces and the relationship between the traffic diversity and code coverage is still an issue. In this paper, the traffic diversity is defined using the number of packets and the size of the subnets involved, and traces having various diversity are used to evaluate the corresponding code coverage for the programs in a network device. Experiment results show that more number of packets or larger size of network segments can generate larger diversity indices and thus larger code coverage. For Snort, as the number of packets increases from 1 to 10,000,000, representative diversity index and the code coverage can increase from 0 to 0.95 on the basis of Simpson's index and from 19.1% to 32.2%, respectively. As the size of network segments increases, representative diversity index and the code coverage can increase from 0.41 to 0.82 and from 28.2% to 32.2%, respectively. Similar results can be obtained in the case of Linux kernel. If the mappings among different diversity indices and the corresponding code coverage can be built beforehand, the quality of the tests can improved. Copyright © 2014 John Wiley & Sons, Ltd.},
author_keywords={code coverage;  diversity index;  network test;  traffic diversity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Elsaka201685,
author={Elsaka, E. and Memon, A.},
title={Disqover: Debugging via code sequence covers},
journal={2015 IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2015},
year={2016},
pages={85-92},
doi={10.1109/ISSREW.2015.7392051},
art_number={7392051},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974621999&doi=10.1109%2fISSREW.2015.7392051&partnerID=40&md5=8d11d963bbd221c7bfdd87207633bc65},
affiliation={Department of Computer Science, University of Maryland, College Park, MD, United States},
abstract={Automated model-based test generation has seen an undeniable trend towards obtaining large numbers of test cases. However, the full benefits of this trend have not yet percolated to downstream activities, such as debugging. We present Disqover for automated software debugging based on code sequence covers that leverages execution traces, or alternatively, sequence covers of large numbers of failing test cases to quickly identify causes of test failures, thereby aiding debugging. We develop a new algorithm that efficiently extracts commonalities between sequence covers in the form of ordered subsequences and values of variables contained in these subsequences that contribute to each failure. The results of our experimental evaluation suggest that users of Disqover need only 30% of the time needed to identify faults compared to the baseline in a user study. Furthermore, we show that the number of inspected statements using our approach is smaller than that of other state-of-The-Art systems by multiple orders of magnitude. Additionally, we show that increasing the number and diversity of test cases improves our results by further decreasing the length of output subsequences to be examined. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Frtunikj2016160,
author={Frtunikj, J. and Frohlich, J. and Rohlfs, T. and Knoll, A.},
title={Qualitative evaluation of fault hypotheses with non-intrusive fault injection},
journal={2015 IEEE International Symposium on Software Reliability Engineering Workshops, ISSREW 2015},
year={2016},
pages={160-167},
doi={10.1109/ISSREW.2015.7392062},
art_number={7392062},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974574090&doi=10.1109%2fISSREW.2015.7392062&partnerID=40&md5=ab0708f3f5725e7a6b9399dc62da3c26},
affiliation={Fortiss GmbH, Guerickestrabe 25, München, 80805, Germany; Siemens AG, Otto Hahn Ring 6, München, 81739, Germany; Jambit GmbH, Erika-Mann-Strabe 63, München, 80636, Germany; Technische Universität München, Boltzmannstrabe 3, Garching, 85748, Germany},
abstract={This paper presents a new approach for demonstrating whether safety-critical, hard real-Time systems implement fault hypotheses correctly and timely. In the forefront are tests which non-intrusively and deterministically stimulate and monitor the system under test. The tests use a domain-specific language which can formalize logical truths on system properties derived from fault hypotheses. Test results are strong arguments in safety cases. In this way the tests support both development and certification of safety-critical systems. Advantages over existing approaches to evaluating safety properties of complex and diverse safety-critical systems are discussed briefly, and fundamental work is referenced. © 2015 IEEE.},
author_keywords={fault hypothesis;  fault injection test;  safety;  safety case;  safety properties},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2016451,
author={Lee, C.-S. and Wong, K.D. and Lau, S.B.-Y.},
title={Scaffolds and design factors to increase creative outcomes in teaching software design and testing},
journal={IEEE International Conference on Industrial Engineering and Engineering Management},
year={2016},
volume={2016-January},
pages={451-454},
doi={10.1109/IEEM.2015.7385687},
art_number={7385687},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962010086&doi=10.1109%2fIEEM.2015.7385687&partnerID=40&md5=6117b3f24cdaeb2d7a7da03b5098c447},
affiliation={Faculty of Science and Technology, Sunway University, Malaysia; Faculty of Information Technology, Malaysia University of Science and Technology, Malaysia; School of Computing, University College of Technology Sarawak, Malaysia},
abstract={Graduates are expected to be able to provide holistic solutions, capable of meeting diverse objectives simultaneously. Furthermore, opportunities are often found or made. The quality of solutions, however, is mediated by different conceptualizations of contexts. Thus, we aim to investigate how students would conceptualize, make sense, desire to know, find solutions and subsequently progress to collaborate, communicate and create new artefacts. We hypothesize that there would be a higher likelihood of better quality design process, explanations as well as modelling outcomes if first, students can relate between the ability to analyse problems with the ability to conceptualize/model and second, the design of the task focuses on not only functional but more importantly sustaining positive user experience. Findings highlight the importance of key design factors contributing to more creative outcomes. © 2015 IEEE.},
author_keywords={Conceptualization;  creative thinking;  Engineering education;  scaffolding},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Altaf20161378,
author={Altaf, I. and Dar, J.A. and Rashid, F.U. and Rafiq, M.},
title={Survey on selenium tool in software testing},
journal={Proceedings of the 2015 International Conference on Green Computing and Internet of Things, ICGCIoT 2015},
year={2016},
pages={1378-1383},
doi={10.1109/ICGCIoT.2015.7380682},
art_number={7380682},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966671563&doi=10.1109%2fICGCIoT.2015.7380682&partnerID=40&md5=6a2c1f3efd058cd26c18c6f10a5d4c27},
affiliation={Department of Computer Science and Engineering, Amity University, Noida, India; Department of Computer Science and Engineering, Islamic University of Science and Technology, Kashmir, India},
abstract={We are moving towards a general public where web is the need of hour. Today the vast majority of the product applications executed, are composed as online applications which are keep running in a web program. Testing programming applications is critical. Numerous associations make utilization of a specific web application, so the same web applications are tried habitually by diverse clients from distinctive regions physically. Testing a web application physically is tedious, so we go for test automation. In test automation we make utilization of a product device to run repeatable tests against the application to be tried. There are various focal points of test automation. They are exceptionally exact and have more prominent preparing pace when contrasted with manual automation. There are various open source and business devices accessible for test mechanization. Selenium is one of the broadly utilized open source device for test computerization. Test automation enhances the effectiveness of programming testing procedures. Test automation gives quick criticism to engineers. It additionally discovers the imperfections when one may miss in the manual testing. In test automation we can perform boundless emphases for testing the same example of code ceaselessly commonly. © 2015 IEEE.},
author_keywords={aka.selenium;  RC;  selenium;  selenium control;  selenium grid;  selenium ide;  test case;  web driver},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mamrot2016701,
author={Mamrot, M. and Marchlewitz, S. and Nicklas, J.-P. and Winzer, P. and Tetzlaff, T. and Kemper, P. and Witkowski, U.},
title={Model-Based Test and Validation Support for Autonomous Mechatronic Systems},
journal={Proceedings - 2015 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2015},
year={2016},
pages={701-706},
doi={10.1109/SMC.2015.132},
art_number={7379264},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964450426&doi=10.1109%2fSMC.2015.132&partnerID=40&md5=1c48d81d7e4985c4b2c217bfb3adecb2},
affiliation={Product Safety and Quality Engineering, University of Wuppertal, Wuppertal, Germany; Electronics and Circuit Technology, University of Applied Sciences Soest, Soest, Germany},
abstract={Test and validation of autonomous mechatronic systems is a major challenge. Due to more complex tasks as well as dynamic environments, existing test and validation methods are reaching their limits. The complexity and diversity of their elements and interrelations of these as well as interrelations with environmental elements have to be handled because established methods do not consider the characteristics of autonomous mechatronic systems. Therefore Systems Engineering seems to be a proper solution. Based on system thinking an approach for analyzing autonomous mechatronic systems will be developed. For this purpose a procedure for model development will be aligned with the robot's system model. This system model combines hardware and software elements, analyzes their interrelations and prepares a later test and validation. With the help of this new system model, which decomposes to the hardware and software level, new test and validation methods can be developed. © 2015 IEEE.},
author_keywords={autonomous mechatronic systems;  product development;  system modeling;  systems engineering;  test and validation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fabien-Ouellet2016,
author={Fabien-Ouellet, G. and Gloaguen, E. and Giroux, B.},
title={Viscoelastic forward and adjoint modeling with OpenCL on heterogeneous clusters},
journal={78th EAGE Conference and Exhibition 2016: Efficient Use of Technology - Unlocking Potential},
year={2016},
doi={10.3997/2214-4609.201600565},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088063517&doi=10.3997%2f2214-4609.201600565&partnerID=40&md5=f6007526e1280ecd92e3821feb01e514},
affiliation={INRS-ETE, France},
abstract={Efficient seismic modeling is more and more needed because of the advent of full waveform inversion (FWI). For real case FWI, an efficient usage of the available computer resources is paramount. With the diversity of processor architectures found today, this is not a trivial task. In this study, we investigate the use of OpenCL to take advantage of large heterogeneous clusters in the context of FWI. The main objective is to present a scalable, multi-device code for the resolution of the viscoelastic wave equation that can compute the gradient of the objective function by the adjoint state method. We present several algorithmic aspects of our program in details, with an emphasis on its different levels of parallelism. The performance of the program is shown with several tests performed on large clusters with nodes containing three types of processors: Intel CPUs, NVidia GPUs and Intel Xeon PHI. We obtain a speed-up of more than 80 when using GPUs compared to a single threaded implementation and a linear scaling when computations are divided on separate nodes. Our results show that OpenCL allows a better usage of the computing resources available using a single source code for a multitude of devices.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Junsik2016,
author={Junsik, Y. and Jeongmin, H. and Beomseop, K. and Jeonghun, J.},
title={Code coverage measurement in automated hils with sw specification-based testcase},
journal={FISITA 2016 World Automotive Congress - Proceedings},
year={2016},
page_count={8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060464133&partnerID=40&md5=926837916f3be3605a8f3aebf718031a},
affiliation={Hyundai Autron Co., Ltd, South Korea},
abstract={As car electronization has been rapidly progressing, efficient and effective verifying methods are required to secure reliability of vehicular electronic control software. There are many difficulties in verifications – including diverse integrated functions of vehicle system – with code coverage techniques for structural verification of the existing code. Thus, by presenting code coverage calculation methods utilizing requirement specification-based testcases in HILS (Hardware In the Loop Simulation), the reliability of vehicular electronic control software will be secured with efficient and effective verification solutions. © 2016, FISITA. All rights reserved.},
author_keywords={Automation Test;  Code Coverage;  HILS;  Reliability of the verification;  SW Specifiaction-based test},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Proskiw2016197,
author={Proskiw, G. and Spewak, R. and Knight, K. and Carson, C.},
title={Commercial building airtightness testing—lessons learned from the red river college airtightness testing program},
journal={Thermal Performance of the Exterior Envelopes of Whole Buildings},
year={2016},
volume={2016-December},
pages={197-205},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053676509&partnerID=40&md5=2514250cda35da4970f41bff092e32b8},
affiliation={Proskiw Engineering Ltd., Winnipeg, MB, Canada; Building Envelope Technology Access Centre, Red River College, Winnipeg, MB, Canada},
abstract={In 2012, Red River College in Winnipeg, Canada, embarked on a program to expand the knowledge base on the airtightness characteristics of commercial buildings by conducting tests on a cross section of Manitoba’s commercial building stock. This provided quantitative data on the performance of these buildings and offered critical, practical experience with airtightness testing procedures for commercial buildings. Over three years, airtightness tests were carried out on a diverse sample of 26 commercial buildings ranging in age from 1 to over 100 years in age, floor areas of 150 to 19,788 m2 (1615 to 212,918 ft2), and building heights from 1 to 16 stories. Using the normalized leakage rate (NLR75) as the primary metric, the measured airtightness was found to vary by a factor of 18:1 between the tightest and loosest buildings in the sample (0.19 to 3.44 L/s·m2 [0.037 to 0.678 ft3/min]), with a mean of 1.70 L/s·m2 (0.335 ft3/min). The tightest building in the survey, a one-year-old middle school, achieved an airtightness of 0.19 L/ s·m2(0.037 ft3/min), better than almost all commercial building results reported in the literature. Overall, these results are particularly interesting given that current standards for new construction recommend (maximum) leakage rates of 1.27 to 2.03 L/ s·m2(0.250 to 0.400 ft3/min·ft2.). The study also revealed that then-current testing standards had serious weaknesses in their treatment of the building’s intentional openings (such as HVAC inlets and outlets). Rather than employing a single sealing schedule, separate envelope and energy schedules are required depending on whether envelope integrity and durability or energy performance is of primary interest. This insight was subsequently incorporated into ASTM WK35913, New Test Method for Whole Building Enclosure Air Tightness Compliance. © 2016 U.S. Government.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Whitsett2016,
author={Whitsett, A. and Holmedal, J. and Leonard, D.},
title={Maximizing efficiency in haynesville restimulations: A case study in improving lateral coverage to maximize incremental gas recovery},
journal={SPE/AAPG/SEG Unconventional Resources Technology Conference 2016},
year={2016},
doi={10.15530/urtec-2016-2443071},
art_number={2443071},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048575000&doi=10.15530%2furtec-2016-2443071&partnerID=40&md5=c891bb39a874ba820f7ae1b181939597},
affiliation={Halliburton, United States; Comstock Resources, United States; ProTechnics, Division of Core Laboratories, LP, United States},
abstract={The interest in restimulating unconventional wells in recent years has been driven by multiple factorsâ€”the desire to arrest the steep decline curves of unconventional wells, the depressed prices for both oil and natural gas, and the optimization efforts to fully drain reservoir rock that might have been inefficiently stimulated during original completions. Numerous publications have documented that a restimulation treatment can either add to the estimated ultimate recovery (EUR) of a well or accelerate the recovery rate of that wellâ€™s EUR. Efforts are now focused on making restimulation treatments more effective with more repeatable results. Early diagnostic methods have shown that in some horizontal restimulation treatments, the majority of the fluid and proppant pumped are delivered only into the first 1,000 to 2,000 ft of the lateral. These diagnostic methods include microseismic monitoring, radioactive proppant tracer, and production logging. In June 2015, one Haynesville shale well was restimulated and evaluated using radioactive proppant tracer to provide insight into the stimulation coverage of the lateral. The restimulation design for the case history well focused on treating the entire lateral with the use of a biodegradable particulate diverter. This was accomplished by increasing the amount of material per diversion cycle and by increasing the number of cycles in the restimulation treatment. Additional consideration was given to delivering an optimized proppant amount per lateral foot, ideal cluster spacing, treatment fluid selection, and appropriate treatment rate. The case history shows that 63 of the 70 clusters identified with the tracer log were stimulated during the restimulation treatment. Additionally, the clusters showing proppant were located throughout the entirety of the lateral. The size and composition of the restimulation design is analyzed with respect to treatment performance, and possible improvements are considered. The economic viability of a restimulation program should be weighed against the entire cost of a restimulation treatment, which includes wellbore preparation, wellbore integrity testing, cost of restimulation services, and other miscellaneous logistical challenges encountered before the treatment begins. Copyright 2016, Unconventional Resources Technology Conference (URTeC).},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2016,
author={Zhang, W. and Higham, N.J.},
title={Matrix Depot: An extensible test matrix collection for Julia},
journal={PeerJ Computer Science},
year={2016},
volume={2016},
number={4},
doi={10.7717/peerj-cs.58},
art_number={e58},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025704567&doi=10.7717%2fpeerj-cs.58&partnerID=40&md5=6bd9b333d02f03d0603cbde60c43c619},
affiliation={School of Mathematics, University of Manchester, Manchester, United Kingdom},
abstract={Matrix Depot is a Julia software package that provides easy access to a large and diverse collection of test matrices. Its novelty is threefold. First, it is extensible by the user, and so can be adapted to include the user's own test problems. In doing so, it facilitates experimentation and makes it easier to carry out reproducible research. Second, it amalgamates in a single framework two different types of existing matrix collections, comprising parametrized test matrices (including Hansen's set of regularization test problems and Higham's Test Matrix Toolbox) and real-life sparse matrix data (giving access to the University of Florida sparse matrix collection). Third, it fully exploits the Julia language. It uses multiple dispatch to help provide a simple interface and, in particular, to allow matrices to be generated in any of the numeric data types supported by the language. © 2016 Zhang and Higham.},
author_keywords={Julia;  Matrix algorithm;  Software package;  Test matrices;  Test problems},
document_type={Article},
source={Scopus},
}

@CONFERENCE{SalimAzzouz2016,
author={Salim Azzouz, M. and Brink, J.},
title={Twists and turns of a senior design project},
journal={ASME International Mechanical Engineering Congress and Exposition, Proceedings (IMECE)},
year={2016},
volume={5},
doi={10.1115/IMECE201666194},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021686074&doi=10.1115%2fIMECE201666194&partnerID=40&md5=efe5359af1b12c5c29c2d38766adf6f0},
affiliation={Midwestern State University, Wichita Falls, TX, United States},
abstract={Teaching senior design courses and labs has not been an easy task for the two authors. It has been rather a daunting working task associated with great learning experiences. It was decided early on from the initiation of the mechanical engineering program at the McCoy School of Engineering at Midwestern State University that the senior design project within the senior design class is a testing and enriching experience for senior mechanical engineering students as well as the teaching faculty. The senior design course and labs are conducted as a research experience for undergraduate students and their assigned faculty. The proposed senior project spans over two semesters, fall and spring, where the students experience a full mechanical engineering related project from the inception phase, through the design and construction phases, and finishing with the testing and analysis phases. The inception phase stands essentially for the brainstorming phase where the students are required to come-up with a set of diverse solutions to their assigned project problem. The design and construction phases stand for choosing an optimal particular solution for their problem according to a set of defined criteria. Then, the students start the preliminary design phase with related cost estimation, and then finalize the design with a set of final drawings. After the design phase, the students start building a machine, an apparatus, a prototype or putting together the elements of a process. In this period they work intensely, with their faculty, the purchasing department, and mostly the department machinist, or the surrounding town machine shops. The testing and analysis phase stands for designing an experimental set-up, writing a testing procedure, and obtaining real time recorded data and proceeding with its analysis. In this technical paper, the authors talk about the requirements for a senior project known as the deliverables, the teaching tools used throughout the class work and labs, the students' partial and final PowerPoints presentations and weekly and final reports. The authors describe the students overall achievements, and the archiving of the projects. Additionally, the authors talk about the twists and turns encountered during a senior project, with students, other faculty, the machinist, the lab technician, the secretary, and suppliers, and other difficulties experienced in running a full project with real final products. Finally, the authors talk about the aftermath of a senior project, eventual publications related to the project, and what is the view point of the American Board of Engineering and Technology (ABET) on these senior projects. Copyright © 2016 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Grant201635,
author={Grant, E. and Salmon, P.M. and Stevens, N.J. and Goode, N. and Read, G.J.},
title={Exposing resistant problems in complex systems: A review of accident causation tenets},
journal={51st Annual Conference of the Human Factors and Ergonomics Society of Australia 2016: Healthy, Safe and Productive By Design},
year={2016},
pages={35-49},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018425920&partnerID=40&md5=41fa56ac2a9b20ccb422c0cb0fe44ba7},
affiliation={Centre for Human Factors and Sociotechnical Systems, University of the Sunshine Coast, Maroochydore, QLD, Australia},
abstract={The plateau of incident rates in many domains, including road and rail transport, aviation and workplaces indicates that system-based ergonomics models used to understand accident causation are either underutilised or underperforming in preventing adverse events. Whether this plateau is due to an increase in system complexity or out-of-date toolkits, the problems are seemingly resistant to the current ergonomics methods that pursue them. With this knowledge it is pertinent to ask what changes, if any, are required to capture contemporary system complexity and uncover resistant problems. Are current methods being used to their full advantage or are new methods required to progress safety science? The paper presents a review of the dominant contemporary safety science methods and underpinning models that could potentially provide a solution to uncovering resistant problems in complex systems. Accident causation theorists with the greatest number of citations were identified from the safety science literature. The citation information was derived from Scopus (April 2016). These included, Nancy Leveson (3950 citations Scopus, April 2016) Jens Rasmussen (3486 citations Scopus, April 2016), Charles Perrow (2041 citations Scopus, April 2016), Sidney Dekker (789 citations Scopus, April 2016) and Erik Hollnagel (672 citations Scopus, April 2016). Over ninety published works were coded into categories of safe and unsafe system behaviours. The outcome of the review produced a list of principle tenets extracted from the literature. The review has shown that despite the diversity in approaches there is considerable agreement about the core tenets of system safety and accident causation. Using this information a research program is proposed to test the quality of the tenets extracted from the literature review and the extent to which they can be used to support a proactive approach to safety in complex systems. This will provide a test of sociotechnical systems theory when used to predict accidents.},
author_keywords={DRIFT;  FRAM;  Normal accident theory;  Rasmussen's risk management framework;  STAMP},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Haiguang2016,
author={Haiguang, L. and Keqin, L.},
title={Study on life cycle quality management of the aerospace advanced development flight demonstration project},
journal={Proceedings of the International Astronautical Congress, IAC},
year={2016},
volume={0},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016505945&partnerID=40&md5=19bb5b2dfc8abd782ed532a0aaccb6d0},
affiliation={R and D Center, China Academy of Launch Vehicle Technology, No. 1 Nan Dahongmen Road, Feng Tai District, Beijing, 100076, China},
abstract={The development of aerospace pre research flight demonstration project is conforms to the current trend of space equipment, and it provides technical reserves for the future development of space equipment. The development process of aerospace pre research project is similar to the traditional space project. But the aerospace pre research project has its own characteristics of tight schedule, less fund, great technical uncertainty and diversity suppliers. In order to achieve the flight demonstration test of aerospace pre research project fast, the quality management in the whole life cycle of the project needs to explore an innovation road which is suitable for the project features and guides the quality management work of high efficiency. The development process of aerospace pre research project is divided into three stages: concept design, prototype and flight demonstration. At the beginning of the project, tailor-made quality management regulations are defined. The "Quality Assurance Program" is established, and each development stage has focused on the quality work of his own, for instance, the concept design stage pays close attention to the correctness and feasibility of the principle, the prototype stage is focused on the ground test results and the realization of the design and production, the flight demonstration stage focus on the impact of changes in the state of technology, and concerns about whether the reliability and manufacturing process of products meet the requirements. During the developing course, we work in an innovative manner. A number of new fine quality management methods are widely used, such as "quality tips everyday", "quality regulations release to the final suppliers", "experts' comments Itemization", "Quality supervisor Tracking production and test", "field investigation and scientific supplier management". We, meanwhile, make full use of the information and network technology to make the quality management becomes more efficient. Through developing and implicating of the electronic network quality board, the quality consciousness of whole staff is improved. By establishing and operating the quality information management and assistant decision system, the leaders of the project can grasp the dynamic quality information quickly, and the assistance decisions are also provided to them. Through the implementation of the whole life cycle quality control methods, the flight test of the aerospace pre research project achieves a complete success. It also provides a useful reference for the quality management of future aerospace pre research project. Copyright © 2016 by the International Astronautical Federation (IAF). All rights reserved.},
author_keywords={Advanced development;  Flight demonstration project;  Life cycle;  Quality management},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lin20168.1,
author={Lin, W.},
title={A study on the management of online teaching resources for track and field courses in colleges of physical education},
journal={International Journal of Simulation: Systems, Science and Technology},
year={2016},
volume={17},
number={48},
pages={8.1-8.10},
doi={10.5013/IJSSST.a.17.48.08},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010791614&doi=10.5013%2fIJSSST.a.17.48.08&partnerID=40&md5=d9669db12beb0fc65a2ce11664ae15e3},
affiliation={College of physical education and sports science, Guangzhou University, Guangzhou, China},
abstract={The purpose of this study is to improve the management model and its application to track and field (T&F) web course teaching activity resources. The research methods included literature, expert interview, questionnaire survey, mathematical statistics, system settings, software editing etc. Results show: i) user Information includes students’ and teachers’ basic information and authorized management system; ii) My Resources has the features of synchronization and updating of teaching activity resources; iii) Course Property Resources include course introduction, teaching tasks and achievements of teachers, development and trends of courses; iv) Teaching Task collects, classifies and edits the resources such as assignments, tests and forums, which meet the management demand of diverse teaching task arrangements and the evaluation system. Our conclusion: the resource modules have characteristics, and they are interactive between each other and, complementary with each other, which is conducive to the integration and organization of T&F teaching activity resources. © 2016, UK Simulation Society. All rights reserved.},
author_keywords={College;  Management;  Physical education;  Teaching resource;  Track and field;  Web course},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tanci2016,
author={Tanci, C. and Tosti, G. and Antolini, E. and Gambini, G.F. and Bruno, P. and Canestrari, R. and Conforti, V. and Lombardi, S. and Russo, F. and Sangiorgi, P. and Scuderi, S.},
title={Software design and code generation for the engineering graphical user interface of the ASTRI SST-2M prototype for the Cherenkov Telescope Array},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9913},
doi={10.1117/12.2232005},
art_number={99133X},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006482322&doi=10.1117%2f12.2232005&partnerID=40&md5=c81f714fe2ee491ea8b30b9970718226},
affiliation={Università di Perugia, Dipartimento di Fisica e Geologia, v. Pascoli, Perugia, 06123, Italy; INAF, Osservatorio Astronomico di Brera, v. Bianchi, 46, Merate (Lc), 23807, Italy; INAF, Osservatorio Astrofisico di Catania, v. Sofia 78, Catania, 95123, Italy; INAF, Istituto di Astrofisica Spaziale e Fisica Cosmica di Bologna, v. Gobetti 101, Bologna, 40129, Italy; INAF, Osservatorio Astronomico di Roma, v. Di Frascati 33, Monteporzio Catone, Roma, 00040, Italy; INAF, Osservatorio Astrofisico di Torino, via Osservatorio 20, Pino Torinese (TO), 10025, Italy; INAF, Istituto di Astrofisica Spaziale e Fisica Cosmica di Palermo, v. La Malfa 153, Palermo, 90146, Italy},
abstract={ASTRI is an on-going project developed in the framework of the Cherenkov Telescope Array (CTA). An end-to-end prototype of a dual-mirror small-size telescope (SST-2M) has been installed at the INAF observing station on Mt. Etna, Italy. The next step is the development of the ASTRI mini-array composed of nine ASTRI SST-2M telescopes proposed to be installed at the CTA southern site. The ASTRI mini-array is a collaborative and international effort carried on by Italy, Brazil and South-Africa and led by the Italian National Institute of Astrophysics, INAF. To control the ASTRI telescopes, a specific ASTRI Mini-Array Software System (MASS) was designed using a scalable and distributed architecture to monitor all the hardware devices for the telescopes. Using code generation we built automatically from the ASTRI Interface Control Documents a set of communication libraries and extensive Graphical User Interfaces that provide full access to the capabilities offered by the telescope hardware subsystems for testing and maintenance. Leveraging these generated libraries and components we then implemented a human designed, integrated, Engineering GUI for MASS to perform the verification of the whole prototype and test shared services such as the alarms, configurations, control systems, and scientific on-line outcomes. In our experience the use of code generation dramatically reduced the amount of effort in development, integration and testing of the more basic software components and resulted in a fast software release life cycle. This approach could be valuable for the whole CTA project, characterized by a large diversity of hardware components. © COPYRIGHT SPIE. Downloading of the abstract is permitted for personal use only.},
author_keywords={ASTRI;  Code generation;  CTA;  GUI;  Imaging atmospheric cherenkov telescope;  Telescope system software},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mossige20164200,
author={Mossige, M. and Gotlieb, A. and Meling, H.},
title={Generating tests for robotized painting using constraint programming},
journal={IJCAI International Joint Conference on Artificial Intelligence},
year={2016},
volume={2016-January},
pages={4200-4204},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006160484&partnerID=40&md5=4b15ffdff416fed0e44377ab7b6e63d0},
affiliation={ABB Robotics, Switzerland; Simula Research Laboratory, Norway; University of Stavanger, Norway},
abstract={Designing industrial robot systems for welding, painting, and assembly, is challenging because they must perform with high precision, speed, and endurance. ABB Robotics has specialized in building highly reliable and safe robotized paint systems using an integrated process control system. However, current validation practices are mainly limited to manual test scenarios, which makes it difficult to exercise important aspects of a paint robot system, such as the need to coordinate the timing of paint activation with the robot motion control. To address these challenges, we have developed and deployed a cost-effective, automated test generation technique aimed at validating the timing behavior of the process control system. The approach is based on a constraint optimization model written in Prolog. This model has been integrated into an automated continuous integration environment, allowing the model to be solved on demand prior to test execution, which allows us to obtain the most optimal and diverse set of test scenarios for the current system configuration.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stergiopoulos201628,
author={Stergiopoulos, G. and Katsaros, P. and Gritzalis, D. and Apostolopoulos, T.},
title={Combining invariant violation with execution path classification for detecting multiple types of logical errors and race conditions},
journal={ICETE 2016 - Proceedings of the 13th International Joint Conference on e-Business and Telecommunications},
year={2016},
volume={4},
pages={28-40},
doi={10.5220/0005947200280040},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85003848807&doi=10.5220%2f0005947200280040&partnerID=40&md5=d5398fe8d3668a4e8aed6a5cbf985b8e},
affiliation={Information Security and Critical Infrastructure Protection Laboratory, Dept. of Informatics, Athens University of Economics and Business, 76 Patission Ave., Athens, GR-10434, Greece; Dept. of Informatics, Aristotle University of Thessaloniki, Thessaloniki, Greece},
abstract={Context: Modern automated source code analysis techniques can be very successful in detecting a priori defined defect patterns and security vulnerabilities. Yet, they cannot detect flaws that manifest due to erroneous translation of the software's functional requirements into the source code. The automated detection of logical errors that are attributed to a faulty implementation of applications' functionality, is a relatively uncharted territory. In previous research, we proposed a combination of automated analyses for logical error detection. In this paper, we develop a novel business-logic oriented method able to filter mathematical depictions of software logic in order to augment logical error detection, eliminate previous limitations in analysis and provide a formal tested logical error detection classification without subjective discrepancies. As a proof of concept, our method has been implemented in a prototype tool called PLATO that can detect various types of logical errors. Potential logical errors are thus detected that are ranked using a fuzzy logic system with two scales characterizing their impact: (i) a Severity scale, based on the execution paths' characteristics and Information Gain, (ii) a Reliability scale, based on the measured program's Computational Density. The method's effectiveness is shown using diverse experiments. Albeit not without restrictions, the proposed automated analysis seems able to detect a wide variety of logical errors, while at the same time limiting the false positives. Copyright © 2016 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved.},
author_keywords={Analysis;  Assertions;  Automatic;  Code classification;  Dynamic invariants;  Execution path;  Exploit;  Fuzzy logic;  Information gain;  Logical errors;  Source code;  Vulnerability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu201679,
author={Liu, X. and Chen, Y.},
title={Research on urban traffic optimal path planning method based on improved genetic algorithm},
journal={International Journal of Smart Home},
year={2016},
volume={10},
number={10},
pages={79-86},
doi={10.14257/ijsh.2016.10.10.08},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002551066&doi=10.14257%2fijsh.2016.10.10.08&partnerID=40&md5=b8a37b4e58b20312657891fc8487dcee},
affiliation={School of Urban Design, Wuhan University, Wuhan, China},
abstract={The traditional genetic algorithm randomly selects nodes in two chromosomes for crossover operation, which may result in individuals of disconnected or loop circuit and lead to issues as meaningless crossover operations. In order to increase the diversity of the population and prevent the occurrence of premature mutation algorithm which might cause local convergence, this essay presents a new urban traffic optimal path planning method. Initialized from the improvement of population genetic algorithm, it designs the fitness function and optimizes crossover and mutation operators so that the optimal or near-optimal solution can be quickly figured out. Moreover, the Matlab software simulation test exhibits the feasibility and effectiveness of the method. © 2016 SERSC.},
author_keywords={Genetic algorithm;  Matlab simulation;  Path planning;  Urban traffic},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Witzel2016,
author={Witzel, G. and Lu, J.R. and Ghez, A.M. and Martinez, G.D. and Fitzgerald, M.P. and Britton, M. and Sitarski, B.N. and Do, T. and Campbell, R.D. and Service, M. and Matthews, K. and Morris, M.R. and Becklin, E.E. and Wizinowich, P.L. and Ragland, S. and Doppmann, G. and Neyman, C. and Lyke, J. and Kassis, M. and Rizzi, L. and Lilley, S. and Rampy, R.},
title={The AIROPA software package: Milestones for testing general relativity in the strong gravity regime with AO},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2016},
volume={9909},
doi={10.1117/12.2233872},
art_number={99091O},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002156837&doi=10.1117%2f12.2233872&partnerID=40&md5=570de468ec0eab33035c94e703895e80},
affiliation={Department of Physics and Astronomy, University of California Los Angeles, 430 Portola Plaza, Los Angeles, CA  90095-1547, United States; Institute for Astronomy, University of Hawaii, 2680 Woodlawn Drive, Honolulu, HI  96822, United States; W. M. Keck Observatory, 65-1120 Mamalahoa Hwy., Kamuela, HI  96743, United States; Division of Physics, Mathematics, and Astronomy, California Institute of Technology, Pasadena, CA  91125, United States; Aerospace Corporation, El Segundo, CA  90245, United States},
abstract={General relativity can be tested in the strong gravity regime by monitoring stars orbiting the supermassive black hole at the Galactic Center with adaptive optics. However, the limiting source of uncertainty is the spatial PSF variability due to atmospheric anisoplanatism and instrumental aberrations. The Galactic Center Group at UCLA has completed a project developing algorithms to predict PSF variability for Keck AO images. We have created a new software package (AIROPA), based on modified versions of StarFinder and Arroyo, that takes atmospheric turbulence profiles, instrumental aberration maps, and images as inputs and delivers improved photometry and astrometry on crowded fields. This software package will be made publicly available soon. © 2016 SPIE.},
author_keywords={Astrometry;  Atmospheric Anisoplanatism;  Galactic Center;  General Relativity;  Keck Observatory;  Near-infrared;  NIRC2;  Phase Diversity;  Photometry;  PSF-fitting;  PSF-R},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Steininger201622,
author={Steininger, D. and Zweng, A. and Beleznai, C. and Netousek, T.},
title={Development and evaluation of a text recognition framework using synthetic data},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1711},
pages={22-27},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996844896&partnerID=40&md5=d1b8d31a4d1b92637e8fadbe374f57e5},
affiliation={AIT Austrian Institute of Technology GmbH, Vienna, Austria; EMedia Monitor GmbH, Vienna, Austria},
abstract={Text recognition is an intricate Computer Vision task. The main complexity arises from the fact that text as a character sequence spans a very large space of possible appearances, induced by combinatorially vast character orderings, diverse font styles, weights, colors and backgrounds. In order to encode a rich representation of variations and to generate an informative model by statistical learning, image data balanced along all dimensions of variations are needed. In this paper we present a synthetic text pattern generation framework and its use for localizing text lines and recognizing characters in individual frames of broadcast videos. The paper demonstrates that detection and recognition accuracies can be significantly enhanced by employing synthetic text image patches for training an Aggregated Channel Features (ACF) detector and a Convolutional Neural Network (CNN) character recognizer for the text recognition task. Moreover, an efficient annotation tool is presented for ground truth data generation from videos, enabling evaluation experiments on large-scale (several thousands of frames) video datasets. A quantitative evaluation of the detection functionality and qualitative experiments for character recognition are presented, exhibiting promising results on challenging (low-resolution, compression artifacts) realworld test data.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Blaschitz20162,
author={Blaschitz, B. and Soukup, D. and Penz, H. and Krattenthaler, W. and Huber-M'örk, R.},
title={Testing a banknote checking system},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1711},
pages={2-7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996619307&partnerID=40&md5=f134c75fcaf8bc5dd61da01abf72ee81},
affiliation={Digital Safety and Security Department, AIT Austrian Institute of Technology GmbH, Donau-City-Straße 1, Vienna, 1220, Austria},
abstract={We present our work in progress in the direction of generating realistic, challenging and diverse test data in order to evaluate a banknote checking system. Test image generation can be divided into, firstly, the design and composition of highly diverse patterns to form the digital design of a banknote, secondly, the application of different variations which are caused by ink, paper and physically printing the note and, thirdly, the simulation of the subsequent image acquisition, which includes variations due to banknote transport, illumination, camera optics and electronics. These simulations are based on and compared to scans of demo banknotes. We present initial results in simulation of banknote paper, printing and image acquisition. We demonstrate the approach in an initial investigation on synthetic defect generation and its impact on banknote checking results. Algorithmic validation of checking routines is also demonstrated. First conclusions are drawn and further work is discussed.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yilmaz201671,
author={Yilmaz, U. and Moralioǧlu, Ö.F.},
title={Gomulu Yazilim Testinde Farkli bir Yaklasim: ScalaTest ile Test Otomasyon Arac},
journal={CEUR Workshop Proceedings},
year={2016},
volume={1721},
pages={71-79},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996549590&partnerID=40&md5=02bcafb7417e3f0bb2fa992bb5a91c40},
affiliation={Radar Elektronik Harp ve stihbarat Sistemleri (REHI:S) Grubu, ASELSAN A.S. Ankara, Turkey},
abstract={Embedded software does not usually provide user interface and does interact with systems through high speed interfaces. In addition, the embedded software developed in REWIS (Radar Electronics Warfare and Intelligence Systems) requires various and highly-dense input data for diverse algorithms. It is obligatory that the tests of the software having above features should cover all algorithms and interfaces. Moreover the tests should be completed and reported in a reasonable amount of time. To achieve these goals, automation is a commonly applied and efficient method. This paper presents a ScalaTest based Test Automation Tool that can be adapted to different embedded software from various domains, can easily parse test steps through a human-readable DSL that is provided by ScalaTest, can control the simulated interfaces via an API and can report the test results using same DSL. As a verification, the proceeds of tests from two embedded software of different domains is presented as a case study.},
author_keywords={DSL;  Embedded software test;  Scalatest library;  Test automation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gronau20161,
author={Gronau, N.},
title={Determinants of an Appropriate Degree of Autonomy in a Cyber-physical Production System},
journal={Procedia CIRP},
year={2016},
volume={52},
pages={1-5},
doi={10.1016/j.procir.2016.07.063},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992663760&doi=10.1016%2fj.procir.2016.07.063&partnerID=40&md5=0c204ae98535f229e8df14014dfaf274},
affiliation={University of Potsdam, August-Bebel-Str. 89, Potsdam, 14482, Germany},
abstract={Classical productions systems are migrating step-by-step into cyber-physical production systems. The addition of much more computing power and object-bound data storage will lead to new possibilities for the advancement of autonomy in production systems. Autonomous message exchange and coordination can help to prevent quality problems (for instance wrong pairing of tool and work piece) and improve the disturbance management (for instance by faster information about current and probable disturbances). Due to the fact that nearly all improvements of existing production systems with cyber-physical systems take place in real and active manufacturing sites, on-site experiments for determining an appropriate degree of autonomy for production objects are not feasible. Therefore, a lab approach is necessary. In this contribution a hybrid lab approach to simulate various degrees of autonomy is presented [1]. The paper starts with a definition of autonomy and suggests diverse measurement methods [2]. After a short introduction into the lab concept, the results of some test runs are presented where autonomous objects perform the same production program as "dumb" production objects. Finally, an outlook for further research is given. © 2016 The Authors.},
author_keywords={Autonomy;  Cyber-Physical Systems;  Hybrid Lab Approach;  Production Process Characteristics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shenglong20166359,
author={Shenglong, T. and Lan, Q. and Qingwen, L.},
title={Adopting the in-situ test and numerical simulation to the design of underwater rock plug blasting},
journal={Electronic Journal of Geotechnical Engineering},
year={2016},
volume={21},
number={19},
pages={6359-6370},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992523458&partnerID=40&md5=7507e3fa25591aa399a274823ee7bcf6},
affiliation={Department of Civil Engineering, University of Science and Technology Beijing, Beijing, 100083, China},
abstract={Underwater rock plug blasting has been recognized as the best method for the water entrance method of diversion tunnel construction under deep water, but it is due to the complicated factors and high technical requirements. Particularly, the two free faces could lead to the different mechanical environment, which is the biggest difference from the normal underwater blasting. In this paper, an entrance of diversion tunnel construction in China was chosen as the case, after in-situ similar test and the theoretical analysis, a 3D was built in software LS-DYNA, with accepting the blasting parameter, physical and mechanics parameters, damage model and air state equation, to simulate rock plug blasting with the single free face and two free faces under 45m depth water. Then, the whole cutting surface simulation also carried out to make guidance for the field engineering. © 2016 ejge.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Makedonski2016116,
author={Makedonski, P. and Adamis, G. and Käärik, M. and Kristoffersen, F. and Zeitoun, X.},
title={Evolving the ETSI test description language},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9959 LNCS},
pages={116-131},
doi={10.1007/978-3-319-46613-2_8},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990050274&doi=10.1007%2f978-3-319-46613-2_8&partnerID=40&md5=7634c02ef205d701f029e02f59b4213d},
affiliation={Institute of Computer Science, University of Göttingen, Göttingen, Germany; Test Competence Center, Ericsson Hungary Ltd., Budapest, Hungary; Elvior OU, Talinn, Estonia; Cinderella ApS, Copenhagen, Denmark; CEA, LIST, Gif-sur-yvette, France},
abstract={Increasing software and system complexity due to the integration of more and more diverse sub-systems presents new testing challenges. Standardisation and certification requirements in certain domains such as telecommunication, automotive, aerospace, and health-care contribute further challenges for testing systems operating in these domains. Consequently, there is a need for suitable methodologies, processes, languages, and tools to address these testing challenges. To address some of these challenges, the Test Description Language (TDL) has been developed at the European Telecommunications Standards Institute (ETSI) over the past three years. TDL bridges the gap between declarative test purposes and imperative test cases by offering a standardised language for the specification of test descriptions. TDL started as a standardised meta-model, subsequently enriched with a graphical syntax, exchange format, and a UML profile. A reference implementation of TDL has been developed as a common platform to accelerate the adoption of TDL and lower the barrier to entry for both end-users and tool-vendors. This article tells the story of the evolution of TDL from its conception. © Springer International Publishing AG 2016.},
author_keywords={Domain specific modeling;  Model-based testing;  Test description language},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ragab2016315,
author={Ragab, Kh.A. and Bouazara, M. and Bouaicha, A. and Mrad, H.},
title={Materials performance and design analysis of suspension lower-arm fabricated from Al-Si-Mg castings},
journal={Key Engineering Materials},
year={2016},
volume={710},
pages={315-320},
doi={10.4028/www.scientific.net/KEM.710.315},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989865346&doi=10.4028%2fwww.scientific.net%2fKEM.710.315&partnerID=40&md5=c0c3030493f2df98b3e871bce45563c1},
affiliation={Applied Sciences, University of Quebec at Chicoutimi, Saguenay, QC, Canada; Metallurgical Engineering, Cairo University, Egypt; Mechanical Engineering, University of Quebec at Abitibi, Canada},
abstract={The diversity of physical and mechanical properties of aluminum alloys leads to develop a variety of manufacturing processes including the semi-solid casting process. Fatigue failure is considered the most common problem occurred in automotive engineering applications by which the vehicle components, mainly suspension system parts, fail under conditions of dynamic loading. It is well known that the fatigue life of aluminum castings, mainly A357, is very sensitive to casting design as well as to casting defects and microstructure constituents. The fatigue characteristics of automotive lower suspension arm made of semi-solid A357 aluminum castings have been investigated using metallurgical and analytical approaches. The critical stress areas capable of initiating cracks during fatigue tests are detected by using fatigue experimental design for real part materials by the installation of strain gages on the suspension arm to calculate maximum stress; further more, analytical approach is applied using modelling software. Microstructure characteristics of the semisolid A357 under T6 heat treatment conditions are examined using scanning electron microscope. The results show that using the SEED casting technology (Swirled Enthalpy Equilibration Device) has an efficient effect on the mechanical and metallurgical characteristics of real part materials that are also affected by castings design. © 2016 Trans Tech Publications, Switzerland.},
author_keywords={Aluminum;  Automotive;  Mechanical;  Microstructure;  Semisolid casting},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sybilska201680,
author={Sybilska, W. and Korycki, R.},
title={Analysis of thermal-insulating parameters in two- and three-layer textiles with semi-permeable membranes},
journal={Fibres and Textiles in Eastern Europe},
year={2016},
volume={24},
number={5},
pages={80-87},
doi={10.5604/12303666.1215532},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989336229&doi=10.5604%2f12303666.1215532&partnerID=40&md5=b4ee2663a5a60c59cab0092580da6159},
affiliation={Institute of Architecture of Textiles, Poland; Department of Technical Mechanics and Informatics, Poland; Lodz University of Technology, ul. Żeromskiego 116, Łódź, 90-924, Poland},
abstract={The main goal of paper presented was to evaluate clothing products (including products coated by semi-permeable membranes) in respect of thermal comfort. To achieve this, the thermal-insulating parameters were measured using an Alambeta measuring instrument. The research program introduced a set of diversified materials of different raw material composition, membranes applied, weave, density etc. The test results were analysed in respect of the thermal-insulating parameters of multilayer products, particularly the membrane effect as a separating and protective layer. © 2016, Institute of Biopolymers and Chemical Fibres. All rights reserved.},
author_keywords={Membrane products;  Thermal-insulating parameters},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Tao2016480,
author={Tao, C. and Gao, J.},
title={On building test automation system for mobile applications using GUI ripping},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2016},
volume={2016-January},
pages={480-485},
doi={10.18293/SEKE2016-168},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988358022&doi=10.18293%2fSEKE2016-168&partnerID=40&md5=00aed5e24fdf96cee6de18b3ede0dc8f},
affiliation={Nanjing University of Science and Technology, Nanjing, Jiangsu, China; San Jose State University, San Jose, CA, United States; Taiyuan University of Technology, Taiyuan, China},
abstract={With the rapid advance of mobile computing technology and wireless networking, there is a significant increase of mobile subscriptions. This brings new business requirements and demands in mobile software testing, and causes new issues and challenges in mobile testing and automation. Current existing mobile application testing tools mostly concentrate on GUI, load and performance testing which seldom consider large-scale concurrent automation, coverage analysis, fault tolerance and usage of well-defined models. This paper introduces an implemented system that provides an automation solution across platforms on diverse devices using GUI ripping test scripting technique. Through incorporating open source technologies such as Appium and Selenium Grid, this paper addresses the scalable test automation control with the capability of fault tolerant. Additionally, maximum test coverage can also be obtained by executing parallel test scripts within the model. Finally, the paper reports case studies to indicate the feasibility and effectiveness of the proposed approach.},
author_keywords={GUI ripping;  Large-scale concurrent testing;  Mobile application testing;  Test automation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Solanki2016,
author={Solanki, K. and Singh, Y.V. and Dalal, S.},
title={Experimental analysis of m-ACO technique for regression testing},
journal={Indian Journal of Science and Technology},
year={2016},
volume={9},
number={30},
doi={10.17485/ijst/2016/v9i30/86588},
art_number={86588},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984656915&doi=10.17485%2fijst%2f2016%2fv9i30%2f86588&partnerID=40&md5=7f6f4ddcc60d22a5d2054907fcae41d3},
affiliation={M. D. University, Rohtak, Haryana, 124001, India},
abstract={Objectives: Experimental evaluation of "m-ACO" (Modified Ant Colony Optimization) technique for test case prioritization has been performed on two well known software testing problems namely "Triangle Classification Problem" and "Quadratic Equation Problem". Apart from these two problems, m-ACO has been experimentally evaluated using open source software JFreeChart. Methods: m-ACO finds the optimized solution to test suite prioritization by modifying the phenomenon used by natural ants to reach to its food source and select the food. This paper attempts to experimentally and comparatively evaluate the proposed m-ACO technique for test case prioritization against some contemporary meta-heuristic techniques using two well known software testing problems and open source problem. Performance evaluation has been measured using two metrics namely APFD (Average Percentage of Faults Detected) and PTR (Percentage of Test Suite Required for Complete Fault Coverage). Findings: The proposed technique m-ACO proves its efficiency on both the parameters. m-ACO achieves higher fault detection rate with minimized test suite as comparative to other meta-heuristic techniques for test case prioritization. Improvements: The proposed technique m-ACO basically works by modifying the food source searching and selection pattern of the real ants. Real ants grab every type food source it comes across; while modified ants evaluate the food fitness and uniqueness before selection. This phenomenon enhances the quality and diversity of deposited food source.},
author_keywords={Fault coverage;  Genetic algorithm;  Regression testing;  Software testing;  Test suite prioritization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Solanki2016,
author={Solanki, K. and Singh, Y.V. and Dalal, S.},
title={A comparative evaluation of "m-ACO" technique for test suite prioritization},
journal={Indian Journal of Science and Technology},
year={2016},
volume={9},
number={30},
doi={10.17485/ijst/2016/v9i30/86423},
art_number={86423},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984605114&doi=10.17485%2fijst%2f2016%2fv9i30%2f86423&partnerID=40&md5=1eff2a669c396ad3feb5eb5f20f57fee},
affiliation={M. D. University, Rohtak, Haryana, 124001, India},
abstract={Objectives: The novel test case prioritization technique "m-ACO" ("Modified Ant Colony Optimization") for regression testing has been comparatively evaluated. Methods: "m-ACO" prioritize the test cases by altering the food source selection criteria of natural ants to enhance fault diversity. The code for the proposed technique for prioritizing test case "m-ACO" has been implemented in Perl language. This paper makes a comparative evaluation of proposed "m-ACO" technique for prioritization of test cases with GA ("Genetic Algorithm"), BCO ("Bee Colony Optimization") Algorithms and ACO ("Ant Colony Optimization") Algorithms using three case studies. Two metrics namely APFD ("Average Percentage of Faults Detected") and PTR ("Percentage of Test Suite Required for Complete Fault Coverage") have been used to measure the effectiveness of the proposed "m-ACO" technique. Findings: The proposed technique "m-ACO" produced optimal or near optimal solutions. The proposed "m-ACO" technique proves its efficiency in comparison to GA, BCO and ACO methods individually. Improvements: The proposed technique improves the ACO method by altering food source selection criteria of natural ants. The future work in this direction will comparatively evaluate the proposed "m-ACO" technique using some well known software testing problems and open source software. An automated tool for the proposed technique is being developed.},
author_keywords={Fault coverage;  Genetic algorithm;  Regression testing;  Software testing;  Test suite prioritization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ribeiro2016523,
author={Ribeiro, A.N. and Araújo, C.R.},
title={An automated model based approach to mobile UI specification and development},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2016},
volume={9731},
pages={523-534},
doi={10.1007/978-3-319-39510-4_48},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978871073&doi=10.1007%2f978-3-319-39510-4_48&partnerID=40&md5=2c70c83b4b325e4601754ccfc506aa2e},
affiliation={Departamento de Informática, Universidade Do Minho and HASLab, INESC TEC, Braga, Portugal},
abstract={One of the problems of current software development lies on the existence of solutions to address properly the code portability for the increasing number of platforms. To build abstract models is one efficient and correct way to achieve this. The Model-Driven Software Engineering (MDSE) is a development methodology where models are the key for all project lifecycle, from requisites gathering, through modelling and to the development stage, as well as on testing. Pervasive computing demands the use of several technical specifications, such as wireless connections, advanced electronics, and the Internet, as well as it stresses the need to adjust the user interface layer to each one of the platforms. Using a model-driven approach it is possible to reuse software solutions between different targets, since models are not affected by the device diversity and its evolution. This paper reports on a tool, which is highly parameterizable and driven to support Model-2-Model and Model-2-Code transformations. Also, instead of using a predefined technology, the tool was built to be scalable and extensible for many different targets and also by addressing the user interface layer generation. © Springer International Publishing Switzerland 2016.},
author_keywords={Cross-platform generation;  Model transformation;  Model-driven software engineering;  Pervasive software development},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Purawat20161791,
author={Purawat, S. and Cowart, C. and Amaro, R.E. and Altintas, I.},
title={Biomedical big data training collaborative (BBDTC): An effort to bridge the talent gap in biomedical science and research},
journal={Procedia Computer Science},
year={2016},
volume={80},
pages={1791-1800},
doi={10.1016/j.procs.2016.05.454},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978484058&doi=10.1016%2fj.procs.2016.05.454&partnerID=40&md5=8c5277d4e032de4513cd947f9cd9f3a3},
affiliation={San Diego Supercomputer Center, University of California, San Diego, United States; Department of Chemistry and Biochemistry, University of California, San Diego, United States},
abstract={The BBDTC (https://biobigdata.ucsd.edu) is a community-oriented platform to encourage high-quality knowledge dissemination with the aim of growing a well-informed biomedical big data community through collaborative efforts on training and education. The BBDTC collaborative is an e-learning platform that supports the biomedical community to access, develop and deploy open training materials. The BBDTC supports Big Data skill training for biomedical scientists at all levels, and from varied backgrounds. The natural hierarchy of courses allows them to be broken into and handled as modules. Modules can be reused in the context of multiple courses and reshuffled, producing a new and different, dynamic course called a playlist. Users may create playlists to suit their learning requirements and share it with individual users or the wider public. BBDTC leverages the maturity and design of the HUBzero content-management platform for delivering educational content. To facilitate the migration of existing content, the BBDTC supports importing and exporting course material from the edX platform. Migration tools will be extended in the future to support other platforms. Hands-on training software packages, i.e., toolboxes, are supported through Amazon EC2 and Virtualbox virtualization technologies, and they are available as: (i) downloadable lightweight Virtualbox Images providing a standardized software tool environment with software packages and test data on their personal machines, and (ii) remotely accessible Amazon EC2 Virtual Machines for accessing biomedical big data tools and scalable big data experiments. At the moment, the BBDTC site contains three open Biomedical big data training courses with lecture contents, videos and hands-on training utilizing VM toolboxes, covering diverse topics. The courses have enhanced the hands-on learning environment by providing structured content that users can use at their own pace. A four course biomedical big data series is planned for development in 2016. © The Authors. Published by Elsevier B.V.},
author_keywords={Big data;  Biomedical;  Collaborative;  E-learning;  Education;  Toolbox},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Skrettingland2016,
author={Skrettingland, K. and Ulland, E.N. and Ravndal, O. and Tangen, M. and Kristoffersen, J.B. and Stenerud, V.R. and Dalen, V. and Standnes, D.C. and Fevang, Ø. and Mevik, K.M. and McIntosh, N. and Mebratu, A. and Melien, I. and Stavland, A.},
title={Snorre in-depth water diversion - New operational concept for large scale chemical injection from a shuttle tanker},
journal={Proceedings - SPE Symposium on Improved Oil Recovery},
year={2016},
volume={2016-January},
doi={10.2118/179602-ms},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977137217&doi=10.2118%2f179602-ms&partnerID=40&md5=27d4e31ddfd4c4554b5cc27590980d1a},
affiliation={Statoil ASA, Norway; Knutsen Subsea Solutions, Norway; Halliburton, United States; Intl. Research Inst. of Stavanger, Norway},
abstract={Declining oil production and increasing water cut in mature fields highlight the need for improved conformance control. Here we report on a successful in-depth water diversion treatment using sodium silicate to increase oil recovery at the Snorre field, offshore Norway, utilizing a new operational concept of using a stimulation vessel as a platform for the large-scale injection into a subsea well. A custom modified 35,000 DWT shuttle tanker was employed for the field pilot. This paper describes the vessel preparations and the large-scale interwell silicate injection operation. The operational aspects of the large-scale interwell silicate injection include; identification of injection vessel requirements, major vessel modifications, chemical logistic, general logistics to site, major equipment set-up on vessel, subsea connection, mixing and pumping schedules, onsite QC, and real time monitoring. Experience from these operations and lessons learned are included in this paper. After the injection of approximately 400,000 Sm3 (113,000 Sm3 preflush, followed by 240,000 Sm3 of sodium silicate gelant and 49,000 Sm3 of postflush fluid) at injection rates up to 4,000 Sm3/d, the injection from the vessel was stopped and the well was put on regular seawater injection. Following more than two years of regular production, transient pressure measurements, tracer testing and water cut data are presented from the ongoing comprehensive data acquisition program. These results demonstrate clearly the achieved in-depth flow diversion through a delayed breakthrough of injected tracers and lower water cut in the relevant production well. Copyright 2016, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Caruso20161629,
author={Caruso, C. and Spera, Z. and Hennings, T. and Kantola, J. and Kinchen, R.S. and Levy, W. and Wone, M.},
title={DCCR case study on guidelines for achieving an extended design life in CSO projects},
journal={ITA-AITES World Tunnel Congress 2016, WTC 2016},
year={2016},
volume={2},
pages={1629-1638},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976376875&partnerID=40&md5=b0fa924286a55301d320bb1949d0b1b2},
affiliation={McMillen Jacobs Assoct., Canada; JCK Engineering, Australia; Protecht Consulting, United States; DC Water, United States},
abstract={Owners constructing new combined sewer overflow (CSO) systems increasingly seek to have project design lives of 100 or more years in response to the growing size, complexity, and cost to construct and operate these systems. However, a codified and universally accepted approach to achieving this goal in the United States does not currently exist since service life prediction is a relatively new technical field. This paper discusses how the 100-year design life requirement was met for the design of tunnels, drop shafts, and diversion structures in the DC Clean Rivers Program (DCCRP). The authors identify some commonly referenced publications, present the testing framework used by the program manager to characterize the CSOs, review the design criteria used to guide the 100-year design life development, and establish guidelines to be considered by program managers and designers when implementing similar extended design life requirements in other CSO control programs. Copyright © (2016) by the Society for Mining, Metallurgy and Exploration All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{VanDerVegt2016,
author={Van Der Vegt, W. and Westera, W. and Nyamsuren, E. and Georgiev, A. and Ortiz, I.M.},
title={RAGE Architecture for Reusable Serious Gaming Technology Components},
journal={International Journal of Computer Games Technology},
year={2016},
volume={2016},
doi={10.1155/2016/5680526},
art_number={5680526},
note={cited By 34},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962690376&doi=10.1155%2f2016%2f5680526&partnerID=40&md5=2da6ec1ec655e35c4ca1bea69142d050},
affiliation={Open University of the Netherlands, Valkenburgerweg 177, Heerlen, 6419 AT, Netherlands; Sofia University St. Kliment Ohridski, Boulevard Tzar Osvoboditel 15, Sofia, 1504, Bulgaria; Complutense University of Madrid, Avenida de Séneca 2, Madrid, 28040, Spain},
abstract={For seizing the potential of serious games, the RAGE project - funded by the Horizon-2020 Programme of the European Commission - will make available an interoperable set of advanced technology components (software assets) that support game studios at serious game development. This paper describes the overall software architecture and design conditions that are needed for the easy integration and reuse of such software assets in existing game platforms. Based on the component-based software engineering paradigm the RAGE architecture takes into account the portability of assets to different operating systems, different programming languages, and different game engines. It avoids dependencies on external software frameworks and minimises code that may hinder integration with game engine code. Furthermore it relies on a limited set of standard software patterns and well-established coding practices. The RAGE architecture has been successfully validated by implementing and testing basic software assets in four major programming languages (C#, C++, Java, and TypeScript/JavaScript, resp.). Demonstrator implementation of asset integration with an existing game engine was created and validated. The presented RAGE architecture paves the way for large scale development and application of cross-engine reusable software assets for enhancing the quality and diversity of serious gaming. © 2016 Wim van der Vegt et al.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Karaca201623,
author={Karaca, O. and Kirscher, J. and Maurer, L. and Pelz, G.},
title={Towards simulation based evaluation of safety goal violations in automotive systems},
journal={Lecture Notes in Electrical Engineering},
year={2016},
volume={361},
pages={23-40},
doi={10.1007/978-3-319-24457-0_2},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952764772&doi=10.1007%2f978-3-319-24457-0_2&partnerID=40&md5=18b3a0f3ad0acbcca84b4fe74b8a1bbe},
affiliation={Infineon Technologies AG, Neubiberg, Germany; Bundeswehr University Munich, Neubiberg, Germany},
abstract={With the advent of the ISO 26262 it became crucial to prove that electrical and electronic products delivered into safety-related automotive applications are adequately safe. For this purpose safety goal violations due to random hardware faults need to be evaluated. In order to gain evident results for argumentation within the evaluation, a fault injection based approach is utilised. Potential risk scenarios are initiated by injection of analogue and digital faults into the heterogeneous behavioural model which comprises the safety-related hardware. For fault injection in heterogeneous models, we propose analogue saboteurs, designed in VHDL-AMS, by which amongst electrical or mechanical, diverse energy domain analogue hardware faults may be injected. For demonstration of this approach, a hardware model, comprising lithium-ion battery cells with a cell balancing and monitoring module and safety-related circuitry is used. © Springer International Publishing Switzerland 2016},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sarhadi2015470,
author={Sarhadi, P. and Yousefpour, S.},
title={State of the art: hardware in the loop modeling and simulation with its applications in design, development and implementation of system and control software},
journal={International Journal of Dynamics and Control},
year={2015},
volume={3},
number={4},
pages={470-479},
doi={10.1007/s40435-014-0108-3},
note={cited By 45},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975263079&doi=10.1007%2fs40435-014-0108-3&partnerID=40&md5=22e82a98bbfe07c13b3600016de20258},
affiliation={Department of Control Engineering, Islamic Azad University, South Tehran Branch, Tehran, Iran; Department of Computer Engineering, Amir Kabir University of Technology, Tehran, Iran},
abstract={Nowadays due to the technology development and use of digital computers in various systems, need for development of high performance and robust software is attracting great attentions. Because of increasing complexity in algorithms and implementation hardware for embedded systems, proper simulation tools are required. In sophisticated systems design, hardware in the loop (HIL) simulation is known as a prominent simulation tool before realistic tests of the system and a step after software simulation. Simultaneously it can be used for verification and validation of automation and control software. HIL has had an historical background in aerospace industries. Recently, this tool has spread in different steps of system life cycle such as design, development, implementation and test of various applications including automobile industry, shipbuilding, power lines, robotic systems and etc. Utilizing a suitable hardware in the loop laboratory, in system design stages is a practical way to increase the system reliability and efficiency as well as value of product. Also, by proper investigation in this modelling and simulation method, many errors can be avoided in design procedure of software and hardware as well as their interconnections. In this study, structure and components of an hardware in the loop laboratory for different systems are explored, also it is tried to more evaluate the applications of HIL simulations in dynamics and control engineering. At last, general structure of an hardware in the loop lab for diverse industries is proposed and discussed. © 2014, Springer-Verlag Berlin Heidelberg.},
author_keywords={Embedded systems;  Hardware in the loop (HIL);  Modelling & Simulation;  System design},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ramasubbu2015787,
author={Ramasubbu, N. and Bharadwaj, A. and Tayi, G.K.},
title={Software process diversity: Conceptualization, measurement, and analysis of impact on project performance},
journal={MIS Quarterly: Management Information Systems},
year={2015},
volume={39},
number={4},
pages={787-808},
doi={10.25300/misq/2015/39.4.3},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961813326&doi=10.25300%2fmisq%2f2015%2f39.4.3&partnerID=40&md5=86cc714817e32ec58d8bd36c59b8d6ea},
affiliation={Joseph M. Katz Graduate School of Business, University of Pittsburgh, 354 Mervis Hall, Pittsburgh, PA  15260, United States; Goizueta Business School, Emory University, 1300 Clifton Road NE, Atlanta, GA  30322, United States; School of Business, State University of New York at Albany, 1400 Washington Avenue, Albany, NY  12222, United States},
abstract={This article investigates software process diversity, defined as the project condition arising out of the simultaneous use of multiple software development process frameworks within a single project. Software process diversity is conceptualized as the response of a project team to such contingencies as requirements volatility, design and technological novelty, customer involvement, and the level of organizational process compliance enforced on the project. Moreover, we conceptualize that the degree of fit (or match) between a project's software process diversity and the level of process compliance enforced on the project impacts overall project performance. This conceptualization was empirically tested by utilizing data collected from 410 large commercial software projects of a multinational firm. The results show that higher levels of requirements volatility, design and technological novelty, and customer involvement increased software process diversity within a project. However, software process diversity decreased relative to increases in the level of process compliance enforced on the project. A higher degree of fit between the process diversity and process compliance of a project, rather than the effects of those variables independently, was found to be significantly associated with a higher level of project performance, as measured in terms of project productivity and software quality. These results indicate that increasing software process diversity in response to project-level contingencies improves project performance only when there is a concomitant increase in organizational process compliance efforts. The implications of these results for research are discussed and prescriptive guidelines derived to manage the fit between process diversity and process compliance for improving software project performance.},
author_keywords={Agile processes;  Fit as matching;  Plan-based processes;  Process compliance;  Productivity;  Quality;  Software engineering;  Software process diversity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tao20153043,
author={Tao, C.-Q. and Li, B.-X. and Gao, J.},
title={Complexity metrics for regression testing of component-based software},
journal={Ruan Jian Xue Bao/Journal of Software},
year={2015},
volume={26},
number={12},
pages={3043-3061},
doi={10.13328/j.cnki.jos.004876},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957837504&doi=10.13328%2fj.cnki.jos.004876&partnerID=40&md5=884e73765b5b7a857aba8301b3c02f9b},
affiliation={School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, 210094, China; State Key Laboratory for Novel Software Technology (Nanjing University), Nanjing, 210023, China; School of Computer Science and Engineering, Southeast University, Nanjing, 211189, China; Institute of Software Engineering, Southeast University, Nanjing, 211189, China; Department of Computer Engineering, San Jose State University, San Jose, CA, United States},
abstract={Component-based software construction is a widely used approach in software development, aiming to reduce the engineering effort and speed up development cycle. During software maintenance, various software update approaches can be utilized to realize specific change requirements of component-based software. Different update approaches might lead to diverse regression testing complexity. However, there is a lack of research work addressing regression testing complexity in software maintenance. In this paper, a framework is proposed to measure and analyze regression testing complexity based on a set of change and impact complexity models and metrics. The paper presents an approach to complexity metrics for regression testing of component-based software. A graphic model and several measurements for the complexity metrics, which consist of both maintenance and retesting complexity, are also proposed. An experimental study is conducted to compare the complexity of regression testing using the data from several independent groups. The study results indicate the presented approach is feasible in providing visual comparison on various complexity of regression testing from different methods. © Copyright 2015, Institute of Software, the Chinese Academy of Sciences. All rights reserved.},
author_keywords={Component-based software;  Re-testing complexity;  Regression testing;  Software maintenance},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou20152849,
author={Zhou, G. and Cheng, W.-M. and Xu, C.-C. and Nie, W.},
title={Characteristic analysis of 13C-NMR for the wettability difference of coal dust with diverse degrees of metamorphism},
journal={Meitan Xuebao/Journal of the China Coal Society},
year={2015},
volume={40},
number={12},
pages={2849-2855},
doi={10.13225/j.cnki.jccs.2015.0293},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955140406&doi=10.13225%2fj.cnki.jccs.2015.0293&partnerID=40&md5=ec90fb0ff3d5814265e4899832862e8a},
affiliation={State Key Laboratory of Mining Disaster Prevention and Control Co-Founded by Shandong Province and the Ministry of Science and Technology, Shandong University of Science and Technology, Qingdao, 266590, China; Key Laboratory of Safety and High-Efficiency Coal Mining of the Ministry of Education, Anhui University of Science and Technology, Huainan, 232001, China; College of Mining and Safety Engineering, Shandong University of Science and Technology, Qingdao, 266590, China},
abstract={To explore the effect of microscopic molecular structure parameters' changes on the wettability of coal dust with the different degrees of metamorphism, a 13C-NMR experiment was conducted to obtain the structural parameters of the carbon skeletons of six kinds of coal dust with varying metamorphic degrees, and an optical droplet shape analysis system was used to determine the wetting contact angles of the coal dust. According to the test results, the structural parameters' variation of aromatic carbon and aliphatic carbon with metamorphic grades was achieved, which shows that the main impact factors of coal dust wettability are f'a, faH, faB, faP, falH and falO from the analysis of the SPSS software. For aromatic carbon, with the increase of f'a, faB, faH and the decrease of faP, the dust hydrophobic was enhanced; with the decrease of falH and falO for aliphatic carbon, the dust hydrophilic was weakened. Finally, with the analysis of the changing rates of coal dust wetting impact factors, the microscopic mechanism about the wettability difference of different coal dust was achieved, which is useful for improving and expanding the traditional wetting theory of coal dust. © 2015, China Coal Society. All right reserved.},
author_keywords={13C-NMR;  Coal dust with different degrees of metamorphism;  Impact factors;  Microscopic wetting mechanism;  Molecular structure parameter;  Wettability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2015205,
author={Chen, J. and Xu, X. and Osterweil, L.J. and Zhu, L. and Brun, Y. and Bass, L. and Xiao, J. and Li, M. and Wang, Q.},
title={Using simulation to evaluate error detection strategies: A case study of cloud-based deployment processes},
journal={Journal of Systems and Software},
year={2015},
volume={110},
pages={205-221},
doi={10.1016/j.jss.2015.08.043},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944071192&doi=10.1016%2fj.jss.2015.08.043&partnerID=40&md5=b124bbc146fb09a9d9d2fc2cfa43cc50},
affiliation={Laboratory for Internet Software Technologies, Institute of Software, Chinese Academy of Sciences, Beijing, China; NICTA (National ICT Australia), Australian Technology Park, Eveleigh, Australia; College of Information and Computer Sciences, University of Massachusetts, Amherst, MA, United States; School of Computer Science and Engineering, University of New South Wales, Sydney, Australia; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China},
abstract={The processes for deploying systems in cloud environments can be the basis for studying strategies for detecting and correcting errors committed during complex process execution. These cloud-based processes encompass diverse activities, and entail complex interactions between cloud infrastructure, application software, tools, and humans. Many of these processes, such as those for making release decisions during continuous deployment and troubleshooting in system upgrades, are highly error-prone. Unlike the typically well-tested deployed software systems, these deployment processes are usually neither well understood nor well tested. Errors that occur during such processes may require time-consuming troubleshooting, undoing and redoing steps, and problem fixing. Consequently, these processes should ideally be guided by strategies for detecting errors that consider trade-offs between efficiency and reliability. This paper presents a framework for systematically exploring such trade-offs. To evaluate the framework and illustrate our approach, we use two representative cloud deployment processes: a continuous deployment process and a rolling upgrade process. We augment an existing process modeling language to represent these processes and model errors that may occur during process execution. We use a process-aware discrete-event simulator to evaluate strategies and empirically validate simulation results by comparing them to experiences in a production environment. Our evaluation demonstrates that our approach supports the study of how error-handling strategies affect how much time is taken for task-completion and error-fixing. © 2015 Elsevier Inc. All rights reserved.},
author_keywords={Deployment process;  Process modeling;  Simulation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gogolla2015312,
author={Gogolla, M. and Vallecillo, A. and Burgueño, L. and Hilken, F.},
title={Employing classifying terms for testing model transformations},
journal={2015 ACM/IEEE 18th International Conference on Model Driven Engineering Languages and Systems, MODELS 2015 - Proceedings},
year={2015},
pages={312-321},
doi={10.1109/MODELS.2015.7338262},
art_number={7338262},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961644713&doi=10.1109%2fMODELS.2015.7338262&partnerID=40&md5=825b4e6ffcb13cc3a5a211cc5fdc6fa8},
affiliation={University of Bremen, Germany; University of Málaga, Spain},
abstract={This contribution proposes a new technique for developing test cases for UML and OCL models. The technique is based on an approach that automatically constructs object models for class models enriched by OCL constraints. By guiding the construction process through so-called classifying terms, the built test cases in form of object models are classified into equivalence classes. A classifying term can be an arbitrary OCL term on the class model that calculates for an object model a characteristic value. From each equivalence class of object models with identical characteristic values one representative is chosen. The constructed test cases behave significantly different with regard to the selected classifying term. By building few diverse object models, properties of the UML and OCL model can be explored effectively. The technique is applied for automatically constructing relevant source model test cases for model transformations between a source and target metamodel. © 2015 IEEE.},
author_keywords={Buildings;  Computational modeling;  Context;  Data models;  Software testing;  Unified modeling language},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Szczecinski2015,
author={Szczecinski, N.S. and Martin, J.P. and Bertsch, D.J. and Ritzmann, R.E. and Quinn, R.D.},
title={Neuromechanical model of praying mantis explores the role of descending commands in pre-strike pivots},
journal={Bioinspiration and Biomimetics},
year={2015},
volume={10},
number={6},
doi={10.1088/1748-3190/10/6/065005},
art_number={065005},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951742594&doi=10.1088%2f1748-3190%2f10%2f6%2f065005&partnerID=40&md5=e65fc1b3df202fad1305c3541a73a16d},
affiliation={Case Western Reserve University, 10900 Euclid Ave., Cleveland, OH  44106, United States},
abstract={Praying mantises hunt by standing on their meso- and metathoracic legs and using them to rotate and translate (together, 'pivot') their bodies toward prey. We have developed a neuromechanical software model of the praying mantis Tenodera sinensis to use as a platform for testing postural controllers that the animal may use while hunting. Previous results showed that a feedforward model was insufficient for capturing the diversity of posture observed in the animal (Szczecinski et al 2014 Biomimetic and Biohybrid Syst. 3 296-307). Therefore we have expanded upon this model to make a flexible controller with feedback that more closely mimics the animal. The controller actuates 24 joints in the legs of a dynamical model to orient the head and translate the thorax toward prey. It is controlled by a simulation of nonspiking neurons assembled as a highly simplified version of networks that may exist in the mantid central complex and thoracic ganglia. Because of the distributed nature of these networks, we hypothesize that descending commands that orient the mantis toward prey may be simple direction-of-intent signals, which are turned into motor commands by the structure of low-level networks in the thoracic ganglia. We verify this through a series of experiments with the model. It captures the speed and range of mantid pivots as reported in other work (Yamawaki et al 2011 J. Insect Physiol. 57 1010-6). It is capable of pivoting toward prey from a variety of initial postures, as seen in the animal. Finally, we compare the model's joint kinematics during pivots to preliminary 3D kinematics collected from Tenodera. © 2015 IOP Publishing Ltd.},
author_keywords={descending commands;  neuromechanical modeling;  neuromorphic control;  praying mantis},
document_type={Article},
source={Scopus},
}

@ARTICLE{DiAlesio2015,
author={Di Alesio, S. and Briand, L.C. and Nejati, S. and Gotlieb, A.},
title={Combining genetic algorithms and constraint programming to support stress testing of task deadlines},
journal={ACM Transactions on Software Engineering and Methodology},
year={2015},
volume={25},
number={1},
doi={10.1145/2818640},
art_number={4},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952064666&doi=10.1145%2f2818640&partnerID=40&md5=867389abeae5177ed44c29c2bae4b40c},
affiliation={Software Engineering Department, Simula Research Laboratory, P.O. Box 134, Lysaker, 1325, Norway; University of Luxembourg, SnT Centre, 4 rue Alphonse Weicker, Luxembourg, L-2721, Luxembourg},
abstract={Tasks in real-time embedded systems (RTES) are often subject to hard deadlines that constrain how quickly the system must react to external inputs. These inputs and their timing vary in a large domain depending on the environment state and can never be fully predicted prior to system execution. Therefore, approaches for stress testing must be developed to uncover possible deadline misses of tasks for different input arrival times. In this article, we describe stress-test case generation as a search problem over the space of task arrival times. Specifically, we search for worst-case scenarios maximizing deadline misses, where each scenario characterizes a test case. In order to scale our search to large industrial-size problems, we combine two state-of-the-art search strategies, namely, genetic algorithms (GA) and constraint programming (CP). Our experimental results show that, in comparison with GA and CP in isolation, GA+CP achieves nearly the same effectiveness as CP and the same efficiency and solution diversity as GA, thus combining the advantages of the two strategies. In light of these results, we conclude that a combined GA+CP approach to stress testing is more likely to scale to large and complex systems. 2015 Copyright is held by the owner/author(s).},
author_keywords={Constraint programming;  Genetic algorithms;  Real-time systems;  Search-based software testing;  Stress testing;  Task deadline},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Le2015386,
author={Le, V. and Sun, C. and Su, Z.},
title={Finding deep compiler bugs via guided stochastic program mutation},
journal={Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
year={2015},
volume={25-30-Oct-2015},
pages={386-399},
doi={10.1145/2814270.2814319},
note={cited By 64},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958604903&doi=10.1145%2f2814270.2814319&partnerID=40&md5=c27cf1a912eb2accdc444e8b154f7b68},
affiliation={Department of Computer Science, University of California, Davis, United States},
abstract={Compiler testing is important and challenging. Equivalence Modulo Inputs (EMI) is a recent promising approach for compiler validation. It is based on mutating the unexecuted statements of an existing program under some inputs to produce new equivalent test programs w.r.t. these inputs. Orion is a simple realization of EMI by only randomly deleting unexecuted statements. Despite its success in finding many bugs in production compilers, Orion's effectiveness is still limited by its simple, blind mutation strategy. To more effectively realize EMI, this paper introduces a guided, advanced mutation strategy based on Bayesian optimization. Our goal is to generate diverse programs to more thoroughly exercise compilers. We achieve this with two techniques: (1) the support of both code deletions and insertions in the unexecuted regions, leading to a much larger test program space; and (2) the use of an objective function that promotes control-flow-diverse programs for guiding Markov Chain Monte Carlo (MCMC) optimization to explore the search space. Our technique helps discover deep bugs that require elaborate mutations. Our realization, Athena, targets C compilers. In 19 months, Athena has found 72 new bugs-many of which are deep and important bugs-in GCC and LLVM. Developers have confirmed all 72 bugs and fixed 68 of them. © 2015 ACM.},
author_keywords={Automated testing;  Compiler testing;  Equivalent program variants;  Markov Chain Monte Carlo},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Madison2015481,
author={Madison, J.C. and Hayes, J.C. and Keller, D.T. and Lombardo, N.J.},
title={Combining systems engineering with Technology and Manufacturing Readiness Levels to advance research and development},
journal={1st IEEE International Symposium on Systems Engineering, ISSE 2015 - Proceedings},
year={2015},
pages={481-488},
doi={10.1109/SysEng.2015.7302801},
art_number={7302801},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954526616&doi=10.1109%2fSysEng.2015.7302801&partnerID=40&md5=a98df43f43c0409bb01c27d00d3f4d08},
affiliation={Pacific Northwest National Laboratory, Richland, WA, United States},
abstract={The Nuclear Explosion and Monitoring Program (NEMP) at Pacific Northwest National Laboratory (PNNL) performs R& D to develop gas processing and nuclear detection systems that can monitor and detect atmospheric signatures that indicate a nuclear test has occurred. Since the program is multidisciplinary and diverse, a graded systematic approach for how technology is developed and transferred is needed. This approach would standardize the technology development process across the program for consistent product delivery and facilitate technology transfer to outside entities. This process would be made available to other research and development (R&D) projects within PNNL to standardize their technology development processes. Since the NEMP has a strong record of successful laboratory system development, there was not a need to design a new R&D and technology development process, rather to refine it with systems engineering principles. An analysis was performed that compared the existing NEMP technology development process to the US Department of Homeland Security (DHS) Technology Readiness Levels (TRLs) and Manufacturing Readiness Levels (MRLs). A gap analysis was performed that identified the areas in which more rigor could help to transition R&D to the community and where the systems engineering principles could be applied. Based on the analysis, processes surrounding requirements and testing, manufacturing preparedness, and verification and validation were strengthened. © 2015 IEEE.},
author_keywords={manufacturing readiness levels;  research and development;  systems engineering;  technology readiness levels;  technology transfer},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mäntylä20151384,
author={Mäntylä, M.V. and Adams, B. and Khomh, F. and Engström, E. and Petersen, K.},
title={On rapid releases and software testing: a case study and a semi-systematic literature review},
journal={Empirical Software Engineering},
year={2015},
volume={20},
number={5},
pages={1384-1425},
doi={10.1007/s10664-014-9338-4},
note={cited By 54},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940719377&doi=10.1007%2fs10664-014-9338-4&partnerID=40&md5=1c1ee4066c600a8622cc168b563e5997},
affiliation={Department of Computer Science and Engineering, Aalto University, Espoo, Finland; MCIS, Polytechnique Montréal, Québec, Canada; SWAT, Polytechnique Montréal, Québec, Canada; Department of Computer Science, Lund University, Lund, Sweden; School of Computing, Blekinge Institute of Technology, Karlskrona, Sweden},
abstract={Large open and closed source organizations like Google, Facebook and Mozilla are migrating their products towards rapid releases. While this allows faster time-to-market and user feedback, it also implies less time for testing and bug fixing. Since initial research results indeed show that rapid releases fix proportionally less reported bugs than traditional releases, this paper investigates the changes in software testing effort after moving to rapid releases in the context of a case study on Mozilla Firefox, and performs a semi-systematic literature review. The case study analyzes the results of 312,502 execution runs of the 1,547 mostly manual system-level test cases of Mozilla Firefox from 2006 to 2012 (5 major traditional and 9 major rapid releases), and triangulates our findings with a Mozilla QA engineer. We find that rapid releases have a narrower test scope that enables a deeper investigation of the features and regressions with the highest risk. Furthermore, rapid releases make testing more continuous and have proportionally smaller spikes before the main release. However, rapid releases make it more difficult to build a large testing community , and they decrease test suite diversity and make testing more deadline oriented. In addition, our semi-systematic literature review presents the benefits, problems and enablers of rapid releases from 24 papers found using systematic search queries and a similar amount of papers found through other means. The literature review shows that rapid releases are a prevalent industrial practice that are utilized even in some highly critical domains of software engineering, and that rapid releases originated from several software development methodologies such as agile, open source, lean and internet-speed software development. However, empirical studies proving evidence of the claimed advantages and disadvantages of rapid releases are scarce. © 2014, Springer Science+Business Media New York.},
author_keywords={Agile releases;  Bugs;  Builds;  Mozilla;  Open-source;  Release model;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Le2015386,
author={Le, V. and Sun, C. and Su, Z.},
title={Finding deep compiler bugs via guided stochastic program mutation},
journal={ACM SIGPLAN Notices},
year={2015},
volume={50},
number={10},
pages={386-399},
doi={10.1145/2814270.2814319},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030783808&doi=10.1145%2f2814270.2814319&partnerID=40&md5=bbb290c240241acc602f2f8ef04743c9},
affiliation={Department of Computer Science, University of California, Davis, United States},
abstract={Compiler testing is important and challenging. Equivalence Modulo Inputs (EMI) is a recent promising approach for compiler validation. It is based on mutating the unexecuted statements of an existing program under some inputs to produce new equivalent test programs w.r.t. these inputs. Orion is a simple realization of EMI by only randomly deleting unexecuted statements. Despite its success in finding many bugs in production compilers, Orion's effectiveness is still limited by its simple, blind mutation strategy. To more effectively realize EMI, this paper introduces a guided, advanced mutation strategy based on Bayesian optimization. Our goal is to generate diverse programs to more thoroughly exercise compilers. We achieve this with two techniques: (1) the support of both code deletions and insertions in the unexecuted regions, leading to a much larger test program space; and (2) the use of an objective function that promotes control-flow-diverse programs for guiding Markov Chain Monte Carlo (MCMC) optimization to explore the search space. Our technique helps discover deep bugs that require elaborate mutations. Our realization, Athena, targets C compilers. In 19 months, Athena has found 72 new bugs - many of which are deep and important bugs - in GCC and LLVM. Developers have confirmed all 72 bugs and fixed 68 of them. © 2015 ACM.},
author_keywords={automated testing;  Compiler testing;  equivalent program variants;  Markov Chain Monte Carlo},
document_type={Article},
source={Scopus},
}

@ARTICLE{McIntosh2015456,
author={McIntosh, L.D. and Sharma, M.K. and Mulvihill, D. and Gupta, S. and Juehne, A. and George, B. and Khot, S.B. and Kaushal, A. and Watson, M.A. and Nagarajan, R.},
title={CaTissue Suite to OpenSpecimen: Developing an extensible, open source, web-based biobanking management system},
journal={Journal of Biomedical Informatics},
year={2015},
volume={57},
pages={456-464},
doi={10.1016/j.jbi.2015.08.020},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949486509&doi=10.1016%2fj.jbi.2015.08.020&partnerID=40&md5=65b77f032fbb7dad6a6737e203b0e77c},
affiliation={Department of Pathology and Immunology, Washington University School of Medicine, St. Louis, MO, United States},
abstract={The National Cancer Institute (NCI) Cancer Biomedical Informatics Grid® (caBIG®) program established standards and best practices for biorepository data management by creating an infrastructure to propagate biospecimen resource sharing while maintaining data integrity and security. caTissue Suite, a biospecimen data management software tool, has evolved from this effort. More recently, the caTissue Suite continues to evolve as an open source initiative known as OpenSpecimen. The essential functionality of OpenSpecimen includes the capture and representation of highly granular, hierarchically-structured data for biospecimen processing, quality assurance, tracking, and annotation. Ideal for multi-user and multi-site biorepository environments, OpenSpecimen permits role-based access to specific sets of data operations through a user-interface designed to accommodate varying workflows and unique user needs. The software is interoperable, both syntactically and semantically, with an array of other bioinformatics tools given its integration of standard vocabularies thus enabling research involving biospecimens. End-users are encouraged to share their day-to-day experiences in working with the application, thus providing to the community board insight into the needs and limitations which need be addressed. Users are also requested to review and validate new features through group testing environments and mock screens. Through this user interaction, application flexibility and interoperability have been recognized as necessary developmental focuses essential for accommodating diverse adoption scenarios and biobanking workflows to catalyze advances in biomedical research and operations. Given the diversity of biobanking practices and workforce roles, efforts have been made consistently to maintain robust data granularity while aiding user accessibility, data discoverability, and security within and across applications by providing a lower learning curve in using OpenSpecimen. Iterative development and testing cycles provide continuous maintenance and up-to-date capabilities for this freely available, open-access, web-based software application that is globally-adopted at over 25 institutions. © 2015 Elsevier Inc.},
author_keywords={Biobanking;  Biospecimen;  Databases;  Medical informatics;  Specimen collection;  Tissue banks},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ricciardi2015248,
author={Ricciardi, L. and Gélain, T. and Soares, S.},
title={Experimental and numerical characterization of wind-induced pressure coefficients on nuclear buildings and chimney exhausts},
journal={Nuclear Engineering and Design},
year={2015},
volume={292},
pages={248-260},
doi={10.1016/j.nucengdes.2015.06.014},
art_number={8359},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939509125&doi=10.1016%2fj.nucengdes.2015.06.014&partnerID=40&md5=a4bcdbbd3d5a56e947c0c1e47047fefb},
affiliation={Institut de Radioprotection et de Sûreté Nucléaire (IRSN), PSN-RES, SCA, BP 68, Saclay, Gif-sur-Yvette, 91192, France},
abstract={Wind creates pressure effects on different surfaces of buildings according to their exposure to the wind, in particular at external communications. In nuclear facilities, these effects can change contamination transfers inside the building and can even lead to contamination release into the environment, especially in damaged (ventilation stopped) or accidental situations. The diversity of geometries of facilities requires the use of a validated code for predicting pressure coefficients, which characterize the wind effect on the building walls and the interaction between the wind and chimney exhaust. The first aim of a research program launched by the French Institut de Radioprotection et de Sûreté Nucléaire (IRSN), was therefore to acquire experimental data of the mean pressure coefficients for different geometries of buildings and chimneys through wind tunnel tests and then to validate a CFD code (ANSYS CFX) from these experimental results. The simulations were performed using a steady RANS approach and a two-equation SST k-ω turbulence model. After a mesh sensitivity study for one configuration of building and chimney, a comparison was carried out between the numerical and experimental values for other studied configurations. This comparison was generally satisfactory, averaged over all measurement points, with values of Root Mean Square Deviations lower than 0.15 for most cases. © 2015 Elsevier B.V.},
author_keywords={JEL classification L. Safety and risk analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Chourey2015772,
author={Chourey, V. and Sharma, M.},
title={Component based reliability assessment from UML models},
journal={2015 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2015},
year={2015},
pages={772-778},
doi={10.1109/ICACCI.2015.7275704},
art_number={7275704},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946233967&doi=10.1109%2fICACCI.2015.7275704&partnerID=40&md5=0c46c8081fb3d093a15a9f517eea13cd},
affiliation={Computer Science and Engineering Department, Medi-Caps Institute, Indore, Madhya Pradesh, India; Computer Engineering Department, IET, DAVV, Indore, Madhya Pradesh, India},
abstract={Model based development and testing techniques have ventured diverse research directions to assure quality of the software product. Models developed during architecture and design phases are efficient tools to assess quality at an early development stage. However, testing the extra-functional or non-functional properties of software systems is not frequently practised eg. reliability. The motivation to our work is to model the context of execution which is significant in system reliability analysis. In this paper we visualize the components of complex software systems and their interactions in the form of Functional Flow Diagram (FFD). This notation specifies the dynamic aspect of system behavior as the context of execution. To further asses reliability, the FFD is translated into Reliability Block Diagram (RBD). The relative importance of the components in terms of reliability is evaluated and is associated with prioritization of the component. The model is simple but significant for system maintenance, improvisation and modification. This model supports analysis and testing through better understanding of the interacting components and their reliabilities. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feng2015225,
author={Feng, Y. and Chen, Z. and Jones, J.A. and Fang, C. and Xu, B.},
title={Test report prioritization to assist crowdsourced testing},
journal={2015 10th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2015 - Proceedings},
year={2015},
pages={225-236},
doi={10.1145/2786805.2786862},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953837204&doi=10.1145%2f2786805.2786862&partnerID=40&md5=9148dbf9eb130223365b45c18663546b},
affiliation={State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Informatics, University of California, Irvine, United States},
abstract={In crowdsourced testing, users can be incentivized to per- form testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a financial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called \test reports" and are composed of simple natural language and screenshots. Back at the software-development organization, developers must manually inspect the test reports to judge their value for revealing faults. Due to the nature of crowdsourced work, the number of test reports are often difficult to comprehensively inspect and process. In order to help with this daunting task, we created the first technique of its kind, to the best of our knowledge, to prioritize test re- ports for manual inspection. Our technique utilizes two key strategies: (1) a diversity strategy to help developers inspect a wide variety of test reports and to avoid duplicates and wasted effort on falsely classified faulty behavior, and (2) a risk strategy to help developers identify test reports that may be more likely to be fault-revealing based on past observations. Together, these strategies form our DivRisk strategy to prioritize test reports in crowdsourced testing. Three industrial projects have been used to evaluate the effectiveness of these methods. The results of the empirical study show that: (1) DivRisk can significantly outperform random prioritization; (2) DivRisk can approximate the best theoretical result for a real-world industrial mobile application. In addition, we provide some practical guidelines of test report prioritization for crowdsourced testing based on the empirical study and our experiences. © 2015 ACM.},
author_keywords={Crowdsourcing testing;  Natural language processing;  Test diversity;  Test report prioritization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{VanDerAalst20151,
author={Van Der Aalst, W.},
title={Big software on the run: In vivo software analytics based on process mining (Keynote)},
journal={ACM International Conference Proceeding Series},
year={2015},
volume={24-26-August-2015},
pages={1-5},
doi={10.1145/2785592.2785593},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958538737&doi=10.1145%2f2785592.2785593&partnerID=40&md5=b225cea0ec89b5638b69b649400b8159},
affiliation={Eindhoven University of Technology, P.O. Box 513, Eindhoven, 5600 MB, Netherlands},
abstract={Software-related problems have an incredible impact on society, organizations, and users that increasingly rely on information technology. Specification, verification and testing techniques aim to avoid such problems. However, the growing complexity, scale, and diversity of software complicate matters. Since software is evolving and operates in a changing environment, one cannot anticipate all problems at design-time. Hence, we propose to analyze software "in vivo", i.e., we study systems in their natural habitat rather than through testing or software design. We propose to observe running systems, collect and analyze data on them, generate descriptive models, and use these to respond to failures. We focus on process mining as a tool for in vivo software analytics. Process discovery techniques can be used to capture the real behavior of software. Conformance checking techniques can be used to spot deviations. The alignment of models and real software behavior can be used to predict problems related to performance or conformance. Recent developments in process mining and instrumentation of software make this possible. This keynote paper provides pointers to process mining literature and introduces the "Big Software on the Run" (BSR) research program that just started.},
author_keywords={Conformance checking;  Event logs;  Process discovery;  Process mining;  Software analytics;  Software engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boussaa201540,
author={Boussaa, M. and Barais, O. and Sunyé, G. and Baudry, B.},
title={A Novelty Search Approach for Automatic Test Data Generation},
journal={Proceedings - 8th International Workshop on Search-Based Software Testing, SBST 2015},
year={2015},
pages={40-43},
doi={10.1109/SBST.2015.17},
art_number={7173590},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946944440&doi=10.1109%2fSBST.2015.17&partnerID=40&md5=1d8bae8f56076c158fe63de0291b9e68},
affiliation={Inria/IRISA Rennes, France},
abstract={In search-based structural testing, metaheuristic search techniques have been frequently used to automate the test data generation. In Genetic Algorithms (GAs) for example, test data are rewarded on the basis of an objective function that represents generally the number of statements or branches covered. However, owing to the wide diversity of possible test data values, it is hard to find the set of test data that can satisfy a specific coverage criterion. In this paper, we introduce the use of Novelty Search (NS) algorithm to the test data generation problem based on statement-covered criteria. We believe that such approach to test data generation is attractive because it allows the exploration of the huge space of test data within the input domain. In this approach, we seek to explore the search space without regard to any objectives. In fact, instead of having a fitness-based selection, we select test cases based on a novelty score showing how different they are compared to all other solutions evaluated so far. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Muterspaw2015,
author={Muterspaw, K. and Urner, T. and Lewis, R. and Babic, I. and Srinath, D. and Peck, C.},
title={Multidisciplinary research and education with open tools: Metagenomic analysis of 16S rRNA using Arduino, Android, Mothur and XSEDE},
journal={ACM International Conference Proceeding Series},
year={2015},
volume={2015-July},
doi={10.1145/2792745.2792767},
art_number={a22},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942770157&doi=10.1145%2f2792745.2792767&partnerID=40&md5=eae3a6bc59589fab29eac8bbc38a2e62},
affiliation={Earlham College, Richmond, IN  47374, United States},
abstract={Modern scientific research is often multidisciplinary, involving scientists from two or more backgrounds. A multidisciplinary approach is frequently necessary to advance our knowledge in a diverse range of fields, from genomics to climate change. Many of the projects undertaken in these areas involve a combination of field, lab, and computational analysis components. Our research initiatives demonstrate how the principles of active learning { performing tasks while engaging in analysis, synthesizing and evaluating the tasks being performed [10] { can be applied to undergraduate science education using 16S rRNA metagenomics as the basis. Beginning with development of the scientific questions, students work through the entire process of designing, testing and implementing physical and digital sampling protocols, hardware and software platforms for collecting geographically coded (geocoded) environmental metadata, and lab protocols; they work in the field taking samples and in the lab preparing them; they perform the computational analysis of the sequencer output and synthesis of metadata and metagenomic data; and finally they disseminate the results. The students come primarily from backgrounds in computer science, biology, geology, and physics. This broad range makes it possible to select teams that cover many of the traditionally underrepresented groups in science. Working together for a year or more, the students learn the science, vocabularies, skill sets, etc. of all the disciplines, as well as how their own discipline, in conjunction with others, contributes to addressing large, complex questions. © 2015 Copyright held by the owner/author(s).},
author_keywords={Active learning;  Archaeology;  Bioinformatics;  Biology;  Computer science;  Ecology;  Education;  Geology;  GIS;  International collaboration;  Metagenomics;  Mobile app;  Multidisciplinary science},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hataba2015120,
author={Hataba, M. and Elkhouly, R. and El-Mahdy, A.},
title={Diversified remote code execution using dynamic obfuscation of conditional branches},
journal={Proceedings - 2015 IEEE 35th International Conference on Distributed Computing Systems Workshops, ICDCSW 2015},
year={2015},
pages={120-127},
doi={10.1109/ICDCSW.2015.37},
art_number={7165094},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945415145&doi=10.1109%2fICDCSW.2015.37&partnerID=40&md5=2558bc2b7387ff3f72d819f9e38e2fa4},
affiliation={Parallel Computing Lab, Computer Science and Engineering Department, Egypt-Japan University of Science and Technology (E-JUST), Alexandria, Egypt},
abstract={Information leakage via timing side-channel attacksis one of the main threats that target code executing on remoteplatforms such as the cloud computing environment. Theseattacks can be further leveraged to reverse-engineer or eventamper with the running code. In this paper, we propose asecurity obfuscation technique, which helps making the generatedcode more resistant to these attacks, by means of increasinglogical complexity to hinder the formulation of a solid hypothesisabout code behavior. More importantly, this software solutionis portable, generic and does not require special setup orhardware or software modifications. In particular, we considermangling the control-flow inside a program via converting arandom set of conditional branches into linear code, using ifconversiontransformation. Moreover, our method exploits thedynamic compilation technology to continually and randomlyalter the branches. All of this mangling should diversify codeexecution, hence it becomes difficult for an attacker to infertiming correlations through statistical analysis. We extend theLLVM JIT compiler to provide for an initial investigation of thisapproach. This makes our system applicable to a wide varietyof programming languages and hardware platforms. We havestudied the system using a simple test program and selectedbenchmarks from the standard SPEC CPU 2006 suite withdifferent input loads and experimental setups. Initial results showsignificant changes in program's control-flow and hence datadependences, resulting in noticeable different execution timeseven for the same input data, thereby complicating such attacks.More notably, the performance penalty is within reasonablemargins. © 2015 IEEE.},
author_keywords={If-;  JIT Compilation;  Obfuscation;  Side-Channels},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yatoh2015316,
author={Yatoh, K. and Sakamoto, K. and Ishikawa, F. and Honiden, S.},
title={Feedback-controlled random test generation},
journal={2015 International Symposium on Software Testing and Analysis, ISSTA 2015 - Proceedings},
year={2015},
pages={316-326},
doi={10.1145/2771783.2771805},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975764104&doi=10.1145%2f2771783.2771805&partnerID=40&md5=063dd58589f4929b185398669ec75b20},
affiliation={University of Tokyo, Japan; Google Inc., Japan; National Institute of Informatics, Japan},
abstract={Feedback-directed random test generation is a widely used technique to generate random method sequences. It leverages feedback to guide generation. However, the validity of feedback guidance has not been challenged yet. In this paper, we investigate the characteristics of feedback-directed random test generation and propose a method that exploits the obtained knowledge that excessive feedback limits the diversity of tests. First, we show that the feedback loop of feedback-directed generation algorithm is a positive feedback loop and amplifies the bias that emerges in the candidate value pool. This over-directs the generation and limits the diversity of generated tests. Thus, limiting the amount of feedback can improve diversity and effectiveness of generated tests. Second, we propose a method named feedbackcontrolled random test generation, which aggressively controls the feedback in order to promote diversity of generated tests. Experiments on eight different, real-world application libraries indicate that our method increases branch coverage by 78% to 204% over the original feedback-directed algorithm on large-scale utility libraries. Copyright is held by the owner/author(s).},
author_keywords={Diversity;  Random testing;  Test generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hothersall-Thomas201537,
author={Hothersall-Thomas, C. and Maffeis, S. and Novakovic, C.},
title={BrowserAudit: Automated testing of browser security features},
journal={2015 International Symposium on Software Testing and Analysis, ISSTA 2015 - Proceedings},
year={2015},
pages={37-47},
doi={10.1145/2771783.2771789},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975727091&doi=10.1145%2f2771783.2771789&partnerID=40&md5=984ed5e584d36bfeee421c57e3cd118b},
affiliation={Netcraft Ltd, United Kingdom; Department of Computing, Imperial College London, United Kingdom},
abstract={The security of the client side of a web application relies on browser features such as cookies, the same-origin policy and HTTPS. As the client side grows increasingly powerful and sophisticated, browser vendors have stepped up their offering of security mechanisms which can be leveraged to protect it. These are often introduced experimentally and informally and, as adoption increases, gradually become standardised (e.g., CSP, CORS and HSTS). Considering the diverse landscape of browser vendors, releases, and customised versions for mobile and embedded devices, there is a compelling need for a systematic assessment of browser security. We present BrowserAudit, a tool for testing that a deployed browser enforces the guarantees implied by the main standardised and experimental security mechanisms. It includes more than 400 fully-automated tests that exercise a broad range of security features, helping web users, application developers and security researchers to make an informed security assessment of a deployed browser. We validate BrowserAudit by discovering both fresh and known security-related bugs in major browsers. Copyright is held by the owner/author(s).},
author_keywords={Click-jacking;  Content Security Policy;  Cookies;  Cross-origin resource sharing;  Same-origin policy;  Web browser testing;  Web security},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fieldsend20151031,
author={Fieldsend, J.E. and Moraglio, A.},
title={Strength through diversity: Disaggregation and multi-objectivisation approaches for genetic programming},
journal={GECCO 2015 - Proceedings of the 2015 Genetic and Evolutionary Computation Conference},
year={2015},
pages={1031-1038},
doi={10.1145/2739480.2754643},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963650753&doi=10.1145%2f2739480.2754643&partnerID=40&md5=fb597cca6fa5e9e3a040e353623cb6f6},
affiliation={Computer Science University of Exeter, United Kingdom},
abstract={An underlying problem in genetic programming (GP) is how to ensure sufficient useful diversity in the population during search. Having a wide range of diverse (sub)component structures available for recombination and/or mutation is important in preventing premature converge. We propose two new fitness disaggregation approaches that make explicit use of the information in the test cases (i.e., program semantics) to preserve diversity in the population. The first method preserves the best programs which pass each individual test case, the second preserves those which are nondominated across test cases (multi-objectivisation). We use these in standard GP, and compare them to using standard fitness sharing, and using standard (aggregate) fitness in tournament selection. We also examine the effect of including a simple anti-bloat criterion in the selection mechanism. We find that the non-domination approach, employing antibloat, significantly speeds up convergence to the optimum on a range of standard Boolean test problems. Furthermore, its best performance occurs with a considerably smaller population size than typically employed in GP. © 2015 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
author_keywords={Diversity;  Genetic programming;  Multi-objectivisation;  Optimisation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Boussaa20151359,
author={Boussaa, M. and Barais, O. and Sunye, G. and Baudry, B.},
title={A novelty search-based test data generator for object-oriented programs},
journal={GECCO 2015 - Companion Publication of the 2015 Genetic and Evolutionary Computation Conference},
year={2015},
pages={1359-1360},
doi={10.1145/2739482.2764716},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959440220&doi=10.1145%2f2739482.2764716&partnerID=40&md5=36f703ba4665120070cfa93bbe422460},
affiliation={INRIA Rennes, France},
abstract={In search-based structural testing, meta-heuristic search techniques have been frequently used to automate test data generation. In this paper, we introduce the use of novelty search algorithm to the test data generation problem based on statement-covered criterion. In this approach, we seek to explore the search space by considering diversity as the unique objective function to be optimized. In fact, instead of having a fitness-based selection, we select test cases based on a novelty score showing how different they are compared to all other solutions evaluated so far.},
author_keywords={Automated test data generation;  Genetic Algorithm;  Novelty Search;  Search-based software testing;  Structural coverage},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{FernándezRibaya2015,
author={Fernández Ribaya, Y. and Álvarez, E. and Paredes Sánchez, J.P. and Xiberta Bernat, J.},
title={Simulations of hybrid system varying solar radiation and microturbine response time},
journal={AIP Advances},
year={2015},
volume={5},
number={7},
doi={10.1063/1.4926436},
art_number={077110},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935439298&doi=10.1063%2f1.4926436&partnerID=40&md5=aca40da007f3d9c09a92230e20e6e4d1},
affiliation={Department of Energy E.I.M.E.M., University of Oviedo, 13 Independencia Street, Oviedo, 36004, Spain},
abstract={Hybrid power systems, such as combinations of renewable power sources with intermittent power production and non-renewable power sources, theoretically increase the reliability and thus integration of renewable sources in the electrical system. However, a recent increase in the number of hybrid installations has sparked interest in the effects of their connection to the grid, especially in remote areas. This paper analyses a photovoltaic-gas microturbine hybrid system dimensioned to be installed in La Paz (Mexico).The research presented in this paper studies and quantifies the effects on the total electric power produced, varying both the solar radiation and the gas microturbine response time. The gas microturbine and the photovoltaic panels are modelled using Matlab/Simulink software, obtaining a platform where different tests to simulate real conditions have been executed. They consist of diverse ramps of irradiance that replicate solar radiation variations, and different microturbine response times reproduced by the time constants of a first order transfer function that models the microturbine dynamic response. The results obtained show that when radiation varies quickly it does not produce significant differences in the power guarantee or the microturbine gas consumption, to any microturbine response time. However, these two parameters are highly variable with smooth radiance variations. The maximum total power variation decreases greatly as the radiation variation gets lower. In addition, by decreasing the microturbine response time, it is possible to appreciably increase the power guarantee although the maximum power variation and gas consumption increase. Only in cases of low radiation variation is there no appreciable difference in the maximum power variation obtained by the different turbine response times. © 2015 Author(s).},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Zhang2015179,
author={Zhang, T. and Gao, J. and Cheng, J. and Uehara, T.},
title={Compatibility testing service for mobile applications},
journal={Proceedings - 9th IEEE International Symposium on Service-Oriented System Engineering, IEEE SOSE 2015},
year={2015},
volume={30},
pages={179-186},
doi={10.1109/SOSE.2015.35},
art_number={7133527},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990998492&doi=10.1109%2fSOSE.2015.35&partnerID=40&md5=76f48e8af4ab6e126e077a9af99dab1a},
affiliation={School of Software and Microelectronics, Northwest Polytechnical University Xi'an, China; Department of Computer Engineering, San Jose State University, San Jose, United States; Software Innovation Laboratories, Fujitsu Aboratories LTD., Japan},
abstract={As more and more mobile applications are developed, mobile app testing and quality assurance have become very important. Due to the diversity of mobile devices and platforms, compatibility testing for mobile apps has been identified as one urgent and challenging issue. There are two major reasons contributing to this issue. They are: a) the large number of mobile devices with diverse features and platforms which are upgraded frequently; b) a higher cost and complexity in mobile app compatibility testing. This paper proposes one optimized compatibility testing strategy using a statistical approach to reduce test costs, and improve engineer's operation efficiency. The paper provides a solution to generate an optimized compatibility test sequence for mobile apps using the K-Means statistical algorithm. A compatibility testing service has been proposed for mobile apps. Moreover, two case study results are reported to demonstrate its potential application and effectiveness. © 2015 IEEE.},
author_keywords={Clustering algorithm;  Compatibility testing;  Mobile testing;  Software testing;  Test coverage},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tao2015326,
author={Tao, C. and Gao, J. and Li, B.},
title={A model-based framework to support complexity analysis service for regression testing of component-based software},
journal={Proceedings - 9th IEEE International Symposium on Service-Oriented System Engineering, IEEE SOSE 2015},
year={2015},
volume={30},
pages={326-331},
doi={10.1109/SOSE.2015.42},
art_number={7133549},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990957268&doi=10.1109%2fSOSE.2015.42&partnerID=40&md5=fae0512cf3b8700895f2ebe11c19d60f},
affiliation={School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, Jiangsu, China; School of Computer Engineering, San Jose State University, San Jose, CA, United States; School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China},
abstract={Today, software components have been widely used in software construction to reduce the cost of project and speed up software development cycle. During software maintenance, various software change approaches can be used to realize specific change requirements of software components. Different change approaches lead to diverse regression testing complexity. Such complexity is one of the key contributors to the cost and effectiveness of software maintenance. However, there is a lack of research work addressing regression testing complexity analysis service for software components. This paper proposes a framework to measure and analyze regression testing complexity based on a set of change and impact complexity models and metrics. The framework can provide services for complexity modeling, complexity factor classification, and regression testing complexity measurements. The initial study results indicate the proposed framework is feasible and effective in measuring the complexity of regression testing for component-based software. © 2015 IEEE.},
author_keywords={Component-based software regression testing;  Regression testing complexity;  Software maintenance;  Testing service},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheng2015302,
author={Cheng, J. and Zhu, Y. and Zhang, T. and Zhu, C. and Zhou, W.},
title={Mobile compatibility testing using multi-objective genetic algorithm},
journal={Proceedings - 9th IEEE International Symposium on Service-Oriented System Engineering, IEEE SOSE 2015},
year={2015},
volume={30},
pages={302-307},
doi={10.1109/SOSE.2015.36},
art_number={7133545},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990890405&doi=10.1109%2fSOSE.2015.36&partnerID=40&md5=60f91a559c00aa0f860d33fac20e2e9b},
affiliation={School of Computer, Northwest Polytechnical University, Xi'an, China; School of Software and Microelectronics, Northwest Polytechnical University, Xi'an, China},
abstract={Mobile compatibility testing has been identified as one urgent and challenging issue. Mobile apps are expected to work on thousand kinds of mobile devices with diverse device features and mobile platforms. So mobile compatibility testing is complex and costly, it is impossible to test mobile apps on all mobile devices and in all environments with limited test resources. Then the question is how to select test devices in cost-effective mobile app compatibility testing. This paper proposes a novel test device selection approach using multiobjective genetic algorithm. Using the proposed approach, the minimum number of mobile devices is selected, and the multiple test coverage requirements are met simultaneously. Furthermore, the case study results have successfully demonstrated that the proposed approach is effective for mobile compatibility testing. © 2015 IEEE.},
author_keywords={Compatibility testing;  Genetic algorithm;  Mobile testing;  Software testing;  Test coverage},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Baskar20151590,
author={Baskar, S. and Ertin, E.},
title={A software defined radar platform for waveform adaptive MIMO radar research},
journal={IEEE National Radar Conference - Proceedings},
year={2015},
volume={2015-June},
number={June},
pages={1590-1594},
doi={10.1109/RADAR.2015.7131251},
art_number={7131251},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937871118&doi=10.1109%2fRADAR.2015.7131251&partnerID=40&md5=6518de4f9b8371a9940c03e89af6af78},
affiliation={Department of Electrical and Computer Engineering, Ohio State University, Columbus, OH  43210, United States},
abstract={MIMO Radar systems that can adaptively select independent waveforms on transmit antennas can provide spatial and temporal diversity gains for improved detection, classification and tracking capability in challenging propagation environments. In this paper we discuss design, implementation and validation of a Software Defined Radar (SDR) test bed to derive MIMO waveform adaptive radar research. The OSU micro SDR platform features a custom integrated X-Band RF frontend with two independent transmit and a single receive channel multiplexed to four receiver antennas with bandwidth of 250 MHz and a digital back-end with an FPGA and an embedded PC for real-time receive processing and waveform scheduling. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kunze201530,
author={Kunze, S. and Poschl, R. and Grzemba, A.},
title={Experimental test system for distance estimation of standardized passive UHF RFID systems},
journal={Proceedings of 25th International Conference Radioelektronika, RADIOELEKTRONIKA 2015},
year={2015},
pages={30-33},
doi={10.1109/RADIOELEK.2015.7128981},
art_number={7128981},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942749215&doi=10.1109%2fRADIOELEK.2015.7128981&partnerID=40&md5=db77ce574824b42e7cbf5980b131bfe5},
affiliation={Deggendorf Institute of Technology, Campus Freyung, Freyung, D-94078, Germany; Deggendorf Institute of Technology, Deggendorf, D-94441, Germany},
abstract={In this paper an experimental test setup for the signal-strength based distance estimation of passive radio-frequency identification (RFID) tags is proposed. The system shall use freely available ultra high frequency (UHF) tags, and shall require no modifications of the tags or the communication protocol. A test setup based on a software defined radio (SDR) system is implemented. However, first measurements with this simple setup show, that a reliable distance estimation is not possible. Due to interference effects, such as multi-path propagation, the signal strength is not mainly dominated by its distance dependency. Therefore, the test system is modified and a number of optimization approaches is proposed. The goal hereby is to reduce the impact of interference on the signal strength, so that the measured value exhibits a mainly distance dependent behavior. Besides frequency hopping, various forms of antenna diversity are discussed. Using these optimization techniques it's possible to implement a test system that allows a fairly reliable distance estimation under controlled laboratory conditions. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rushing2015,
author={Rushing, D. and Guidry, J. and Alkadi, I.},
title={Collaborative penetration-testing and analysis toolkit (CPAT)},
journal={IEEE Aerospace Conference Proceedings},
year={2015},
volume={2015-June},
doi={10.1109/AERO.2015.7119262},
art_number={7119262},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940676322&doi=10.1109%2fAERO.2015.7119262&partnerID=40&md5=2c2142af454fdbeafb86704a0a2c2aaf},
affiliation={CSIT Department, SLU, 18256 Log Cabin Rd., Prairieville, LA  70769, United States; CSIT Department, SLU, 9108 Lockhart Rd. #4, Denham Springs, LA, United States; CSIT Department, SLU, SLU Box 10847, Hammond, LA  70402, United States},
abstract={Penetration testing (or 'pentesting') is critical to both maintaining and increasing the reliability of computer networks while lessening their vulnerability. The number, importance and value of these networks has been growing over the past decade, and their capabilities and respective uses have been integrated into many aspects of our lives. Without penetration testing, our networks can fall victim to a myriad of malicious mayhem which has the potential for serious, large-scale ramifications, and when these networks are not operating as expected it is often individuals who suffer. However, penetration testing poses its own new and diverse set of problems to security analysts. Due to the abstract nature of performing a pentest, the near complete lack of design geared toward effective collaboration and teamwork in many widely used penetration testing tools can create a notable hindrance for security teams. This paper describes a software project surrounding network penetration testing from a collaborative standpoint and the problems associated with team-based efforts utilizing present network analysis tools and technologies. © 2015 IEEE.},
author_keywords={Meteor framework;  penetration testing;  real time data},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Karaca2015,
author={Karaca, O. and Kirscher, J. and Maurer, L. and Pelz, G.},
title={Towards simulation based evaluation of safety goal violations in automotive systems},
journal={Forum on Specification and Design Languages},
year={2015},
volume={2015-June},
doi={10.1109/FDL.2014.7119346},
art_number={7119346},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940471260&doi=10.1109%2fFDL.2014.7119346&partnerID=40&md5=a73eb9aa82cb6165a657faaa5746d351},
affiliation={Infineon Technologies AG, Neubiberg, Germany; Bundeswehr University Munich, Neubiberg, Germany},
abstract={With the advent of the ISO 26262 it became crucial to prove that electrical and electronic products delivered into safety-related automotive applications are adequately safe. For this purpose safety goal violations due to random hardware failures need to be evaluated. In order to gain evident results for argumentation within the evaluation, a fault injection based approach is utilized. Potential risk scenarios are initiated by injection of analog and digital faults into the heterogeneous behavioral model which comprises the safety-related hardware. For fault injection in heterogeneous models, we propose analog saboteurs, designed in VHDL-AMS, by which amongst electrical or mechanical, diverse energy domain analog hardware faults may be injected. For demonstration of this approach, a hardware model, comprising lithium-ion battery cells with a cell balancing module and safety-related circuitry is used. © 2014 ECSI.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Idrees2015121,
author={Idrees, Z. and Rashdi, A.},
title={PCA Based Spatial Spectrum Sensing for MIMO Cognitive Radios},
journal={Proceedings - 12th International Conference on Frontiers of Information Technology, FIT 2014},
year={2015},
pages={121-126},
doi={10.1109/FIT.2014.31},
art_number={7118385},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937604916&doi=10.1109%2fFIT.2014.31&partnerID=40&md5=adc42dcef3437567978e10bcdddac0fd},
affiliation={Department of Electrical Engineering, Military College of Signals, NUST, Islamabad, Pakistan},
abstract={In the recent years spatial spectrum sensing become a promising approach due to the convergence of almost all wireless standards to incorporate spatial dimensions and use of multiple antennas at both transmitter and receiver. Keeping in consideration such wireless environment, we proposed a spectrum sensing algorithm based on principal component (PC) of spatially received signals. The proposed algorithm is analyzed under SISO (single input single output), SIMO (single input multiple output) and MIMO (multiple input multiple output) (employing stream multiplexing and Alamouti space time coding) scenario. Performance comparison was done by receiver operating curve (ROC) with other proposed algorithms in literature i.e. Maximum minimum Eigen value (MME). No prior information about the channel or primary user's (PU) signal is assumed. Simulations show the improved performance when info about spatial diversity of PU is incorporated in the proposed PCA. All the algorithms were tested using experimental data while using USRP (universal software radio peripheral) test bed that was controlled by GNU radio software. © 2014 IEEE.},
author_keywords={cognitive radios;  covariance-based detection;  software-defined radio;  spectrum sensing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nieminen2015379,
author={Nieminen, M. and Raty, T.},
title={Adaptable Design for Root Cause Analysis of a Model-Based Software Testing Process},
journal={Proceedings - 12th International Conference on Information Technology: New Generations, ITNG 2015},
year={2015},
pages={379-384},
doi={10.1109/ITNG.2015.67},
art_number={7113502},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936803758&doi=10.1109%2fITNG.2015.67&partnerID=40&md5=1c81fd7dbcfb4d61b285ba368c535c8e},
affiliation={Digital Systems and Services, VTT Technical Research Centre of Finland, Oulu, Finland},
abstract={Model-Based Testing (MBT) is a method for automating software testing and improving fault detection by using an abstracted model of the system under test to systematically generate and execute test cases. One of the main challenges in MBT is the efficient locating of fundamental causes of errors, which can be achieved using root cause analysis. As MBT is being applied to many diverse domains using various tools and tool chains, tool adaptability is an important factor to consider. We define the design for an automated Root Cause Analyzer (RCA) tool adaptable for different MBT testing scenarios and environments. In our RCA design, analysis is configured using rules for evaluating output data of other MBT components. We present an example of RCA application for a case study in mobile telecommunications testing to demonstrate the integration and adapting of our design into an existing tool chain. Within the case study, the RCA tool is successful in locating causes of errors, and is able to interface with MBT components and exploit their data in analysis. The research is based on the constructive method of the related publications and technologies and the results are derived by the implemented RCA tool. © 2015 IEEE.},
author_keywords={Adaptability;  Model-Based Testing;  Root Cause Analysis;  Rule Generation;  Rule-Based Analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chasins20151321,
author={Chasins, S. and Phothilimthana, P.M.},
title={Dicer: A framework for controlled, large-scale web experiments},
journal={WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web},
year={2015},
pages={1321-1326},
doi={10.1145/2740908.2741699},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968547907&doi=10.1145%2f2740908.2741699&partnerID=40&md5=baccac3530d07992dd101b72cefdf941},
affiliation={University of California, Berkeley, United States},
abstract={As dynamic, complex, and non-deterministic webpages pro- liferate, running controlled web experiments on live web- pages is becoming increasingly dificult. To compare algo- rithms that take webpages as inputs, an experimenter must worry about ever-changing webpages, and also about scal- ability. Because webpage contents are constantly changing, experimenters must intervene to hold webpages constant, in order to guarantee a fair comparison between algorithms. Because webpages are increasingly customized and diverse, experimenters must test web algorithms over thousands of webpages, and thus need to implement their experiments ef- ciently. Unfortunately, no existing testing frameworks have been designed for this type of experiment. We introduce Dicer, a framework for running large-scale controlled experiments on live webpages. Dicer's program- ming model allows experimenters to easily 1) control when to enforce a same-page guarantee and 2) parallelize test ex- ecution. The same-page guarantee ensures that all loads of a given URL produce the same response. The frame- work utilizes a specialized caching proxy server to enforce this guarantee. We evaluate Dicer on a dataset of 1,000 real webpages, and find it upholds the same-page guarantee with little overhead.},
author_keywords={JavaScript;  Testing Framework;  Web Algorithm Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Appelt2015,
author={Appelt, D. and Nguyen, C.D. and Briand, L.},
title={Behind an application firewall, are we safe from SQL injection attacks?},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015 - Proceedings},
year={2015},
doi={10.1109/ICST.2015.7102581},
art_number={7102581},
note={cited By 31},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935108505&doi=10.1109%2fICST.2015.7102581&partnerID=40&md5=64af00f025351cf0150c1a660bd0ac43},
affiliation={Interdisciplinary Centre for Security, Reliability and Trust (SnT Centre), University of Luxembourg, Luxembourg},
abstract={Web application firewalls are an indispensable layer to protect online systems from attacks. However, the fast pace at which new kinds of attacks appear and their sophistication require that firewalls be updated and tested regularly as otherwise they will be circumvented. In this paper, we focus our research on web application firewalls and SQL injection attacks. We present a machine learning-based testing approach to detect holes in firewalls that let SQL injection attacks bypass. At the beginning, the approach can automatically generate diverse attack payloads, which can be seeded into inputs of web- based applications, and then submit them to a system that is protected by a firewall. Incrementally learning from the tests that are blocked or passed by the firewall, our approach can then select tests that exhibit characteristics associated with bypassing the firewall and mutate them to efficiently generate new bypassing attacks. In the race against cyber attacks, time is vital. Being able to learn and anticipate more attacks that can circumvent a firewall in a timely manner is very important in order to quickly fix or fine-tune the firewall. We developed a tool that implements the approach and evaluated it on ModSecurity, a widely used application firewall. The results we obtained suggest a good performance and efficiency in detecting holes in the firewall that could let SQLi attacks go undetected. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mondal2015,
author={Mondal, D. and Hemmati, H. and Durocher, S.},
title={Exploring test suite diversification and code coverage in multi-objective test case selection},
journal={2015 IEEE 8th International Conference on Software Testing, Verification and Validation, ICST 2015 - Proceedings},
year={2015},
doi={10.1109/ICST.2015.7102588},
art_number={7102588},
note={cited By 43},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84935106287&doi=10.1109%2fICST.2015.7102588&partnerID=40&md5=9a4b1c3c5f4a74145afb790a752b847c},
affiliation={Department of Computer Science, University of Manitoba, Winnipeg, MB, Canada},
abstract={Test case selection is a classic testing technique to choose a subset of existing test cases for execution, due to the limited budget and tight deadlines. While 'code coverage' is the state of practice among test case selection heuristics, recent literature has shown that 'test case diversity' is also a very promising approach. In this paper, we first compare these two heuristics for test case selection in several real-world case studies (Apache Ant, Derby, JBoss, NanoXML and Math). The results show that neither of the two techniques completely dominates the other, but they can potentially be complementary. Therefore, we next propose a novel approach that maximizes both code coverage and diversity among the selected test cases using NSGA-II multi- objective optimization, and the results show a significant improvement in fault detection rate. Specifically, sometimes this novel approach detects up to 16\%(Ant), 10\%(JBoss), and 14\% (Math) more faults compared to either of coverage or diversity-based approaches, when the testing budget is less than 20\% of the entire test suite execution cost. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{En-Nahli2015698,
author={En-Nahli, L. and Allaoui, H. and Nouaouri, I.},
title={A multi-objective modelling to human resource assignment and routing problem for home health care services},
journal={IFAC-PapersOnLine},
year={2015},
volume={28},
number={3},
pages={698-703},
doi={10.1016/j.ifacol.2015.06.164},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953866958&doi=10.1016%2fj.ifacol.2015.06.164&partnerID=40&md5=5124f296a8efcd3d35d5ae48d2bfc461},
affiliation={Univ. Lille Nord France, Lille, F-59000, France; U. Artois, LGL2A, Béthune, F-62400, France},
abstract={Solving the assignment and routing problem for Home Health Care Services (HHCS) is a very complex task because of the high number and the diversity of actors that participate in the process, and the criteria to be optimized. Given the multi-objective nature of the problem, we propose in this paper a multi-objective approach based on a mixed integer linear programming to support decision making in Home Health Care Services. The purpose is to find an effective feasible working plan for each resource on a daily basis, which ensures the patients' and the caregivers' satisfaction while controlling costs and respecting patients' preferences. To solve this model, we used the solver ILOG CPLEX Optimization Studio. The program has been tested on existing TSP benchmark instances and additional random data. Numerical results show that a good balancing between patients' and caregivers' preferences and travel cost can be reached by using the proposed approach. © 2015, IFAC (International Federation of Automatic Control) Hosting by Elsevier Ltd. All rights reserved.},
author_keywords={Decision support system;  Home health care;  Human resource planning;  Mixed integer linear programming;  Multi-objective optimization;  Routing problem},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen20151,
author={Chen, T.Y. and Kuo, F.-C. and Towey, D. and Zhou, Z.Q.},
title={A revisit of three studies related to random testing},
journal={Science China Information Sciences},
year={2015},
volume={58},
number={5},
pages={1-9},
doi={10.1007/s11432-015-5314-x},
note={cited By 55},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939939769&doi=10.1007%2fs11432-015-5314-x&partnerID=40&md5=a1d77d5d36ad55213e1804ef644d6061},
affiliation={Department of Computer Science and Software Engineering, Swinburne University of Technology, Victoria, 3122, Australia; School of Computer Science, The University of Nottingham Ningbo China, Ningbo, 315100, China; School of Computer Science and Software Engineering, University of Wollongong, Wollongong, NSW  2522, Australia},
abstract={Software testing is an approach that ensures the quality of software through execution, with a goal being to reveal failures and other problems as quickly as possible. Test case selection is a fundamental issue in software testing, and has generated a large body of research, especially with regards to the effectiveness of random testing (RT), where test cases are randomly selected from the software’s input domain. In this paper, we revisit three of our previous studies. The first study investigated a sufficient condition for partition testing (PT) to outperform RT, and was motivated by various controversial and conflicting results suggesting that sometimes PT performed better than RT, and sometimes the opposite. The second study aimed at enhancing RT itself, and was motivated by the fact that RT continues to be a fundamental and popular testing technique. This second study enhanced RT fault detection effectiveness by making use of the common observation that failure-causing inputs tend to cluster together, and resulted in a new family of RT techniques: adaptive random testing (ART), which is random testing with an even spread of test cases across the input domain. Following the successful use of failure-causing region contiguity insights to develop ART, we conducted a third study on how to make use of other characteristics of failure-causing inputs to develop more effective test case selection strategies. This third study revealed how best to approach testing strategies when certain characteristics of the failure-causing inputs are known, and produced some interesting and important results. In revisiting these three previous studies, we explore their unexpected commonalities, and identify diversity as a key concept underlying their effectiveness. This observation further prompted us to examine whether or not such a concept plays a role in other areas of software testing, and our conclusion is that, yes, diversity appears to be one of the most important concepts in the field of software testing. © 2015, Science China Press and Springer-Verlag Berlin Heidelberg.},
author_keywords={adaptive random testing;  diversity;  metamorphic testing;  proportional sampling strategy;  random testing;  software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2015937,
author={Zhang, X. and Shin, K.G.},
title={Cooperation without Synchronization: Practical Cooperative Relaying for Wireless Networks},
journal={IEEE Transactions on Mobile Computing},
year={2015},
volume={14},
number={5},
pages={937-950},
doi={10.1109/TMC.2014.2341611},
art_number={6862056},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926364722&doi=10.1109%2fTMC.2014.2341611&partnerID=40&md5=a51aa4fbaa68d5eaeab644c66ff05dcc},
affiliation={Department of Electrical and Computer Engineering, University of Wisconsin-Madison, Madison, WI, United States; Department of Electrical Engineering and Computer Science, Real-Time Computing Laboratory, University of Michigan, Ann Arbor, MI  48109-2121, United States},
abstract={Cooperative relay aims to realize the capacity of multi-antenna arrays in a distributed manner. However, the symbol-level synchronization requirement among distributed relays limits its use in practice. We propose to circumvent this barrier with a cross-layer protocol called Distributed Asynchronous Cooperation (DAC). With DAC, multiple relays can schedule concurrent transmissions with packet-level (hence coarse) synchronization. The receiver then extracts multiple versions of each relayed packet via a collision-resolution algorithm, thus realizing the diversity gain of cooperative communication. We demonstrate the feasibility of DAC by prototyping and testing it on the GNURadio/USRP software radio platform. To explore its relevance at the network level, we introduce a DAC-based medium access control (MAC) protocol, and a generic approach to integration of the DAC MAC/PHY layer into a typical routing algorithm. Considering the use of DAC for multiple network flows, we analyze the fundamental tradeoff between the improvement in diversity gain and the reduction in multiplexing opportunities. DAC is shown to improve the throughput and delay performance of lossy networks with intermediate link quality. Our analytical results have also been confirmed via network-level simulation with ns-2. © 2002-2012 IEEE.},
author_keywords={asynchronous cooperative relaying;  cooperative communications;  cross-layer design;  diversity-multiplexing tradeoff;  Relay network},
document_type={Article},
source={Scopus},
}

@ARTICLE{Panichella2015358,
author={Panichella, A. and Oliveto, R. and Di Penta, M. and De Lucia, A.},
title={Improving multi-objective test case selection by injecting diversity in genetic algorithms},
journal={IEEE Transactions on Software Engineering},
year={2015},
volume={41},
number={4},
pages={358-383},
doi={10.1109/TSE.2014.2364175},
art_number={6936894},
note={cited By 82},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928157034&doi=10.1109%2fTSE.2014.2364175&partnerID=40&md5=5e42e04910e85827be98bff147e40d9c},
affiliation={Department of Mathematics and Computer Science, University of Salerno, Fisciano, Salerno, 84084, Italy; Department of Bioscience and Territory, University of Molise, Pesche, Isernia, 86090, Italy; Department of Engineering, University of Sannio, Benevento, 82100, Italy},
abstract={A way to reduce the cost of regression testing consists of selecting or prioritizing subsets of test cases from a test suite according to some criteria. Besides greedy algorithms, cost cognizant additional greedy algorithms, multi-objective optimization algorithms, and multi-objective genetic algorithms (MOGAs), have also been proposed to tackle this problem. However, previous studies have shown that there is no clear winner between greedy and MOGAs, and that their combination does not necessarily produce better results. In this paper we show that the optimality of MOGAs can be significantly improved by diversifying the solutions (sub-sets of the test suite) generated during the search process. Specifically, we introduce a new MOGA, coined as DIversity based Genetic Algorithm (DIV-GA), based on the mechanisms of orthogonal design and orthogonal evolution that increase diversity by injecting new orthogonal individuals during the search process. Results of an empirical study conducted on eleven programs show that DIV-GA outperforms both greedy algorithms and the traditional MOGAs from the optimality point of view. Moreover, the solutions (sub-sets of the test suite) provided by DIV-GA are able to detect more faults than the other algorithms, while keeping the same test execution cost. © 1976-2012 IEEE.},
author_keywords={Empirical Studies;  Genetic Algorithms;  Orthogonal Design;  Regression Testing;  Singular Value Decomposition;  Test Case Selection},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Huges20152627,
author={Huges, J.T. and Domínguez-García, A.D. and Poolla, K.},
title={Virtual battery models for load flexibility from commercial buildings},
journal={Proceedings of the Annual Hawaii International Conference on System Sciences},
year={2015},
volume={2015-March},
pages={2627-2635},
doi={10.1109/HICSS.2015.316},
art_number={7070131},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944252434&doi=10.1109%2fHICSS.2015.316&partnerID=40&md5=8b9748f1675f03e610257829020762c3},
affiliation={University of Illinois at Urbana-Champaign, United States; University of California, Berkeley, United States},
abstract={Frequency regulation is becoming increasingly important with deeper penetration of variable generation resources. Flexible loads have been proposed as a low-cost provider of frequency regulation. For example, the flexibility of loads with inherent thermal energy storage resides in their ability to vary their electricity consumption without compromising their end function. In this context, the aggregate flexibility of a collection of diverse residential air-conditioning loads has previously been shown to be well modeled as a virtual battery using first principles load models. This analytical method will not scale to more complex flexible loads such as commercial HVAC systems. This paper presents a method to identify virtual battery model parameters for these more complex flexible loads. The method extracts the parameters of the virtual battery model by stress-testing a detailed software model of the physical system. Synthetic examples reveal the effectiveness of the proposed identification technique. © 2015 IEEE.},
author_keywords={Distributed Energy Resource (DER);  Frequency Regulation;  System ID},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Abdelazim2015,
author={Abdelazim, A. and Hamza, A.M.},
title={Adaptive hierarchical motion estimation optimization for scalable HEVC},
journal={2015 IEEE 8th GCC Conference and Exhibition, GCCCE 2015},
year={2015},
doi={10.1109/IEEEGCC.2015.7060088},
art_number={7060088},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929095991&doi=10.1109%2fIEEEGCC.2015.7060088&partnerID=40&md5=331fb51e6a3424333527ac1d2e0dea64},
affiliation={Dept. Electrical and Computer Engineering, American University of the Middle-East, Eqaila, Kuwait},
abstract={The scalable extension of the HEVC Video Coding Standard (H.265) offers elaborate mechanisms for motion vector prediction and estimation. S-HEVC builds on the standard by extending predictor lists for Coding Unit blocks, utilizing base-layer information in the inference of enhancement-layer Coding Units. The complex, exhaustive search schemes in use can be aided by hierarchical optimizations in subpixel motion estimation, which we propose for slow-moving CUs per frame. In this paper we implement and test an adaptive optimization of motion estimation in the standard (SHM 6.1 software release), based on a statistical analysis of the behavior of subpixel motion vector differentials in each spatial mode per Coding Unit. We propose that the least granular mode (64×64 PEL macro-block in current release) contains sufficient information at subpixel levels to decide best-mode selection, i.e., whether a complete recursion through the inner partitions (higher granularity) is required in the estimation of a CU motion vector. We further propose that subpixel motion estimation overheads can be avoided below a set threshold, given conditions set in base and enhancement layer motion estimation for priorly computed modes in the same CU. Both optimization methods are tested across a diverse set of video sequences, producing negligible quality penalties at for a sizable reduction in encoding time. © 2015 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sakti2015294,
author={Sakti, A. and Pesant, G. and Guéhéneuc, Y.-G.},
title={Instance generator and problem representation to improve object oriented code coverage},
journal={IEEE Transactions on Software Engineering},
year={2015},
volume={41},
number={3},
pages={294-313},
doi={10.1109/TSE.2014.2363479},
art_number={6926828},
note={cited By 51},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925086159&doi=10.1109%2fTSE.2014.2363479&partnerID=40&md5=a0b9727fc95fd640ad4d9503858d671d},
affiliation={Department of Computer and Software Engineering, École Polytechnique de Montral, Montral, QC  H3C 3A7, Canada},
abstract={Search-based approaches have been extensively applied to solve the problem of software test-data generation. Yet, test-data generation for object-oriented programming (OOP) is challenging due to the features of OOP, e.g., abstraction, encapsulation, and visibility that prevent direct access to some parts of the source code. To address this problem we present a new automated search-based software test-data generation approach that achieves high code coverage for unit-class testing. We first describe how we structure the test-data generation problem for unit-class testing to generate relevant sequences of method calls. Through a static analysis, we consider only methods or constructors changing the state of the class-under-test or that may reach a test target. Then we introduce a generator of instances of classes that is based on a family of means-of-instantiation including subclasses and external factory methods. It also uses a seeding strategy and a diversification strategy to increase the likelihood to reach a test target. Using a search heuristic to reach all test targets at the same time, we implement our approach in a tool, JTExpert, that we evaluate on more than a hundred Java classes from different open-source libraries. JTExpert gives better results in terms of search time and code coverage than the state of the art, EvoSuite, which uses traditional techniques. © 2014 IEEE.},
author_keywords={Automatic Test Data Generation;  Diversification Strategy;  Java Testing;  Search Based Software Testing;  Seeding Strategy;  Unit Class Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Holtkamp2015136,
author={Holtkamp, P. and Jokinen, J.P.P. and Pawlowski, J.M.},
title={Soft competency requirements in requirements engineering, software design, implementation, and testing},
journal={Journal of Systems and Software},
year={2015},
volume={101},
pages={136-146},
doi={10.1016/j.jss.2014.12.010},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921656831&doi=10.1016%2fj.jss.2014.12.010&partnerID=40&md5=60aa037d623cc34b18aad0f887393623},
affiliation={Mattilanniemi 2, Agora Building, Jyväskylä, FI-40014, Finland; Ruhr West University of Applied Sciences, Mülheim, 45407, Germany},
abstract={Global software development changes the requirements in terms of soft competency and increases the complexity of social interaction by including intercultural aspects.While soft competency is often seen as crucial for the success of global software development projects, the concrete competence requirements remain unknown. Internationalization competency represents one of the first attempts to structure and describe the soft competence requirements for global software developers. Based on the diversity of tasks, competence requirements will differ among the various phases of software development. By conducting a survey on the importance of internationalization competences for the different phases of global software development, we identified differences in terms of competence importance and requirements in the phases. "Adaptability" (of one's working style) and "Cultural Awareness" were the main differences. "Cultural Awareness" distinguishes requirements engineering and software design from testing and implementationwhile "Adaptability" distinguishes implementation and software design from requirements engineering and testing. © 2014 Elsevier Inc. All rights reserved.},
author_keywords={Competency requirements;  Global software development;  Soft competency},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mao201523,
author={Mao, C. and Xiao, L. and Yu, X. and Chen, J.},
title={Adapting ant colony optimization to generate test data for software structural testing},
journal={Swarm and Evolutionary Computation},
year={2015},
volume={20},
pages={23-36},
doi={10.1016/j.swevo.2014.10.003},
note={cited By 43},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921319684&doi=10.1016%2fj.swevo.2014.10.003&partnerID=40&md5=2752f968443e77dd9689e2bed71c6a60},
affiliation={School of Software and Communication Engineering, Jiangxi University of Finance and Economics, Nanchang, 330013, China; State Key Laboratory of Software Engineering, Wuhan University, Wuhan, 430072, China; School of Computer Science and Telecommunication Engineering, Jiangsu University, Zhenjiang, Jiangsu, 212013, China},
abstract={In general, software testing has been viewed as an effective way to improve software quality and reliability. However, the quality of test data has a significant impact on the fault-revealing ability of software testing activity. Recently, search-based test data generation has been treated as an operational approach to settle this difficulty. In the paper, the basic ACO algorithm is reformed into discrete version so as to generate test data for structural testing. First, the technical roadmap of combining the adapted ACO algorithm and test process together is introduced. In order to improve algorithm's searching ability and generate more diverse test inputs, some strategies such as local transfer, global transfer and pheromone update are defined and applied. The coverage for program elements is a special optimization objective, so the customized fitness function is constructed in our approach through comprehensively considering the nesting level and predicate type of branch. To validate the effectiveness of our ACO-based test data generation method, eight well-known programs are utilized to perform the comparative analysis. The experimental results show that our approach outperforms the existing simulated annealing and genetic algorithm in the quality of test data and stability, and is comparable to particle swarm optimization-based method. In addition, the sensitivity analysis on algorithm parameters is also employed to recommend the reasonable parameter settings for practical applications. © 2014 Elsevier B.V. All rights reserved.},
author_keywords={Ant colony optimization;  Branch coverage;  Experimental evaluation;  Fitness function;  Meta-heuristic search;  Test data generation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pei2015840,
author={Pei, Y. and Christi, A. and Fern, X. and Groce, A. and Wong, W.-K.},
title={Taming a fuzzer using delta debugging trails},
journal={IEEE International Conference on Data Mining Workshops, ICDMW},
year={2015},
volume={2015-January},
number={January},
pages={840-843},
doi={10.1109/ICDMW.2014.58},
art_number={7022682},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936855360&doi=10.1109%2fICDMW.2014.58&partnerID=40&md5=3cc0c716d333ff812383c0aa43adc1c6},
affiliation={School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR  97330, United States},
abstract={Fuzzers, or random testing tools, are powerful tools for finding bugs. A major problem with using fuzzersis that they often trigger many bugs that are already known. The fuzzer taming problem addresses this issue by ordering bug-triggering random test cases generated by a fuzzer such that test cases exposing diverse bugs are found early in the ranking. Previous work on fuzzer taming first reduces each test case into a minimal failure-inducing test case using delta debugging, then finds the ordering by applying the Furthest Point First algorithm over the reduced test cases. During the delta debugging process, a sequence of failing test cases is generated (the 'delta debugging trail'). We hypothesize that these additional failing test cases also contain relevant information about the bug and could be useful for fuzzertaming. In this paper, we propose to use these additional failing test cases generated during delta debugging to help tame fuzzers. Our experiments show that this allows for more diverse bugs to be found early in the furthest point first ranking. © 2014 IEEE.},
author_keywords={Automated Testing;  Fuzzer Taming;  Fuzzing;  Software Testing;  Test-Case Reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brey2015,
author={Brey, J.A. and Geer, I.W. and Mills, E.W. and Nugnes, K.A. and Asokan, A.},
title={Energizing students' earth science literacy},
journal={2014 Oceans - St. John's, OCEANS 2014},
year={2015},
doi={10.1109/OCEANS.2014.7003014},
art_number={7003014},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921725212&doi=10.1109%2fOCEANS.2014.7003014&partnerID=40&md5=acfc992a89a17679357ba86917c86997},
affiliation={Education Program, American Meteorological Society, Washington, DC, United States},
abstract={The future state of the world's oceans greatly depends on the leaders of tomorrow. That's why increasing current students' ocean and overall Earth science literacy is so important and a primary goal of the American Meteorological Society (AMS). To this end, the AMS has created a suite of introductory, undergraduate-level courses that have already engaged approximately a hundred thousand students in the Earth sciences. AMS Ocean Studies, AMS Weather Studies, and AMS Climate Studies have been developed by the AMS with support from NSF, NOAA, and NASA and are designed to be adaptable to traditional, hybrid, or online instructional settings. Course components include a comprehensive textbook (ebook only for AMS Climate Studies; printed or ebook options for AMS Ocean and Weather Studies), eInvestigations Manual, course website, faculty website, and a faculty resource CD. Instructors can use these materials in any combination that best suits their needs. The 3rd edition Ocean Studies textbook features topics such as ocean acidification, harmful algal blooms, the use of ocean currents as an alternative energy source, along with a detailed look at the essential role the ocean plays in Earth's climate system. The textbook also reviews the latest technology for monitoring ocean properties including floats and gliders and cabled ocean observatories. The newest version of the climate studies textbook, Our Changing Climate, is currently under revision and was released for the fall 2014 semester. A greater focus on climate change issues as well as new chapters detailing human and ecosystem vulnerabilities to climate change, energy and geopolitical issues, and climate change denial are a few highlights in the new text. Additionally, new findings from the IPCC AR5 report and the USGCRP National Climate Assessment will be included along with mitigation and adaptation solutions to climate change issues. Updated yearly, the eInvestigations Manuals contain 30 laboratory activities and innovatively connects with a third online component, Current Ocean/Weather/Climate Studies, via the RealTime Ocean/Weather/Climate Portals. These online investigations reference data from NOAA, NOS, reports from the IPCC, USGCRP and contain real-world data from other lead scientific organizations. The RealTime Ocean/Weather/Climate Portals is an all-inclusive webpage that provides links to numerous external sources that further engage students. Instructor support materials are available with each course and include a faculty CD that contains a faculty manual with learning objectives and suggestions for course implementation, as well as investigations manual answer forms compatible with any course management system, test bank questions and answers, textbook images, and PowerPoint® presentations for each chapter. The investigations manual answer forms are files compatible with Respondus®, test-generating software for which many institutions are licensed (answer forms are also provided in Respondus® format). The faculty member has the option of delivering questions through their course management system to allow automatic scoring and immediate results for their students. This feature allows for full integration to a college's e-learning environment. AMS Ocean Studies also offers a unique way to study marine science through the use of Google Earth-based animations and other similar data visualization tools, such as NOAA View that allows students to view satellite, model, and other data and export high-resolution images. These tools bring oceanography to life! The AMS believes that students are truly motivated to learn when they can relate to the topic. That is why use of current, real-world data is vital to all three of the courses. A few examples include the 2011 Japan earthquake/tsunami event, the 2012 record-low Arctic sea ice, and Superstorm Sandy. In all of these instances, online activities were created as these events were occurring; students learned the science as it was happening! AMS Ocean Studies, AMS Weather Studies, and AMS Climate Studies can be taught by experienced science faculty or those new to teaching the subject matter. Mentoring by AMS-trained course instructors is available to all new instructors. These courses aim to interest all students in the geosciences and increase scientific literacy through the use of real-world data. © 2014 IEEE.},
author_keywords={climate;  course;  diversity;  ocean;  online;  water},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hebert2015,
author={Hebert, P.W., Sr. and Elliot, A.C. and Graves, A.R.},
title={NASA data acquisition system software development for rocket propulsion test facilities},
journal={31st AIAA Aerodynamic Measurement Technology and Ground Testing Conference},
year={2015},
page_count={17},
doi={10.2514/6.2015-2561},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088357621&doi=10.2514%2f6.2015-2561&partnerID=40&md5=cfbee63383a90958cf8a9e7d9c23cde2},
affiliation={NASA, Stennis Space Center, EA-34MS  39529, United States; Camgian Microsystems, Test Operations Contractor, Stennis Space Center, B-8306MS  39529, United States},
abstract={Current NASA propulsion test facilities include Stennis Space Center in Mississippi, Marshall Space Flight Center in Alabama, Plum Brook Station in Ohio, and White Sands Test Facility in New Mexico. Within and across these NASA centers, a diverse set of data acquisition systems exist, each with different hardware and software platforms. To address the varying needs of these different facilities, the NASA Data Acquisition System (NDAS) software suite was designed to operate and control many critical aspects of rocket engine testing. NDAS combines real-time data visualization, data recording to a variety formats, short-term and long-term acquisition system calibration capabilities, test stand configuration control, and a variety of data post-processing capabilities. There are also data stream conversion functions that can be used to translate test facility data streams to and from downstream systems, including engine customer systems. The software is designed to be modular to accommodate test facility and test program variability. For example, one test program may require real-time displays and data recording; others may require more complex data stream conversion, measurement filtering, or test stand configuration management. The NDAS suite allows test facility users to choose which components to incorporate based on their specific needs. An additional novel feature is that the NDAS code is written primarily in LabVIEW™4, a graphical, data-flow driven language. Although LabVIEW™ is a general-purpose programming language, using it for large-scale software development is relatively rare. The NDAS software suite also integrates a new, advanced development framework called the Actor Framework. Utilization of the Actor Framework provides a level of code reuse and extensibility that has previously been difficult to achieve using LabVIEW™. The NDAS team also uses the agile development for all of the development activities. © 2015, American Institute of Aeronautics and Astronautics Inc, AIAA.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Iqbal2015,
author={Iqbal, J. and Ponsonby, W.},
title={Implementation of an occupational health management system in the majnoon oil field in the Republic of Iraq},
journal={Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference, ADIPEC 2015},
year={2015},
doi={10.2118/177877-ms},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088070033&doi=10.2118%2f177877-ms&partnerID=40&md5=cfbd39e85eeb52a7a1b5c75a4f48c2a5},
affiliation={Shell, Netherlands},
abstract={This paper reviews a project that required the implementation of a comprehensive occupational health system in a brown field (named Majnoon) in the Basra Province of Iraq. This paper reports outcomes from the project and builds on an earlier published paper describing planning and preparation for the project. A multidisciplinary team from Shell Limited was responsible for implementation of the project comprising of Health, Industrial Hygienist, HSE and Social Performance team. The project required implementation of a comprehensive occupational health management system where none had existed previously. This was required in a diverse work force that was from different ethnic cultural backgrounds with very different experiences of the oil & gas industry. The Key elements of the occupational health programme were: 1. Medical Emergency Response a. First aid and defibrillation in four minutes b. Tier 2 medical response from a paramedic or doctor in one hour for advanced care c. Admission to an approved hospital in four hours d. Referral to specialist centre as required 2. Health Hazard Management a. Carry out a health risk assessment for all activities by Industrial Hygienist b. Identify assess control and arrange recovery measures for all health hazards to reduce risks ALARP c. Assign and track all remedial actions to close out d. Set up a HACCP system to manage food hygiene and safety e. Set up a system to manage the risk of Legionella f. Set up and roll out a system to manage working in a hot climate (temperatures up to 55 C in summer) g. Set up systems to manage health of workers during the Holy Month of Ramadan 3. Fitness to work a. Programme to check all international staff are fit to come and work in arduous conditions in Iraq (OGUK standard or equivalent) b. Task specific medicals for food handlers, drivers, crane operators, BA wearers, Emergency Response team workers 4. Drug and alcohol program a. Develop and educate workers on a program to control use of drugs and alcohol in the work place b. Set up system for carrying out drug and alcohol test as required 5. Workers Welfare a. Ensure all worker camps meet minimum requirements b. Air conditioned bed rooms c. No over crowding d. Sufficient and good quality food (HACCP system in place) e. Exercise and leisure facilities available f. Facilities to communicate with families g. Fatigue management for workers, adequate sleep time, good sleep hygiene 6. Social Performance a. Ensure health projects included in the health programme are fit for purpose b. Improve health capacity in local communities and in Basra Province c. Working in conjunction with NGOs and the Ministry of Health to bring health care to communities which previously had none d. Inclusion of Health education programs in local schools and training of Lady Health Workers e. Recruitment and training of national doctors to take over health management as a part of the company's commitment to local content 7. Incident Reporting and Investigation a. All occupational illnesses and injuries should be reported. They are classified according severity under the OSHA system. Targets are set for Total Recordable Case frequency, and the number of man hours worked and Lost time Incident were tracked. All serious incidents are investigated with root cause analysis. Copyright 2015, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tsukernikov20151601,
author={Tsukernikov, I. and Shubin, I. and Tikhomirov, L. and Nevenchannaya, T.},
title={Software quality testing for calculation of outdoor noise},
journal={Euronoise 2015},
year={2015},
pages={1601-1603},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080888814&partnerID=40&md5=424a3c6da9a7a701f68c5b3342cfd816},
affiliation={Research Institute of Building Physics, Russian Academy of Architecture and Building Science, Moscow, Russian Federation; Moscow State University of Printing Arts, Moscow, Russian Federation},
abstract={Currently a wide range of software tools are used for calculations in different fields of building physics. This takes place for building acoustics as well. However such diversity in the absence of clearly defined requirements to software products leads to the fact that calculation results may differ significantly even in the programs that are using the same calculation methods and the same initial data. In this paper an attempt is made of compared analysis of three documents to be intended for testing of software quality for calculation of outdoor noise: Finish Standard NT ACOU 107-2001, German Standard DIN 45687:2006-05 and Draft International standard ISO/DIS 17534-1. The information of Russian Standard GOST R 56234-2014 stating quality requirements and test conditions for such software products is given also. Copyright © (2015) by EAA-NAG-ABAV, All rights reserved},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Guise2015317,
author={Guise, M. and Loureiro, I. and Rodrigues, N. and Teixeira, S.},
title={Testing a human thermal software using field investigation from an industrial plant},
journal={Occupational Safety and Hygiene III},
year={2015},
pages={317-320},
doi={10.1201/b18042},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054755017&doi=10.1201%2fb18042&partnerID=40&md5=614b91a2c1477591e79ad4eab48a5f2c},
affiliation={Department of Production and Systems, School of Engineering, University of Minho, Portugal},
abstract={The analysis of a thermal comfort or to a thermal stress situation can be achieved using diverse techniques, models and numerical simulations. The main purpose of this paper is to use a human thermal software to analyze the human response to different thermal environmental conditions. The human thermal model is based on equations of heat and mass transfer and, based on the parameters of thermal environment that influence comfort, it can predict the temperatures and humidity at the human body and clothing. A simulation was done using the experimental data obtained from a field investigation at an industrial plant. Results indicate that the software can differentiate body parts concerning its thermal behavior according to their adaptability and in all cases, the temperature values tend to stabilization. A verification of the coefficients of heat transfer between the cloth and the environment is required being pointed as future work. © 2015 Taylor & Francis Group, London.},
document_type={Book Chapter},
source={Scopus},
}

@BOOK{Mistrik20151,
author={Mistrik, I. and Soley, R.M. and Ali, N. and Grundy, J. and Tekinerdogan, B.},
title={Software quality assurance: In large scale and complex software-intensive systems},
journal={Software Quality Assurance: In Large Scale and Complex Software-intensive Systems},
year={2015},
pages={1-373},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017565269&partnerID=40&md5=1c3e14ac6b2989dc7994d60b259d99b6},
affiliation={Heidelberg, Germany; Object Management Group, Needham, MA, United States; University of Brighton, Brighton, United Kingdom; Swinburne University of Technology, Hawthorn, VIC, Australia; Information Technology Group, Wageningen University, Wageningen, Netherlands},
abstract={Software Quality Assurance in Large Scale and Complex Software-intensive Systems presents novel and high-quality research related approaches that relate the quality of software architecture to system requirements, system architecture and enterprise-architecture, or software testing. Modern software has become complex and adaptable due to the emergence of globalization and new software technologies, devices and networks. These changes challenge both traditional software quality assurance techniques and software engineers to ensure software quality when building today (and tomorrow's) adaptive, context-sensitive, and highly diverse applications. This edited volume presents state of the art techniques, methodologies, tools, best practices and guidelines for software quality assurance and offers guidance for future software engineering research and practice. Each contributed chapter considers the practical application of the topic through case studies, experiments, empirical validation, or systematic comparisons with other approaches already in practice. Topics of interest include, but are not limited, to: quality attributes of system/software architectures; aligning enterprise, system, and software architecture from the point of view of total quality; design decisions and their influence on the quality of system/software architecture; methods and processes for evaluating architecture quality; quality assessment of legacy systems and third party applications; lessons learned and empirical validation of theories and frameworks on architectural quality; empirical validation and testing for assessing architecture quality. Focused on quality assurance at all levels of software design and development. Covers domain-specific software quality assurance issues e.g. for cloud, mobile, security, context-sensitive, mash-up and autonomic systems. Explains likely trade-offs from design decisions in the context of complex software system engineering and quality assurance. Includes practical case studies of software quality assurance for complex, adaptive and context-critical systems. © 2016 Elsevier Inc. All rights reserved.},
document_type={Book},
source={Scopus},
}

@CONFERENCE{Lee2015,
author={Lee, A.G. and Wu, M.G. and Abramsonz, M.},
title={Modeling of complex and diverse aircraft trajectories with the trajectory synthesizer generalized profile interface},
journal={AIAA Modeling and Simulation Technologies Conference, 2015},
year={2015},
page_count={12},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013498907&partnerID=40&md5=f25bd1728e92b4f52491307f0ea6321e},
affiliation={NASA Ames Research Center, Aerospace Simulation Research & Development Branch, MS 210-8, Moffett Field, CA  94035, United States; University Affliated Research Center, Moffett Field, CA  94035, United States; University of California, Santa Cruz, Moffett Field, CA  94035, United States},
abstract={A flexible interface for prescribing complex and diverse aircraft trajectories called Gen- Prof was developed and tested on the Center TRACON Automation System (CTAS) software research platform. This interface models various pilot procedures, supports common flight constraints imposed by air traffic control, and allows the building of trajectories in accordance with new flight procedures. In addition to serving trajectory generation for air traffic management decision support tools and concepts, the GenProf methodology has enabled a variety of research and validation tasks to be performed. This paper describes the interface and details these applications. © 2015, American Institute of Aeronautics and Astronautics Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yanmei20158598,
author={Yanmei, J. and Tao, Z. and Hongen, Z. and Liping, Z. and Yufeng, H.},
title={Payloads integration process on chiese space station},
journal={Proceedings of the International Astronautical Congress, IAC},
year={2015},
volume={11},
pages={8598-8607},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991510595&partnerID=40&md5=78db0079e48ec360d0978970144abba1},
affiliation={Technology and Engineering Center for Space Utilization, Chinese Academy of Sciences, Beijing, 100094, China},
abstract={The objective of this paper is to introduce a new concept for the process of payloads integration and to provide a more practical template for PDs and Pls' payloads integration on Chinese space station. Payloads integration characteristic of Chinese space station is given. There will be many scientific experiments on the space station. These scientific experimental devices will be operational and changeable. Science modules or scientific samples will be delivered by space shuttle transportation. Science program will be planned and integrated in a short time and transported to space station. The life cycle of the Chinese space station is longer than any Chinese spacecraft before. These devices which will stay on the station until the end must be reliable and endurable. These modules which will be change in a short time must be developed quickly and adaptive for the launch and space environment. Traditional payload integration process of Chinese spacecraft is introduced. Many payloads have been integrated on Chinese manned spacecraft. They are earth or atmosphere observational payloads, science experimental payloads, new technology validation payloads etc. When payloads will be integrated on spacecraft, the first consideration is development flow. Hardware, software, reliability, test, science research and ground support development have to be considered parallel. The drawing up method of development flow for the Chinese space station payload integration is based on the analysis above and the traditional payload integration methods. The development flow of payloads on Chinese space station will be more complex and challenging. More flexible and diverse development flow is required in order to meet different needs of the long life cycle and short life cycle payloads, More reasonable and effective development templates are needed for the process of payload planning, integration and operations. It is suggested that scientific research process should be independent which device development. Integration process should be different with each other for short life cycle and long life cycle payloads.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{AliEdris2015,
author={Ali Edris, M.A. and Haggag Amin, M. and Al Benali, K. and Shinde, A.L. and Ghadimipour, A. and Perumalla, S.V. and Hartley, L.J. and Baxter, S.},
title={Implementation of coupled 3D geomechanics and discrete fracture network (DFN) models in field development optimisation: A case study from Carbonate Reservoir, Abu Dhabi},
journal={Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference, ADIPEC 2015},
year={2015},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979920505&partnerID=40&md5=c8978d60472daed37d735afd4be42ab2},
affiliation={ADCO, United Arab Emirates; Baker Hughes Inc., United States; AMEC, United Kingdom},
abstract={An onshore oilfield in Abu Dhabi has complex faulted structure of early Cretaceous carbonate reservoir. The porosity in these reservoirs ranges up to 25%. These reservoirs are overlaid by dense carbonate which acts as a cap rock. The field has been produced for 35 years with peripheral water and gas injection scheme to maintain the reservoir pressure. The objectives of this geomechanical study were to verify the impact of in-situ stresses and rock mechanical properties on reservoir performance during production/injection activities, as well as to evaluate the mechanical and hydraulic responses of natural fractures and faults to fluid flow within the reservoir. In addition, evaluation of cap rock integrity and fault seal analyses are also considered as necessary risk assessment inputs for field development program. Extensive rock mechanics testing program has been carried out on reservoir, the interlay dense and cap rock to quantify the contrast in mechanical rock properties and their response to in-situ stresses. Diverse set of data have been integrated including geological, geophysical, petrophysical, reservoir and drilling data to build robust 1D and 3D Geomechanical models. Natural fractures were characterized using core inspection, image logs and CT-Scans which have been calibrated to production logging (PLT) and well tests. Multi-attribute analysis with inputs from petrophysical, structural and geophysical data has been used to build and calibrate the 3D DFN model. The developed 3D Geomechanical model was coupled with the 3D reservoir dynamic model in order to predict the changes in total stresses due to reservoir pressure changes. Furthermore, this model was integrated with the 3D DFN model to help understand the impact of production/injection activities on the mechanical behaviour of the fractures, fault seal and cap rock integrity. This paper discusses the role of coupled 3D Geomechanics and the DFN modeling in evaluating and quantifying the mechanical behaviour of reservoir rock matrix, fractures, faults and cap rock that can influence the decisions on optimum field development planning. Copyright ©2015, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang201580,
author={Zhang, T. and Gao, J. and Aktouf, O.-E.-K. and Uehara, T.},
title={Test model and coverage analysis for location-based mobile services},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2015},
volume={2015-January},
pages={80-86},
doi={10.18293/SEKE2015-199},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969785228&doi=10.18293%2fSEKE2015-199&partnerID=40&md5=704b653e17648de2c1fdb41e5ebf968b},
affiliation={School of Software and Microelectronic, Northwest Polytechnical University, Xi'an, China; Department of Computer Engineering, San Jose State University, San Jose, United States; LCIS Laboratory, University of Grenoble, France; Software Innovation Laboratories, Fujitsu Laboratories LTD, Japan},
abstract={Location-based services (LBS) are very important mobile app services, which provide diverse mobility services for mobile users anywhere and anytime. This brings new demands, issues, and challenges in mobile application testing. Today, mobile applications provide location-based service functions based on dynamic location contexts, mobile users and their travel patterns to deliver location-based mobile data, and service actions. Current software testing methods do not consider location-based validation coverage. Hence, there is a lack of research results addressing location-based mobile application testing. This paper focuses on mobile LBS testing. A novel test object model is proposed for quality validation of location-based mobile information services. In addition, the related test coverage metrics are also presented. These metrics can be useful for test engineers in designing test cases. A case study based on student testers is reported to demonstrate the potential application of the proposed model. Copyright © 2015 by KSI Research Inc. and Knowledge Systems Institute Graduate School.},
author_keywords={Component;  Location-based service;  Mobile app;  Mobile testing;  Test coverage;  Test metrics;  Test model},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bishop2015297,
author={Bishop, P.},
title={Modeling the impact of testing on diverse programs},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9337},
pages={297-309},
doi={10.1007/978-3-319-24255-2_22},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969784589&doi=10.1007%2f978-3-319-24255-2_22&partnerID=40&md5=7ed611a0ca8dc9b0525bed93a1a05e99},
affiliation={City University, Adelard LLP, London, United Kingdom},
abstract={This paper presents a model of diverse programs that assumes there are a common set of potential software faults that are more or less likely to exist in a specific program version. Testing is modeled as a specific ordering of the removal of faults from each program version. Different models of testing are examined where common and diverse test strategies are used for the diverse program versions. Under certain assumptions, theory suggests that a common test strategy could leave the proportion of common faults unchanged, while diverse test strategies are likely to reduce the proportion of common faults. A review of the available empirical evidence gives some support to the assumptions made in the fault-based model. We also consider how the proportion of common faults can be related to the expected reliability improvement. © Springer International Publishing Switzerland 2015.},
author_keywords={Diverse test strategies;  Multi-version programs;  Software diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liang201540,
author={Liang, Y.},
title={OCCI and TTCN-3: Towards a standardized cloud quality assessment framework},
journal={CLOSER 2015 - 5th International Conference on Cloud Computing and Services Science, Proceedings},
year={2015},
pages={40-48},
doi={10.5220/0005451900400048},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969761335&doi=10.5220%2f0005451900400048&partnerID=40&md5=03de55f674b9da8610f872526222e337},
affiliation={Bwcon GmbH, Breitscheidstrasse 4, Stuttgart, 70174, Germany},
abstract={Impacting basically all types of IT infrastructures The Cloud is one of the most important evolving IT paradigms. A standard-based Cloud quality and compliance assessment framework will be therefore of utmost importance. Bringing together the Open Cloud Computing Interface OCCI and the ETSI standardized test specification language TTCN-3 and related test methodologies this paper is going to demonstrate initial steps towards such a framework. Taking into account the diversity of Cloud infrastructures, of service providers, and related architectural, harmonization and standardization effort this approach is mainly motivated by studying Cloud-related effort of the NIST Cloud Computing Program and the ETSI Cloud Standards Coordination (CSC). Reflecting the "Cloudiness" of the Software Defined Network (SDN) and ETSI Network Functions Virtualization (NFV) this paper is considering these initiatives as necessary elements of the scope of every future standardized Cloud quality assessment framework as well.},
author_keywords={Cloud Quality Assessment;  Cloud Standards;  Network Functions Virtualization;  OCCI;  Software Defined Network;  Standardized Testing;  TTCN-3},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Aljanad2015339,
author={Aljanad, A. and Mohamed, A. and Shareef, H.},
title={Impact study of plug-in electric vehicles on electric power distribution system},
journal={2015 IEEE Student Conference on Research and Development, SCOReD 2015},
year={2015},
pages={339-344},
doi={10.1109/SCORED.2015.7449352},
art_number={7449352},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84966601459&doi=10.1109%2fSCORED.2015.7449352&partnerID=40&md5=fc6ce68cd00a2bb1526d0e9e94b931da},
affiliation={Department of Electrical, Electronic and System Engineering, University Kebangsaan Malaysia, Bangi, Malaysia; Department of Electrical Engineering, United Arab Emirates University, United Arab Emirates},
abstract={This paper presents the impact study of plug-in electric vehicles (PHEVs) on a power distribution system and investigates how it would affect the distribution systems from different perspectives. PHEVs are modelled as storage energy systems in which its dispatch mode will follow the load shape patterns for charging behavior. The IEEE 37 test system with PHEVs is simulated in OpenDss, and a control algorithm is developed using the Matlab program. The study considers the impact of PHEVs on system voltage profile, power loss and line loading by taking into account low and high penetration level of PHEVs. An evaluation is also made on the diversification of charging behavior over a 24 hour period by considering the existing system load profile. © 2015 IEEE.},
author_keywords={Battery Charger;  Plug-In Hybrid Electric Vehicles;  Smart Grids},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chauvel2015132,
author={Chauvel, F. and Song, H. and Fleurey, F.},
title={Diversity: A Heuristic to Improve Robustness of Self-Adaptive Cloud Architectures},
journal={Proceedings - 2015 IEEE/ACM 8th International Conference on Utility and Cloud Computing, UCC 2015},
year={2015},
pages={132-141},
doi={10.1109/UCC.2015.29},
art_number={7431404},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965058033&doi=10.1109%2fUCC.2015.29&partnerID=40&md5=93774a91f495e4124163fd69404aa65b},
affiliation={SINTEF ICT, Oslo, Norway},
abstract={In complex biological systems, the hypothesis that bio-diversity contributes to stability or robustness is an active debate. The FP7 DIVERSIFY project tests whether this hypothesis holds for software systems, and explores the use of diversity as a heuristic to increase robustness in self-adaptive architectures. Inspired by Ecology, we present here a technique to evaluate diversity of software architectures and we report preliminary investigations of its correlation with robustness. Given existing cloud-based architectures, we artificially inject predefined levels of diversity and measure the resulting robustness. In four out of our five industrial case studies, a higher diversity appeared correlated with a higher robustness. © 2015 IEEE.},
author_keywords={cloud-computing;  correlation;  diversity;  models;  robustness;  service architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bilyi2015,
author={Bilyi, D. and Gerling, D.},
title={New test bench for vehicle power network with outstanding accuracy, resolution and data rate},
journal={28th International Electric Vehicle Symposium and Exhibition 2015, EVS 2015},
year={2015},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962859138&partnerID=40&md5=ad74d929d8261dbc0effb14471dfbc60},
affiliation={FEAAM GmbH, Neubiberg, D-85577, Germany; Institute of Electrical Drives, University of Federal Defense Munich, Neubiberg, D-85577, Germany},
abstract={New test bench for vehicle power network with outstanding accuracy, resolution and data rate allows a high quality and at the same time low cost research on typical test cases and situations that appear in commercial vehicles. All different signals e.g. load profiles, speed ramps or other process variables could be run in real time. All test cases are automated i.e. more than one variable is changing during the test case. Differentiation between test cases is made through the software solution and appears on the fly with no need for additional hardware or software preparation. The measurements are triggered and acquisied time synchronously that makes the precise post processing within 100 μS possible. The results provide the information about potentially critical points of power network and components. The test bench is widely expandable. Potential of the test bench for possible diverse test cases is near to limitless.},
author_keywords={Data rate;  Generator;  High accuracy;  Power net;  Resolution;  Test bench;  Vehicle power network;  Voltage stability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rao201556,
author={Rao, P. and Dubey, A. and Virdi, G.},
title={Crowdsourced testing for enterprises: Experiences},
journal={CEUR Workshop Proceedings},
year={2015},
volume={1519},
pages={56-57},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962591236&partnerID=40&md5=0a0df13b8cc965b28de6261a717f3c9d},
affiliation={Accenture Digital, Bangalore, India; Accenture Technology Labs, Bangalore, India},
abstract={Crowdsourced testing is an emerging phenomenon in software testing which utilizes benefits of crowdsourcing for software testing. It brings cost and quality advantage with faster delivery of software products. Among all the software development activities, testing is one of the primary activities considered for crowdsourcing. Certain types of testing demand for testers from diverse ethnicity, culture, or geography; crowdsourced testing has become a viable option to address such needs. This paper presents some of the initial attempts done at Accenture to adopt crowdsourced testing. We present some of the software development projects where testing had been crowdsourced using a crowd based platform and yielded promising results. We also showcase how crowd wisdom, gleaned through a sentiment analysis performed on the users' feedback, have helped in improving software. © 2015 for the individual papers by the papers' authors.},
author_keywords={Crowdsourcing;  Software development;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Okamura2015416,
author={Okamura, H. and Dohi, T.},
title={Analysis of optimal restart policies for software systems},
journal={Journal of Japan Industrial Management Association},
year={2015},
volume={66},
number={4E},
pages={416-425},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960360867&partnerID=40&md5=90b705179f308ac43f42d4d93314fb5d},
affiliation={Hiroshima University, Japan},
abstract={The restart is one of the typical environmental diversity techniques in dependable computing, and is quite effective to rejuvenate software systems at low cost. In this paper we generalize the seminal results on restart mechanisms by van Moorsel and Wolter (2004, 2006) and analyze optimal restart policies under more general conditions. We further develop a statistical algorithm to estimate the optimal restart policies from the empirical data of task processing time.},
author_keywords={Fault tolerance;  Restart policy;  Software systems;  Statistical estimation;  Total time on test},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jablonski2015,
author={Jablonski, A.M. and Showalter, D. and Boutilier, J.T. and Rajakareyar, P.},
title={Recent advances in assembly, integration and testing (AIT) at the david Florida laboratory–an update},
journal={AIAA SPACE 2015 Conference and Exposition},
year={2015},
page_count={15},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960330593&partnerID=40&md5=1028ef9f8381e708b77ae92787e45b32},
affiliation={David Florida Laboratory (DFL), Canadian Space Agency, 3701 Carling Avenue, P.O.Box 11490, Station ‘H’, Ottawa, ON  K2H 8S2, Canada; Department of Physics, Carleton University, Ottawa, ON  K1S 5B6, Canada; Department of Mechanical and Aerospace Engineering, Carleton University, Ottawa, ON  K1S 5B6, Canada},
abstract={The David Florida Laboratory (DFL) of the Canadian Space Agency is the Canadian national space qualification and testing facility, which offers back-to-back Assembly, Testing and Integration services for Canadian Space Program, Canadian and international industry and other space agencies and organizations. The DFL is an ISO 9001:2008 registered facility and consists of three qualification facilities: Structural Qualification Facilities (SQF), Thermal Qualification Facilities (TQF) and Radio Frequency Qualification Facilities (RFQF). In addition, there are three important areas of operation: Client Support and Business Development (CSBD), Life Cycle Support (LCS) and AIT Development. Technically-advanced Assembly, Integration and Testing (AIT) services of satellites and any other space systems are essential to meet the operational requirements in a specific space environment, including planetary environments for planetary exploration systems Highly specialized AIT technologies and environmental testing facilities are necessary and important for the assembly, integration and testing (AIT) of any space hardware. AIT facilities at DFL were constructed first in the 1970s for the Canadian Communications Technology Satellite (CTS) program. Since then, AIT services have evolved, with continuous augmentation and upgrade to meet new requirements of diverse and space systems serving the needs of various national and international space programs. In addition, DFL offers its AIT services to private clients from Canada and outside world. A continuous test technology development program ensures that the facilities will remain state-of-the-art in measurement technology, provide efficiency and performance improvements, and meet new demands of evolving test requirement for present and future space programs. Some examples of recent qualification and testing campaigns at DFL are presented in the context of risk-reduction. This paper also presents some examples of the recent AIT test technologies studies, as well as some implemented test technologies to increase the reliability level of tested space hardware. © 2015, American Institute of Aeronautics and Astronautics Inc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ardrey2015763,
author={Ardrey, D. and Gimler, G. and Pippitt, M.},
title={Spatial diversity combining using blind estimation techniques},
journal={Proceedings of the International Telemetering Conference},
year={2015},
volume={82},
pages={763-766},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958521782&partnerID=40&md5=c682b1e480ae35022fe2af9f3c0230db},
affiliation={MIT Lincoln Laboratory, Lexington, MA, United States},
abstract={This paper proposes a spatial diversity combining approach by which spatially diverse telemetry signals from multiple antennas are combined before they are demodulated. The combined signal is guaranteed to at least replicate and in many cases improve upon the performance of any single antenna. By taking advantage of blind channel estimation, the combined signal can be computed as a time varying weighted sum of digital I and Q samples from multiple antennas. Multiple antenna combining is enabled by improved computation capability, high speed network connectivity, and accurate clock synchronization. The algorithm will be demonstrated at the Reagan Test Site (RTS), whose modernization program encompasses multiple antenna sites with network capability and a state of the art software defined radio back end. This paper details the spatial diversity combining algorithm and discusses its merits and challenges.},
author_keywords={Best source selection;  Blind estimation;  Multiple antenna combining;  Reagan test site;  Spatial diversity combining;  Telemetry},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vahldick2015523,
author={Vahldick, A. and Mendes, A.J. and Marcelino, M.J.},
title={Analysing the enjoyment of a serious game for programming learning with two unrelated higher education audiences},
journal={Proceedings of the European Conference on Games-based Learning},
year={2015},
volume={2015-January},
pages={523-531},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84955167930&partnerID=40&md5=86186f24150405ea15e03e4d221ef741},
affiliation={CISUC, Department of Informatics Engineering, University of Coimbra, Coimbra, Portugal},
abstract={Serious games are generally considered a good alternative to improve motivation to learn. A game should have meaningful mechanics and elements to involve the players and to keep their attention. Each person has his own motivation to play games. This leads to different types of players and consequently to the need of different types of elements to keep them engaged. Understanding the player's behaviour is an important issue to improve support in the game. While the designer of a serious game focus mainly on its educational value, it is also important to incorporate fun elements in the game. We are developing a serious game aimed to help the development of basic computer programming skills. Our challenge is to create a game that provides effective self-learning support while keeping the student engaged in the game's missions. This paper deals with the game features and their evaluation. The game has missions covering basic concepts of computer programming: Sequence of actions, variable manipulation and conditionals. In this research, we are concerned with evaluating the game enjoyment. We conducted two experiments with groups of students from completely distinct knowledge areas. First, we applied the game in a Portuguese Education Sciences class that included essentially female students. Next, we performed a play test in a Brazilian first semester of Software Engineering undergraduate class composed essentially by male students. We believed that we would identify a larger set of needs if we diversified the types of players that tried the game. Consequently, if the game satisfies their needs, we would reduce the possibility of failure of the game. The EGameFlow instrument was used to measure the enjoyment. Findings show that the game entertains the players even if the programming tasks are hard. However, some improvements are necessary to foster self-learning. Finally, the paper presents future work that aims to include new challenges and missions in the game and to improve the enjoyment.},
author_keywords={Computer programming;  Educational game;  Enjoyment;  Programming learning;  Serious game},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Miller2015392,
author={Miller, R. and Jakobsen, E. and Kalfayan, L. and Troelsen, B.},
title={Exploitation of Danish Sector North Sea Chalk Field: A Case Study of the Hess South Arne Field - Acid Stimulation Design, Optimization and Execution},
journal={SPE - European Formation Damage Conference, Proceedings, EFDC},
year={2015},
volume={2015-January},
pages={392-413},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951122087&partnerID=40&md5=b268be817102dc470566c0c14e774e60},
affiliation={Hess Denmark APS, Denmark; Hess Corporation, Denmark},
abstract={The South Arne Field, located in the Danish sector of the North Sea, has been producing oil and gas since 1999 with an active waterflood since 2000. Producers and injectors are primarily horizontal with sliding sleeve completions targeting both the Ekofisk and Tor chalk formations. South Arne is currently in exploitation modewith a new (Phase III) development program. Phase III includes two new platform installations, a two-kilometer umbilical bundle and an eleven-well drilling program targeting the northern extension of the field. This paper discusses the first five Phase III wells, focusing on stimulation design, lessons learned from the fluid testing, experiences, and the continuous improvement of the operational execution process. New producer and injector stimulation designs are generally based on a pad-acid procedure in which a cross-linked gel pad is followed by an in-situ cross-linked hydrochloric (HCl) acid treatment, to create viscosity contrast for diversion and differential acid fracture etching for sustained conductivity. The acid fracture treatment is then followed by a closed fracture acidizing (CFA) step to enhance near-wellbore fracture conductivity. For wells completed in zones that have the potential of short-circuiting to neighboring wells, lower rate and smaller volume HCl matrix treatments are pumped. The Phase III stimulations to date have been conducted through ball drop sliding sleeves, with the number of zones ranging from 10 to 14. Establishing Phase III success has required extensive QA/QC testing to optimize the stimulation fluid systems and, in particular, the acid corrosion inhibition program. Use of real-time bottom-hole temperature measurements during stimulation treatments has enabled optimization of corrosion inhibitor loadings, resulting in both an environmental benefit and financial savings. The stimulation treatments are performed simultaneously with drilling operations, therefore extensive work was required to ensure that stimulation and drilling activities are safely executed without interference. Both platforms were initially designed to optimize productive time during stimulation operations, however as the project evolved, thorough risk evaluations pointed to the need to modify operating plans. Rigorous marine and vessel audits were performed to ensure that stimulation vessels could operate safely and effectively from both a marine and stimulation job execution standpoint. The design, testing and advance preparation for the South Arne Phase III program has resulted in safe and effective treatment execution. Copyright 2015, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sahinoglu2015147,
author={Sahinoglu, M. and Incki, K. and Aktas, M.S.},
title={Mobile application verification: A systematic mapping study},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9159},
pages={147-163},
doi={10.1007/978-3-319-21413-9_11},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948952720&doi=10.1007%2f978-3-319-21413-9_11&partnerID=40&md5=8348202ac51d34046dd610cd38ef9b0b},
affiliation={TUBITAK BILGEM Center for Software Test and Quality Assessment, Kocaeli, Turkey; Department of Computer Engineering, Adana Science and Technology University, Seyhan/Adana, Turkey; Department of Computer Engineering, Yıldız Technical University, Istanbul, Turkey},
abstract={The proliferation of mobile devices and applications has seen an unprecedented rise in recent years. Application domains of mobile systems range from personal assistants to point-of-care health informatics systems. Software development for such diverse application domains requires stringent and well defined development process. Software testing is a type of verification that is required to achieve more reliable system. Even though, Software Engineering literature contains many research studies that address challenging issues in mobile application development, we could not have identified a comprehensive literature review study on this subject. In this paper, we present a systematic mapping of the Software Verification in the field of mobile applications. We provide definitive metrics and publications about mobile application testing, which we believe will allow fellow researchers to identify gaps and research opportunities in this field. © Springer International Publishing Switzerland 2015.},
author_keywords={Literature review;  Mobile application;  Software testing;  Systematic mapping;  Verification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Romli2015149,
author={Romli, R. and Sulaiman, S. and Zamli, K.Z.},
title={Improving the reliability and validity of test data adequacy in programming assessments},
journal={Jurnal Teknologi},
year={2015},
volume={77},
number={9},
pages={149-163},
doi={10.11113/jt.v77.6201},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947259710&doi=10.11113%2fjt.v77.6201&partnerID=40&md5=353e0f623fabc2442bdd704341855337},
affiliation={School of Computing(SOC), College of Arts and Sciences, Universiti Utara Malaysia UUM, Sintok, Kedah  06010, Malaysia; Universiti Teknologi Malaysia UTM, Johor Bahru, Johor  81310, Malaysia; Universiti Malaysia Pahang, Lebuhraya Tun Razak, Gambang, Kuantan, Pahang  26300, Malaysia},
abstract={Automatic Programming Assessment (or APA) has recently become a notable method in assisting educators of programming courses to automatically assess and grade students’ programming exercises as its counterpart; the typical manual tasks are prone to errors and lead to inconsistency. Practically, this method also provides an alternative means of reducing the educators’ workload effectively. By default, test data generation process plays an important role to perform a dynamic testing on students’ programs. Dynamic testing involves the execution of a program against different inputs or test data and the comparison of the results with the expected output, which must conform to the program specifications. In the software testing field, there have been diverse automated methods for test data generation. Unfortunately, APA rarely adopts these methods. Limited studies have attempted to integrate APA and test data generation to include more useful features and to provide a precise and thorough quality program testing. Thus, we propose a framework of test data generation known as FaSt-Gen covering both the functional and structural testing of a program for APA. Functional testing is a testing that relies on specified functional requirements and focuses the output generated in response to the selected test data and execution, Meanwhile, structural testing looks at the specific program logic to verify how it works. Overall, FaSt-Gen contributes as a means to educators of programming courses to furnish an adequate set of test data to assess students’ programming solutions regardless of having the optimal expertise in the particular knowledge of test cases design. FaSt-Gen integrates the positive and negative testing criteria or so-called reliable and valid test adequacy criteria to derive desired test data and test set schema. As for the functional testing, the integration of specification-derived test and simplified boundary value analysis techniques covering both the criteria. Path coverage criterion guides the test data selection for structural testing. The findings from the conducted controlled experiment and comparative study evaluation show that FaSt-Gen improves the reliability and validity of test data adequacy in programming assessments. © 2015 Penerbit UTM Press. All rights reserved.},
author_keywords={Automatic programming assessment (APA);  Functional testing;  Negative testing;  Positive testing;  Structural testing;  Test data adequacy;  Test data generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ludi2015157,
author={Ludi, S. and Jordan, S.},
title={A JBrick: Accessible robotics programming for visually impaired users},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2015},
volume={9177},
pages={157-168},
doi={10.1007/978-3-319-20684-4_16},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947213448&doi=10.1007%2f978-3-319-20684-4_16&partnerID=40&md5=0f27b64454c42572880bf1afe5d20c9a},
affiliation={Department of Software Engineering, Department of Computer Science, Rochester Institute of Technology, Rochester, United States},
abstract={Despite advances in assistive technology, challenges remain in precollege computer science outreach and university programs for visually impaired students. The use of robotics has been popular in pre-college classrooms and outreach programs, including those that serve underrepresented groups. This paper describes the specific accessibility features implemented in software that provides an accessible Lego Mindstorms NXT programming environment for teenage students who are visually impaired. JBrick is designed to support students with diverse visual acuity and who use needed assistive technology. Field tests over several days showed that JBrick has the potential to accommodate students who are visually impaired as they work together to program Lego Mindstorms NXT robots. © Springer International Publishing Switzerland 2015.},
author_keywords={Accessibility;  Robotics;  Visual impairment},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tang20151248,
author={Tang, C.K. and Pietryka, T.S. and Federico, P.A. and Williams, J.C.},
title={Limerick BWR turbine control and protection system upgrade success},
journal={9th International Topical Meeting on Nuclear Plant Instrumentation, Control, and Human-Machine Interface Technologies, NPIC and HMIT 2015},
year={2015},
volume={2},
pages={1248-1257},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946145957&partnerID=40&md5=180b1c394f604bc34f2d876f8c9cd75d},
affiliation={Westinghouse Electric Company LLC, 1000 Westinghouse Drive, Cranberry Township, PA  16066, United States; Exelon Nuclear, 4300 Winfield Road, Warrenville, Jonathan, IL  60555, United States},
abstract={Westinghouse and Exelon have successfully designed and commissioned a digital electro-hydraulic control (DEHC) system for the Limerick Generating Station Unitl boiling water reactor (BWR), to perform the turbine control, protection, and pressure control functions. The DEHC replacement of the existing analog control system addressed the issues of equipment obsolescence and reliability. The distributed control system (DCS) platform utilized is used in modem control applications in power plants world-wide, but this was the first application for control, protection, and monitoring of the main turbine and pressure control in a BWR. A dynamic model of the Limerick BWR was used for development of control system logics, alarms, operator interface graphics, testing of application software, and factory acceptance testing. The integration of the dynamic model and the control software running in the DCS development system allowed verification of system design functions, operator reviews and validation of system operability, operating procedure development, and testing of the system prior to system installation. Additionally, the existing turbine mechanical overspeed trip devices in the turbine front standard were replaced with a separate and diverse online testable 2-out-of-3 overspeed trip system and speed sensors. A pair of redundant hydraulic trip blocks was used for tripping the turbine, to provide online testability of the trip solenoids and trip block operability. The installation of certain equipment and cable runs were completed with the plant online, with no impact on the outage schedule. The demolition of existing analog equipment, control panel modifications in the main control room (MCR), front standard modifications, and modification acceptance testing were completed during the normal 2014 outage. Power ascension testing with the new system installed was successfully completed, and lessons learned will be applied to future units. Key aspects of the project that facilitated this success will be discussed and presented in this paper; including the project planning, project management/risks, software and hardware development, testing program, simulator, installation, and commissioning.},
author_keywords={BWR;  DCS;  DEHC;  Limerick},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zamli2015223,
author={Zamli, K.Z. and Mohd Hassin, M.H. and Al-Kazemi, B.},
title={tReductSA – test redundancy reduction strategy based on simulated annealing},
journal={Communications in Computer and Information Science},
year={2015},
volume={513},
pages={223-236},
doi={10.1007/978-3-319-17530-0_16},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942693246&doi=10.1007%2f978-3-319-17530-0_16&partnerID=40&md5=87dae3149782ed04c7f01fd9ccbd6fcf},
affiliation={Universiti Malaysia Pahang, Gambang, Malaysia; College of Computer and Information Systems, Umm Al-Qura University, Makkah, Saudi Arabia},
abstract={Software testing relates to the process of accessing the functionality of a program against some defined specifications. To ensure conformance, test engineers often generate a set of test cases to validate against the user requirements. When dealing with large line of codes (LOCs), there are potentially issue of redundancies as new test cases may be added and old test cases may be deleted during the whole testing process. In order to address this issue, we have developed a new strategy, called tReductSA, to systematically minimize test cases for testing consideration. Unlike existing works which rely on the Greedy approaches, our work adopts the random sequence permutation and optimization algorithm based on Simulated Annealing with systematic merging technique. Our benchmark experiments demonstrate that tReductSA scales well with existing works (including that of GE, GRE and HGS) as far as optimality is concerned. On the other note, tReductSA also offers more diversified solutions as compared to existing work. © Springer International Publishing Switzerland 2015.},
author_keywords={Optimization;  Search based software engineering;  Simulated Annealing;  Test suite redundancy reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guise2015317,
author={Guise, M. and Loureiro, I. and Rodrigues, N. and Teixeira, S.},
title={Testing a human thermal software using field investigation from an industrial plant},
journal={Occupational Safety and Hygiene III - Selected Extended and Revised Contributions from the International Symposium on Safety and Hygiene},
year={2015},
pages={317-320},
doi={10.1201/b18042-64},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941662244&doi=10.1201%2fb18042-64&partnerID=40&md5=bbd8ea86f5aee025545ed8d4fb090e92},
affiliation={Department of Production and Systems, School of Engineering, University of Minho, Portugal},
abstract={The analysis of a thermal comfort or to a thermal stress situation can be achieved using diverse techniques, models and numerical simulations. The main purpose of this paper is to use a human thermal software to analyze the human response to different thermal environmental conditions. The human thermal model is based on equations of heat and mass transfer and, based on the parameters of thermal environment that influence comfort, it can predict the temperatures and humidity at the human body and clothing. A simulation was done using the experimental data obtained from a field investigation at an industrial plant. Results indicate that the software can differentiate body parts concerning its thermal behavior according to their adaptability and in all cases, the temperature values tend to stabilization. A verification of the coefficients of heat transfer between the cloth and the environment is required being pointed as future work. © 2015 Taylor & Francis Group, London.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mikhaylov2015279,
author={Mikhaylov, D. and Konev, V. and Starikovskiy, A. and Khabibullin, T. and Kuzminova, A.},
title={Development of the software for recognition of pathologies in the images obtained by the wireless endoscopic capsule and the atlas of abnormalities},
journal={International Journal of Soft Computing},
year={2015},
volume={10},
number={4},
pages={279-286},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936991871&partnerID=40&md5=77e03b2c1219ed3849bf27648e983dd6},
affiliation={Moscow Engineering Physics Institute (MEPhI), National Research Nuclear University, Moscow, Russian Federation},
abstract={The wireless capsule endoscopy is a method of non-invasive examination of gastrointestinal tract widely used for screening purposes. The review of images taken by capsule usually takes several hours and requires a high degree of concentration by a doctor. To reduce the time and enhance the accuracy of data reviewing the special software for image processing is being developed. It is able to recognize abnormalities of the digestive tract. The study provides the information about the proposed software architecture and its implementation, the function it is to perform, user-friendly report interface development and the proposal for the creation of the digestive diseases' atlas that will provide information useful for both the professional community of physicians and for the developers of software for detection of pathologies and their symptoms in the images of gastrointestinal tract. The study provides the results of the software testing showing 94% accuracy. The research is underway to increase the accuracy of the algorithm to diversify the pathologies that can be identified in the images obtained by the wireless endoscopic capsule and to implement the atlas of diseases. © Medwell Journals, 2015.},
author_keywords={Accuracy;  Disease atlas;  Ref channel;  Report interface;  Software architecture implementation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2015113,
author={Zhang, N. and Wu, B. and Bao, X.},
title={Automatic generation of test cases based on multi-population genetic algorithm},
journal={International Journal of Multimedia and Ubiquitous Engineering},
year={2015},
volume={10},
number={6},
pages={113-122},
doi={10.14257/ijmue.2015.10.6.11},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936763591&doi=10.14257%2fijmue.2015.10.6.11&partnerID=40&md5=b8760fc5881b0fae21c639b7700caa18},
affiliation={Zhejiang Sci-Tech University, Hangzhou, China},
abstract={The design of automatic generation technology of test case is an important part of the software test automation implementation, having an important guiding role in testing of late work, which is the fundamental guarantee to improve the reliability of software. In this paper, considering the lack of adequacy of control flow testing, using the data flow testing as the test adequacy criteria, and then on the basis of the single population genetic algorithm search efficiency is not high, combining with previous methods on the improvement of the genetic algorithm, introducing the concept of multi-population, and then designs a kind of improved parallel evolutionary algorithm (IPEA) based on multipopulation is used to automatically generate test cases. The algorithm defined the concept of external pressure which as the degree of competition between individuals. Fully considering the influence of coverage, branch condition and degree of competition between individual species of three aspects, and give different weights, we design a fitness function to evaluate the merits of the individual species. Experiments show that the IPEA has obviously improved in convergence speed, search time, coverage, scale of the test cases on key performance than the single population genetic algorithm and random search algorithm. © 2015 SERSC.},
author_keywords={Diversity of the population;  Evolutionary algorithm;  Multi-population;  Test case generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ng2015210,
author={Ng, C.W.W. and Liu, J. and Chen, R. and Xu, J.},
title={Physical and numerical modeling of an inclined three-layer (silt/gravelly sand/clay) capillary barrier cover system under extreme rainfall},
journal={Waste Management},
year={2015},
volume={38},
number={1},
pages={210-221},
doi={10.1016/j.wasman.2014.12.013},
note={cited By 55},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933674723&doi=10.1016%2fj.wasman.2014.12.013&partnerID=40&md5=b59b032c7ed81eef81934dfed7189d3c},
affiliation={Guangzhou HKUST Fok Ying Tung Research Institute, Guangzhou, 511458, China; Department of Civil and Environmental Engineering, The Hong Kong University of Science and Technology, Hong Kong Special Administrative Region, Hong Kong; Key Laboratory of Ministry of Education for Geomechanics and Embankment Engineering, Hohai University, Nanjing, 210098, China; Shenzhen Key Laboratory of Urban and Civil Engineering for Disaster Prevention and Mitigation, Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, 518055, China},
abstract={As an extension of the two-layer capillary barrier, a three-layer capillary barrier landfill cover system is proposed for minimizing rainfall infiltration in humid climates. This system consists of a compacted clay layer lying beneath a conventional cover with capillary barrier effects (CCBE), which is in turn composed of a silt layer sitting on top of a gravelly sand layer. To explore the effectiveness of the new system in minimizing rainfall infiltration, a flume model (3.0. m. ×. 1.0. m. ×. 1.1. m) was designed and set up in this study. This physical model was heavily instrumented to monitor pore water pressure, volumetric water content, surface runoff, infiltration and lateral drainage of each layer, and percolation of the cover system. The cover system was subjected to extreme rainfall followed by evaporation. The experiment was also back-analyzed using a piece of finite element software called CODE_BRIGHT to simulate transient water flows in the test. Based on the results obtained from various instruments, it was found that breakthrough of the two upper layers occurred for a 4-h rainfall event having a 100-year return period. Due to the presence of the newly introduced clay layer, the percolation of the three-layer capillary barrier cover system was insignificant because the clay layer enabled lateral diversion in the gravelly sand layer above. In other words, the gravelly sand layer changed from being a capillary barrier in a convention CCBE cover to being a lateral diversion passage after the breakthrough of the two upper layers. Experimental and back-analysis results confirm that no infiltrated water seeped through the proposed three-layer barrier system. The proposed system thus represents a promising alternative landfill cover system for use in humid climates. © 2014 Elsevier Ltd.},
author_keywords={Capillary barrier;  Extreme rainfall;  Flume model;  Landfill cover;  Suction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sadowski2015,
author={Sadowski, T. and Balawender, T. and Golewski, P.},
title={Technological aspects of manufacturing and numerical modelling of clinch-adhesive joints},
journal={SpringerBriefs in Applied Sciences and Technology},
year={2015},
volume={138},
page_count={59},
doi={10.1007/978-3-319-14902-8_1},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930616937&doi=10.1007%2f978-3-319-14902-8_1&partnerID=40&md5=90ad45b5c885109c07e87106bcf8f9f5},
affiliation={Department of Civil Engineering, University of Technology, Lublin, Poland; Department of Materials Forming and Processing, RzeszÃ³w University of Technology Faculty of Mech Eng and Aeronautics, Rzeszow, Poland; Department of Solid Mechanics, Lublin University of Technology Faculty of Civil and Architecture, Lublin, Poland},
abstract={The analysis of technological factors that influence the plastic forming of the clinch joint and its strength is described. Basing on literature date and authors’ studies, it was found that quality and strength of the joint are mainly related to plastic deformation of joined sheets and sheets’ adhesion on the joint interface. These conclusions determined the subsequent investigations of the clinching process. The clinch joints made of one or two different materials with diversified plastic and strength properties were experimentally tested. The basic samples were single overlap clinch joints with one clinch bulge. The joints mechanical response was analysed in the pull and peer tests. The obtained results showed the relation of the clinch joinability to the exponent of materials strain hardening curves. The good quality and good strength joints were obtained for materials with low value of strain hardening curve exponent ‘n’ in the range of about 0,14–0,22. The book includes also experimental results and numerical calculations of clinch joint forming process with different sheets’ interface preparation (degreased, greased and separated by thin PTFE film). The investigations covered the effect of the above specified contact conditions on the clinch joint geometrical parameters and the shear strength. The obtained results showed crucial role of interface friction conditions, apart from geometrical parameters, on the joint shear strength. The hybrid joints combining clinching and adhesive bonding techniques were also investigated. Application of the hybrid clinch-bonded joints lead to the significant increase of the joint quality and strength. It was found that an adhesive plays the role of grease when the joint is clinched and then, after curing it causes great and advantages adhesion between sheets, stopping their displacement and bending. The book includes: (a) very wide experimental testing program with analysis of the obtained results, (b) advanced finite element numerical model of the hybrid joints behavior with application of 2 degradation processes: in the adhesive layer and the plastic adherends. © The Author(s) 2015.},
author_keywords={Clinch-adhesive joint;  Clinching;  Finite element analysis;  Friction;  Hybrid joint;  Strain hardening exponent},
document_type={Article},
source={Scopus},
}

@ARTICLE{Foreman2015101,
author={Foreman, V.L. and Favaró, F.M. and Saleh, J.H. and Johnson, C.W.},
title={Software in military aviation and drone mishaps: Analysis and recommendations for the investigation process},
journal={Reliability Engineering and System Safety},
year={2015},
volume={137},
pages={101-111},
doi={10.1016/j.ress.2015.01.006},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922151638&doi=10.1016%2fj.ress.2015.01.006&partnerID=40&md5=622283dc695b360fd67362aeb83c92b8},
affiliation={School of Aerospace Engineering, Georgia Institute of Technology, Atlanta, GA, United States; School of Computing Science, University of Glasgow, United Kingdom},
abstract={Software plays a central role in military systems. It is also an important factor in many recent incidents and accidents. A safety gap is growing between our software-intensive technological capabilities and our understanding of the ways they can fail or lead to accidents. Traditional forms of accident investigation are poorly equipped to trace the sources of software failure, for instance software does not age in the same way that hardware components fail over time. As such, it can be hard to trace the causes of software failure or mechanisms by which it contributed to accidents back into the development and procurement chain to address the deeper, systemic causes of potential accidents. To identify some of these failure mechanisms, we examined the database of the Air Force Accident Investigation Board (AIB) and analyzed mishaps in which software was involved. Although we have chosen to focus on military aviation, many of the insights also apply to civil aviation. Our analysis led to several results and recommendations. Some were specific and related for example to specific shortcomings in the testing and validation of particular avionic subsystems. Others were broader in scope: for instance, we challenged both the investigation process (aspects of) and the findings in several cases, and we provided recommendations, technical and organizational, for improvements. We also identified important safety blind spots in the investigations with respect to software, whose contribution to the escalation of the adverse events was often neglected in the accident reports. These blind spots, we argued, constitute an important missed learning opportunity for improving accident prevention, and it is especially unfortunate at a time when Remotely Piloted Air Systems (RPAS) are being integrated into the National Airspace. Our findings support the growing recognition that the traditional notion of software failure as non-compliance with requirements is too limited to capture the diversity of roles that software plays in military and civil aviation accidents. The identification of several specific mechanisms by which software contributes to accidents can help populate a library of patterns and triggers of software contributions to adverse events, a library which in turn can be used to help guide better software development, better coding, and better testing to avoid or eliminate these particular patterns and triggers. Finally, we strongly argue for the examination of software 's causal role in accident investigations, the inclusion of a section on the subject in the accident reports, and the participation of software experts in accident investigations. © 2015 Elsevier Ltd. All rights reserved.},
author_keywords={Accident investigation;  Military aviation;  Mishap;  Remotely piloted air systems (rpas);  Software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ferreira2015613,
author={Ferreira, A.M.S. and De Oliveira Fontes, C.H. and Cavalcante, C.A.M.T. and Marambio, J.E.S.},
title={Pattern recognition as a tool to support decision making in the management of the electric sector. Part II: A new method based on clustering of multivariate time series},
journal={International Journal of Electrical Power and Energy Systems},
year={2015},
volume={67},
pages={613-626},
doi={10.1016/j.ijepes.2014.12.001},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919935351&doi=10.1016%2fj.ijepes.2014.12.001&partnerID=40&md5=94688c0036fb675275d448f271e98652},
affiliation={Program of Industrial Engineering, Polytechnic School, Federal University of Bahia, Salvador, BA, Brazil; Norsul LTD, Salvador, BA, Brazil},
abstract={This work presents a new method for the clustering and pattern recognition of multivariate time series (CPT-M) based on multivariate statistics. The algorithm comprises four steps that extract essential features of multivariate time series of residential users with emphasis on seasonal and temporal profile, among others. The method was successfully implemented and tested in the context of an energy efficiency program carried out by the Electric Company of Alagoas (Brazil) that considers, among others, the analysis of the impact of replacing refrigerators in low-income consumers' homes in several towns located within the state of Alagoas (Brazil). The results were compared with a well-known method of time series clustering already established in the literature, the Fuzzy C-Means (FCM). Unlike C-means models of clustering, the CPT-M method is also capable to obtain directly the number of clusters. The analysis confirmed that the CPT-M method was capable to identify a greater diversity of patterns, showing the potential of this method in better recognition of consumption patterns considering simultaneously the effect of other variables in additional to load curves. This represents an important aspect to the process of decision making in the energy distribution sector. © 2014 Elsevier Ltd. All rights reserved.},
author_keywords={Clustering;  Electricity distribution;  Multivariate time series;  Pattern},
document_type={Article},
source={Scopus},
}

@ARTICLE{Binder201440,
author={Binder, R.V. and Legeard, B. and Kramer, A.},
title={Model-based testing: Where does it stand? : MBT has positive effects on efficiency and effectiveness, even if it only partially fulfills high expectations},
journal={Queue},
year={2014},
volume={13},
number={1},
pages={40-48},
doi={10.1145/2716276.2723708},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921905086&doi=10.1145%2f2716276.2723708&partnerID=40&md5=36251cbe147886e66d83b2b427a1535c},
affiliation={System Verification, Germany; University of Franche-Comté, France; Sepp.med gmbh, Germany},
abstract={You have probably heard about MBT (model-based testing), but like many software-engineering professionals who have not used MBT, you might be curious about others' experience with this test-design method. From mid-June 2014 to early August 2014, we conducted a survey to learn how MBT users view its efficiency and effectiveness. The 2014 MBT User Survey, a follow-up to a similar 2012 survey (http:// robertvbinder.com/real-users-of-model-based-testing/), was open to all those who have evaluated or used any MBT approach. Its 32 questions included some from a survey distributed at the 2013 User Conference on Advanced Automated Testing. Some questions focused on the efficiency and effectiveness of MBT, providing the figures that managers are most interested in. Other questions were more technical and sought to validate a common MBT classification scheme. As there many significantly different MBT approaches, beginners are often confused. A common classification scheme could help users understand both the general diversity and specific approaches. © 2014 ACM.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rao2014,
author={Rao, E. and Remer, J. and Bauer, D.},
title={A model for development, transition and technology transfer leading to commercialization of security technology},
journal={Proceedings - International Carnahan Conference on Security Technology},
year={2014},
volume={2014-October},
number={October},
doi={10.1109/CCST.2014.6987007},
art_number={6987007},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931077320&doi=10.1109%2fCCST.2014.6987007&partnerID=40&md5=add506b63e16d536f251556c6326ef3d},
affiliation={Department of Homeland Security, Transportation Security Administration, Washington, DC, United States; Department of Homeland Security Science and Technology, Atlantic City, NJ, United States; University of Connecticut, Washington, DC, United States},
abstract={Various agencies of the United States' Federal Government develop technologies in support of their missions based on requirements derived from a number of sources-among them end users (customers) of those agencies, as well as by internal Research and Development staff. Transition of these technologies to customers within and outside those agencies generally takes place after an extensive program of research, development, test and evaluation in laboratory and operational environments. A large number of different technology transition /transfer processes are available to realize the journey from Lab to operational deployment. These processes form a basis of choices available to the federal program manager form which he/she can choose, dependent upon various technical, political and environmental requirements developed over the life cycle of the end product. This paper will outline the diverse processes available to move forward the transition/ transfer of aviation security technologies, from requirements development, assessment and evaluation, certification, qualification and approval to end-user application. Alternative routes for technologies not ready for deployment will be proposed and illustrated. The ultimate goal being to find exit criteria for the technology through commercialization activities, or through classical application of Federal Technology transfer law, regulation, executive orders, and best practices of the public and private sector. The exit criterion for developed and transitioned technologies is its successful acquisition and application is driven by budget and policy constraints. Transition of the developed technologies to commercialization and transfer to private sector and application is constrained by budget, policy and political considerations. This paper will outline the process from requirements development, assessment and evaluation, certification, qualification, transition and technology transfer for end user application. A successful technology transition, technology transfer and commercialization model with illustrative examples will be detailed. The paper will describe some ways the current process of transitioning R&D to commercial product might be improved for different purchasers toward the end of continuous product improvement. © 2014 IEEE.},
author_keywords={Commercialization;  Security Technology;  Technology Transfer},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Romli201484,
author={Romli, R. and Sulaiman, S. and Zamli, K.Z.},
title={Test data generation framework for Automatic Programming Assessment},
journal={2014 8th Malaysian Software Engineering Conference, MySEC 2014},
year={2014},
pages={84-89},
doi={10.1109/MySec.2014.6985993},
art_number={6985993},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929347083&doi=10.1109%2fMySec.2014.6985993&partnerID=40&md5=799b5ca6501106716983405bfbd539ce},
affiliation={School of Computing, College of Arts and Sciences, Universiti Utara Malaysia, UUM Sintok, Kedah, 01000, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, UTM Skudai, Johor, 81310, Malaysia; Faculty of Computer System and Engineering, Universiti Malaysia Pahang, Lebuhraya Tun Razak, Gambang, Kuantan, 26300, Malaysia},
abstract={Automatic Programming Assessment (APA) has recently become a significant method in assisting educators of programming courses to automatically mark and grade students' programming as its counterpart; the typical manual tasks are prone to errors and leading to inconsistency. By default, test data generation process plays an important role to perform a dynamic testing on students' programs. In software testing field, there have been diverse automated methods for test data generation. Unfortunately, APA seldom adopts these methods. Merely limited studies have attempted to integrate APA and test data generation to include more useful features and to provide a precise and thorough quality of program testing. Thus, we propose a framework of test data generation so-called FaSt-Gen to cover both the functional and structural testing of a program for APA. It aims to assist the lecturers of programming courses to furnish an adequate set of test data to assess students' programming solutions regardless of having the concrete expertise in the particular knowledge of test cases design. FaSt-Gen integrates the positive and negative testing criteria (or reliable and valid test adequacy criteria) to derive desired test data and test set schema. The findings from the conducted experiment depict that FaSt-Gen improves the reliability and validity test data adequacy in programming assessments. © 2014 IEEE.},
author_keywords={Automatic Programming Assessment (APA);  functional testing;  negative testing;  positive testing;  structural testing;  test data generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Grace20141,
author={Grace, P. and Barbosa, J. and Pickering, B. and Surridge, M.},
title={Taming the interoperability challenges of complex IoT systems},
journal={Proceedings of the 1st ACM Workshop on Middleware for Context-Aware Applications in the IoT, M4IOT 2014 - In conjunction with ACM/IFIP/USENIX ACM International Middleware Conference},
year={2014},
pages={1-6},
doi={10.1145/2676743.2676744},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943256899&doi=10.1145%2f2676743.2676744&partnerID=40&md5=35a8766da54aa8ceef937d80ac91130c},
affiliation={IT Innovation, University of Southampton, UK, IT Innovation Centre, Gamma House Enterprise Road, Southampton SO167NS, UK},
abstract={The Internet of Things is characterised by extreme heterogeneity of communication protocols and data formats; hence ensuring diverse devices can interoperate with one another remains a significant challenge. Model-driven development and testing solutions have been proposed as methods to aid software developers achieve interoperability compliance in the face of this increasing complexity. However, current approaches often involve complicated and domain specific models (e.g. web services described by WSDL). In this paper, we explore a lightweight, middleware independent, model-driven development framework to help developers tame the challenges of composing IoT services that interoperate with one another. The framework is based upon two key contributions: i) patterns of interoperability behaviour, and ii) a software framework to monitor and reason about interoperability success or failure. We show using a case-study from the FI-WARE Future Internet Service domain that this interoperability framework can support non-expert developers address interoperability challenges. We also deployed tools built atop the framework and made them available in the XIFI large-scale FI-PPP test environment. Copyright 2014 ACM.},
author_keywords={Architectural patterns;  Internet of Things;  Interoperability;  Model-driven software engineering;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vanderveen2014516,
author={Vanderveen, P. and Janzen, M. and Tappenden, A.F.},
title={A web service test generator},
journal={Proceedings - 30th International Conference on Software Maintenance and Evolution, ICSME 2014},
year={2014},
pages={516-520},
doi={10.1109/ICSME.2014.85},
art_number={6976129},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931032701&doi=10.1109%2fICSME.2014.85&partnerID=40&md5=2af9576e44449a3a8cc6c41a4d938bed},
affiliation={Department of Computing Science, King's University, Edmonton, Canada},
abstract={An automated process for generating test inputs for web services from a WSDL is presented. A grammatical representation of the web service is extracted from the WSDL and used to produce test cases. A context-free grammar (CFG) is generated from the XSD that is stored in the WSDL. The CFG is provided as input into a constraint-satisfaction problem solver to automatically generate a diverse set of structurally correct XML documents. Testing data is then inserted into the XML templates in accordance with any constraints specified in the XSD. Web service-specific testing can be performed with the inclusion of external datasets and service-specific configurations. © 2014 IEEE.},
author_keywords={Software Testing;  Web Service testing;  Web Services;  WSDL;  XML;  XML Generation;  XSD},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tarakji201497,
author={Tarakji, A. and Salscheider, N.O.},
title={Runtime behavior comparison of modern accelerators and coprocessors},
journal={Proceedings - IEEE 28th International Parallel and Distributed Processing Symposium Workshops, IPDPSW 2014},
year={2014},
pages={97-108},
doi={10.1109/IPDPSW.2014.16},
art_number={6969375},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918828348&doi=10.1109%2fIPDPSW.2014.16&partnerID=40&md5=bbd83d8439387bdf3cf9193436327ec2},
affiliation={Faculty of Electrical Engineering, RWTH Aachen University, Aachen, Germany},
abstract={Recently, a variety of accelerator architectures became available in the field of high performance computing. Intel's MIC (Many Integrated Core) and both GPU architectures, NVIDIA's Kepler and AMD's Graphics Core Next, all represent the latest innovation in the field of general purpose computing accelerators. This paper explores several important characteristics of these architectures and investigates the impact of certain design factors on the achieved performance using the uCLbench micro-benchmarks, the NPB (NAS Parallel Benchmark) suite and diverse real-world applications from the field of physics. Based on the single unified programming interface OpenCL, we observe the run-time behavior of each test program on several test platforms. Major architectural discrepancies are studied and a higher level examination is discussed in details. © 2014 IEEE.},
author_keywords={Accelerator;  Benchmarking;  GPGPU;  Graphics Core Next;  Kepler;  MIC;  OpenCL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Aruna20142244,
author={Aruna, C. and Prasad, R.S.R.},
title={Metamorphic relations to improve the test accuracy of Multi Precision Arithmetic software applications},
journal={Proceedings of the 2014 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2014},
year={2014},
pages={2244-2248},
doi={10.1109/ICACCI.2014.6968586},
art_number={6968586},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927582804&doi=10.1109%2fICACCI.2014.6968586&partnerID=40&md5=60024a475bd9de215421e0c6a4d17704},
affiliation={Department of Computer Science and Engineering, KKR and KSR Institute of Technology and Sciences, Guntur, Andhra Pradesh, India; Department of CSE, Acharya Nagarjuna University, Guntur, Andhra Pradesh, India},
abstract={Recent advances in computing technologies are increasing the expectations of high accuracy and reliability from sophisticated arithmetic programs. Multi Precision Arithmetic (MPA) plays a vital role in majority of scientific applications, where the accuracy levels are more considerable and even a small mistake may misguide the downstream experimental results. Normal testing strategies rely on test oracles to predict the expected output to compare with target output. Testing of MPA programs is a crucial work with normal testing strategies, due to the complexity of generating oracles to verify the correctness of test outputs. In this paper we propose a novel software testing technique called Metamorphic Testing (MT), to test the non-testable MPA programs with the help of Metamorphic Relations (MRs) to alleviate the oracle problem.MT uses the data diversity technique to generate the follow-up test cases based on the existed successful test cases, which are low cost, scalable, efficient and leads to 'N-Version Programming'. Experimental results are showing that our approach is identifying the hidden errors and improving the testing accuracy by finding more mutants. © 2014 IEEE.},
author_keywords={Follow-up test cases;  Metamorphic relations;  Metamorphic Testing;  Multi Precision Arithmetic;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Galwey20141,
author={Galwey, N.W.},
title={Introduction to Mixed Modelling: Beyond Regression and Analysis of Variance: 2nd Edition},
journal={Introduction to Mixed Modelling: Beyond Regression and Analysis of Variance: 2nd Edition},
year={2014},
volume={9781119945499},
pages={1-487},
doi={10.1002/9781118861769},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927677412&doi=10.1002%2f9781118861769&partnerID=40&md5=d9eb231375c910df8279def1908db67c},
affiliation={Statistical Consulting Group, GlaxoSmithKline, United Kingdom},
abstract={Mixed modelling is very useful, and easier than you think! Mixed modelling is now well established as a powerful approach to statistical data analysis. It is based on the recognition of random-effect terms in statistical models, leading to inferences and estimates that have much wider applicability and are more realistic than those otherwise obtained. Introduction to Mixed Modelling leads the reader into mixed modelling as a natural extension of two more familiar methods, regression analysis and analysis of variance. It provides practical guidance combined with a clear explanation of the underlying concepts. Like the first edition, this new edition shows diverse applications of mixed models, provides guidance on the identification of random-effect terms, and explains how to obtain and interpret best linear unbiased predictors (BLUPs). It also introduces several important new topics, including the following: Use of the software SAS, in addition to GenStat and R. Meta-analysis and the multiple testing problem. The Bayesian interpretation of mixed models. Including numerous practical exercises with solutions, this book provides an ideal introduction to mixed modelling for final year undergraduate students, postgraduate students and professional researchers. It will appeal to readers from a wide range of scientific disciplines including statistics, biology, bioinformatics, medicine, agriculture, engineering, economics, archaeology and geography. © 2014 John Wiley & Sons, Ltd. All rights reserved.},
document_type={Book},
source={Scopus},
}

@CONFERENCE{Shi2014246,
author={Shi, A. and Gyori, A. and Gligoric, M. and Zaytsev, A. and Marinov, D.},
title={Balancing trade-offs in test-suite reduction},
journal={Proceedings of the ACM SIGSOFT Symposium on the Foundations of Software Engineering},
year={2014},
volume={16-21-November-2014},
pages={246-256},
doi={10.1145/2635868.2635921},
note={cited By 77},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986890458&doi=10.1145%2f2635868.2635921&partnerID=40&md5=20dfd82a9eee54bf0f0e60fc94aeb52c},
affiliation={Department of Computer Science, University of Illinois, Urbana-Champaign, United States},
abstract={Regression testing is an important activity but can get expensive for large test suites. Test-suite reduction speeds up regression testing by identifying and removing redundant tests based on a given set of requirements. Traditional research on test-suite reduction is rather diverse but most commonly shares three properties: (1) requirements are defined by a coverage criterion such as statement coverage; (2) the reduced test suite has to satisfy all the requirements as the original test suite; and (3) the quality of the reduced test suites is measured on the software version on which the reduction is performed. These properties make it hard for test engineers to decide how to use reduced test suites. We address all three properties of traditional test-suite reduction: (1) we evaluate test-suite reduction with requirements defined by killed mutants; (2) we evaluate inadequate reduction that does not require reduced test suites to satisfy all the requirements; and (3) we propose evolution-aware metrics that evaluate the quality of the reduced test suites across multiple software versions. Our evaluations allow a more thorough exploration of trade-offs in test-suite reduction, and our evolution-aware metrics show how the quality of reduced test suites can change after the version where the reduction is performed. We compare the trade-offs among various reductions on 18 projects with a total of 261,235 tests over 3,590 commits and a cumulative history spanning 35 years of development. Our results help test engineers make a more informed decision about balancing size, coverage, and fault-detection loss of reduced test suites. Copyright 2014 ACM.},
author_keywords={Software evolution;  Test-suite reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ferguson2014123,
author={Ferguson, B. and Tall, A. and Olsen, D.},
title={National cyber range overview},
journal={Proceedings - IEEE Military Communications Conference MILCOM},
year={2014},
pages={123-128},
doi={10.1109/MILCOM.2014.27},
art_number={6956748},
note={cited By 54},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912564725&doi=10.1109%2fMILCOM.2014.27&partnerID=40&md5=8395729b0f728ea21a68707316d31948},
affiliation={Test Resource Management Center, Office of the under Secretary of Defense for Acquisition, Technology and Logistics, Alexandria, VA, United States; MITRE Corporation, McLean, VA, United States},
abstract={The National Cyber Range (NCR) is an innovative Department of Defense (DoD) resource originally established by the Defense Advanced Research Projects Agency (DARPA) and now under the purview of the Test Resource Management Center (TRMC). It provides a unique environment for cyber security testing throughout the program development life cycle using unique methods to assess resiliency to advanced cyberspace security threats. This paper describes what a cyber security range is, how it might be employed, and the advantages a program manager (PM) can gain in applying the results of range events. Creating realism in a test environment isolated from the operational environment is a special challenge in cyberspace. Representing the scale and diversity of the complex DoD communications networks at a fidelity detailed enough to realistically portray current and anticipated attack strategies (e.g., Malware, distributed denial of service attacks, cross-site scripting) is complex. The NCR addresses this challenge by representing an Internet-like environment by employing a multitude of virtual machines and physical hardware augmented with traffic emulation, port/protocol/service vulnerability scanning, and data capture tools. Coupled with a structured test methodology, the PM can efficiently and effectively engage with the Range to gain cyberspace resiliency insights. The NCR capability, when applied, allows the DoD to incorporate cyber security early to avoid high cost integration at the end of the development life cycle. This paper provides an overview of the resources of the NCR which may be especially helpful for DoD PMs to find the best approach for testing the cyberspace resiliency of their systems under development. © 2014 IEEE.},
author_keywords={cyberspace;  range;  security;  test},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jiao20142629,
author={Jiao, J. and Fu, Y. and Wen, S.},
title={Accelerated assessment of fine-grain AVF in NoC using a Multi-Cell Upsets considered fault injection},
journal={Microelectronics Reliability},
year={2014},
volume={54},
number={11},
pages={2629-2640},
doi={10.1016/j.microrel.2014.06.008},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911390601&doi=10.1016%2fj.microrel.2014.06.008&partnerID=40&md5=4b6bad1bef6a070f49afdcc8254474da},
affiliation={School of Micro Electronics, Shanghai Jiao Tong University, China; Cisco Research Center, United States},
abstract={With the increasing threat of soft errors induced bits upset, Network on Chip (NoC) as the communication infrastructure in many-core systems has been proven a reliability bottleneck in a fault tolerant parallel system. The often-used metric Architecture Vulnerability Factor (AVF), measures the architecture-level soft error impacts to compromise the design cost of fault tolerant schemes and reliability well. As a complementary of existing estimation methods about standard IP like processor and Cache, this work aims at an accelerated fault injection methodology for the fine-grain AVF assessment in NoC via two components: (1) modeling the complex fault patterns of both Multi-Cell Upsets (MCU) and Single Bit Upset (SBU) in the standard Fault Injection (FI) method; (2) accelerating the estimation via classifying and exploiting the fine-grain metrics according to different error impacts. The comprehensive simulation results using the diverse configures (e.g., varying fault model, benchmark, traffic load, network size and fault list size) also demonstrate that the proposed approach (i) shrinks the estimation inaccuracy due to MCU patterns 18.89% underestimation in no protection case and 88.92% overestimation under ECC (Error Correction Coding) protection on average; (ii) achieves about 5× speedup without estimation accuracy loss via phased pre-analysis based on fine-grain classification; (iii) verifies ECC a cost-effective mechanism to protect NoC router: soft errors reduced by about 50% over the no protection case, with only less than 2% area overhead. © 2014 Elsevier Ltd.},
author_keywords={Acceleration;  AVF assessment;  Fault injection;  MCU;  NoC;  Soft error},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sela20142453,
author={Sela, H. and Ezrati, S. and Ben-Yehuda, P. and Manisterski, J. and Akhunov, E. and Dvorak, J. and Breiman, A. and Korol, A.},
title={Linkage disequilibrium and association analysis of stripe rust resistance in wild emmer wheat (Triticum turgidum ssp. dicoccoides) population in Israel},
journal={Theoretical and Applied Genetics},
year={2014},
volume={127},
number={11},
pages={2453-2463},
doi={10.1007/s00122-014-2389-5},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919934400&doi=10.1007%2fs00122-014-2389-5&partnerID=40&md5=7f7f63ebf38c555761a7bd89015eb989},
affiliation={The Institute for Cereal Crops Improvement, Tel-Aviv University, P.O. Box 39040, Tel Aviv, 69978, Israel; Department of Plant Pathology, Kansas State University, Manhattan, KS  66506, United States; Department of Plant Sciences, University of California, Davis, CA  95616, United States; Department of Evolutionary and Environmental Biology and Institute of Evolution, University of Haifa, Haifa, 31905, Israel},
abstract={Key Message: Rapid LD decay in wild emmer population from Israel allows high-resolution association mapping. Known and putative new stripe rust resistance genes were found.
Abstract: Genome-wide association mapping (GWAM) is becoming an important tool for the discovery and mapping of loci underlying trait variation in crops, but in the wild relatives of crops the use of GWAM has been limited. Critical factors for the use of GWAM are the levels of linkage disequilibrium (LD) and genetic diversity in mapped populations, particularly in those of self-pollinating species. Here, we report LD estimation in a population of 128 accessions of self-pollinating wild emmer, Triticum turgidum ssp. dicoccoides, the progenitor of cultivated wheat, collected in Israel. LD decayed fast along wild emmer chromosomes and reached the background level within 1 cM. We employed GWAM for the discovery and mapping of genes for resistance to three isolates of Puccinia striiformis, the causative agent of wheat stripe rust. The wild emmer population was genotyped with the wheat iSelect assay including 8643 gene-associated SNP markers (wheat 9K Infinium) of which 2,278 were polymorphic. The significance of association between stripe rust resistance and each of the polymorphic SNP was tested using mixed linear model implemented in EMMA software. The model produced satisfactory results and uncovered four significant associations on chromosome arms 1BS, 1BL and 3AL. The locus on 1BS was located in a region known to contain stripe rust resistance genes. These results show that GWAM is an effective strategy for gene discovery and mapping in wild emmer that will accelerate the utilization of this genetic resource in wheat breeding. © 2014, Springer-Verlag Berlin Heidelberg.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Boljevic2014,
author={Boljevic, S.},
title={Planning algorithm for optimal CHP generation plant connection in urban distribution network (UDN) according LCTA principle},
journal={Proceedings of the Universities Power Engineering Conference},
year={2014},
doi={10.1109/UPEC.2014.6934631},
art_number={6934631},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910028252&doi=10.1109%2fUPEC.2014.6934631&partnerID=40&md5=7f0592423aff624f4cfe2f9103c625c5},
affiliation={Department of Electrical and Electronic Engineering, Cork Institute of Technology, Cork, Ireland},
abstract={Environmental awareness and sustainable development based on long-term diversification of energy sources are key points on the agenda of energy policy-makers. As part of electrical power systems UDNs are evolving from the present traditional electricity supply network towards more decentralized system with smaller, more efficient generation unit normally connected directly to the network at consumer sites. Combined Heat & Power (CHP) generation is the most efficient way of energy supply in urban area available today. It delivers significant benefits to its host facilities and urban distributed network (UDN) to which is connected. Economic viability of CHP generation for many sites requires integration with the UDN for backup and supplementary power needs and in some case the export of excess power to the UDN. CHP system integration into existing UDN entail installation costs. How these integration costs are distributed will have considerable impact on development and implementation of CHP generation in urban areas. The objective of this paper is to use analytical and statistical methods to develop an algorithm that provide means of determining the optimum capacity of a CHP generating plant that can be accommodated within the UDN, which correspond to Least Cost Technically Acceptable (LCTA) principle, and the UDN long term network planning policy. In order to determine optimal size of CHP generating plant that could be connected at any particular busbar on the UDN without causing a significant adverse impact on performance of the UDN and with minimum connection cost an algorithm is created that incorporates an analytical and multiple regression analysis model. It is tested using data obtained from ERAC power analysing software incorporating load flow, fault current level and power losses analysis. Additional data needed for effective algorithm creation was obtained via surveys of local UDN operators and planners. These analyses are performed on a 34 busbar network resembling part of the real UDN of Cork city for validation purposes and accuracy of the algorithm proposed. © 2014 IEEE.},
author_keywords={CHP Plant;  Connection;  Cost;  LCTA principle;  Multiple Regression Analysis;  Optimization;  UDN},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ludi2014237,
author={Ludi, S. and Ellis, L. and Jordan, S.},
title={An accessible robotics programming environment for visually impaired users},
journal={ASSETS14 - Proceedings of the 16th International ACM SIGACCESS Conference on Computers and Accessibility},
year={2014},
pages={237-238},
doi={10.1145/2661334.2661385},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911376346&doi=10.1145%2f2661334.2661385&partnerID=40&md5=0e3e029b0013b35dd130fa4cfd4ea17e},
affiliation={Department of Software Engineering, Rochester Institute of Technology, Rochester, NY, United States; School of Interactive Games and Media, Rochester Institute of Technology, Rochester, NY, United States; Department of Computer Science, Rochester Institute of Technology, Rochester, NY, United States},
abstract={Despite advances in assistive technology, challenges remain in pre-college computer science outreach and university programs for visually impaired students. The use of robotics has been popular in pre-college classrooms and outreach programs, including those that serve underrepresented groups. This paper describes the specific accessibility features implemented in software that provides an accessible Lego Mindstorms NXT programming environment for teenage students who are visually impaired. JBrick is designed to support students with diverse visual acuity and who use needed assistive technology. Field tests over several days showed that JBrick has the potential to accommodate students who are visually impaired as they work together to program Lego Mindstorms NXT robots.},
author_keywords={Accessibility;  Visual impairment},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fraser201441,
author={Fraser, S. and Mancl, D. and Namioka, A. and Salama, R. and Wirfs-Brock, A.},
title={East meets west: The influences of geography on software production},
journal={SPLASH 2014 - Companion Publication of the 2014 ACM SIGPLAN Conference on Systems, Programming, and Applications: Software for Humanity},
year={2014},
pages={41-42},
doi={10.1145/2660252.2661293},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910649714&doi=10.1145%2f2660252.2661293&partnerID=40&md5=b911c6c70fb8524a4001b154fe50aee6},
affiliation={Research Relations and Tech Transfer, United States; Alcatel-Lucent, United States; Marchex, United States; Millennium Partners, United States; Mozilla Corporation, United States},
abstract={How do software development practices differ from coast-tocoast? What should practitioners learn about the influences of geography - and why is it important? Each community of software professionals has its own technical biases: preferred programming languages, software tools, design paradigms, software testing approaches, and techniques for collaboration within a working group. Conferences like SPLASH provide an opportunity to compare notes, to learn from the successes (and failures) of others, to learn about new technologies, and to learn about howother groups communicate and collaborate. This panel will focus on the diversity of software development practices in North America and the broader influences of geography. Copyright is held by the owner/author(s).},
author_keywords={Geography;  Organizational learning},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Zhu2014171,
author={Zhu, H. and Zhang, Y.},
title={A test automation framework for collaborative testing of web service dynamic compositions},
journal={Advanced Web Services},
year={2014},
volume={9781461475354},
pages={171-197},
doi={10.1007/978-1-4614-7535-4_8},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930435122&doi=10.1007%2f978-1-4614-7535-4_8&partnerID=40&md5=05e192a8bb98e7b48bef978307e2f50b},
affiliation={Department of Computing and Communication Technologies, Oxford Brookes University, Oxford, OX33 1HX, United Kingdom; National Laboratory for Parallel, Processing School of Computer Science, National University of Defense Technology, Changsha, China},
abstract={The dynamic composition of services owned bydifferent vendors demands a high degree of test automation, which must be able to cope with the diversity of service implementation techniques and to meet a wide range of test requirements on-the-fly. These goals are hard to achieve because of the lack of software artefacts of the composed services and the lack of the means of control over test executions and the means of observations on the internal behaviours of composed services. Yet, such integration testing on-the-fly must be non-intrusive and non-disruptive while the composed services are in operation. This chapter presents a test automation framework for such on-the-fly testing of service compositions to facilitate the collaboration between test services through utilisation of Semantic Web Services techniques. In this framework, an ontology of software testing called STOWS are used for the registration, discovery and invocation of test services. The composition of test services is realized by using test brokers, which are also test services but specialized in the coordination of other test services. The ontology can be extended and updated through an ontology management service so that it can support a wide open range of test activities, methods, techniques and types of software artefacts. We also demonstrate the uses of the framework by two running examples. © Springer Science+Business Media New York 2014. All rights are reserved.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{An201413,
author={An, J. and Quercia, D. and Crowcroft, J.},
title={Partisan sharing: Facebook evidence and societal consequences},
journal={COSN 2014 - Proceedings of the 2014 ACM Conference on Online Social Networks},
year={2014},
pages={13-23},
doi={10.1145/2660460.2660469},
note={cited By 39},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912120714&doi=10.1145%2f2660460.2660469&partnerID=40&md5=ae0d6bb9debaa1205b9dd34602ca930a},
affiliation={Qatar Computing Research Institute, Qatar; Yahoo Labs, Barcelona, Spain; University of Cambridge, United Kingdom},
abstract={The hypothesis of selective exposure assumes that people seek out information that supports their views and eschew information that conflicts with their beliefs, and that has negative consequences on our society. Few researchers have recently found counter evidence of selective exposure in social media: users are exposed to politically diverse articles. No work has looked at what happens after exposure, particularly how individuals react to such exposure, though. Users might well be exposed to diverse articles but share only the partisan ones. To test this, we study partisan sharing on Facebook: the tendency for users to predominantly share like-minded news articles and avoid conflicting ones. We verified four main hypotheses. That is, whether partisan sharing: 1) exists at all; 2) changes across individuals (e.g., depending on their interest in politics); 3) changes over time (e.g., around elections); and 4) changes depending on perceived importance of topics. We indeed find strong evidence for partisan sharing. To test whether it has any consequence in the real world, we built a web application for BBC viewers of a popular political program, resulting in a controlled experiment involving more than 70 individuals. Based on what they share and on survey data, we find that partisan sharing has negative consequences: distorted perception of reality. However, we do also find positive aspects of partisan sharing: it is associated with people who are more knowledgeable about politics and engage more with it as they are more likely to vote in the general elections. Copyright © 2014 ACM.},
author_keywords={Facebook;  News aggregators;  Online social network;  Partisan sharing;  Politics;  Selective exposure;  Social media;  Twitter},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murugesan2014287,
author={Murugesan, L. and Balasubramanian, P.},
title={Cloud based mobile application testing},
journal={2014 IEEE/ACIS 13th International Conference on Computer and Information Science, ICIS 2014 - Proceedings},
year={2014},
pages={287-289},
doi={10.1109/ICIS.2014.6912148},
art_number={6912148},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84936806641&doi=10.1109%2fICIS.2014.6912148&partnerID=40&md5=95663de782a9f71eb60bd9f566d51c38},
affiliation={School of Computing Science and Engineering, VIT University, Chennai, India},
abstract={The testing of a mobile application is a difficult task keeping in mind the diversity in mobile devices and the runtime environment. Also the need of resources to test a mobile application varies from mobile phones to tablet. At present there are tools which can simulate the mobile environment to test mobile applications. These stimulators simulate only the functionality of a particular operating system but not the processor cores, speed, memory, cache size of the mobile device. In order to overcome the above short comings we move to cloud testing. In cloud computing we can easily allocate the required resources using virtualization and also it is easy to scale up our resources anytime without affecting the entire system. The major advantage in cloud is that it is cost-effective. In this paper we define a cloud based mobile testing model which can test application for different mobile environments and platforms. © 2014 IEEE.},
author_keywords={Cloud Computing;  Cloud Testing;  Mobile Application Testing;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gundecha2014,
author={Gundecha, P. and Barbier, G. and Tang, J. and Liu, H.},
title={User vulnerability and its reduction on a social networking site},
journal={ACM Transactions on Knowledge Discovery from Data},
year={2014},
volume={9},
number={2},
doi={10.1145/2630421},
art_number={12},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907502828&doi=10.1145%2f2630421&partnerID=40&md5=dc88f65da6661033ec89a89bbdfa8703},
affiliation={Computer Science Department, Arizona State University, 699 S Mill Ave, Tempe, AZ  85281, United States; Air Force Research Laboratory (AFRL), Wright-Patterson Air Force Base, OH, United States},
abstract={Privacy and security are major concerns for many users of social media. When users share information (e.g., data and photos) with friends, they can make their friends vulnerable to security and privacy breaches with dire consequences. With the continuous expansion of a user's social network, privacy settings alone are often inadequate to protect a user's profile. In this research, we aim to address some critical issues related to privacy protection: (1) How can we measure and assess individual users' vulnerability? (2) With the diversity of one's social network friends, how can one figure out an effective approach to maintaining balance between vulnerability and social utility? In this work, first we present a novel way to define vulnerable friends from an individual use's perspective. User vulnerability is dependent on whether or not the use's friends' privacy settings protect the friend and the individual's network of friends (which includes the user). We show that it is feasible to measure and assess user vulnerability and reduce one's vulnerability without changing the structure of a social networking site. The approach is to unfriend one's most vulnerable friends. However, when such a vulnerable friend is also socially important, unfriending him or her would significantly reduce one's own social status. We formulate this novel problem as vulnerability minimization with social utility constraints. We formally define the optimization problem and provide an approximation algorithm with a proven bound. Finally, we conduct a large-scale evaluation of a new framework using a Facebook dataset. We resort to experiments and observe how much vulnerability an individual user can be decreased by unfriending a vulnerable friend. We compare performance of different unfriending strategies and discuss the security risk of new friend requests. Additionally, by employing different forms of social utility, we confirm that the balance between user vulnerability and social utility can be practically achieved. This work is supported by grants of ARO (025071), ONR (N000141010091, N000141410095), and AFOSR (FA95500810132). This work was also funded, in part, by OSD-T&E (Office of Secretary Defense-Test and Evaluation), DefenseWide/PE0601120D8Z National Defense Education Program (NDEP)/BA-1, Basic Research; SMART Program Office, www.asee.org/fellowships/smart, Grant Number N00244-09-1-0081. The majority of this work was conducted when Geoffrey Barbier was affiliated with the Computer Science Denartment at Arizona Sate University. © 2014 ACM.},
author_keywords={Privacy;  Social network;  Vulnerability},
document_type={Article},
source={Scopus},
}

@CONFERENCE{DeOliveira2014344,
author={De Oliveira, G.S. and Duarte, A.},
title={A framework for automated software testing on the cloud},
journal={Parallel and Distributed Computing, Applications and Technologies, PDCAT Proceedings},
year={2014},
pages={344-349},
doi={10.1109/PDCAT.2013.61},
art_number={6904278},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907966076&doi=10.1109%2fPDCAT.2013.61&partnerID=40&md5=1d73e69c499ebd7693e845c22245dc13},
affiliation={Informatics Centre, Federal University of Paraiba, Joao Pessoa, Paraiba, Brazil},
abstract={This work presents the framework Cloud Testing, a solution to parallelize the execution of a test suite over a distributed cloud infrastructure. The use of a cloud as runtime environment for automated software testing provides a more efficient and effective solution when compared to traditional methods regarding the exploration of diversity and heterogeneity for testing coverage. The objective of this work is evaluate our solution regarding the performance gains achieved with the use of the framework showing that it is possible to improve the software testing process with very little configuration overhead and low costs. © 2013 IEEE.},
author_keywords={cloud computing;  software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{O'Connell2014,
author={O'Connell, T.C. and McCarthy, K. and Paquette, A. and McCormick, D. and Pigg, P. and Lamm, P.T.},
title={Enhancements to software tools and progress in model-based design of EOA on the INVENT program},
journal={SAE Technical Papers},
year={2014},
volume={2014-September},
number={September},
doi={10.4271/2014-01-2118},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937809592&doi=10.4271%2f2014-01-2118&partnerID=40&md5=996628df4c797aa41813082d3b6ff6b2},
affiliation={PC Krause and Assoc., 3000 Kent Ave., West Lafayette, IN  47906-1075, United States; Boeing Research and Technology, United States; US Air Force, United States},
abstract={The diverse and complex requirements of next-generation energy optimized aircraft (EOA) demand detailed transient and dynamic model-based design (MBD) to ensure the proper operation of numerous interconnected and interacting subsystems across multiple disciplines. In support of the U.S. Air Force's Integrated Vehicle Energy Technology (INVENT) program, several MBD-derived software tools, including models of EOA technologies, have been developed. To validate these models and demonstrate the performance of EOA technologies, a series of Integrated Ground Demonstration (IGD) hardware tests are planned. Several of the numerous EOA software tools and MBD-based processes have been updated and adapted to support this activity. In this paper, the following key enhancements to the INVENT software tools and the MBD process are discussed: distributed version control; batch launch capability for co-simulation software; automated test script (ATS) library upgrades; and data visualization tool enhancements. For each, details are given about the motivation behind, and the implementation of, the enhancement. The described improvements reduce the potential for MBD errors and design cycle time and allow for a greater number of IGD experiments to be simulated prior to being conducted in hardware. Copyright © 2014 SAE International. © 2014 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wagner20142885,
author={Wagner, M.},
title={Maximising axiomatization coverage and minimizing regression testing time},
journal={Proceedings of the 2014 IEEE Congress on Evolutionary Computation, CEC 2014},
year={2014},
pages={2885-2892},
doi={10.1109/CEC.2014.6900324},
art_number={6900324},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908584251&doi=10.1109%2fCEC.2014.6900324&partnerID=40&md5=84fd15561729e1fec7300677a9c19ea0},
affiliation={Optimisation and Logistics Group, School of Computer Science, University of Adelaide, Australia},
abstract={The correctness of program verification systems is of great importance, as they are used to formally prove that safety- and security-critical programs follow their specification. One of the contributing factors to the correctness of the whole verification system is the correctness of the background axiom-atization, which captures the semantics of the target program language. We present a framework for the maximization of the proportion of the axiomatization that is used ('covered') during testing of the verification tool. The diverse set of test cases found not only increases the trust in the verification system, but it can also be used to reduce the time needed for regression testing. © 2014 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lopez-Herrejon2014387,
author={Lopez-Herrejon, R.E. and Ferrer, J. and Chicano, F. and Egyed, A. and Alba, E.},
title={Comparative analysis of classical multi-objective evolutionary algorithms and seeding strategies for pairwise testing of Software Product Lines},
journal={Proceedings of the 2014 IEEE Congress on Evolutionary Computation, CEC 2014},
year={2014},
pages={387-396},
doi={10.1109/CEC.2014.6900473},
art_number={6900473},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907860137&doi=10.1109%2fCEC.2014.6900473&partnerID=40&md5=3a1a245b185469d51517866a00097915},
affiliation={Software Systems Engineering, Johannes Kepler University Linz, Austria; Universidad de Málaga, Andalucía Tech, Spain},
abstract={Software Product Lines (SPLs) are families of related software products, each with its own set of feature combinations. Their commonly large number of products poses a unique set of challenges for software testing as it might not be technologically or economically feasible to test of all them individually. SPL pairwise testing aims at selecting a set of products to test such that all possible combinations of two features are covered by at least one selected product. Most approaches for SPL pairwise testing have focused on achieving full coverage of all pairwise feature combinations with the minimum number of products to test. Though useful in many contexts, this single-objective perspective does not reflect the prevailing scenario where software engineers do face trade-offs between the objectives of maximizing the coverage or minimizing the number of products to test. In contrast and to address this need, our work is the first to propose a classical multi-objective formalisation where both objectives are equally important. In this paper, we study the application to SPL pairwise testing of four classical multi-objective evolutionary algorithms. We developed three seeding strategies - techniques that leverage problem domain knowledge - and measured their performance impact on a large and diverse corpus of case studies using two well-known multi-objective quality measures. Our study identifies the performance differences among the algorithms and corroborates that the more domain knowledge leveraged the better the search results. Our findings enable software engineers to select not just one solution (as in the case of single-objective techniques) but instead to select from an array of test suite possibilities the one that best matches the economical and technological constraints of their testing context. © 2014 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Han20141247,
author={Han, L. and Zhang, L.-B. and An, Y. and Li, C.-R. and Anwar, S.},
title={Key technology of the electric-hydraulic system for continuous variable transmission},
journal={Jilin Daxue Xuebao (Gongxueban)/Journal of Jilin University (Engineering and Technology Edition)},
year={2014},
volume={44},
number={5},
pages={1247-1252},
doi={10.7964/jdxbgxb201405004},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907878942&doi=10.7964%2fjdxbgxb201405004&partnerID=40&md5=b0bcce015cd866bd3ef6ce9e221e8de7},
affiliation={College of Transportation, Jilin University, Changchun, 130022, China; College of Automotive Engineering, Hu'nan University, Changsha, 410000, China; College of Mechanical Engineering, Jilin Agriculture University, Changchun, 130000, China; College of Mechanical Engineering, Purdue University, Indiana, 46202, United States},
abstract={The control consistency of Drive-Neutral-Reverse (DNR) clutch in the software development of the electric-hydraulic control system of Continuous Variable Transmission (CVT) is analyzed. The parameter diversity, uncertainty, and time-varying introduce difficulties to the open-loop control of DNR clutch. Regarding the filling time and the slipping process, a self-learning method is proposed, which takes the smooth turbine acceleration as the target. The learning rule is derived from the Lyapunov argument. The asymptotic convergence of the self-learning process is theoretically proved. To verify the effectiveness of the learning control algorithm in practical application, the control algorithm modeling is set up in the ETAS software, and the control algorithm is tested using rapid prototype simulation on real vehicle. The test results indicate that application effect of the control method is good.},
author_keywords={Continuously variable transmission (CVT);  DNR clutch;  Electric-hydraulic control system;  Vehicle engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang20141691,
author={Zhang, C. and Chu, W. and Hou, J. and Chen, X. and Wu, X. and Liu, N.},
title={In-situ test on diversion tunnel at Jinping II hydropower station I-test design},
journal={Yanshilixue Yu Gongcheng Xuebao/Chinese Journal of Rock Mechanics and Engineering},
year={2014},
volume={33},
number={8},
pages={1691-1701},
doi={10.13722/j.cnki.jrme.2014.08.020},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906952265&doi=10.13722%2fj.cnki.jrme.2014.08.020&partnerID=40&md5=fe2bf5f6491799f8513f7c0620e37b2f},
affiliation={PowerChina Huadong Engineering Corporation, Hangzhou, Zhejiang  310014, China; HydroChina ITASCA R and D Center, Hangzhou, Zhejiang  310014, China},
abstract={The large scale in-situ test is an important method to solve complex engineering problems. During the construction of diversion tunnels at Jinping II hydropower station, an in-situ test program to monitor the realtime response of the surrounding rocks of the diversion tunnel excavated with TBM was designed and implemented. The contents of the in-situ test program include the stress measurement of rock mass with the rock stress meters of vibrating wire and CSIRO HI Cells to record the stress change in rock mass during tunneling, the acoustic emission monitoring to detect the crack initiation and propagation in rock mass during excavation, the sonic velocity measurement to determine the profile of excavation damage zone around tunnel, the displacement measurement with the multipoint displacement meters and fiber Bragg grating sensors to obtain the deformation of rock mass during tunneling, the digital photographing of borehole to observe the process of the macrocrack initiation. This article is the first of the series and introduce the background of the project, the designing of the in-situ test program, and the detailed geological models of the tested region.},
author_keywords={Acoustic emission;  Deep tunnel;  In-situ test;  Marble;  Rock mechamics;  Surrounding rock mass stress},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhou201469,
author={Zhou, X. and Yang, Y. and Wang, J. and Zhao, T. and Gao, B.},
title={Genetic diversity of Ralstonia solanacearum in tobacco},
journal={Acta Tabacaria Sinica},
year={2014},
volume={20},
number={4},
pages={69-74},
doi={10.3969/j.issn.1004-5708.2014.04.014},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922641010&doi=10.3969%2fj.issn.1004-5708.2014.04.014&partnerID=40&md5=5f26bc5a01e16b54850588ef323de8df},
affiliation={Institute of Plant Protection, Chinese Academy of Agricultural Sciences, Beijing, 100193, China; College of Bio-safety Science and Technology, Hunan Agricultural University, Changsha, 410128, China; Tobacco Research Institute, Chinese Academy of Agricultural Sciences, Qingdao, 266101, China},
abstract={Genetic diversity was analyzed with data from 93 R. solanacearum strains collected from 11 main tobacco-growing areas by Multilocus Sequence Typingin in order to reveal relationship between tobacco variety, geographical origin and genetic diversity of if. solanacearum. There were 51 sequence types (STs) out of 93 R. solanacearum strains, and all tested strains were classified into 4 groups and 4 singletons by using eBURST V3 software sharing 5/7 gene criterion. MLST analysis showed that R. solanacearum was widely spread in China with rich genetic diversity and no significant correlation was found between geographical area, variety and group classification of R. solanacearum. ©, 2014, State Tobacco Monopoly Bureau and China Tobacco Society. All right reserved.},
author_keywords={Genetic diversity;  MLST;  Ralstonia solanacearum},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Baudry2014149,
author={Baudry, B. and Allier, S. and Monperrus, M.},
title={Tailored source code transformations to synthesize computationally diverse program variants},
journal={2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
year={2014},
pages={149-159},
doi={10.1145/2610384.2610415},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942805875&doi=10.1145%2f2610384.2610415&partnerID=40&md5=d497c131c7c9e5ee5ae792721ad6ea96},
affiliation={INRIA, IRISA, Rennes, France; University of Lille and INRIA, Lille, France},
abstract={The predictability of program execution provides attackers a rich source of knowledge who can exploit it to spy or remotely control the program. Moving target defense ad- dresses this issue by constantly switching between many diverse variants of a program, which reduces the certainty that an attacker can have about the program execution. The effectiveness of this approach relies on the availability of a large number of software variants that exhibit different executions. However, current approaches rely on the natural diversity provided by off-the-shelf components, which is very limited. In this paper, we explore the automatic synthesis of large sets of program variants, called sosies. Sosies provide the same expected functionality as the original program, while exhibiting different executions. They are said to be computationally diverse. This work addresses two objectives: comparing different transformations for increasing the likelihood of sosie synthesis (densifying the search space for sosies); demonstrating computation diversity in synthesized sosies. We synthesized 30 184 sosies in total, for 9 large, real-world, open source applications. For all these programs we identified one type of program analysis that systematically increases the density of sosies; we measured computation diversity for sosies of 3 programs and found diversity in method calls or data in more than 40% of sosies. This is a step towards controlled massive unpredictability of software. Copyright 2014 ACM.},
author_keywords={Computation diversity;  Program sosie;  Program transformation;  Software diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nejati2014351,
author={Nejati, S. and Briand, L.C.},
title={Identifying optimal trade-offs between CPU time usage and temporal constraints using search},
journal={2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
year={2014},
pages={351-361},
doi={10.1145/2610384.2610396},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942790194&doi=10.1145%2f2610384.2610396&partnerID=40&md5=b776b10f07946d175fd8b64269e5a10a},
affiliation={SnT Centre, University of Luxembourg, Luxembourg},
abstract={Integration of software from different sources is a critical activity in many embedded systems across most industry sectors. Software integrators are responsible for producing reliable systems that fulfil various functional and performance requirements. In many situations, these requirements inversely impact one another. In particular, embedded system integrators often need to make compromises regarding some of the functional system properties to optimize the use of various resources, such as CPU time. In this paper, motivated by challenges faced by industry, we introduce a multi-objective decision support approach to help balance the minimization of CPU time usage and the satisfaction of temporal constraints in automotive systems. We develop a multi-objective, search-based optimization algorithm, specifically designed to work for large search spaces, to identify optimal trade-off solutions fulfilling these two objectives. We evaluated our algorithm by applying it to a large automotive system. Our results show that our algorithm can find solutions that are very close to the estimated ideal optimal values, and further, it finds significantly better solutions than a random strategy while being faster. Finally, our approach efficiently identifies a large number of diverse solutions, helping domain experts and other stakeholders negotiate the solutions to reach an agreement. Copyright 2014 ACM.},
author_keywords={Multi-objective search optimization;  Software integration;  Static cyclic scheduling;  Temporal coupling constraints},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pradel201413,
author={Pradel, M. and Huggler, M. and Gross, T.R.},
title={Performance regression testing of concurrent classes},
journal={2014 International Symposium on Software Testing and Analysis, ISSTA 2014 - Proceedings},
year={2014},
pages={13-25},
doi={10.1145/2610384.2610393},
note={cited By 61},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908285972&doi=10.1145%2f2610384.2610393&partnerID=40&md5=d383ffbbfeab7c39d262a279a5723f2f},
affiliation={EECS Department, Univ. of California, Berkeley, United States; Dept. of Computer Science, ETH Zurich, Switzerland},
abstract={Developers of thread-safe classes struggle with two opposing goals. The class must be correct, which requires synchronizing concurrent accesses, and the class should pro- vide reasonable performance, which is difficult to realize in the presence of unnecessary synchronization. Validating the performance of a thread-safe class is challenging because it requires diverse workloads that use the class, because existing performance analysis techniques focus on individual bottleneck methods, and because reliably measuring the performance of concurrent executions is difficult. This paper presents Speed Gun, an automatic performance regression testing technique for thread-safe classes. The key idea is to generate multi-threaded performance tests and to com- pare two versions of a class with each other. The analysis notifies developers when changing a thread-safe class significantly influences the performance of clients of this class. An evaluation with 113 pairs of classes from popular Java projects shows that the analysis effectively identifies 13 performance differences, including performance regressions that the respective developers were not aware of. Copyright 2014 ACM.},
author_keywords={Performance measurement;  Test generation;  Thread safety},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhu201421,
author={Zhu, J. and Sun, L. and Liu, X. and Wu, C. and Zhang, B.},
title={Design and optimization of transplanting mechanism with planetary gear train composed of helical gears and noncircular bevel gears},
journal={Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
year={2014},
volume={30},
number={11},
pages={21-29},
doi={10.3969/j.issn.1002-6819.2014.11.003},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902598237&doi=10.3969%2fj.issn.1002-6819.2014.11.003&partnerID=40&md5=f0b043bb13982c7cf691efeed20bf75c},
affiliation={College of Mechanical Engineering and Automation, Zhejiang Sci-Tech University, Hangzhou 310018, China; Zhejiang Province Key Laboratory of Transplanting Equipment and Technology, Hangzhou 310018, China; Wenzhou Nanfang Amusement Rides Company Limited, Wenzhou 325005, China},
abstract={In order to obtain high-quality wide-narrow distance planting trajectory that can solve the problems such as large horizontal offset of picking seeding trajectory in inclined and shaping wider plant hole, a wide-narrow distance transplanting mechanism with spatial planetary gear train composed of non-circular bevel gears and crossed cylindrical gears was put forward based on spatial planetary gear train composed of elliptical bevel gears and crossed cylindrical gears. Non-circular bevel gears were better than traditional gears, such as circular gears and elliptical gears. In addition, the pitch curve of the non-circular bevel gear was general so that this mechanism could achieve more potential spatial planting trajectories. Transplanting mechanism with this kind of bevel gears was one kind of spatial non-uniform velocity transmission mechanism which could obtain the spatial planting trajectory that met diverse agronomic requirements. The theory of wide-narrow distance transplanting mechanism was introduced and the optimization goal of planting trajectory was designed. A smooth, continuous and closed spherical curve was designed by fitting the free form spline, which was used to represent the big end pitch curve of non-circular bevel gear. The equations of spherical big end pitch curve were given. Based on the D-H matrix transformation, the kinematics optimization model of mechanism was built up. The key points' displacements of transplanting mechanism were calculated. Several main parameters affecting the wide-narrow distance planting trajectory were analyzed. The shape of non-circular bevel gear's big end pitch curve played a major role in the shape of wide-narrow distance planting trajectory; the initial angle of planetary carrier was a key parameter affecting the posture of wide-narrow distance planting trajectory; the staggered angle of non-circular bevel gear pair and bevel gear pair's cone distance had decided the offset of picking and taking seeding trajectory. A optimization program about wide-narrow distance transplanting mechanism was written to study the kinematics, and a set of optimal parameters meeting the working requirements of wide-narrow distance rice transplanter was obtained by the method of human-computer conversation afterward. The lateral optimal trajectory and ratio formed by spatial planetary gear train composed of non-circular bevel gears and spatial planetary gear train composed of elliptical bevel gears were compared. The advantage was the smaller horizontal offset and lower speed of picking seeding trajectory; the time of pushing seedling period and returning period were much less; the probability that took back the rice seedling was decreased; and the overall height of wide-narrow distance planting trajectory was raised. A three-dimensional model of wide-narrow distance transplanting mechanism was obtained. Virtual prototype test was performed by the software named Adams and a model machining of the transplanting mechanism as well as its testing was made, and the shape of tow trajectory was almost same. The result showed that the transplanting mechanism with spatial planetary gear train composed of non-circular bevel gears and crossed cylindrical gears could fulfill the requirement of wild-narrow distance rice transplanter and had better performance than transplanting mechanism with spatial planetary gear train composed of elliptical bevel gears and crossed cylindrical gears.},
author_keywords={Agricultural machiney;  Crossed cylindrical gear;  Gears;  Non-circular bevel gear;  Optimization;  Spatial planetary gear;  Transplanting mechanism;  Transplanting trajectory},
document_type={Article},
source={Scopus},
}

@ARTICLE{Narciso2014653,
author={Narciso, E.N. and Delamaro, M.E. and De Lourdes Dos Santos Nunes, F.},
title={Test case selection: A systematic literature review},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2014},
volume={24},
number={4},
pages={653-676},
doi={10.1142/S0218194014500259},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929314072&doi=10.1142%2fS0218194014500259&partnerID=40&md5=ce1e3dbd9e3349c6b57406c7526efec2},
affiliation={Escola de Artes, Ciências e Humanidades - EACH, Universidade de São Paulo, Avenida Arlindo Bettio 1000, Ermelino Matarazzo, São Paulo, SP, Cep 03828-000, Brazil; Instituto de Ciências Matemáticas e de Computação - ICMC, Universidade de São Paulo, Avenida Trabalhador São-Carlense 400, São Carlos, SP, Cep 13560-970, Brazil},
abstract={Time and resource constraints should be taken into account in software testing activities, and thus optimizing the test suite is fundamental in the development process. In this context, the test case selection aims to eliminate redundant or unnecessary test data, which is crucial for the definition of test strategies. This paper presents a systematic review on the test case selection conducted through a selection of 449 articles published in leading journals and conferences in Computer Science. We addressed the state-of-art by collecting and comparing existing evidence on the methods used in the different software domains and the methods used to evaluate the test case selection. Our study identified 32 papers that met the research objectives, which featured 18 different selection methods and were evaluated through 71 case studies. The most commonly reported methods are adaptive random testing, genetic algorithms and greedy algorithm. Most approaches rely on heuristics, such as diversity of test cases and code or model coverage. This paper also discusses the key concepts and approaches, areas of application and evaluation metrics inherent to the methods of test case selection available in the literature. © 2014 World Scientific Publishing Company.},
author_keywords={software test;  systematic review;  Test case selection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zakharov2014713,
author={Zakharov, A.V. and Peach, M.L. and Sitzmann, M. and Nicklaus, M.C.},
title={A new approach to radial basis function approximation and its application to QSAR},
journal={Journal of Chemical Information and Modeling},
year={2014},
volume={54},
number={3},
pages={713-719},
doi={10.1021/ci400704f},
note={cited By 35},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896988861&doi=10.1021%2fci400704f&partnerID=40&md5=4a17641a93b456a265ea4e096451b69e},
affiliation={Chemical Biology Laboratory, National Cancer Institute, National Institutes of Health, 376 Boyles St., Frederick, MD 21702, United States; Leidos Biomedical, Inc., Computer-Aided Drug Design Group, Frederick National Laboratory for Cancer Research, 376 Boyles St., Frederick, MD 21702, United States},
abstract={We describe a novel approach to RBF approximation, which combines two new elements: (1) linear radial basis functions and (2) weighting the model by each descriptors contribution. Linear radial basis functions allow one to achieve more accurate predictions for diverse data sets. Taking into account the contribution of each descriptor produces more accurate similarity values used for model development. The method was validated on 14 public data sets comprising nine physicochemical properties and five toxicity endpoints. We also compared the new method with five different QSAR methods implemented in the EPA T.E.S.T. program. Our approach, implemented in the program GUSAR, showed a reasonable accuracy of prediction and high coverage for all external test sets, providing more accurate prediction results than the comparison methods and even the consensus of these methods. Using our new method, we have created models for physicochemical and toxicity endpoints, which we have made freely available in the form of an online service at http://cactus.nci.nih.gov/chemical/apps/cap. © 2014 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bushuev2014257,
author={Bushuev, F.I. and Kalyuzhny, N.A. and Slivinsky, A.P. and Shulga, A.V.},
title={Ranging geostationary telecommunication satellites with satellite TV signals},
journal={Telecommunications and Radio Engineering (English translation of Elektrosvyaz and Radiotekhnika)},
year={2014},
volume={73},
number={3},
pages={257-269},
doi={10.1615/TelecomRadEng.v73.i3.50},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900661652&doi=10.1615%2fTelecomRadEng.v73.i3.50&partnerID=40&md5=79906bb5a5236da74feb07bc8db527be},
affiliation={Mykolayiv Astronomical Observatory Research Institute, 1, Observatorna St., Mykolayiv, 54030, Ukraine; Ukrainian Institute of Radio Engineering, 238, Kirov St., Mykolayiv, 54031, Ukraine},
abstract={A complex of radio frequency equipment and software has been developed and tested, which is intended for estimating the difference in slant ranges to geostationary telecommunication satellites through the use of correlation parameters of satellite TV signals. The signal processing that has been adopted makes account of the existing signal structure, while being independent of the information content carried by the signals. The error of estimating the difference of slant ranges is 5.6 m, which value is determined by the accuracy of hardware synchronization through the use of Resolution-T single-frequency GPS receivers. The error of orbit determination for a telecommunication geostationary satellite might be reduced to above 18 m ( 0.1′ ), should the synchronization system be improved, based on an atomic frequency standard of high stability and optimally selected spatial diversity of a few hard- and software complexes separated by about 1.000 km. The radio frequency complex discussed may become a prototype for a cost-effective all-weather regional network of facilities aimed at orbit determination and continual monitoring of orbit parameters for geostationary telecommunication satellites. © 2014 by Begell House, Inc.},
author_keywords={Correlation function;  DVB-S signal;  Radio interferometer;  Space monitoring},
document_type={Article},
source={Scopus},
}

@ARTICLE{Romli201441,
author={Romli, R. and Sulaiman, S. and Zamli, K.Z.},
title={Designing a test set for structural testing in automatic programming assessment},
journal={International Journal of Advances in Soft Computing and its Applications},
year={2014},
volume={5},
number={SPECIALISSUE.3},
pages={41-64},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894092381&partnerID=40&md5=638f5d7b51c68ed3273ef972287b92df},
affiliation={School of Computing, College of Arts and Sciences, Universiti Utara Malaysia, 06010 UUM Sintok, Kedah, Malaysia; Faculty of Computing, Universiti Teknologi Malaysia, 81310 UTM Skudai, Johor, Malaysia; Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, Lebuhraya Tun Razak, 26300 Gambang, Kuantan, Pahang, Malaysia},
abstract={An automatic programming assessment (APA) method aims to support marking and grading of students' programming exercises. APA requires a test data generation to perform a dynamic testing on students' programs. In software testing field, diverse automated methods for test data generation are proposed. Unfortunately, APA seldom adopts these methods. Merely limited studies have attempted to integrate APA and test data generation to include more useful features and to provide a precise and thorough quality of program testing coverage. Thus, we propose a test data generation approach to cover both the functional and structural testing of a program for APA by focusing the structural testing in this paper. We design a test set based on the integration of positive and negative testing criteria that enhanced path coverage criterion to select the desired test data. It supports lecturers of programming courses to furnish an adequate set of test data to assess students' programming solutions in term of structural testing without necessarily having the expertise in a particular knowledge of test cases. The findings from the experiment depict that the test set improves the criteria of reliability and validity for test data adequacy in programming assessments. © SCRG Publication, 2013.},
author_keywords={Automatic programming assessment (APA);  Negative testing;  Path coverage;  Positive testing;  Structural testing;  Test data generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Bueno2014490,
author={Bueno, P.M.S. and Jino, M. and Wong, W.E.},
title={Diversity oriented test data generation using metaheuristic search techniques},
journal={Information Sciences},
year={2014},
volume={259},
pages={490-509},
doi={10.1016/j.ins.2011.01.025},
note={cited By 37},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889670641&doi=10.1016%2fj.ins.2011.01.025&partnerID=40&md5=5b02793eecfc8672a6c608650daaccd9},
affiliation={Information Technology Center Renato Archer, Rodovia Dom Pedro I km 143, 6 Campinas, São Paulo, Brazil; State University of Campinas, Av. Albert Einstein 400, Campinas, São Paulo, Brazil; Department of Computer Science, University of Texas at Dallas, Richardson, TX 75083, United States},
abstract={We present a new test data generation technique which uses the concept of diversity of test sets as a basis for the diversity oriented test data generation - DOTG. Using DOTG we translate into an automatic test data generation technique the intuitive belief that increasing the variety, or diversity, of the test data used to test a program can lead to an improvement on the completeness, or quality, of the testing performed. We define the input domain perspective for diversity (DOTG-ID), which considers the distances among the test data in the program input domain to compute a diversity value for test sets. We describe metaheuristics which can be used to automate the generation of test sets for the DOTG-ID testing technique: simulated annealing; a genetic algorithm; and a proposed metaheuristic named simulated repulsion. The effectiveness of DOTG-ID was evaluated by using a Monte Carlo simulation, and also by applying the technique to test simple programs and measuring the data-flow coverage and mutation scores achieved. The standard random testing technique was used as a baseline for these evaluations. Results provide an understanding of the potential gains in terms of testing effectiveness of DOTG-ID over random testing and also reveal testing factors which can make DOTG-ID less effective. © 2013 Elsevier Inc. All rights reserved.},
author_keywords={Genetic algorithms;  Random testing;  Simulated annealing;  Simulated repulsion;  Software testing;  Test data generation},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Garroppo2014,
author={Garroppo, R.G. and Giordano, S. and Roma, S. and Foddis, G. and Topazzi, S.},
title={Test and monitoring of LTE network: A step towards low cost solutions based on NetFPGA},
journal={2014 IEEE Latin-America Conference on Communications, IEEE LATINCOM 2014},
year={2014},
doi={10.1109/LATINCOM.2014.7041893},
art_number={7041893},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946690566&doi=10.1109%2fLATINCOM.2014.7041893&partnerID=40&md5=b0c09a85d03642401e546f3fc91183b1},
affiliation={Dipartimento di Ingegneria dell'Informazione, University of Pisa, Italy; Telecom Italia Lab, Torino, Italy},
abstract={The paper presents the lessons learned during the test activity of LTE network in Telecom Italia Lab. The issues mainly regard the lack of low cost and flexible monitoring and test platforms easy to reconfigure and adapt to the diverse network interfaces. The paper presents the design guideline for the development of FPGA-based devices able to overcome the presented issues. Exploiríng the NetFPGA platform and re-using the available open-source software, a preliminary prototype implementing some ideas is presented Furthermore, the paper shows the preliminary results obtained during the utilization of the NetFPGA prototype in the test activities. © 2014 IEEE.},
author_keywords={LTE;  monitoring;  NetFPGA;  troubleshooting},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhou2014453,
author={Zhou, R.-G. and Shen, C.-Y. and Xiao, T.-R. and Li, Y.-C.},
title={Improvement of NSGAII on diversity and spread},
journal={ICIC Express Letters, Part B: Applications},
year={2014},
volume={5},
number={2},
pages={453-458},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893158995&partnerID=40&md5=22fc1a8656cb61a59b05e3ed3cbeb81f},
affiliation={College of Information Engineering, East China Jiaotong University, No. 808, Shuanggang East Street, Nanchang 330013, China},
abstract={This paper proposed modifications of NSGAII on diversity and spread. There are two improvements: one is the use of a novel truncation strategy to keep the dominant population; the other is the substitution of crowding distance calculation. The two modifications both brought a better Pareto front. And we realized it using Matlab program to test both modifications. Besides it was verified and proved effectiveness by experiment results. © 2014 ISSN 2185-2766.},
author_keywords={Crowding distance;  Diversity;  Nsgaii;  Spread;  Truncation strategy},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Sokolov2014,
author={Sokolov, O. and Meszynski, S. and Groemer, G. and Sattler, B. and Carbognani, F. and Salotti, J.-M. and Jozefowicz, M.},
title={Human-mobile agents partnerships in complex environment},
journal={IEEE SSCI 2014 - 2014 IEEE Symposium Series on Computational Intelligence - CIHLI 2014: 2014 IEEE Symposium on Computational Intelligence for Human-Like Intelligence, Proceedings},
year={2014},
doi={10.1109/CIHLI.2014.7013396},
art_number={7013396},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922932687&doi=10.1109%2fCIHLI.2014.7013396&partnerID=40&md5=1bc10d26091fabe767f45834a9a97bfd},
affiliation={Department of Informatics, Faculty of Physics, Astronomy and Informatics, Nicolaus Copernicus University, Toruń, Poland; Austrian Space Forum, Austria; University of Innsbruck, Austria; Italian Mars Society, Italy; Laboratoire de l'Intégration du Matériau Au Systeme, Bordeaux University, France; Polish Mars Society, Poland},
abstract={This article is devoted to explore the robotic and software support strategies based on a sample activity providing optimal inputs, namely a simulated human missions. This mission will be treated as a clean-sheet approach for operating multiple, diverse and adaptive agents in complex environments. Building upon existing state-of-the-art hardware, like mobile robots, astrobiological instruments and software architectures, results and experiences from previous missions involving the partners, high-fidelity analog field tests shall demonstrate the added value, potential and limitations of adaptive machines supporting humans in a challenging environment. © 2014 IEEE.},
author_keywords={adaptive agent;  knowledge base;  mobile robots;  multiagent system},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Strain2014,
author={Strain, S. and Kugele, S. and Franklin, S.},
title={The learning intelligent distribution agent (LIDA) and medical agent X (MAX): Computational intelligence for medical diagnosis},
journal={IEEE SSCI 2014 - 2014 IEEE Symposium Series on Computational Intelligence - CIHLI 2014: 2014 IEEE Symposium on Computational Intelligence for Human-Like Intelligence, Proceedings},
year={2014},
doi={10.1109/CIHLI.2014.7013390},
art_number={7013390},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922883274&doi=10.1109%2fCIHLI.2014.7013390&partnerID=40&md5=6d7965701bb13e477709a94ceb3e9193},
affiliation={Cognitive Computing Research Group (CCRG), University of Memphis, Memphis, TN, United States},
abstract={The complexity of medical problem solving presents a formidable challenge to current theories of cognition. Building on earlier work, we claim that the systemslevel cognitive model LIDA (for 'Learning Intelligent Distribution Agent') offers a number of specific advantages for modeling diagnostic thinking. The LIDA Model employs a consciousness mechanism in an iterative cognitive cycle of understanding, attention, and action, endowing it with the ability to integrate multiple sensory modalities into flexible, dynamic, multimodal representations according to strategies that support specific task demands. These representations enable diverse, asynchronous cognitive processes to be dynamically activated according to rapidly changing contexts, much like in biological cognition. The recent completion of the LIDA Framework, a software API supporting the domain-independent LIDA Model, allows the construction of domain-specific agents that test the Model and/or enhance traditional machine learning algorithms with human-style problem solving. Medical Agent X (MAX) is a medical diagnosis agent under development using the LIDA Model and Framework. We review LIDA's approach to exploring cognition, assert its appropriateness for problem solving in complex domains such as diagnosis, and outline the design of an initial implementation for MAX. © 2014 IEEE.},
author_keywords={cognitive modeling;  LIDA;  medical diagnosis;  topic modeling},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Contractor2014385,
author={Contractor, J. and Musau, F.},
title={Potential for net zero energy neighbourhoods in the Ahmedabad urban and solar contexts},
journal={30th International PLEA Conference: Sustainable Habitat for Developing Societies: Choosing the Way Forward - Proceedings},
year={2014},
volume={2},
pages={385-392},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088358136&partnerID=40&md5=b69765ccae8dd914a888aefa8c4212a3},
affiliation={Glasgow School of Art, United Kingdom},
abstract={Many net zero buildings have been proposed in different parts of the world. However, there is an argument that an individual building is not the right scale to develop Net Zero Energy Housing. The neighbourhood scale has the potential to integrate not only individual building systems, but also multibuilding systems as well as of integrating neighbourhood geometry. This scale also offers opportunities for load sharing between buildings and diversity in functions. Inspite of India having a rich solar energy resource, there are no Net Zero Energy Neighbourhoods being developed. This paper tests the potential of three existing neighbourhoods in Ahmedabad, with different building typology and geometry, to achieve Net Zero Energy status by way of retrofitting Photovoltaic Technology. After a review of historical energy bills to assess the energy demand, the PVSyst software package (version 6.0) is used to test the potential performance of solar retrofits in the three different neighbourhoods. The results show that each of the three neighbourhoods can achieve Net Zero Energy status by retrofitting PV Panels. However, the investment cost and payback periods are prohibitive for the economic contexts of the three neighbourhoods. The paper further proposes neighbourhood scale retrofitting strategies. It also proposes government support policies, based on the neighbourhood scale, to overcome the cost limitations in achieving Net Zero Energy status. © Copyright: CEPT University, Center for Advanced Research in Building Science & Energy, Ahmedabad First Published 2014.All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Narendran2014507,
author={Narendran, A. and Musau, F.},
title={Flexible and environment responsive mass housing in Bangalore, India},
journal={30th International PLEA Conference: Sustainable Habitat for Developing Societies: Choosing the Way Forward - Proceedings},
year={2014},
volume={2},
pages={507-514},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088356425&partnerID=40&md5=4f72f2478dfc8813fe2d2af93b157bf9},
affiliation={Glasgow School of Art, United Kingdom},
abstract={Bangalore is one of the fastest urbanizing cities in India due to rapid increase in population and migration of people from varied and distinct cultural backgrounds. This has resulted in rapid development of high density housing characterized by towers of repetitive units. Most modern housing developments are focused on the repetition of units suitable for an average dweller, without taking into consideration the diverse and dynamic needs and wants of individuals and society. What is ironic is that for centuries now, most societies have produced housing it requires, naturally and indigenously. The traditional vernacular architecture has always been in empathy with the environment. This project develops options for a prototype housing unit and tests it by an analysis using the TAS software package for achieving flexibility without compromising on natural ventilation. It then develops a residential cluster which can be used as a model for future developments in Bangalore. © Copyright: CEPT University, Center for Advanced Research in Building Science & Energy, Ahmedabad First Published 2014.All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schloßer2014,
author={Schloßer, A. and Khastgir, S. and Jentges, M. and Jakoby, B. and Richenhagen, J.},
title={Test driven model based series software development for automotive systems},
journal={FISITA 2014 World Automotive Congress - Proceedings},
year={2014},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058009440&partnerID=40&md5=077aab0b003e8cdf731018696ca46b4d},
affiliation={FEV GmbH, Germany; FEV India Pvt. Ltd, India},
abstract={With concerns growing over depleting fuel reserves and global warming, automotive manufacturers are adopting various methods to meet these challenges. In order to do so, the electronic content in the vehicles is increasing rapidly. Software is a key part of the electronic systems and the focus on quality, reliability, safety and traceability within the software systems is gathering momentum. To meet these developments, new standards have been established by manufacturers and suppliers to assure software quality with focus on safety systems. This paper focusses on the comprehensive testing and traceability process for lean and efficient software development at FEV GmbH. The test-driven model based software development approach involves the definition of test cases for the automotive software prior to the software implementation. While the specification follows the Autosar standard, implementation is realized model-based, generating control code fully automatically with established code generation tools. Test cases are defined e.g. via the classification tree method for this purpose. In-house developed tools are tailored to automotive needs: the agile software development method “Continuous Integration” is applied to software models embedding diverse verification and validation methodologies as well as code integration and software documentation. The bi-directional traceability between the work products is ensured by an optimized interplay between the process and applied tools. This ensures software quality across various release versions and is also critical for safety functions within the software. The processes and tools developed in-house meet the requirements as laid down by the ISO 26262 standard for automotive systems. The discussed process leads to an increase in the software quality and early detection of faults in the software during the development process. This frontloading approach reduces time and costs of hardware-in-loop testing and in-Vehicle testing and improves the quality of the software at the same time. This is crucial in the current cost and quality driven environment. © 2014, FISITA. All rights reserved.},
author_keywords={Agile software development;  AUTOSAR;  Bidirectional traceability;  Classification tree method;  ISO 26262},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DallaTorre2014,
author={Dalla Torre, S.},
title={Micro pattern gas detector technologies and applications - The work of the RD51 collaboration},
journal={Proceedings of Science},
year={2014},
volume={0},
art_number={002},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011596177&partnerID=40&md5=a73148f6af9b49fe357aaf546f69416b},
affiliation={INFN, Sezione di Trieste, Trieste, Italy},
abstract={Driven by the availability of modern photolithographic techniques, the Micro Pattern Gas Detectors have been introduced in the last years of the 20th century by pioneer activities: Gas Electron Multipliers (GEM) and Micromegas, later followed by thick-GEM, resistive GEM (RETGEM) and novel micro-pattern devices. Nowadays, a flourishing of R&amp;D activities dedicated to MPGDs and of diversified applications is ongoing, largely favored by the technological collaboration RD51. The collaboration mission is to facilitate the development of these advanced gas-avalanche detector technologies and associated electronic-readout systems, to be used in basic and applied research. The areas of activities within RD51 include MPGD technology and new structures, device characterization, software and simulations, electronics, MPGD production, common test facilities, and applications of MPGDs. By this coverage of all aspects of MPGDs, RD51 brings together the leading experts in the field of MPGD developments and MPGD users in a wide array of applications. The RD51 achievements are reviewed by summarising the first five years of the Collaboration activity and by anticipating the future programmes, planned over the next five years. © Copyright owned by the author(s) under the terms of the Creative Commons Attribution-NonCommercial-ShareAlike Licence.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Edris20141928,
author={Edris, M.A.A. and Amin, M.H. and Al Benali, K. and Shinde, A.L. and Ghadimipour, A. and Perumalla, S.V. and Hartley, L.J. and Baxter, S.},
title={Implementation of coupled 3D Geomechanics and discrete fracture network (DFN) models in field development optimisation: A case study from carbonate reservoir, Abu Dhabi},
journal={Society of Petroleum Engineers - 30th Abu Dhabi International Petroleum Exhibition and Conference, ADIPEC 2014: Challenges and Opportunities for the Next 30 Years},
year={2014},
volume={3},
pages={1928-1951},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994121193&partnerID=40&md5=6f7da197107506dd084d12905e9be161},
affiliation={ADCO, United Arab Emirates; Baker Hughes Inc, United States; AMEC, United Kingdom},
abstract={An onshore oilfield in Abu Dhabi has complex faulted structure of early Cretaceous carbonate reservoir. The porosity in these reservoirs ranges up to 25%. These reservoirs are overlaid by dense carbonate which acts as a cap rock. The field has been produced for 35 years with peripheral water and gas injection scheme to maintain the reservoir pressure. The objectives of this geomechanical study were to verify the impact of in-situ stresses and rock mechanical properties on reservoir performance during production/injection activities, as well as to evaluate the mechanical and hydraulic responses of natural fractures and faults to fluid flow within the reservoir. In addition, evaluation of cap rock integrity and fault seal analyses are also considered as necessary risk assessment inputs for field development program. Extensive rock mechanics testing program has been carried out on reservoir, the interlay dense and cap rock to quantify the contrast in mechanical rock properties and their response to in-situ stresses. Diverse set of data have been integrated including geological, geophysical, petrophysical, reservoir and drilling data to build robust ID and 3D Geomechanical models. Natural fractures were characterized using core inspection, image logs and CT-Scans which have been calibrated to production logging (PLT) and well tests. Multi-attribute analysis with inputs from petrophysical, structural and geophysical data has been used to build and calibrate the 3D DFN model. The developed 3D Geomechanical model was coupled with the 3D reservoir dynamic model in order to predict the changes in total stresses due to reservoir pressure changes. Furthermore, this model was integrated with the 3D DFN model to help understand the impact of production/injection activities on the mechanical behaviour of the fractures, fault seal and cap rock integrity. This paper discusses the role of coupled 3D Geomechanics and the DFN modeling in evaluating and quantifying the mechanical behaviour of reservoir rock matrix, fractures, faults and cap rock that can influence the decisions on optimum field development planning. Copyright 2014, Society of Petroleum Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ionescu20148,
author={Ionescu, T.B. and Laurien, E. and Scheuermann, W.},
title={Improving the reliability of decision-support systems for nuclear emergency management using software diversity methods},
journal={HARMO 2014 - 16th International Conference on Harmonisation within Atmospheric Dispersion Modelling for Regulatory Purposes, Proceedings},
year={2014},
pages={8-13},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983174917&partnerID=40&md5=dbae1fd56e50221f41f31e5c985f8af4},
affiliation={Institute for Nuclear Technology and Energy Systems, University of Stuttgart, Stuttgart, Germany},
abstract={Decision-support systems for nuclear emergency management (DSNE) are currently used to assist decision makers around the world in taking emergency response countermeasures in case of accidental releases of radioactive materials into the atmosphere. The present work has been motivated by the fact that, up until now, DSNE systems and the underlying atmospheric dispersion simulation codes have not been regarded as safety critical software systems and have therefore not been treated as such during the software testing and verification phase. The main goal of the current work is to improve the reliability of DSNE systems by adapting well-established methods from the domain of software reliability engineering to the case of atmospheric dispersion simulation codes. The effectiveness of the approach has been assessed using the atmospheric dispersion forecasts of two test versions of the widely used RODOS system. © Crown Copyright 2014 Dstl.},
author_keywords={Decision-support;  Nuclear emergency;  RODOS;  Simulation codes;  Software reliability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Meinel2014893,
author={Meinel, D. and Paryanto, P. and Franke, J.},
title={Chances of the application of multi-domain simulation tools in the field of train system engineering},
journal={FAIM 2014 - Proceedings of the 24th International Conference on Flexible Automation and Intelligent Manufacturing: Capturing Competitive Advantage via Advanced Manufacturing and Enterprise Transformation},
year={2014},
pages={893-899},
doi={10.14809/faim.2014.0893},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960884501&doi=10.14809%2ffaim.2014.0893&partnerID=40&md5=ac1f95dfdf2141834685d79f26f7fcf2},
affiliation={Institute for Factory Automation and Production Systems, Friedrich-Alexander-University of Erlangen-Nuremberg, Erlangen, Bavaria, 91058, Germany},
abstract={Complex mechatronic systems, such as modern trains, demand interdisciplinary software tools in order to test systems as a whole and predict operational behavior. Caused by the intention of cost savings and pushing the market launch time, computer-aided modelling and simulating of the behavior of single devices, such as thermal effects, stresses, deformations and also electrical state variables, have long since become state-of-the-art. This method leads to well-matured products. The component integration into the system as a whole, however, often causes trouble due to the lack of computer and software assistance. Furthermore, the diverse engineering fields do not have the expertise to collaborate. Therefore, joined system testing and improving is difficult and uncommon. Since the different mechanical and electrical components are being developed and tested independently, overall system behavior can hardly be forecast. As a consequence, during the system assembly phase, unpredicted incompatibilities and system malfunctions can appear. In order to integrate the train subsystems into an overall system and allow for a better synchronization and proper system testing, the application of multi-domain modelling and simulating is recommended. In this regard, the benefits brought by multi-domain applications will be discussed in this publication, using the example of a modern train. Thereafter, a case study of a pneumatic train brake application follows. Its results are exemplarily being shown to demonstrate future perspectives. Models and simulation results of the brake and air supply components already turn out allowing for comparisons with the actual system. System start-up times of the simulation match actually measured times adequately. © Copyright 2014 by DEStech Publications, Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zamli2014818,
author={Zamli, K.Z. and Mohd Hassin, M.H. and Al-Kazemi, B. and Naseer, A.},
title={Simulated annealing based strategy for test redundancy reduction},
journal={Frontiers in Artificial Intelligence and Applications},
year={2014},
volume={265},
pages={818-832},
doi={10.3233/978-1-61499-434-3-818},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948799310&doi=10.3233%2f978-1-61499-434-3-818&partnerID=40&md5=80a837c838b1030239dc3f7c62dcd3cf},
affiliation={Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, Malaysia; College of Computer and Information Systems, Umm Al-Qura University Makkah, Saudi Arabia},
abstract={Software testing relates to the process of accessing the functionality of a program against some defined specifications. To ensure conformance, test engineers often generate a set of test cases to validate against the user requirements. When dealing with large line of codes (LOCs), there are potentially issue of redundancies as new test cases may be added and old test cases may be deleted during the whole testing process. In order to address this issue, we have developed a new strategy, called tReductSA, to systematically minimize test cases for testing consideration. Unlike existing works which rely on the Greedy approaches, our work adopts the random sequence permutation and optimization algorithm based on Simulated Annealing with systematic merging technique. Our benchmark experiments demonstrate that t Reduct SA scales well with existing works (including that of GE, GRE and HGS) as far as optimality is concerned. On the other note, tReductSA also offers more diversified solutions as compared to existing work. © 2014 The authors and IOS Press. All rights reserved.},
author_keywords={Optimization;  Search based software engineering;  Simulated annealing;  Test suite redundancy reduction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Seyedtabaii2014921,
author={Seyedtabaii, S. and Khalaji, A.},
title={Single chip digital CMOS implementation of a reconfigurable fuzzy logic traffic controller},
journal={Journal of Intelligent and Fuzzy Systems},
year={2014},
volume={27},
number={2},
pages={921-928},
doi={10.3233/IFS-131051},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946574017&doi=10.3233%2fIFS-131051&partnerID=40&md5=927084cd3a4d10c924cc4a430c6ce8fd},
affiliation={Electrical Engineering Department, Shahed University, P.O. Box 18155/159, Tehran, Iran},
abstract={In this paper, single chip CMOS design of a reconfigurable fuzzy traffic light controller is studied. The chip inputs are traffic data and parameters of fuzzy membership degrees (MD). MD's are allowed to have variable numbers of reconfigurable trapezoidal membership functions. Based on the traffic density, the fuzzy traffic controller decides either to terminate the current green phase or to prolong it for better intersection traffic handling. Import/export of data and working parameters are carried out through a serial link. The chip is designed using Very High Speed Integrated Circuit (VHSIC) Hardware Description Language (VHDL) and its layout is derived using SOC Encounter. Parallel and sequential architectures for fuzzy processing which have diverse impacts on the chip area (cost) and fuzzy process delay (speed) are evaluated. The design of choice is a sequential architecture maintaining low power, low die area (cost) and adequately fast speed, to be capable of containing the entire fuzzy traffic control and data collection hardware. The simulated chip performance is tested versus full software implementation of the algorithm. © 2014-IOS Press and the authors. All rights reserved.},
author_keywords={CMOS design;  Fuzzy logic;  traffic light control;  VHDL},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Birrane20146890,
author={Birrane, E. and Zhang, D.L. and Berardino, R.},
title={A system-independent ground software (SIGS) approach for telemetry distribution},
journal={Proceedings of the International Astronautical Congress, IAC},
year={2014},
volume={10},
pages={6890-6899},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938257663&partnerID=40&md5=db994c617ba005bb82cbab81aa068155},
affiliation={Johns Hopkins University Applied Physics Laboratory, United States, United States},
abstract={Ground segments visualize, analyze, fuse, condition, and otherwise process telemetry received from spacecraft. Dedicated server software, referred to as Command and Telemetry (C&T) Cores, feed and manage telemetry archives. The diversity of commercially available C&T Core products provides flexibility for missions of varying size and budget, but complicates the engineering of re-usable software applications. Individual C&T Cores provide different features, have different Application Programming Interfaces (APIs), and are written in different programming languages; they may be deployed across hardware platforms with incompatible bit representations, data precision, and operating systems. Missions often must balance the cost savings of a less-expensive C&T Core with the added expense of refactoring ground software applications, even when these applications represent mission-independent capabilities. Avoiding redactors dramatically reduces software development, testing, training, and maintenance while allowing missions to contain costs by selecting the C&T Core that best matches their budget. Applications operating on mission telemetry must be built on an abstracting layer that hides the underlying C&T Core. Implementing that layer as a messaging protocol insulates software from incompatibilities in hardware and operating systems. This paper presents work performed by The Johns Hopkins University Applied Physics Laboratory (JHU/APL) Space Exploration Sector (SES) Ground Application Group (SIG) to develop a Telemetry Request Protocol (TRP) and build protocol-driven ground software applications for telemetry distribution. We demonstrate the efficiency and flexibility of the TRP, as part of the ground architecture, in requesting telemetry of various types and formats common to our NASA missions. We describe the desirable properties, architecture, and messages of the TRP and show examples of its deployment as a standard way of communicating amongst telemetry providers and consumers. We quantify performance benefits from using binary representations (Google Protocol Buffers) versus ASCII data (XML). We discuss the implementation of this protocol and its associated servers, clients, and applications in support of NASA missions such as the Van Allen Probes and Solar Probe Plus. Additionally, we discuss lessons learned from this standardization and implementation effort to include time representations and sorting, protocol streaming performance, state transitions, and error handling. We conclude that targeting standardization to ground system functions, such as telemetry retrieval, provides opportunity for incremental application and thus wider adoption. We observe that our implementation of the TRP proves the price/performance of this approach and demonstrates the feasibility of cross-mission, cross product reuse. Finally, we discuss the implications of protocols as a model for incrementally standardizing other ground segment functions. Copyright © 2014 by the International Astronautical Federation.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ahlbrandt2014743,
author={Ahlbrandt, S. and Huish, D. and Burr, C. and Hamilton, R.},
title={The space operations simulation center: A 6DOf laboratory for testing relative navigation systems},
journal={Advances in the Astronautical Sciences},
year={2014},
volume={151},
pages={743-756},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930173534&partnerID=40&md5=a2b3360042354cedfc691c3c1e5f6548},
affiliation={SOSC Lockheed Martin Space Systems Company, Littleton, CO  80125, United States; Lockheed Martin Space Systems Company, Littleton, CO  80125, United States},
abstract={The Space Operations Simulation Center (SOSC) on the Lockheed Martin campus southwest of Denver Colorado is a sophisticated, high fidelity laboratory designed for testing hardware-in-the-loop relative navigation systems. Using six degree-of-freedom (6DOF) mechanisms, or robots, that precisely maneuver on an ultra-stable pier throughout a large high bay, the SOSC is capable of simulating full scale spacecraft motion relative to another object or point in space. The carrying capacity of the robots and range of motion allow for integration of complete sensor suites and spacecraft systems. The SOSC has proved to be a unique test environment for a diverse user base such as development teams from NASA centers, space sensor suppliers, internal Lockheed Martin R&D projects and even university senior design teams. Testing has been performed for all phases of project development; from proof of concepts through flight hardware and flight software design and integration. The laboratory supports the evaluation of all the components of relative navigation missions, including passive and active sensors, mechanisms, algorithms, models and software, as well as the integration of these elements into subsystems and systems for development and test-like-you-fly verification. Both closed and open-loop control of the relative robot motion has been implemented in these activities. This paper gives a brief introduction to the lab and presents the lab's superior capabilities and operational flow by describing some recent test campaigns, major challenges overcome, and the test outcomes. Examples of projects include cross-country remote operations and ongoing closed loop rendezvous and docking maneuvers to a full scale model of an ISS docking port using the STORRM VNS LIDAR from STS-134.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schindler2014412,
author={Schindler, I. and Mendrok, R. and Kawulok, P. and Unucka, P. and Kawulok, R. and Opěla, P. and Rusz, S. and Turoň, R. and Turoňová, P. and Buřuta, A.},
title={Plastometric study of hot deformation behaviour of steel X10CRWMOVNB9-2},
journal={METAL 2014 - 23rd International Conference on Metallurgy and Materials, Conference Proceedings},
year={2014},
pages={412-417},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924970465&partnerID=40&md5=055867c69badb96ffd24cdb74325c18d},
affiliation={VSB - Technical University of Ostrava, Faculty of Metallurgy and Materials Engineering, Ostrava, Czech Republic; Material and Metallurgical Research S.R.O., Ostrava, Czech Republic; Třinecké Železárny, A.S., Třinec, Czech Republic},
abstract={Phase composition of the studied high-alloy martensitic steel has been analyzed by metallographic methods, plastometric tests and thermodynamic calculations performed in the Thermo-Calc Software. Basic parameters of the formability and deformation resistance were determined depending on the forming temperature (800-1250 °C) and strain rate (0.1 - 17 s-1). Continuous tests to fracture were performed at two individual plastometers which enable performance of the uniaxial tension (Gleeble 3800) and/or torsion (SETARAM). The results were processed into the illustrative 3D-maps that also reflect the effect of the phase composition at various deformation conditions. The mean forming temperature increased by tens of °C during some torsion tests due to deformation heating and this value had to be corrected. Since both plastometers work with different forming conditions and diverse state of stresses in the deformed sample, the mutual comparison of thus obtained results is interesting from the methodology viewpoint. The results of tension and torsion tests are quite compatible and display the positive influence of higher temperature and strain rate on the formability of the given steel.},
author_keywords={Deformation resistance;  Hot formability;  Martensitic steel;  Torsion test;  Uniaxial tension test},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chang20141098,
author={Chang, Y.H.},
title={Based on digital image forensics double JPEG compression algorithm},
journal={Advanced Materials Research},
year={2014},
volume={1044-1045},
pages={1098-1101},
doi={10.4028/www.scientific.net/AMR.1044-1045.1098},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922056519&doi=10.4028%2fwww.scientific.net%2fAMR.1044-1045.1098&partnerID=40&md5=53857f05312726e689861c054f3ef41d},
affiliation={Shandong Yingcai University, Shandong, China},
abstract={Means of digital image tampering diverse, new technical approach increasingly renovation, therefore, the detection of forged digital tampering issues are complex, and must not universal, once and for all solutions. In this paper, digital image tampering common practices through research, proposed a re-deposit operations for double JPEG image processing software compression testing technology method using image compression and decompression process lossy compression of images tamper with forensic, this method can play an effective role in the detection. © (2014) Trans Tech Publications, Switzerland.},
author_keywords={Double compression;  Image forensics;  Quantization matrix},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2014265,
author={Liu, Y. and Zhang, Y.X.},
title={Application of optimized parameters SVM in deformation prediction of creep landslide tunnel},
journal={Applied Mechanics and Materials},
year={2014},
volume={675-677},
pages={265-268},
doi={10.4028/www.scientific.net/AMM.675-677.265},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920829157&doi=10.4028%2fwww.scientific.net%2fAMM.675-677.265&partnerID=40&md5=a8c6cfba26df2b3869093493406cbaff},
affiliation={Jilin Railway Technology College, Jilin, Jilin  132200, China; Institue of Electric and Information Engineering, Beihua University Jilin, Jilin, 132021, China},
abstract={Creep landslide tunnel deformation is a diversity, changeability, less information, complicated nonlinear problem, it’s unable to establish accurate mathematical model. A creep landslide tunnel deformation prediction model based on SVM was constructed in this paper to enhance prediction accuracy, and penalty parameter c and Kernel function parameter g of SVM were optimized by genetic algorithms (GA).nine closely related factors in creep landslide tunnel deformation were selected as Input vector of SVM, creep landslide tunnel deformation measured value as a model target output. In Matlab 2011b simulation software,80 groups observation data from 2012 to 2013 of Laoyeling tunnel in Changchun to Hunchun highway of Jilin province as the sample data, 70 groups were used as training set, other 10 groups were used as testing set. The simulation result shows that testing value is very close to the true value in this method, the average relative error close to 2%. Effectiveness of the creep landslide tunnel deformation prediction based on GA_SVM model is verified by experiments. © (2014) Trans Tech Publications, Switzerland. All Rights Reserved.},
author_keywords={Creep landslide tunnel;  Displacement;  GA;  Prediction;  SVM},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{He20141972,
author={He, C. and Liu, Y.F.},
title={Research on software testing to ensure web application usability, reliability and security},
journal={Advanced Materials Research},
year={2014},
volume={1049-1050},
pages={1972-1976},
doi={10.4028/www.scientific.net/AMR.1049-1050.1972},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920658622&doi=10.4028%2fwww.scientific.net%2fAMR.1049-1050.1972&partnerID=40&md5=0f6418392c162ba71abbe0532c219d8d},
affiliation={Zhongshan polytechnic, China},
abstract={Compared with traditional web sites, there are some new features on modern web applicat ions, as follows: dynamic functionalities, diverse representation, uncertainty for running performance, innovative data handling and data transferring mechanism, vulnerability Subsequently, the problems in testing web application are discussed from functional testing, reliability testing and security testing. At last, in order to solve these problems, new testing methods are proposed, which are systematic we b application testing method, random test methods, reliability testing methods and security testing methods. © (2014) Trans Tech Publications, Switzerland.},
author_keywords={Software testing;  Web application reliability;  Web application security;  Web application usability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2014,
author={Zhang, L. and Li, H. and Ding, L.},
title={Effects of axis ratio on the vortex-induced vibration and energy harvesting of rhombus cylinder},
journal={American Society of Mechanical Engineers, Power Division (Publication) POWER},
year={2014},
volume={2},
doi={10.1115/POWER2014-32156},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84911976475&doi=10.1115%2fPOWER2014-32156&partnerID=40&md5=314cfd4217ed781340c315c7b9ff9d56},
affiliation={Key Lab. of Low-Grade Ener. Utiliz. Technol. and Syst. of Min. of Educ. of China, Chongqing University, Chongqing, China},
abstract={The vortex-induced vibrations of a rhombus cylinder are investigated using two-dimensional unsteady Reynolds-Averaged Navier-Stokes simulations at high Reynolds numbers ranging from 10,000 to 120,000. The rhombus cylinder is constrained to oscillate in the transverse direction, which is perpendicular to the flow velocity direction. Three rhombus cylinders with different axis ratio (AR=0.5, 1.0, 1.5) are considered for comparison. The simulation results indicate that the vibration response and the wake modes are dependent on the axis ratio of the rhombus cylinder. The amplitude ratios are functions of the Reynolds numbers. And as the AR increases, higher peak amplitudes can be made over a significant wide band of Re. On the other hand, a narrow lock-in area is observed for AR=0.5 and AR=1.5 when 30,000<Re<50,000, but the frequency ratio of AR=1.0 monotonically increases at a nearly constant slope in the whole Re range. The vortex shedding mode is always 2S mode in the whole Re range for AR=0.5. However, the wake patterns become diverse with the increasing of Re for AR=1.0 and 1.5. In addition, the mechanical power output of each oscillating rhombus cylinder is calculated to evaluate the efficiency of energy transfer in this paper. The theoretical mechanical power P between water and a transversely oscillating cylinder is achieved. On the base of analysis and comparison, the rhombus cylinder with AR=1.0 is more suitable for harvesting energy from fluid. Copyright © 2014 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20141,
author={Chen, L. and May, J.},
title={A diversity model based on failure distribution and its application in safety cases},
journal={Proceedings - 8th International Conference on Software Security and Reliability, SERE 2014},
year={2014},
pages={1-10},
doi={10.1109/SERE.2014.13},
art_number={6895410},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910028614&doi=10.1109%2fSERE.2014.13&partnerID=40&md5=6009514257e85485c50565d58c65d92d},
affiliation={Safety Systems Research Centre, University of Bristol, Bristol, BS8 1TR, United Kingdom},
abstract={This work develops a new basis for evaluating the reliability benefits of diverse software, based on fault injection testing. In particular, the work investigates forms of argumentation that could be used to justify diversity as a basis for the construction of safety claims. Failure distributions of two versions of diverse software under various fault conditions are revealed separately by fault injection methods, and then the common failure probability of the version-pair can be estimated. The approach is justified theoretically, and cross validated with other work. This method is also used to explain the fundamental influence of failure distributions on diversity. © 2014 IEEE.},
author_keywords={Fault injection;  Multi-version;  Reliability;  Safety case;  Safety critical system;  Software diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jameel2014,
author={Jameel, T. and Lin, M. and Li, H. and Hou, X.},
title={Test image generation using segmental symbolic evaluation for unit testing},
journal={2014 IEEE/ACIS 15th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2014 - Proceedings},
year={2014},
doi={10.1109/SNPD.2014.6888718},
art_number={6888718},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908689423&doi=10.1109%2fSNPD.2014.6888718&partnerID=40&md5=70c94d1052ca03831fd05c3c891fbff5},
affiliation={State Key Lab of Software Development Environment, Beihang University, Beijing, China},
abstract={This paper presents a novel technique to generate test images using segmental symbolic evaluation for testing of image processing applications. Images are multidimensional and diverse in nature, which leads to different challenges for the testing process. A technique is required to generate test images capable of finding program paths derived by image pixels. The proposed technique is based on symbolic execution which is extensively used for test data generation in recent years. In image processing applications, pixel operations such as averaging, convolution etc. are applied on a segment of input image pixels called window for a single iteration and repeated for the entire image. Our key idea is to imitate operations on pixel window using symbolic values rather than concrete ones to generate path constraints in the program under test. The path constraints generated for different paths are solved for concrete values using our simple SAT solver and the solutions are capable to guide program execution to the specific paths. The solutions of path constraints are used to generate synthetic test images for each identified path and the paths constraints which are not solvable for concrete pixel values are reported as infeasible paths. We have developed a tool IMSUITthat takes an image processing function as input and executes the program symbolically for the given pixels window to generate test images. Effectiveness of IMSUIT is tested on different modules of an optical character recognition system and the result shows that it can successfully create test images for each path of the program under test and capable of identifying infeasible paths. © 2014 IEEE.},
author_keywords={symbolic execution;  testing image processing;  unit testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reveco2014,
author={Reveco, J. and Mora, M. and Shen, T.-C. and Soto, R. and Sepulveda, J. and Ibsen, J.},
title={Implementing kanban for agile process management within the aLMA Software Operations Group},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2014},
volume={9152},
doi={10.1117/12.2055646},
art_number={91521M},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906901601&doi=10.1117%2f12.2055646&partnerID=40&md5=ee4f3f94a09b3581fdef1c8321f9d681},
affiliation={Atacama Large Millimeter Array (ALMA), Kilometro 121 Carretera CH 23, San Pedro de Atacama, Chile; National Radio Astronomy Observatory (NRAO), Charlottesville, VA, United States; European Southern Observatory (ESO), Av Alonso de Cordova 3107, Santiago, RM, Chile},
abstract={After the inauguration of the Atacama Large Millimeter/submillimeter Array (ALMA), the Software Operations Group in Chile has refocused its objectives to: (1) providing software support to tasks related to System Integration, Scientific Commissioning and Verification, as well as Early Science observations; (2) testing the remaining software features, still under development by the Integrated Computing Team across the world; and (3) designing and developing processes to optimize and increase the level of automation of operational tasks. Due to their different stakeholders, each of these tasks presents a wide diversity of importances, lifespans and complexities. Aiming to provide the proper priority and traceability for every task without stressing our engineers, we introduced the Kanban methodology in our processes in order to balance the demand on the team against the throughput of the delivered work. The aim of this paper is to share experiences gained during the implementation of Kanban in our processes, describing the difficulties we have found, solutions and adaptations that led us to our current but still evolving implementation, which has greatly improved our throughput, prioritization and problem traceability. © 2014 SPIE.},
author_keywords={agile;  ALMA;  Kanban;  software support},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang2014,
author={Wang, H. and Chan, W.K. and Tse, T.H.},
title={Improving the effectiveness of testing pervasive software via context diversity},
journal={ACM Transactions on Autonomous and Adaptive Systems},
year={2014},
volume={9},
number={2},
doi={10.1145/2620000},
art_number={9},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906849956&doi=10.1145%2f2620000&partnerID=40&md5=331a1bb559cfa23e1a51923f1d5fabd9},
affiliation={Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong},
abstract={Context-aware pervasive software is responsive to various contexts and their changes. A faulty implementation of the context-aware features may lead to unpredictable behavior with adverse effects. In software testing, one of the most important research issues is to determine the sufficiency of a test suite to verify the software under test. Existing adequacy criteria for testing traditional software, however, have not explored the dimension of serial test inputs and have not considered context changes when constructing test suites. In this article, we define the concept of context diversity to capture the extent of context changes in serial inputs and propose three strategies to study how context diversity may improve the effectiveness of the data-flow testing criteria. Our case study shows that the strategy that uses test cases with higher context diversity can significantly improve the effectiveness of existing data-flow testing criteria for context-aware pervasive software. In addition, test suites with higher context diversity are found to execute significantly longer paths, which may provide a clue that reveals why context diversity can contribute to the improvement of effectiveness of test suites. © 2014 ACM.},
author_keywords={Context diversity;  Context-aware program;  Test adequacy},
document_type={Article},
source={Scopus},
}

@ARTICLE{Barbieri2014,
author={Barbieri, F.A.A. and Lima, V.D.A. and Garbin, L. and Boaretto, J.},
title={Rollover study of a heavy truck combination with two different semi-trailer suspension configurations},
journal={SAE Technical Papers},
year={2014},
volume={3},
doi={10.4271/2014-36-0025},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906751771&doi=10.4271%2f2014-36-0025&partnerID=40&md5=db52ce4c0de011e17f38dde9e510b87d},
affiliation={Try Testes e Simulações, Brazil; Virtual CAE, Brazil; Guerra Implementos Rodoviários, Brazil},
abstract={Brazil presents a very diverse road and traffic conditions and due to several factors the number of truck accidents is very high. Inside truck accidents group, the one that causes the highest number of losses and fatalities is the rollover crash and understanding rollover dynamics is very important to prevent such events. The diversity of cargo vehicles arrangements requires a detailed study regarding the dynamic behavior these vehicle combinations in order to increase operation safety. The same tractor unit can be used with different types and numbers of trailers and/or semi-trailers, each one with different suspension configurations. These truck combinations have distinct dynamic performances that need evaluation. In this sense, this work presents a first phase study on the dynamic behavior of different types of cargo vehicle configuration. A 6×2 tractor is combined with a two distinct grain semi-trailer with different types of suspension: pneumatic and leaf spring. The study is conducted in order to verify the difference in dynamic behavior and the resulting stability of the two configurations in different conditions of speed and maneuvers. The study was conducted with simulation models developed within TruckSim® simulation software, with calibrated data from tests previously performed. The simulations were carried out according to the main standards tests used for this kind of analysis - Slalom, Double Lane Change (DLC) and Step Steer maneuvers, to check the directional behavior and limit rollover of both truck combinations. The simulations showed significant changes with respect to the behavior of the combinations, depending on the type of used suspension, especially with regard to the rollover tendency. Different types of combinations with the same tractor show different lateral load transfer, not only depending on the arrangements and quantity of suspension axles, but also due to the different possible locations of loads. These results are interesting for a better understanding of the lateral dynamics of different cargo vehicle compositions, in order to establish better safety margins against the rollover threshold and to accomplish future setups for the electronic control stability systems of combination trucks. Copyright © 2014 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mäntylä20141597,
author={Mäntylä, M.V. and Itkonen, J.},
title={How are software defects found? the role of implicit defect detection, individual responsibility, documents, and knowledge},
journal={Information and Software Technology},
year={2014},
volume={56},
number={12},
pages={1597-1612},
doi={10.1016/j.infsof.2013.12.005},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906314016&doi=10.1016%2fj.infsof.2013.12.005&partnerID=40&md5=6bb2d284936b504b09e8f28ff2b0808d},
affiliation={Department of Computer Science and Engineering, Aalto University, Finland},
abstract={Context Prior research has focused heavily on explicit defect detection, such as formal testing and reviews. However, in reality, humans find software defects in various activities. Implicit defect detection activities, such as preparing a product demonstration or updating a user manual, are not designed for defect detection, yet through such activities defects are discovered. In addition, the type of documentation, and knowledge used, in defect detection is diverse. Objective To understand how defect detection is affected by the perspectives of responsibility, activity, knowledge, and document use. To provide illustrative numbers concerning the multidimensionality of defect detection in an industrial context. Method The data were collected with a survey on four software development organizations in three different companies. We designed the survey based on our prior extensive work with these companies. Results We found that among our subjects (n = 105), implicit defect detection made a higher contribution than explicit defect detection in terms of found defects, 62% vs. 38%. We show that defect detection was performed by subjects in various roles supporting the earlier reports of testing being a cross-cutting activity in software development organizations. We found a low use of test cases (18%), but a high use of other documents in software defect detection, and furthermore, we found that personal knowledge was applied as an oracle in defect detection much more often than documented oracles. Finally, we recognize that contextual factors largely affect the transferability of our results, and we provide elaborate discussion about the most important contextual factors. Furthermore, we must be cautious as the results were obtained with a survey, and come from a small number of organizations. Conclusions In this paper, we show the large impact of implicit defect detection activities in four case organizations. Implicit defect detection has a large contribution to defect detection in practice, and can be viewed as an extremely low-cost way of detecting defects. Thus, harnessing and supporting it better may increase quality without increasing costs. For example, if an employee can update the user manual, and simultaneously detect defects from the software, then the defect detection part of this activity can be seen as cost-free. Additionally, further research is needed on how diverse types of useful documentation and knowledge can be utilized in defect detection. © 2014 Elsevier B.V. All rights reserved.},
author_keywords={Activities;  Defect detection;  Documents;  Human factors;  Industrial questionnaire study;  Software testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Stefan2014,
author={Stefan, I. and Ivan, I. and Miclea, L.},
title={Assisted test case design using contextual information by DOM exploration},
journal={Proceedings of 2014 IEEE International Conference on Automation, Quality and Testing, Robotics, AQTR 2014},
year={2014},
doi={10.1109/AQTR.2014.6857861},
art_number={6857861},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905844089&doi=10.1109%2fAQTR.2014.6857861&partnerID=40&md5=bf0ebc03c90f1b399be00c1445057664},
affiliation={Automation Department, Technical University of Cluj-Napoca, Romania},
abstract={The paper proposes a method to use contextual data from the web applications DOM (Document Object Model) to aid test cases generation for functional testing. The objective is to enhance the automation by reducing the time allocated to obtain the input values and the rough steps of the test cases. The DOM architecture for Web applications will be considered the starting point in the development of the method. The discussion remains if the tester inspection is needed in order to choose between the diversity of test cases automatically generated or the tests will be entirely executed without exception. The required system resources will be taken in consideration to repeatedly run all the tests in regression testing. In the case of applying this method to several user interfaces, by saving the extracted properties and the generated test cases and results, statistical data regarding effective templates to use would emerge. © 2014 IEEE.},
author_keywords={automated test vector generatoration;  context;  DOM;  test vector},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Singh2014160,
author={Singh, P. and Smith, K. and Rao, B.},
title={An advanced placement approach for chemical sand and fines control using zeta potential altering chemistry},
journal={Proceedings of the Annual Offshore Technology Conference},
year={2014},
volume={1},
pages={160-168},
doi={10.2118/24690-ms},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905725367&doi=10.2118%2f24690-ms&partnerID=40&md5=f6c18ce8f3aa1fb781ff31c582c37b32},
affiliation={BTechSoft, United States},
abstract={Sand and fines production is one of the oldest problems in petroleum industry and one of the toughest to solve. Today, many active and passive technologies exist; in some cases sand and fines production is manageable, while for others it cannot be tolerated at all. Also, many wells do not produce sand or fines from the onset and may not require an active sand control solution until later in their life. Chemical sand control solutions have been around for many years have always been attractive due to their ability to be installed without any restrictions to the wellbore geometry. However, due to the difficulties with placement and their association with some degree of reduction in permeability, there has been some reservation to use chemical methods as a standard. This paper presents a unique chemistry that not only increases the maximum sand/fines free rate without a significant reduction in permeability, but also discusses the advanced placement techniques essential for successful application of chemical sand control treatments. This paper includes review of laboratory tests, treatment design considerations, few case studies which have been treated by using zeta potential altering chemistry, and depicts analysis of simulation results from an integrated software package for fluid diversion and placement. Copyright 2014, Offshore Technology Conference.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Poulding20141279,
author={Poulding, S. and Feldt, R.},
title={Generating structured test data with specific properties using nested monte-carlo search},
journal={GECCO 2014 - Proceedings of the 2014 Genetic and Evolutionary Computation Conference},
year={2014},
pages={1279-1286},
doi={10.1145/2576768.2598339},
note={cited By 17},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905675476&doi=10.1145%2f2576768.2598339&partnerID=40&md5=2f92f83666be8028b81951182a020f07},
affiliation={University of York, York, YO10 5GH, United Kingdom; Blekinge Institute of Technology, SE-371 79 Karlskrona, Sweden},
abstract={Software acting on complex data structures can be challenging to test: it is dificult to generate diverse test data that satisfies structural constraints while simultaneously exhibiting properties, such as a particular size, that the test engineer believes will be effective in detecting faults. In our previous work we introduced Gödel Test, a framework for generating such data structures using non-deterministic programs, and combined it with Differential Evolution to optimize the generation process. Monte-Carlo Tree Search (MCTS) is a search technique that has shown great success in playing games that can be represented as a sequence of decisions. In this paper we apply Nested Monte-Carlo Search, a single-player variant of MCTS, to the sequence of decisions made by the generating programs used by GödelTest, and show that this combination can efficiently generate random data structures which exhibit the specific properties that the test engineer requires. We compare the results to Boltzmann sampling, an analytical approach to generating random combinatorial data structures. © 2014 ACM.},
author_keywords={Data structures;  Nested Monte-Carlo Search;  Search-based software engineering;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schulte2014281,
author={Schulte, E. and Fry, Z.P. and Fast, E. and Weimer, W. and Forrest, S.},
title={Software mutational robustness},
journal={Genetic Programming and Evolvable Machines},
year={2014},
volume={15},
number={3},
pages={281-312},
doi={10.1007/s10710-013-9195-8},
note={cited By 48},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905644483&doi=10.1007%2fs10710-013-9195-8&partnerID=40&md5=f66918d315b19d3a028f56298953eb92},
affiliation={Department of Computer Science, University of New Mexico, Albuquerque NM, United States; Department of Computer Science, University of Virginia, Charlottesville VA, United States; Department of Computer Science, Stanford University, Palo Alto CA, United States; Santa Fe Institute, Santa Fe NM, United States},
abstract={Neutral landscapes and mutational robustness are believed to be important enablers of evolvability in biology. We apply these concepts to software, defining mutational robustness to be the fraction of random mutations to program code that leave a program's behavior unchanged. Test cases are used to measure program behavior and mutation operators are taken from earlier work on genetic programming. Although software is often viewed as brittle, with small changes leading to catastrophic changes in behavior, our results show surprising robustness in the face of random software mutations. The paper describes empirical studies of the mutational robustness of 22 programs, including 14 production software projects, the Siemens benchmarks, and four specially constructed programs. We find that over 30 % of random mutations are neutral with respect to their test suite. The results hold across all classes of programs, for mutations at both the source code and assembly instruction levels, across various programming languages, and bear only a limited relation to test suite coverage. We conclude that mutational robustness is an inherent property of software, and that neutral variants (i.e., those that pass the test suite) often fulfill the program's original purpose or specification. Based on these results, we conjecture that neutral mutations can be leveraged as a mechanism for generating software diversity. We demonstrate this idea by generating a population of neutral program variants and showing that the variants automatically repair latent bugs. Neutral landscapes also provide a partial explanation for recent results that use evolutionary computation to automatically repair software bugs. © 2013 Springer Science+Business Media New York.},
author_keywords={Genetic programming;  Mutation testing;  Mutational robustness;  N-version programming;  Neutral landscapes;  Proactive diversity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gabmeyer201478,
author={Gabmeyer, S.},
title={Quality assurance in MBE back and forth},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2014},
volume={8570 LNCS},
pages={78-81},
doi={10.1007/978-3-319-09099-3_6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905035504&doi=10.1007%2f978-3-319-09099-3_6&partnerID=40&md5=79f72055de95d8619c8bfd94a934a072},
affiliation={Institute of Software Technology and Interactive Systems, Vienna University of Technology, Vienna, Austria},
abstract={The maturing of model-based engineering (MBE) has led to an increased interest and demand on the verification of MBE artifacts. Numerous verification approaches have been proposed in the past and, in fact, due to their diversity it is sometimes difficult to determine whether a suitable approach for the verification task at hand exists. In the first part of this tutorial, we thus present a classification for verification approaches of MBE artifacts that allows us to categorize approaches, among others, according to their intended verification goal and the capabilities of their verification engine. Based thereon, we briefly overview the landscape of existing verification approaches. In the second part, we iteratively build and verify the behavioral correctness of a small software system with our OCL-based model checker MocOCL. © 2014 Springer International Publishing Switzerland.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zeiada20141385,
author={Zeiada, W.A. and Underwood, B.S. and Kaloush, K.E.},
title={Uniaxial fatigue testing of diverse asphalt concrete mixtures},
journal={Asphalt Pavements - Proceedings of the International Conference on Asphalt Pavements, ISAP 2014},
year={2014},
volume={2},
pages={1385-1395},
doi={10.1201/b17219-168},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904101238&doi=10.1201%2fb17219-168&partnerID=40&md5=4b267ae09d51fdfb7b6a6e643f47a3fe},
affiliation={Department of Civil Environmental and Sustainable Engineering, Arizona State University, Tempe, AZ, United States},
abstract={The uniaxial fatigue test is a useful method for developing constitutive models to describe the fatigue behaviour of asphalt concrete mixture owing to the uniform states of stress across the specimen section. As part of the NCHRP 944- A project, a proposed uniaxial fatigue test protocol and software were developed to assess fatigue damage and healing. In this study, the protocol was used to evaluate a wide range of conventional and modified asphalt mixtures sampled from national and international projects as well as laboratory prepared mixtures with different volumetric properties. The study mixtures included those modified with rubber, polymer, fiber, warm mix additives, and combined rubber and warm mix. The fatigue analysis was performed using the simplified viscoelastic continuum damage (S-VECD) approach where the damage characteristic (C-S) curves were established for each mixture, and then used to obtain the fatigue relationships through simulated predictions. Overall, the proposed uniaxial fatigue test protocol was successfully used with respect to the S-VECD formulation to capture fatigue behaviour of all tested mixtures. © 2014 Taylor & Francis Group, London.},
author_keywords={Asphalt mixture;  Continuum damage;  Fatigue;  Uniaxial;  Viscoelastic},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu20142699,
author={Liu, C. and Meng, C.},
title={Information integration in ATS based on ATML},
journal={Advanced Materials Research},
year={2014},
volume={962-965},
pages={2699-2702},
doi={10.4028/www.scientific.net/AMR.962-965.2699},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904090392&doi=10.4028%2fwww.scientific.net%2fAMR.962-965.2699&partnerID=40&md5=756aa0b2640e4c75322b66c061e61a99},
abstract={We describe the conclusions of a technology and communities survey supported by concurrent and follow-on proof-of-concept prototyping to evaluate feasibility of defining a durable, versatile, reliable, visible software interface to support strategic modularization of test software development. The objective is that test sets and support software with diverse origins, ages, and abilities can be reliably integrated into test configurations that assemble and tear down and reassemble with scalable complexity in order to conduct both parametric tests and monitored trial runs. The resulting approach is based on integration of three recognized technologies that are currently gaining acceptance within the test industry and when combined provide a simple, open and scalable test orchestration architecture that addresses the objectives of the Automation Hooks task. The technologies are automated discovery using multicast DNS Zero Configuration Networking (zeroconf), commanding and data retrieval using resource-oriented Restful Web Services, and XML data transfer formats based on Automatic Test Markup Language (ATML). This open-source standards-based approach provides direct integration with existing commercial off-the-shelf (COTS) analysis software tools. © (2014) Trans Tech Publications, Switzerland.},
author_keywords={Software management;  Software reusability;  Software standards;  Test equipment;  Test facilities;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Swiech2014,
author={Swiech, M. and Hale, K.C. and Dinda, P.},
title={VMM emulation of intel hardware transactional memory},
journal={Proceedings of the 4th International Workshop on Runtime and Operating Systems for Supercomputers, ROSS 2014 - In Conjunction with ICS 2014},
year={2014},
doi={10.1145/2612262.2612265},
art_number={2612265},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903641160&doi=10.1145%2f2612262.2612265&partnerID=40&md5=376fb15e1b2d87ce304289fc6af91ab9},
affiliation={Department of Electrical Engineering and Computer Science, Northwestern University, United States},
abstract={We describe the design, implementation, and evaluation of emulated hardware transactional memory, specifically the Intel Haswell Restricted Transactional Memory (RTM) architectural extensions for x86/64, within a virtual machine monitor (VMM). Our system allows users to investigate RTM on hardware that does not provide it, debug their RTM-based transactional software, and stress test it on diverse emulated hardware configurations, including potential future configurations that might support arbitrary length trans- actions. Initial performance results suggest that we are able to accomplish this approximately 60 times faster than under a full emulator. A noteworthy aspect of our system is a novel page-flipping technique that allows us to completely avoid instruction emulation, and to limit instruction decoding to only that necessary to determine instruction length. This makes it possible to implement RTM emulation, and poten- tially other techniques, far more compactly than would oth- erwise be possible. We have implemented our system in the context of the Palacios VMM. Our techniques are not specific to Palacios, and could be implemented in other VMMs.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2014440,
author={Wang, Y. and Person, S. and Elbaum, S. and Dwyer, M.B.},
title={A framework to advise tests using tests},
journal={36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},
year={2014},
pages={440-443},
doi={10.1145/2591062.2591106},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903633452&doi=10.1145%2f2591062.2591106&partnerID=40&md5=59373af34c99b48e05eaf74a8a696865},
affiliation={University of Nebraska-Lincoln, Lincoln, NE 68588, United States; NASA LaRC, Hampton, VA 23681, United States},
abstract={Tests generated by different approaches can form a rich body of information about the system under test (SUT), which can then be used to amplify the power of test suites. Diversity in test representations, however, creates an obstacle to extracting and using this information. In this work, we introduce a test advice framework which enables extraction and application of information contained in existing tests to help improve other tests or test generation techniques. Our framework aims to 1) define a simple, yet expressive test case language so that different types of tests can be represented using a unified language, and 2) define an advice extraction function that enables the elicitation and application of the information encoded in a set of test cases. Preliminary re-sults show how test advice can be used to generate amplified test suites with higher code coverage and improved mutants killed scores over the original test suite. Copyright © 2014 ACM.},
author_keywords={Automated test generation;  Regression testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ghandehari201468,
author={Ghandehari, L.Sh. and Czerwonka, J. and Lei, Y. and Shafiee, S. and Kacker, R. and Kuhn, R.},
title={An empirical comparison of combinatorial and random testing},
journal={Proceedings - IEEE 7th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2014},
year={2014},
pages={68-77},
doi={10.1109/ICSTW.2014.8},
art_number={6825640},
note={cited By 27},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903585671&doi=10.1109%2fICSTW.2014.8&partnerID=40&md5=80a8a1b89b521b99ac83ebc117bc5fa9},
affiliation={Dept. of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX 76019, United States; Microsoft Research, Redmond, WA 98052, United States; Information Technology Laboratory, National Institute of Standards and Technology, Gaithersburg, MD 20899, United States},
abstract={Some conflicting results have been reported on the comparison between t-way combinatorial testing and random testing. In this paper, we report a new study that applies t-way and random testing to the Siemens suite. In particular, we investigate the stability of the two techniques. We measure both code coverage and fault detection effectiveness. Each program in the Siemens suite has a number of faulty versions. In addition, mutation faults are used to better evaluate fault detection effectiveness in terms of both number and diversity of faults. The experimental results show that in most cases, t-way testing performed as good as or better than random testing. There are few cases where random testing performed better, but with a very small margin. Overall, the differences between the two techniques are not as significant as one would have probably expected. We discuss the practical implications of the results. We believe that more studies are needed to better understand the comparison of the two techniques. © 2014 IEEE.},
author_keywords={Combinatorial Testing;  Random Testing;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Choudhary2014642,
author={Choudhary, S.R.},
title={Cross-platform testing and maintenance of web and mobile applications},
journal={36th International Conference on Software Engineering, ICSE Companion 2014 - Proceedings},
year={2014},
pages={642-645},
doi={10.1145/2591062.2591097},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903554821&doi=10.1145%2f2591062.2591097&partnerID=40&md5=f365f9904b75ecddbe225d33e2874661},
affiliation={College of Computing, Georgia Institute of Technology, 266 Ferst Dr, Atlanta, GA, United States},
abstract={Modern software applications are expected to run on a vari-ety of web and mobile platforms with diverse software and hardware level features. Thus, developers of such software need to duplicate the testing and maintenance effort on a wide range of platforms. Often developers are not able to cope with this increasing demand. Thus, they release soft-ware that is broken on certain platforms affecting a class of customers using such platforms. The goal of my work is to improve the testing and maintenance of cross-platform ap-plications by developing automated techniques for matching such applications across the different platforms. Copyright © 2014 ACM.},
author_keywords={Analysis;  Cross-platform;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dagiene2014644,
author={Dagiene, V. and Skupas, B. and Kurilovas, E.},
title={Programming assignments in virtual learning environments: Developments and opportunities for engineering education},
journal={International Journal of Engineering Education},
year={2014},
volume={30},
number={3},
pages={644-653},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901291434&partnerID=40&md5=3714fae608dfc8946c29fc8ad30f9e1d},
affiliation={Vilnius University, Institute of Mathematics and Informatics, 4 Akademijos Street, Vilnius 08663, Lithuania; Vilnius Gediminas Technical University, Sauletekio Ave. 11, 10223 Vilnius, Lithuania},
abstract={The aim of the paper is to present observations on automatic and semi-automatic assessment for programming assignments used in different e-learning contexts. Teaching of programming is an important part of different Informatics Engineering, Computer Science or Informatics, Computing, Information Technology and Communication courses in Universities and high schools. Students taking these courses have to demonstrate competences in problem solving and programming by creating working programs. Checking program validity is usually based on testing a program on diverse test cases. Testing for batch-type problems involves creating a set of input data cases, running a program submitted by a contestant with those input cases, analysing obtained outputs, etc. Assessment of programming assignments is as complex as testing of software systems. A lot of automatic assessment systems for programming assignments have been created to support teachers in submission assessment. However the problem of balance between the quality and the speed of assessment for programming assignments is important. Authors conducted the research on the possibilities of advanced semi-automatic approach in assessment, which can be used as compromise between manual and automatic assessment. A semi-automatic testing environment for evaluating programming assignments is developed, and the practical use of this system in Lithuania's optional programming maturity examination is presented. Presented research is useful for evaluating results of engineering education in general, and informatics/computer engineering education particularly. © 2014 TEMPUS Publications.},
author_keywords={Automatic and semi-automatic assessment;  Computer program assessment;  Engineering education;  Personalised feedback;  Programming assignments;  Virtual learning environment},
document_type={Article},
source={Scopus},
}

@ARTICLE{Al-Sewari2014255,
author={Al-Sewari, A.A. and Zamli, K.Z.},
title={An orchestrated survey on T-way test case generation strategies based on optimization algorithms},
journal={Lecture Notes in Electrical Engineering},
year={2014},
volume={291 LNEE},
pages={255-264},
doi={10.1007/978-981-4585-42-2_30},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900477548&doi=10.1007%2f978-981-4585-42-2_30&partnerID=40&md5=61747f8dfee12c2d8acf8c9e26139a89},
affiliation={Software Engineering Department, Faculty of Computer Systems and Software Engineering, Universiti Malaysia Pahang, 26300 Gambang, Kuantan Pahang, Malaysia},
abstract={The test case construction is amongst the most labor-intensive tasks and has significant influence on the effectiveness and efficiency in software testing. Due to the market needed for diverse types of tests, recently, several number of t-way testing strategies (where t indicates the interaction strengths) have been developed adopting different approaches Algebraic, Pure computational, and Optimization Algorithms (OpA). This paper presents an orchestrated survey of the existing OpA t-way strategies as Simulated Annealing (SA), Genetic Algorithm (GA), Ant Colony Algorithm (ACA), Particle Swarm Optimization based strategy (PSTG), and Harmony Search Strategy (HSS). The results demonstrate the strength and the limitations of each strategy, thereby highlighting possible research for future work in this area. © 2014 Springer Science+Business Media Singapore.},
author_keywords={Optimization algorithms;  Software and hardware testing;  T-way testing;  Test case generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Phadke2014,
author={Phadke, M.S. and Phadke, K.M.},
title={Utilizing design of experiments to reduce IT system testing cost},
journal={Proceedings - Annual Reliability and Maintainability Symposium},
year={2014},
doi={10.1109/RAMS.2014.6798451},
art_number={6798451},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900013252&doi=10.1109%2fRAMS.2014.6798451&partnerID=40&md5=8b5616dd8ae0e2383ca707e1a5b82b24},
affiliation={Shawnee Court, Colts Neck, NJ 07722, United States},
abstract={Orthogonal arrays are a powerful tool in quality and statistics. This paper shows new applications for improving testing effectiveness and efficiency in a multi-parameter environment, which is commonly encountered in today's software and systems. Unique aspects of this paper are applications in diverse areas - telecommunications, defense, automotive, information technology, and financial systems. Software and system testing cost the commercial and defense industry hundreds of millions of dollars annually. In addition, conducting each set of tests takes multiple man-months, delaying time to market of key technologies. In this current economic environment, organizations are looking for ways to reduce the cost of testing and time to market while ensuring that defects are not passed on to the customer. At the same time, the organizations are very reluctant to change their standard testing processes due to the heavy cost of field failures, regulatory concerns, and risk-averse culture. This paper describes a comparative study undertaken to assess the benefits of using Orthogonal Arrays (OA) for generating test plans in IT systems in the Financial Services industry. The formal process used for the comparative study consisted of enlisting the support of senior management and conducting multiple side-by-side pilots to compare the cost and risk of OA based testing versus the Business as Usual (BAU) testing practices. Our customers ran 20 side-by-side studies to evaluate the effectiveness of OA based testing and realized an average reduction in total test effort by 41%. In addition, all defects detected by the BAU process were detected by the OA based testing process. Further, in 40% of the cases, the OA based testing process found more defects and in the remaining 60% of the cases, BAU and Orthogonal Arrays found the same defects. The cost and schedule savings translated to tens of millions of dollars in labor and schedule. © 2014 IEEE.},
author_keywords={Design of Experiments;  Orthogonal Array;  Software and System Testing;  Test and Evaluation;  Validation;  Verification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Enteria2014243,
author={Enteria, N. and Yoshino, H. and Takaki, R. and Mochida, A. and Satake, A. and Baba, S. and Ishihara, H. and Yoshie, R.},
title={Case analysis of utilizing alternative energy sources and technologies for the single family detached house},
journal={Solar Energy},
year={2014},
volume={105},
pages={243-263},
doi={10.1016/j.solener.2014.03.005},
note={cited By 20},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899845146&doi=10.1016%2fj.solener.2014.03.005&partnerID=40&md5=c3a21f5f8df6fb2791cb49a145613abf},
affiliation={Enteria Grün Energietechnik, Davao 8000, Philippines; Architectural Institute of Japan, Tokyo 108-8414, Japan; Faculty of Engineering, Akita Prefectural University, Akita 010-0195, Japan; Faculty of Engineering, Tohoku University, Sendai 980-8579, Japan; Technical Research Institute, Maeda Corporation, Tokyo 179-8914, Japan; Earth Clean Tohoku Company, Ltd., Sendai 984-0038, Japan; Architecture Institute, Hokushu Company, Ltd., Tomiya 981-3341, Japan; Faculty of Engineering, Tokyo Polytechnic University, Atsugi 243-0297, Japan},
abstract={The new energy policy in Japan is implemented by renewable energy resources which are given a large slice of the total for their contribution to energy sources. In addition, there is a government plan to minimize, if not to eliminate nuclear power sources. Building sector energy consumption is increasing every year, hence, utilization and development of alternative energy sources and technologies to support the requirements of every house are important. Numerical performance evaluation of the alternative energy and technologies supported single family detached house was conducted in a transient system simulation (TRNSYS) program to evaluate its performance and energy requirements, and to test the cases for a possible upgrade to an energy generating house. The evaluation of the house performance is compared for possible application to the situation in different areas of Japan. 82.6% of the total primary energy supply could be supported by renewable energy sources - solar energy and biomass fueled auxiliary heater. 69.7% of the consumed energy is electricity; grid line electricity is still needed in the present installed photovoltaic roof tiles of the house. Solar energy collection can support up to 26% of the primary thermal energy requirement of the single family detached house. The remaining 70% should be supported by the back-up water heater of which fuel can be sourced from different sources such as biomass, kerosene, etc. In general, making a house yield higher thermal performance by employing energy conservation measures (ECM) coupled with the utilization of different alternative energy sources readily available in the house's vicinity has an impact on the reduction of the house's energy consumption. In addition, application of new technologies which could be supported by different energy sources has an impact on the diverse utilization of the available energy sources in the house's vicinity. © 2014 Elsevier Ltd.},
author_keywords={Alternative energy sources;  Clean energy technologies;  Energy efficient house},
document_type={Article},
source={Scopus},
}

@ARTICLE{Adelberg2014,
author={Adelberg, S. and Schrade, F. and Eckert, P. and Kraemer, L.},
title={Model based exhaust aftertreatment system integration for the development and calibration of ultra-low emission concepts},
journal={SAE Technical Papers},
year={2014},
volume={1},
doi={10.4271/2014-01-1554},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899524950&doi=10.4271%2f2014-01-1554&partnerID=40&md5=3d19026207aef276295a7e0a0815bd88},
affiliation={IAV GmbH, Diesel Exhaust Aftertreatment, Carnotstr. 1, 10587 Berlin, Germany},
abstract={The development and calibration of exhaust aftertreatment (EAT) systems for the most diverse applications of diesel powertrain concepts requires EAT models, capable of performing concept analysis as well as control and OBD system development and calibration. On the concept side, the choice of an application-specific EAT layout from a wide technology selection is driven by a number of requirements and constraints. These include statutory requirements regarding emissions of criteria pollutants and greenhouse gases (GHG), technical constraints such as engine-out emissions and packaging, as well as economic parameters such as fuel consumption, and EAT system and system development costs. Fast and efficient execution of the analysis and multi-criteria system optimization can be done by integrating the detailed EAT models into a total system simulation. On the control / OBD side, the software design, testing and calibration, of both EAT and engine, is efficiently supported by the integrated simulation approach. By coupling the EAT models with control algorithms, a virtual EAT test bench can be set up and used to develop control functionality, offline, in a reproducible test environment. After defining and coding control algorithms, the EAT models can be used for initial calibration, reducing the testing effort on engine and vehicle hardware. Additionally, the integrated system simulation can be used to check the robustness of a calibration in different vehicles, and to define the test matrix for the experimental fine-tuning. During the testing phase, the system simulation can support the verification of measurement plausibility and analysis. This paper describes a methodology for the integration of detailed physical-chemical EAT models and EAT control software in a total system simulation environment. The setup and calibration of the models is presented. After validation of the EAT models, the selection of an ultra-low diesel EAT concept is performed. For the selected concept, the model-based development and calibration of software algorithms is described. Finally, the use of the integrated system simulation for the transfer of EAT calibration to different vehicles is assessed. The results for an 8 liter diesel engine in two different heavy duty (HD) vehicle applications indicate sufficiently good performance for Euro VI HD certification. To illustrate a critical case, a bus application in urban "off-cycle" behavior is depicted. Copyright © 2014 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu20144,
author={Liu, H. and Kuo, F.-C. and Towey, D. and Chen, T.Y.},
title={How effectively does metamorphic testing alleviate the oracle problem?},
journal={IEEE Transactions on Software Engineering},
year={2014},
volume={40},
number={1},
pages={4-22},
doi={10.1109/TSE.2013.46},
art_number={6613484},
note={cited By 124},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898462811&doi=10.1109%2fTSE.2013.46&partnerID=40&md5=95f0a5b715d23d942b20802245be6856},
affiliation={Australia-India Centre for Automation Software Engineering, RMIT University, Melbourne 3001 VIC, Australia; Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn 3122 VIC, Australia; Division of Computer Science, University of Nottingham, Ningbo, 315100, Zhejiang, United Kingdom},
abstract={In software testing, something which can verify the correctness of test case execution results is called an oracle. The oracle problem occurs when either an oracle does not exist, or exists but is too expensive to be used. Metamorphic testing is a testing approach which uses metamorphic relations, properties of the software under test represented in the form of relations among inputs and outputs of multiple executions, to help verify the correctness of a program. This paper presents new empirical evidence to support this approach, which has been used to alleviate the oracle problem in various applications and to enhance several software analysis and testing techniques. It has been observed that identification of a sufficient number of appropriate metamorphic relations for testing, even by inexperienced testers, was possible with a very small amount of training. Furthermore, the cost-effectiveness of the approach could be enhanced through the use of more diverse metamorphic relations. The empirical studies presented in this paper clearly show that a small number of diverse metamorphic relations, even those identified in an ad hoc manner, had a similar fault-detection capability to a test oracle, and could thus effectively help alleviate the oracle problem. © 2014 IEEE.},
author_keywords={metamorphic relation;  metamorphic testing;  oracle problem;  Software testing;  test oracle},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Larner2014,
author={Larner, M. and Acker, J. and Dávila, L.P.},
title={The random porous structure and mechanical response of lightweight aluminum foams},
journal={Materials Research Society Symposium Proceedings},
year={2014},
volume={1662},
doi={10.1557/opl.2014.264},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897972492&doi=10.1557%2fopl.2014.264&partnerID=40&md5=a7f2e1c1b82cd0fd63dbc836a0b4253f},
affiliation={Materials Science and Engineering, School of Engineering, University of California Merced, 5200 N. Lake Road, Merced, CA 95343, United States; Computer Science and Engineering, School of Engineering, University of California Merced, 5200 N. Lake Road, Merced, CA 95343, United States},
abstract={Lightweight porous foams have been of particular interest in recent years, since they have a very unique set of properties which can be significantly different from their solid parent materials. These properties arise from their random porous structure which is generated through specialized processing techniques. Their unique structure gives these materials interesting properties which allow them to be used in diverse applications. In particular, highly porous Al foams have been used in aircraft components and sound insulation; however due to the difficulty in processing and the random nature of the foams, they are not well understood and thus have not yet been utilized to their full potential. The objective of this study was to integrate experiments and simulations to determine whether a relationship exists between the relative density (porous density/bulk density) and the mechanical properties of open-cell Al foams. Compression experiments were performed using an Instron Universal Testing Machine (IUTM) on ERG Duocel open-cell Al foams with 5.8% relative density, with compressive loads ranging from 0-6 MPa. Foam models were generated using a combination of an open source code, Voro++, and MATLAB. A Finite Element Method (FEM)-based software, COMSOL Multiphysics 4.3, was used to simulate the mechanical behavior of Al foam structures under compressive loads ranging from 0-2 MPa. From these simulated structures, the maximum von Mises stress, volumetric strain, and other properties were calculated. These simulation results were compared against data from compression experiments. CES EduPack software, a materials design program, was also used to estimate the mechanical properties of open-cell foams for values not available experimentally, and for comparison purposes. This program allowed for accurate prediction of the mechanical properties for a given percent density foam, and also provided a baseline for the Al foam samples tested via the IUTM method. Predicted results from CES EduPack indicate that a 5.8% relative density foam will have a Young's Modulus of 0.02-0.92 GPa while its compressive strength will be 0.34-3.37 MPa. Overall results revealed a relationship between pores per inch and selected mechanical properties of Al foams. The methods developed in this study can be used to efficiently generate open-cell foam models, and to combine experiments and simulations to calculate structure-property relationships and predict yielding and failure, which may help in the pursuit of simulation-based design of metallic foams. This study can help to improve the current methods of characterizing foams and porous materials, and enhance knowledge about theirproperties for novel applications. Copyright © 2014 Materials Research Society.},
author_keywords={foam;  porosity;  stress/strain relationship},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Breu201432,
author={Breu, R. and Kuntzmann-Combelles, A. and Felderer, M.},
title={New perspectives on software quality},
journal={IEEE Software},
year={2014},
volume={31},
number={1},
pages={32-38},
doi={10.1109/MS.2014.9},
art_number={6750444},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896288088&doi=10.1109%2fMS.2014.9&partnerID=40&md5=6e2e5fb2f50c47f1203ae65ab9be581d},
affiliation={University of Innsbruck, Austria},
abstract={This special issue, owing to its fundamental software quality focus, comprises a collection of diverse articles that address the challenges and directions for software quality research. The Web extra at http://youtu.be/ T7V4RSr1KEE is an audio interview in which Davide Falessi speaks with guest editors Annie Kuntzmann-Combelles, Michael Felderer, and Ruth Breu about methods for improving software quality management, testing, and security on intelligent and interconnected devices. © 1984-2012 IEEE.},
author_keywords={quality engineering;  software;  software quality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sun2014138,
author={Sun, H. and Teng, G. and Zhang, X. and He, G.},
title={Design and test on real-time measurement system of mat roller workload for sunlight greenhouse},
journal={Nongye Gongcheng Xuebao/Transactions of the Chinese Society of Agricultural Engineering},
year={2014},
volume={30},
number={1},
pages={138-145},
doi={10.3969/j.issn.1002-6819.2014.01.018},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892868308&doi=10.3969%2fj.issn.1002-6819.2014.01.018&partnerID=40&md5=984fe4f4c715d26d4d5e5c5c3bcd7543},
affiliation={College of Water Resources and Civil Engineering, China Agriculture University, Beijing 100083, China; China Agricultural University Jinwang (Beijing) Agricultural Engineering Technology Co. Ltd., Beijing 100083, China},
abstract={In the paper, a study was conducted on the workload change of a mat rolling machine, which belongs to the agricultural machinery research of a sunlight greenhouse. The specific content is the on-line measurement for the mat roller workload on the foundation of the summarization of current related research. To be exact, first, the paper analyzes the working conditions and operating feature of the mat roller. Then, choosing a dynamic torque sensor of the strain gauge type, the real-time load measurement system was designed and developed on the LabVIEW platform, with the application of VI (virtual instrument) technology. Finally, the on-line test system for mat roller workload was launched and completed, whose main hardware was composed of the sensor and the signal amplifier, the data acquisition card, the electrical control system for mat roller, the stroke limiter, and the accessory device for sensor installation, etc. Additionally, its staple software function consisted of acquiring data, processing data, saving data, and automatically controlling the mat roller. Generally, the rollers can be divided into the reel-type and the rope-type in light of its working principle. We choose the side mat roller with a reel pushing up covering and the rear mat roller with a rope pulling up covering as subjects. Therefore, for one thing, aiming at the reel-type, load change was measured out when employing heat-insulating coverings of two materials and three lengths. For another, aiming at the rope-type, load change was measured out when utilizing heat-insulating coverings of three materials and two conditions (dryness and wetness). Specifically, in the beginning, prior to have the workload test, the sensor was calibrated, together with systematically testing the system. Next, during the test, according to the different rolling machines, diverse sunlight greenhouses were selected while with the same type of roller was utilized. Secondly, regarding test data collection, after configuring a collected sample of 1, 000 and a sampling frequency of 10, 000 S/s, the mean value was acquired as the instant load at the moment by means of using the LabVIEW statistical module to count and analyze the sample. Thirdly, the roller ran continuously, thereby gathering the data that represented the load variation. Eventually, when the test was complete, the acquired data was disposed and analyzed with Excel spreadsheet software. The test, measuring the instant loads of a reel-type roller and a rope-type roller respectively, is the initial study on the real-time measurement for roller workload, whose entire data error does not exceed ± 6N. m. The results of the test showed that first, the mat roller with a reel pushing up acquires the maximum working load at the top of the sunlight greenhouse while the rear mat roller with a rope pulling up receives the maximum working load in an uncertain position. Secondly, the working load of the rope-type, whose change curve is more modest, is far less than that of the reel-type. Additionally, the rope-type mat roller, currently used in the production, has an overlarge maximum load. Finally, the study was initially focused on actually measuring the workload of the mat roller, thereby having some deficiencies. And there are three suggestions that can be carried out on deepening the research, including increasing the length of heat-insulating coverings, analyzing the relationship between the front roof slope of the sunlight greenhouse and the workload of the mat roller, and actually measuring the workload variation curves of the front mat roller with a reel pushing up covering.},
author_keywords={Agricultural machinery;  Greenhouses;  Loads;  Mat rolling machine;  Real time measurement;  Sunlight greenhouses},
document_type={Article},
source={Scopus},
}

@ARTICLE{Moustafa2014191,
author={Moustafa, A. and Al-Marghirani, A. and Al-Shomrani, M.M. and Al-Rababah, A.A.},
title={A new dynamic model for software testing quality},
journal={Research Journal of Applied Sciences, Engineering and Technology},
year={2014},
volume={7},
number={1},
pages={191-197},
doi={10.19026/rjaset.7.239},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888794555&doi=10.19026%2frjaset.7.239&partnerID=40&md5=c7bd6d66583450b796578b6ba0a169ef},
affiliation={Northern Border University, Saudi Arabia; King-Abdulaziz University, Saudi Arabia; Hail University, Saudi Arabia},
abstract={Research and study of Software Quality has Traditionally Focused on the Overall Product Quality Rather than on the sub-phase's milestones. There are many attempts over software testing as a standalone development phase which introduced in the literature; these efforts lacked the dynamic nature which has a diverse effect on the maintenance phase. This study will present the current software testing models; their challenges and at the end it will present our new dynamic model for applying quality assurance requirements over the sub-phases of the testing model. Quality properties may include as performance, efficiency, reliability, etc., new model will present for Improving the effectiveness and efficiency of the testing process through applying the quality requirements, designing high quality products, producing software with high Cost optimization, satisfying the product stakeholders. © Maxwell Scientific Organization, 2014.},
author_keywords={Dynamic model;  Software development life cycle;  Software quality;  Software testing;  Software verification and validation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gupta20131,
author={Gupta, V. and Chauhan, D.S. and Dutta, K.},
title={Incremental development & revolutions of E-learning software systems in education sector: a case study approach},
journal={Human-centric Computing and Information Sciences},
year={2013},
volume={3},
number={1},
pages={1-14},
doi={10.1186/2192-1962-3-8},
art_number={8},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934763530&doi=10.1186%2f2192-1962-3-8&partnerID=40&md5=8672db66a5cbe0d77ea2c145f59bfe0e},
affiliation={Uttarakhand Technical University, Dehradun, Uttarakhand, India; Department of CSE, National Institute of Technology, Hamirpur, India},
abstract={Advancement in the area of software engineering, mobile computing together with web technologies have paved way for myriad range of applications, including good quality E-learning software’s, delivering online classes in real time to unlimited number of students across the world, on a personalized E-learning space for every student. These E-learning software systems have virtually made the whole world as a single campus education hub. However, development of these software’s has been a challenge for industry, as the requirement of various stakeholders–learner, educator, institutional management, accreditation bodies, has to be handled in the software effectively. Software systems developed for E-learning applications should implement all the requirements of its diverse stakeholders and must be delivered well in time. Delays, incomplete software and faulty modules could be a big failure for educational institute. To be able to deliver the software within deadline, software’s are delivered in increments. In order to support incremental delivery, paper proposes a new requirement prioritization method that selects those requirements for implementation that are essentially required by stakeholders and has a lower regression count associated with them, thereby reducing regression testing effort. This paper reports the advantages reaped from E-learning software project of “Virtual Classroom” employed in teaching “Multimedia Technologies” course to undergraduate students of sixth semester. Total 50 students, enrolled under both distance education and full time education (25 in each category), were asked a set of questions. Results indicate that E-learning system would bring revolution in the field of education, whether study program is full time, part time or a distance education program. Students found augmenting classroom teaching with the use of E-learning software systems as an enriching experience. © 2013, Gupta et al.; licensee BioMed Central Ltd.},
author_keywords={Analytical Hierarchical Process (AHP);  E-learning software system;  Regression testing;  Requirement prioritization;  Virtual classroom},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yan20131717,
author={Yan, J. and Wang, X.},
title={From open source to commercial software development - the community based software development model},
journal={International Conference on Information Systems (ICIS 2013): Reshaping Society Through Information Systems Design},
year={2013},
volume={2},
pages={1717-1736},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897730349&partnerID=40&md5=0e249c89a75e9cc0751260b0b605160d},
affiliation={Grenoble Ecole de Management, 12, Rue Pierre Semard - BP 127, 38003 Grenoble Cedex 01, France; Juling Infotech Inc, 21F, Kaipeng International, No.425, Gonghexin Road, 200070, Shanghai, China},
abstract={The successful practice of OSS leads to the intuition that integrating online software engineering community into the value chain of software company may be a solution to access qualified workforce and to reduce product cost. The emerging practice of crowdsourcing offers a potential solution for this attempt. Adopting an action research approach, the researchers collaborated with a software company in China and developed a crowdsourcing based software community development model, which consists of 3 elements: 1. online communities, providing the abundant low cost software developers with diverse technical capability and background; 2. crowdsourcing, providing incentive for developers' participation and also motivating competition; and 3. process management and quality control mechanism, borrowed from in-house software development practice, guaranteeing the product quality and fulfillment of project schedule. This Crowdsourcing Based Community Development (CBCD) model, as a new business model and a new method of organizing software development, was tested with real-life commercial software projects and proved to be effective. © (2013) by the AIS/ICIS Administrative Office All rights reserved.},
author_keywords={Action research;  Crowdsourcing;  Online communities;  Open innovation;  Open source software;  Software development},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lupse2013,
author={Lupse, O.S. and Stoicu-Tivadar, L. and Golie, C.},
title={Assisted prescription based on successful treatments},
journal={2013 E-Health and Bioengineering Conference, EHB 2013},
year={2013},
doi={10.1109/EHB.2013.6707286},
art_number={6707286},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893750018&doi=10.1109%2fEHB.2013.6707286&partnerID=40&md5=851957e1aa10bb2c8755dff6f92ed38d},
affiliation={Automation and Applied Informatics Department, University 'Politehnica' of Timisoara, Timisoara, Romania},
abstract={Medical practice is more and more based on electronic healthcare records. Big data and great amount of information drive medical staff to use decision support software to support their activities. The paper presents a software solution in this area, collecting a set of successful treatments from diverse medical software, based on tested treatments related to illness that makes suggestions to other physicians in likewise cases. The suggested treatments is verified with the characteristics of the patient if is compatible. Based on this application the physicians may use tested treatments that have good results. In order to collect a great diversity of cases and situations from as many applications as possible the technological solution is based on Cloud Computing. We expect physicians to be more confident and to relate better to the application because the suggested treatments are given mainly by physicians and are not a result of an algorithm. © 2013 IEEE.},
author_keywords={Cloud Computing;  e-Prescription;  Electronic Healthcare Record;  treatment;  XML},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Groce2013390,
author={Groce, A. and Zhang, C. and Alipour, M.A. and Eide, E. and Chen, Y. and Regehr, J.},
title={Help, help, I'm being suppressed the significance of suppressors in software testing},
journal={2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013},
year={2013},
pages={390-399},
doi={10.1109/ISSRE.2013.6698892},
art_number={6698892},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893326388&doi=10.1109%2fISSRE.2013.6698892&partnerID=40&md5=f3a9b96170019f35205e28c64996816c},
affiliation={School of Electrical Engineering and Computer Science, Oregon State University, United States; School of Computing, University of Utah, United States},
abstract={Test features are basic compositional units used to describe what a test does (and does not) involve. For example, in API-based testing, the most obvious features are function calls; in grammar-based testing, the obvious features are the elements of the grammar. The relationship between features as abstractions of tests and produced behaviors of the tested program is surprisingly poorly understood. This paper shows how large-scale random testing modified to use diverse feature sets can uncover causal relationships between what a test contains and what the program being tested does. We introduce a general notion of observable behaviors as targets, where a target can be a detected fault, an executed branch or statement, or a complex coverage entity such as a state, predicate-valuation, or program path. While it is obvious that targets have triggers - features without which they cannot be hit by a test - the notion of suppressors - features which make a test less likely to hit a target - has received little attention despite having important implications for automated test generation and program understanding. For a set of subjects including C compilers, a flash file system, and JavaScript engines, we show that suppression is both common and important. © 2013 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Feldt2013350,
author={Feldt, R. and Poulding, S.},
title={Finding test data with specific properties via metaheuristic search},
journal={2013 IEEE 24th International Symposium on Software Reliability Engineering, ISSRE 2013},
year={2013},
pages={350-359},
doi={10.1109/ISSRE.2013.6698888},
art_number={6698888},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893320046&doi=10.1109%2fISSRE.2013.6698888&partnerID=40&md5=809c3ab2a534f782e54e5801e9059282},
affiliation={Dept. of Computer Science and Engineering, Chalmers University of Technology, Sweden; Dept. of Computer Science, University of York, York, United Kingdom},
abstract={For software testing to be effective the test data should cover a large and diverse range of the possible input domain. Boltzmann samplers were recently introduced as a systematic method to randomly generate data with a range of sizes from combinatorial classes, and there are a number of automated testing frameworks that serve a similar purpose. However, size is only one of many possible properties that data generated for software testing should exhibit. For the testing of realistic software systems we also need to trade off between multiple different properties or search for specific instances of data that combine several properties. In this paper we propose a general search-based framework for finding test data with specific properties. In particular, we use a metaheuristic, differential evolution, to search for stochastic models for the data generator. Evaluation of the framework demonstrates that it is more general and flexible than existing solutions based on random sampling. © 2013 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cameron201324,
author={Cameron, S.},
title={Understanding the ice environment is critical to Arctic success},
journal={Offshore Engineer},
year={2013},
volume={38},
number={12},
pages={24-26},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891868246&partnerID=40&md5=480e72552f6a9c58d9a30d40c8415bf6},
affiliation={ION Geophysical, United States},
abstract={Declining reserves from mature fields worldwide, the ongoing demand for oil, and retreating sea ice have brought a flurry of exploration activity to the Arctic. The US Geological Survey estimates undiscovered Arctic petroleum resources of 90 billion bbls of oil and 1,669 tcf of natural gas. Dangerous ice conditions limit the Arctic operating season to less than a year. Early ice navigation technology became fairly standards in Arctic operations, but lacking automation, the process of integrating diverse information remained complex and time-consuming. ION Geophysical has been operating in the Arctic since 2006. During its voyage through the Northwest Passage last summer, the Polar Prince, an ice-classed seismic survey vessel, put the software to the test.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Lu2013545,
author={Lu, Y.-F. and Yin, Y.-F.},
title={A new constructive cost model for software testing project management},
journal={19th International Conference on Industrial Engineering and Engineering Management},
year={2013},
pages={545-556},
doi={10.1007/978-3-642-37270-4_52},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891297834&doi=10.1007%2f978-3-642-37270-4_52&partnerID=40&md5=2484df53ebc7c5b1e0cfcf558bc6f378},
affiliation={School of Reliability and System Engineering, Bei Hang University, Beijing, China},
abstract={Software testing is becoming a relatively independent activity of software development process. To estimate the cost of software testing is an important part of project management. The researchers have focused on the software cost and treated software testing cost as a part of the total software development cost for several years. Few researchers consider software testing as a separate process to estimate its' cost. This paper proposed a new constructive cost model for software testing project management (CCMST) which contains the main cost divers of software testing. The cost drivers included in this model is more comprehensive than former models. Application case study proved the model is usable and valid. © 2013 Springer-Verlag Berlin Heidelberg.},
author_keywords={Constructive;  Cost;  Project management;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Högberg20135664,
author={Högberg, S. and Jensen, B.B. and Bendixen, F.B.},
title={Design and demonstration of a test-rig for static performance-studies of permanent magnet couplings},
journal={IEEE Transactions on Magnetics},
year={2013},
volume={49},
number={12},
pages={5664-5670},
doi={10.1109/TMAG.2013.2274645},
art_number={6567955},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890068085&doi=10.1109%2fTMAG.2013.2274645&partnerID=40&md5=020dcef6d17e356ead4d08e002b8a2a7},
affiliation={Department of Electrical Engineering, Technical University of Denmark, Lyngby, Denmark; Sintex A/s, 9500 Hobro, Denmark},
abstract={The design and construction of an easy-to-use test-rig for permanent magnet couplings is presented. Static torque of permanent magnet couplings as a function of angular displacement is measured of permanent magnet couplings through an semi-automated test system. The test-rig is capable of measuring torque up to 240 Nm (in increments of 0.1 Nm) at a angular step of 0.0011 degrees (mechanical). Axial, radial, and angular misalignment can be imposed on the coupling in order to study abnormal and faulty operating conditions. This can also be used to assess installation tolerances. Measured data is stored in a USB thumb-drive, and no additional software or hardware is needed to operate the test-rig. Tests of the aligned static torque performance of two different cylindrical couplings are presented along with radial and axial misalignment-tests of one. The results demonstrates the diversity and usefulness of the the designed test-rig. Furthermore, the coupling-performance shows a clear influence of end-effects for axially short couplings, and are found to be very robust to small misalignment. © 1965-2012 IEEE.},
author_keywords={Automatic test equipment;  axial misalignment;  permanent magnet coupling;  radial misalignment;  test equipment;  test-rig;  torque measurement},
document_type={Article},
source={Scopus},
}

@ARTICLE{Martin20131360,
author={Martin, A. and Emami, M.R.},
title={A fault-tolerant approach to robot teams},
journal={Robotics and Autonomous Systems},
year={2013},
volume={61},
number={12},
pages={1360-1378},
doi={10.1016/j.robot.2013.07.015},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887234455&doi=10.1016%2fj.robot.2013.07.015&partnerID=40&md5=392d631bb2281442601969b1aa1c447c},
affiliation={University of Toronto, Institute for Aerospace Studies, 4925 Dufferin Street, Toronto, ON M3H 5T6, Canada},
abstract={As the applications of mobile robotics evolve it has become increasingly less practical for researchers to design custom hardware and control systems for each problem. This paper presents a new approach to control system design in order to look beyond end-of-lifecycle performance, and consider control system structure, flexibility, and extensibility. Towards these ends the Control ad libitum philosophy was proposed, stating that to make significant progress in the real-world application of mobile robot teams the control system must be structured such that teams can be formed in real-time from diverse components. The Control ad libitum philosophy was applied to the design of the HAA (Host, Avatar, Agent) architecture: a modular hierarchical framework built with provably correct distributed algorithms. A control system for mapping, exploration, and foraging was developed using the HAA architecture and evaluated in three experiments. First, the basic functionality of the HAA architecture was studied, specifically the ability to: (a) dynamically form the control system, (b) dynamically form the robot team, (c) dynamically form the processing network, and (d) handle heterogeneous teams and allocate robots between tasks based on their capabilities. Secondly, the control system was tested with different rates of software failure and was able to successfully complete its tasks even when each module was set to fail every 0.5-1.5 min. Thirdly, the control system was subjected to concurrent software and hardware failures, and was still able to complete a foraging task in a 216 m2 environment. © 2013 Elsevier B.V. All rights reserved.},
author_keywords={Control system architecture;  Distributed systems;  Fault tolerance;  Hardware-in-the-loop simulation;  Robot teams},
document_type={Article},
source={Scopus},
}

@ARTICLE{Franchini20132366,
author={Franchini, S. and Gentile, A. and Sorbello, F. and Vassallo, G. and Vitabile, S.},
title={Design and implementation of an embedded coprocessor with native support for 5D, quadruple-based clifford algebra},
journal={IEEE Transactions on Computers},
year={2013},
volume={62},
number={12},
pages={2366-2381},
doi={10.1109/TC.2012.225},
art_number={6302125},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885214647&doi=10.1109%2fTC.2012.225&partnerID=40&md5=9fd0b8f1d5b9910fe4d461eaf50e744b},
affiliation={Computer Science and Artificial Intelligence Laboratory, DICGIM Department, University of Palermo, Viale delle Scienze, Edificio 6, 90128 Palermo, Italy; Department of Biopathology, Medical and Forensic Biotechnologies, University of Palermo, Via del Vespro, 129, 90127 Palermo, Italy},
abstract={Geometric or Clifford algebra (CA) is a powerful mathematical tool that offers a natural and intuitive way to model geometric facts in a number of research fields, such as robotics, machine vision, and computer graphics. Operating in higher dimensional spaces, its practical use is hindered, however, by a significant computational cost, only partially addressed by dedicated software libraries and hardware/software codesigns. For low-dimensional algebras, several dedicated hardware accelerators and coprocessing architectures have been already proposed in the literature. This paper introduces the architecture of CliffordALU5, an embedded coprocessing core conceived for native execution of up to 5D CA operations. CliffordALU5 exploits a novel, hardware-oriented representation of the algebra elements that allows for faster execution of Clifford operations. In this paper, a prototype implementation of a complete system-on-chip (SOC) based on CliffordALU5 is presented. This prototype integrates an embedded processing soft-core based on the PowerPC 405 and a CliffordALU5 coprocessor on a Xilinx XUPV2P Field Programmable Gate Array (FPGA) board. Test results show a 5× average speedup for 4D Clifford products and a 4× average speedup for 5D Clifford products against the same operations in Gaigen 2, a CA software library generator running on the general-purpose PowerPC processor. This paper also presents an execution analysis of three different applications in three diverse domains, namely, inverse kinematics of a robot, optical motion capture, and raytracing, showing an average speedup between 3× and 4× with respect to the baseline Gaigen 2 implementation. Finally, a multicore approach to higher dimensional CA based on CliffordALU5 is discussed. © 1968-2012 IEEE.},
author_keywords={Application-specific processors;  Clifford algebra;  Computational geometry;  Embedded coprocessors;  FPGA-based prototyping},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mendez2013,
author={Mendez, A.J. and Lado, M.J. and Vila, X.A. and Rodriguez-Linares, L. and Alonso, R.A. and Garcia-Caballero, A.},
title={Heart of darkness: Heart rate variability on patients with risk of suicide},
journal={Iberian Conference on Information Systems and Technologies, CISTI},
year={2013},
art_number={6615756},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887860486&partnerID=40&md5=e68e7a5478ea262de5aa678d6907d644},
affiliation={MILE, Universidade de Vigo, Ourense, Spain; Servicio de Psiquiatría, CHUO, Ourense, Spain},
abstract={Heart Rate Variability (HRV) is an emerging research field in the study of diverse pathologies, as long as it allows considering another measurement for detecting possible aggravations. The aim of this work is to study the applicability of the analysis of HRV in order to establish if a person is at risk of suffering from suicidal ideation. This work includes the development and testing of a heart rate acquisition and automatic analysis system, with friendly software for clinicians, customized to the necessities of an emergency unit. Furthermore, it includes the analysis of the obtained data with the purpose of assessing possible correlations between HRV parameters and personality impulsive traits. 20 patients and 10 normal cases were selected to develop this pilot study. Results show significant statistical difference (p<0.05) among patients and normal cases for pNN50, IRRR, MADRR, total HRV power, Approximate Entropy and Fractal Dimension. © 2013 AISTI.},
author_keywords={Heart rate variability;  RHRV package;  suicidal ideation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Wang20132060,
author={Wang, B. and Ruchikachorn, P. and Mueller, K.},
title={SketchPadN-D: WYDIWYG sculpting and editing in high-dimensional space},
journal={IEEE Transactions on Visualization and Computer Graphics},
year={2013},
volume={19},
number={12},
pages={2060-2069},
doi={10.1109/TVCG.2013.190},
art_number={6634118},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886478690&doi=10.1109%2fTVCG.2013.190&partnerID=40&md5=02dadd32bd691066c5310947c482aeb3},
affiliation={Visual Analytics and Imaging Laboratory, Computer Science Department, Stony Brook University, NY, United States; Chulalongkorn Business School, Chulalongkorn University, Thailand},
abstract={High-dimensional data visualization has been attracting much attention. To fully test related software and algorithms, researchers require a diverse pool of data with known and desired features. Test data do not always provide this, or only partially. Here we propose the paradigm WYDIWYGS (What You Draw Is What You Get). Its embodiment, SketchPadND, is a tool that allows users to generate high-dimensional data in the same interface they also use for visualization. This provides for an immersive and direct data generation activity, and furthermore it also enables users to interactively edit and clean existing high-dimensional data from possible artifacts. SketchPadND offers two visualization paradigms, one based on parallel coordinates and the other based on a relatively new framework using an N-D polygon to navigate in high-dimensional space. The first interface allows users to draw arbitrary profiles of probability density functions along each dimension axis and sketch shapes for data density and connections between adjacent dimensions. The second interface embraces the idea of sculpting. Users can carve data at arbitrary orientations and refine them wherever necessary. This guarantees that the data generated is truly high-dimensional. We demonstrate our tool's usefulness in real data visualization scenarios. © 1995-2012 IEEE.},
author_keywords={data acquisition and management;  data editing;  high-dimensional data;  interaction;  multiple views;  multivariate data;  N-D navigation;  parallel coordinates;  scatterplot;  Synthetic data generation;  user interface},
document_type={Article},
source={Scopus},
}

@ARTICLE{Arnold201333,
author={Arnold, J. and Alexander, R.},
title={Testing autonomous robot control software using procedural content generation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={8153 LNCS},
pages={33-44},
doi={10.1007/978-3-642-40793-2_4},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886430706&doi=10.1007%2f978-3-642-40793-2_4&partnerID=40&md5=f406f48cc3583f029dc879232a581557},
affiliation={BAE Systems Detica, United Kingdom; Department of Computer Science, University of York, United Kingdom},
abstract={We present a novel approach for reducing manual effort when testing autonomous robot control algorithms. We use procedural content generation, as developed for the film and video game industries, to create a diverse range of test situations. We execute these in the Player/Stage robot simulator and automatically rate them for their safety significance using an event-based scoring system. Situations exhibiting dangerous behaviour will score highly, and are thus flagged for the attention of a safety engineer. This process removes the time-consuming tasks of hand-crafting and monitoring situations while testing an autonomous robot control algorithm. We present a case study of the proposed approach - we generated 500 randomised situations, and our prototype tool simulated and rated them. We have analysed the three highest rated situations in depth, and this analysis revealed weaknesses in the smoothed nearness-diagram control algorithm. © 2013 Springer-Verlag.},
author_keywords={autonomy;  faults;  procedural content generation;  robots;  simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chow2013268,
author={Chow, C. and Chen, T.Y. and Tse, T.H.},
title={The ART of divide and conquer: An innovative approach to improving the efficiency of adaptive random testing},
journal={Proceedings of the International Symposium on the Physical and Failure Analysis of Integrated Circuits, IPFA},
year={2013},
pages={268-275},
doi={10.1109/QSIC.2013.19},
art_number={6605937},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885645956&doi=10.1109%2fQSIC.2013.19&partnerID=40&md5=13af9e37a7fec14c511412b0d1731faf},
affiliation={University of Hong Kong, Pokfulam, Hong Kong; Swinburne University of Technology, Australia},
abstract={Test case selection is a prime process in the engineering of test harnesses. In particular, test case diversity is an important concept. In order to achieve an even spread of test cases across the input domain, Adaptive Random Testing (ART) was proposed such that the history of previously executed test cases are taken into consideration when selecting the next test case. This was achieved through various means such as best candidate selection, exclusion, partitioning, and diversity metrics. Empirical studies showed that ART algorithms make good use of the concept of even spreading and achieve 40 to 50% improvement in test effectiveness over random testing in revealing the first failure, which is close to the theoretical limit. However, the computational complexity of ART algorithms may be quadratic or higher, and hence efficiency is an issue when a large number of previously executed test cases are involved. This paper proposes an innovative divide-and-conquer approach to improve the efficiency of ART algorithms while maintaining their performance in effectiveness. Simulation studies have been conducted to gauge its efficiency against two most commonly used ART algorithms, namely, fixed size candidate set and restricted random testing. Initial experimental results show that the divide-and-conquer technique can provide much better efficiency while maintaining similar, or even better, effectiveness. © 2013 IEEE.},
author_keywords={adaptive random testing;  divide and conquer;  effectiveness;  efficiency;  software testing;  test harness},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jia2013222,
author={Jia, C. and Yu, Y.T.},
title={Using the 5W+1H model in reporting systematic literature review: A case study on software testing for cloud computing},
journal={Proceedings of the International Symposium on the Physical and Failure Analysis of Integrated Circuits, IPFA},
year={2013},
pages={222-229},
doi={10.1109/QSIC.2013.13},
art_number={6605931},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885587150&doi=10.1109%2fQSIC.2013.13&partnerID=40&md5=461d65ba5468149b78b2a9518fcc1e0f},
affiliation={Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong},
abstract={This paper documents a case study of using the 5W+1H model for reporting systematic literature review on software testing for cloud computing. To our knowledge, this is the first systematic literature review that applies the 5W+1H model, which is widely used in the journalism domain, to report the full picture of the research area in both software engineering and services computing. Existing guidelines on systematic literature review heavily rely on the researcher to pose the right research questions, and the review results are tightly focused on these research questions. For researchers new to a field, defining the right research questions that are effective in revealing the critical issues in the field can be challenging. Our case study demonstrates that the 5W+1H model provides an easy aid for the researcher to get over such initial challenges. As the researcher becomes more familiar with the field, he/she may then refine the research questions by adding more topic-specific contexts. In this way, the 5W+1H model serves to provide an exploratory framework to shape a systematic literature review. Applying to software testing for cloud computing, we are able to synthesize a comprehensive picture of recent researches on the field, including publication pattern, article citation immediacy, research topic diversity, research ideas for addressing testing challenges at different cloud service architectural layers. Based on the case study, we summarize the lessons learned on using the 5W+1H model in reporting systematic literature review. © 2013 IEEE.},
author_keywords={5W+1H;  cloud-based application;  software testing;  systematic literature review},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Liu2013210,
author={Liu, D. and Wang, J.},
title={Software test data generation based on improved particle swarm optimization algorithm},
journal={International Journal of Applied Mathematics and Statistics},
year={2013},
volume={44},
number={14},
pages={210-217},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884683038&partnerID=40&md5=b6a81b3b88fddaa81f33cb259ca2cf13},
affiliation={School of Information Science and Technology, Shijiazhuang Tiedao University, Shijiazhuang 050043, China},
abstract={Software testing is an important means of software quality assurance, the automatic generation of test data has been widely studied. By analyzing the advantages and disadvantages of the genetic algorithm the particle swarm optimization algorithm and the ant colony algorithm, the paper proposes a new improved particle swarm optimization algorithm in the automatic generation of test data. By the artificial immune algorithm is introduced into the particle swarm algorithm, the diversity of the individual is kept in the improved strategy, and it can overcome the local optimum problem of standard particle swarm optimization algorithm. The overall search capability as well as the performance of the standard algorithm is enhanced. Finally experiment proves the feasibility and efficiency of the algorithm in software testing. © 2013 by CESER Publications.},
author_keywords={Immune algorithm;  Particle swarm optimization algorithm;  Test data},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Albee2013381,
author={Albee, A.J.},
title={The evolution of ICT: PCB technologies, test philosophies, and manufacturing business models are driving in-Circuit test evolution and innovations},
journal={IPC APEX EXPO Conference and Exhibition 2013, APEX EXPO 2013},
year={2013},
volume={1},
pages={381-401},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883563903&partnerID=40&md5=f596de12b041f2af463acdd23c878a79},
affiliation={Teradyne Inc., North Reading, MA, United States},
abstract={Many manufacturers employ one or more In-Circuit Test (ICT) systems in their PCB manufacturing facilities to help them detect manufacturing process and component defects. These "bed-of-nails" electrical test systems are highly valued for providing the qualities of simple program generation, high fault coverage, fast test throughput, low false fail rates, and exceptional diagnostic accuracy as compared to other available test and inspection techniques. Advancements in PCB technologies, along with changing test philosophies and manufacturing business models in recent years have created new and diverse requirements for manufacturers of in-circuit test systems. Particular challenges that ICT manufacturers have had to address include the erosion of test point access in certain product sectors; the progression of ultra-low voltage components; the variable test requirements of different product applications; the varying test philosophies of different market segments and different manufacturing regions; and the demanding throughput requirements of high volume production facilities. This paper highlights how in-circuit test systems have evolved in recent years to include innovations and advancements to address these challenges and trends. Topics that will be covered include boundary scan and functional test integration strategies; advancements in vectorless test techniques; incorporation of limited access electrical test techniques; test strategy analysis tools; high accuracy pin drivers and sensors; concurrent test throughput improvement options; scalable test performance capability architecture; and program development accelerators. The paper describes how these new ICT advancements contribute to lowering overall manufacturing test costs by improving the fault coverage, reliability, and throughput of in-circuit production tests.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Roper2013272,
author={Roper, M.},
title={Estimating fault numbers remaining after testing},
journal={Proceedings - IEEE 6th International Conference on Software Testing, Verification and Validation, ICST 2013},
year={2013},
pages={272-281},
doi={10.1109/ICST.2013.43},
art_number={6569739},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883426476&doi=10.1109%2fICST.2013.43&partnerID=40&md5=d0038cf4f682f32d366fa6cf130b932e},
affiliation={Dept. Computer and Information Sciences, University of Strathclyde, Glasgow, United Kingdom},
abstract={Testing is an essential component of the software development process, but also one which is exceptionally difficult to manage and control. For example, it is well understood that testing techniques are not guaranteed to detect all faults, but more frustrating is that after the application of a testing technique the tester has little or no knowledge of how many faults might still be left undiscovered. This paper investigates the performance of a range of capture-recapture models to determine the accuracy with which they predict the number of defects remaining after testing. The models are evaluated with data from two empirical testing-related studies and from one larger publicly available project and the factors affecting the accuracy of the models are analysed. The paper also considers how additional information (such as structural coverage data) may be used to improve the accuracy of the estimates. The results demonstrate that diverse sets of faults resulting from different testers using different techniques tend to produce the most accurate results, and also illustrate the sensitivity of the estimators to the patterns of fault data. © 2013 IEEE.},
author_keywords={Capture-Recapture;  Fault Estimation;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2013197,
author={Chen, Y. and Groce, A. and Zhang, C. and Wong, W.-K. and Fern, X. and Eide, E. and Regehr, J.},
title={Taming compiler fuzzers},
journal={Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)},
year={2013},
pages={197-207},
doi={10.1145/2462156.2462173},
note={cited By 63},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883128500&doi=10.1145%2f2462156.2462173&partnerID=40&md5=1a87698914ac55b008203a0a24628c2b},
affiliation={University of Utah Salt, Lake City, UT, United States; Oregon State University, Corvallis, OR, United States},
abstract={Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine. Copyright © 2013 ACM.},
author_keywords={Automated testing;  Bug reporting;  Compiler defect;  Compiler testing;  Fuzz testing;  Random testing;  Test-case reduction},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dehghanian2013,
author={Dehghanian, V. and Nielsen, J. and Lachapelle, G.},
title={Enhanced GNSS frequency tracking in multipath environments},
journal={Canadian Conference on Electrical and Computer Engineering},
year={2013},
doi={10.1109/CCECE.2013.6567732},
art_number={6567732},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883060389&doi=10.1109%2fCCECE.2013.6567732&partnerID=40&md5=e25f8bff9aea67a61b0b1088ef4b6db2},
affiliation={Department of Math, Physics and Engineering, Mount Royal University, Calgary, AB, Canada; Position, Location and Navigation Group, University of Calgary, Calgary, AB, Canada},
abstract={A novel configuration for improving the frequency tracking performance of GNSS signals in multipath environments is proposed. The proposed technique is based on combining the weakly correlated outputs of Early, Prompt and Late match filters to exploit the inherent time-delay diversity when severe multipath conditions prevail. By observing more code-phase offsets simultaneously, the loop is more resilient to fading than the conventional loop. The proposed configuration is implemented in a GPS software-receiver and tested on GPS signals collected through an extensive measurement campaign in an urban canyon in downtown Calgary. The test results demonstrate the effectiveness of the proposed technique in mitigating multipath fading and improving the robustness of frequency tracking under severe multipath conditions. © 2013 IEEE.},
author_keywords={FLL;  GNSS;  Multipath jading},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hummer2013465,
author={Hummer, W. and Raz, O. and Shehory, O. and Leitner, P. and Dustdar, S.},
title={Testing of data-centric and event-based dynamic service compositions},
journal={Software Testing Verification and Reliability},
year={2013},
volume={23},
number={6},
pages={465-497},
doi={10.1002/stvr.1493},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883452900&doi=10.1002%2fstvr.1493&partnerID=40&md5=d1b60db0ffa9dbd3067e22681c7a133c},
affiliation={Distributed Systems Group, Vienna University of Technology, A-1040 Vienna, Austria; IBM Haifa Research Labs, Haifa University Campus, Israel},
abstract={This paper addresses integration testing of data-centric and event-based dynamic service compositions. The compositions under test define abstract services that are replaced by concrete candidate services at runtime. Testing all possible instantiations of a composition leads to combinatorial explosion and is often infeasible. We consider data dependencies between services as potential points of failure and introduce the k-node data flow test coverage metric, which helps to significantly reduce the number of test combinations. We formulate a combinatorial optimization problem for generating minimal sets of test cases. On the basis of this formalization, we present a mapping to the model of FoCuS, a coverage analysis tool. FoCuS efficiently computes near-optimal solutions, which are used to automatically generate test instances. The proposed approach is applicable to various composition paradigms. We illustrate the end-to-end practicability based on an integrated scenario, which uses two diverse composition techniques: on the one hand, the Web Services Business Process Execution Language and on the other hand, WS-Aggregation, a platform for event-based service composition. Copyright © 2013 John Wiley & Sons, Ltd.},
author_keywords={data-centric service compositions;  event-based systems;  k-node data flow coverage;  test coverage;  testing service-based systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Panchapakesan20132525,
author={Panchapakesan, A. and Abielmona, R. and Petriu, E.},
title={Dynamic white-box software testing using a recursive hybrid evolutionary strategy/genetic algorithm},
journal={2013 IEEE Congress on Evolutionary Computation, CEC 2013},
year={2013},
pages={2525-2532},
doi={10.1109/CEC.2013.6557873},
art_number={6557873},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881574412&doi=10.1109%2fCEC.2013.6557873&partnerID=40&md5=b9ac9392ff663cb53d9ae5e543b3970f},
affiliation={School of EECS, University of Ottawa, Ottawa, Canada; Research and Engineering, Larus Technologies Corporation, Ottawa, Canada},
abstract={Software testing is an important and time consuming part of the software development cycle. While automated testing frameworks do help in reducing the amount of programmer time that testing requires, the onus is still upon the programmer to provide such a framework with the inputs on which the software must be tested. This requires static analysis of the source code, which is more effective when performed as a peer review exercise and is highly dependent on the skills of the programmers performing the analysis. Thus, it demands the allocation of precious time of highly skilled programmers. An algorithm that automatically generates inputs to satisfy test coverage criteria for the software being tested would therefore be valuable, as it would imply that the programmer no longer needs to analyze code to generate the relevant test cases. This paper explores a hybrid evolutionary strategy with an evolutionary algorithm to discover such test case synthesis, in an improvement over previous methods which overly focus their search without maintaining the diversity required to cover the entire search space efficiently. © 2013 IEEE.},
author_keywords={black-box testing;  control flow graph;  dynamic white-box testing;  evolutionary algorithm;  evolutionary strategy;  genetic algorithm;  Software testing;  static white-box testing;  white-box testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kifetew2013257,
author={Kifetew, F.M. and Panichella, A. and De Lucia, A. and Oliveto, R. and Tonella, P.},
title={Orthogonal exploration of the search space in evolutionary test case generation},
journal={2013 International Symposium on Software Testing and Analysis, ISSTA 2013 - Proceedings},
year={2013},
pages={257-267},
doi={10.1145/2483760.2483789},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881271344&doi=10.1145%2f2483760.2483789&partnerID=40&md5=a3662fe629f907563ec59f0b2a0104e0},
affiliation={Fondazione Bruno Kessler, Trento, Italy; University of Salerno, Fisciano (SA), Italy; University of Molise, Pesche (IS), Italy},
abstract={The effectiveness of evolutionary test case generation based on Genetic Algorithms (GAs) can be seriously impacted by genetic drift, a phenomenon that inhibits the ability of such algorithms to effectively diversify the search and look for alternative potential solutions. In such cases, the search becomes dominated by a small set of similar individuals that lead GAs to converge to a sub-optimal solution and to stagnate, without reaching the desired objective. This problem is particularly common for hard-to-cover program branches, associated with an extremely large solution space. In this paper, we propose an approach to solve this problem by integrating a mechanism for orthogonal exploration of the search space into standard GA. The diversity in the population is enriched by adding individuals in orthogonal directions, hence providing a more effective exploration of the solution space. To the best of our knowledge, no prior work has addressed explicitly the issue of evolution direction based diversification in the context of evolutionary testing. Results achieved on 17 Java classes indicate that the proposed enhancements make GA much more effective and efficient in automating the testing process. In particular, effectiveness (coverage) was significantly improved in 47% of the subjects and efficiency (search budget consumed) was improved in 85% of the subjects on which effectiveness remains the same. © 2013 ACM.},
author_keywords={genetic algorithms;  genetic drift;  orthogonal exploration;  Search based testing;  test case generation},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Zavala2013381,
author={Zavala, L. and Mendoza, B. and Huhns, M.N.},
title={A test-driven approach to behavioral queries for service selection},
journal={Software Design and Development: Concepts, Methodologies, Tools, and Applications},
year={2013},
volume={1-4},
pages={381-400},
doi={10.4018/978-1-4666-4301-7.ch020},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944685268&doi=10.4018%2f978-1-4666-4301-7.ch020&partnerID=40&md5=b1542453aae4b0ea48d515810fbc56ff},
affiliation={University of Maryland Baltimore County, United States; New York City College of Technology, United States; University of South Carolina, United States},
abstract={Although the areas of Service-Oriented Computing (SOC) and Agile and Lean Software Development (LSD) have been evolving separately in the last few years, they share several commonalities. Both are intended to exploit reusability and exhibit adaptability. SOC in particular aims to facilitate the widespread and diverse use of small, loosely coupled units of functionality, called services. Such services have a decided agility advantage, because they allow for changing a service provider at runtime without affecting any of a group of diverse and possibly anonymous consumers. Moreover, they can be composed at both development-time and run-time to produce new functionalities. Automatic service discovery and selection are key aspects for composing services dynamically. Current approaches attempting to automate discovery and selection make use of only structural and functional aspects of the services, and in many situations, this does not suffice to discriminate between functionally similar but disparate services. Service behavior is difficult to specify prior to service execution and instead is better described based on experience with the execution of the service. In this chapter, the authors present a behavioral approach to service selection and runtime adaptation that, inspired by agile software development techniques, is based on behavioral queries specified as test cases. Behavior is evaluated through the analysis of execution values of functional and non-functional parameters. In addition to behavioral selection, the authors' approach allows for real-time evaluation of non-functional quality-of-service parameters, such as response time, availability, and latency. © 2014, IGI Global.},
document_type={Book Chapter},
source={Scopus},
}

@ARTICLE{NoAuthor2013,
title={2013 International Conference on Applied Science, Engineering and Technology, ICASET 2013},
journal={Advanced Materials Research},
year={2013},
volume={709},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880172780&partnerID=40&md5=d7146851059e5edea31af97dbb5baf50},
abstract={The proceedings contain 204 papers. The special focus in this conference is on Microelectronics and Materials, Applied Chemistry and Chemical Engineering, Information Technology, Computer Engineering and Network Technology, Environmental Science and Management Engineering. The topics include: template synthesis and electrocatalytic properties of palladium hollow spheres; synthesis and characterization of ferrous tungstate nanoparticles; chemical methods for synthesizing gold nanoparticles; synthesis and characterization of triethanolamine borate; hydrophobic flexible silica aerogels felts fabricated by ambient pressure drying; the synthesis and purification of 4-Dimethylamino-N-Methyl -4-Stilbazolium tosylate; optimization of reaction conditions for preparing carboxymethyl cellulose guaze; aromatic components analysis of HongDao clam by HS-SPME and GC-MS; the influence of auxiliary chemicals on the decolorization of reactive blue dye by laccase; electrochemical synthesis of platinum-reduced graphene oxide hybrids for methanol oxidation; epitaxial graphene growth on 6H-SiC (0001) substrate by confinement controlled sublimation of silicon carbide; characterization and investigation of polyamide 6 nanotubes prepared by a novel solution wetting method; a comparative study of UHMWPE multifilament and aramid multifilament; avoidance test for the assessment of microbial degradation of PAHs; mechanism of the oxidation of iron; synthesis of viscosity-variable diverting-acid main agent and diverting performance evaluation; hot deformation behavior and microstructure of U720Li alloy; defects in germanium nanocrystals produced by ion implantation; effect of graphite degradation on thermal analysis cooling curve of ductile iron melt; grain growth behaviour of superalloy U720Li under continuous heating conditions; crystal growth and optical spectroscopy of ytterbium-doped lutetium orthophosphate; first-principle calculations for magnetism of Mn-doped graphene; ADP single crystal growth from solution in defined crystallographic direction; first-principle study on magnetic properties of TM-doped 6H-SiC; motion analysis of powder particle in plasma jet; study on moisture transfer properties of polyester-cotton fabric; the tensile and surface frictional property of soybean protein fibers; test and analysis of SPF pure yarn or SPF tencel blending yarn mechanics property; the mechanical property of milk protein fiber pure yarn and blended yarn; a study of wearabilities of softwarm heating warm fiber fabric; performance of newdal blended knitted fabrics; study on properties of blended fabrics containing phenolic fiber; research on aerodynamic characteristics of lateral jet control technology; research on aerodynamic characteristics of morphing aircraft with a telescopic wing; analysis and optimization of plastic parts moldflow based on mold flow; fault diagnosis of drilling process based on rough set and support vector machine; application of magnetic flux feedback control in hybrid suspension system; fast tool servo driven by electromagnetic force applied in freeform surfaces machining; study on marine diesel engine waste heat recovery system with multi-stage flash; pressure drop and apparent solids concentration inside a novel multi-regime riser; study on the buckling analysis of stiffened spherical shallow shell for gasholder roof; kinematic analysis of lunar exploration manipulator; application of fault tree analysis method in gas explosion accident; the calibration of piezoelectric phase-shifter based on the improved 2-D FTP; the design of an automatic golf collection robot; fatigue life of UAV airframe based on damage coefficient; new blind adaptive channel estimation schemes based on OFDM systems; new channel estimation schemes for MIMO-OFDM systems; a design of controller about LED landscape lamp; autonomous marine monitoring system based on energy convertor; the design of fish tank thermostat controller based on TC89C52; realization of the optimization of Dijkstra algorithm in vehicle navigation; automatic PID temperature control of thermoelectric cooler based on MSP430; design and implementation of mid-frequency magnetron sputtering power supply based on TL494 and MCU; the trust relay QKD network communication research; simulation research on instantaneous control in battery energy storage system; the design of a Rubik's cube robot; innovative teaching research on the course of intelligent control system and simulation; design of intelligent flushing system; design and production of AT89S52 microcontroller tachometer; the design of warehouse temperature and humidity monitoring system; decoupling suspension controller based on magnetic flux feedback; development research on cloud application program based on GFS; combining two detectors for object tracking; solvent extraction of uranium by laminar flow in microfluidic chips; research on non-manifold polyhedron from function-to-form mapping; depth from defocus using geometric optics regularization; image classification base on sparse representation with basis design; computer simulation of complex consecutive reaction kinetics; uniform blow-up rate for diffusion equation with a weighted localized source; image stereo matching based on multi-scale plane set; a novel image contrast restoration algorithm for fog; Bayesian estimation of gold index's change point of time series model; accelerating PQMRCGSTAB algorithm on xeon phi; sampling theorems of wavelet subspaces with M band in higher dimensions; application of three-dimensional modelling in the study of complex geological body; gauss-Newton iteration estimation to the parameters in nonlinear regression models; study on a rapid real-time feature extraction algorithm; using excel to evaluate shear strength parameters of soil; neuron classification based on semi-supervised FCM algorithm; a novel differentiate weight algorithm on heterogeneous network selection; efficient dynamic data possession checking in cloud computing; research of state-base object-oriented software testing; self-revised opinion leader list construction and influence analysis; cloud computing security analysis and reinforcement of hadoop environment; symmetric multiwavelet frames with general lattice; a robust image watermarking algorithm based on PCA and NN; flight control system for UAV based on simulink; distributed construction for power aware connected dominating set; analysis of the process of collapse in supervision control systems; ScanSAR mode sea ice image segmentation; reviews and a new angle of β coefficient prediction method; study on the property management problems and its countermeasures in small city of China; performance evaluation of private college teachers based on analytic hierarchy process (AHP); empirical research on roof reclamation by land-lost farmers in peri-urban areas of China; the analysis of a shares logistics enterprises' financing structure; research on improved shapley value based profit allocation of agricultural supply chain; application of intelligent decision support system in enterprise management; on the feasibility of introducing commercial insurance into major sports events; effect of China's Commercial Banks' Diversified Revenue on Performance; Optimal Pricing Problem of a Closed-Loop Supply Chain with Remanufacturing; research on impcact of interest rate marketization to SME financing; study of energy-saving reform in garment production enterprises; the reliability evaluation of power system based on uncertain linguistic information; the synthesis of cephalosporin intermediates THZ; studies on the biological characteristics of termitomyces albuminosus hypha; forecasting by applying the time series model; effects of water-soluble dietary fiber to wheat dough rheological properties; stress emotion recognition based on RSP and EMG signals; predictive efficiency comparison of ARIMA-time-series and the grey system GM(1,1) the expression of decoy receptor 3 in peripheral blood monocytes of rheumatoid arthritis patient; study on the function of miRNA-155 target using bioinformatics methods; the application of neural network in the bionic control system of an artificial heart based on neuroshell; effect of adding seeds during maceration on quality of wine from vitis vinifera CV. cabernet sauvignon; a epidemic model with inhomogeneity and mobility based on cellular automata; study on the degumming process of abelmoschus manihot (L.) medik by microwave-assisted treatment; graphene modified molecular imprinted electrochemical sensor for specific recognition of bovine serum albumin; removal of silica bodies on oil palm empty fruit bunch surfaces and application for biogas production; environment pollutant standard: the production planning within certain pollution; bioavailability of vanadium in alfalfa in V-Cd contaminated soil; applying AHP and maximal value normalization to evaluate intensity of urban land use in Sujiatun District, Shenyang City; analysis of agarwood waste at different pre-treatment for silica xerogel production; applying ecological niche suitability model to evaluate land-use fitness; study on environmentally-friendly snow-melting agents application; classification and prediction of economic losses - storm surge disasters in Guangdong province of China; influence of rainfall infiltration on landslide treatment engineering; preliminary study of groundwater pollution about sewage irrigation at Malong county; review of key technique research on the field of vegetation restoration in beach wetlands and the research of decentralized wastewater treatment mode in rural areas in China.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Lee2013243,
author={Lee, S.-C. and Cho, J.-Y. and Vecchio, F.J.},
title={Constitutive model for steel fibre reinforced concrete in tension},
journal={Proceedings of the 8th International Conference on Fracture Mechanics of Concrete and Concrete Structures, FraMCoS 2013},
year={2013},
pages={243-252},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879961233&partnerID=40&md5=ef8ec72cea8e954a8d338ecd4c6e440c},
affiliation={KEPCO International Nuclear Graduate School (KINGS), 1456-1 Shinam-ri, Seosaeng-myeon, Ulju-gun, Ulsan, 689-882, South Korea; Department of Civil and Environmental Engineering, Seoul National University, 599 Gwanak-ro, Gwanak-gu, Seoul, 151-744, South Korea; Department of Civil Engineering, University of Toronto, 35 St. George Street, Toronto, ON, M5S 1A4, Canada},
abstract={In order to represent the ductile tensile behaviour of steel fibre reinforced concrete (SFRC), the Diverse Embedment Model (DEM) was recently developed, accounting for both the random distribution of fibres and the pull-out behaviour of fibres. Although the DEM shows good agreement with test results measured from uniaxial tension tests, it entails a double numerical integration which complicates its implementation into computational models and software developed for the analysis of the structural behaviour of SFRC members. In this paper, the DEM is simplified by eliminating the double numerical integration; thus, the Simplified DEM (SDEM) is derived. In order to simplify the DEM, only fibre slip on the shorter embedded side is taken into the account of the fibre tensile stress at a crack, while coefficients for frictional bond behaviour and mechanical anchorage effect are incorporated to prevent overestimation of the tensile stress attained by fibres due to the neglect of fibre slip on the longer embedded side. The tensile stress-crack width response of SFRC predicted by the SDEM shows good agreement with that obtained from the DEM; hence, the model's accuracy has largely been retained despite the simplification. In comparisons with test results reported in the previous literature, the SDEM is shown to simulate well not only the direct tensile behaviour but also the flexural behaviour of SFRC members. The SDEM can easily be implemented in currently available analysis models and programs so that it can be useful in the modelling of structural behaviour of SFRC members or structures.},
author_keywords={Anchorage;  Bond;  Crack Width;  Fibre Reinforced Concrete;  Steel Fibre;  Tensile Stress},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Denis2013269,
author={Denis, S. and Sandro, R. and Denis, Z.},
title={Experimentation of proactive computing in context aware systems: Case study of human-computer interactions in e-learning environment},
journal={2013 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Situation Awareness and Decision Support, CogSIMA 2013},
year={2013},
pages={269-276},
doi={10.1109/CogSIMA.2013.6523857},
art_number={6523857},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879773049&doi=10.1109%2fCogSIMA.2013.6523857&partnerID=40&md5=f854c58eb2a2662505f955af801118aa},
affiliation={Computer Science and Communications Research Unit, University of Luxembourg, 6, rue Richard Coudenhove-Kalergi, L-1359, Luxembourg, Luxembourg},
abstract={In this paper we show the implementation of the concept of Proactivity applied as the core mechanism in our Proactive Context Aware System (PCAS), which is capable to detect and extract the related events of interest from the user's contextual situation and to provide the appropriate goal-oriented actions to this event with the objective to help or assist the user or the group of users. We have chosen the academic environment as the ongoing contextual setting for our system. From this perspective, we designed Proactive Scenarios for an automatic and enhanced management of the online learning and teaching activities on Moodle™ for both student and teacher users. Due to the diversity of the potential contexts and situations arising from the user's activity, we developed two kinds of Proactive Scenarios. The first type or Meta Scenarios are responsible for capturing the changes in the outward context. The second type or Target Scenarios are triggered off by Meta Scenarios and aim to undertake the appropriate actions in response to the conditions of the user's contextual situation. In order to test and validate the capability of our software as well as to analyse its context related outcomes we have performed empirical studies. The experiments consisted in creating two groups of students, on the one hand the study group, which used the Moodle™ platform enhanced by PCAS, and on the other hand the control group, which used the standard version of Moodle™. The subsequent data analysis showed significant differences in specific related results such as notable advantage of the study group outcomes in the category of passing the final exam, where the study group has performed by 11 percentage points better than the control group. © 2013 IEEE.},
author_keywords={Cognitive Scenarios;  Context Aware System;  Context Situation Management;  Proactive Computing;  Proactive System;  Situational Awareness},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Saravani2013473,
author={Saravani, S.R. and Abbasi, B.},
title={Investigating the influence of job rotation on performance by considering skill variation and job satisfaction of bank employees [Ispitivanje utjecaja promjene posla na radnu učinkovitost uzimajući u obzir prekvalifikaciju i zadovoljstvo poslom bankovnih namještenika]},
journal={Tehnicki Vjesnik},
year={2013},
volume={20},
number={3},
pages={473-478},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879274673&partnerID=40&md5=80e6417bd7818f7de22cf5a7c6e888cf},
affiliation={Department of Agricultural Management, Rasht Branch, Islamic Azad University, Rasht, Iran; Department of human resources management, Astara Branch, Islamic Azad University, Astara, Iran},
abstract={Nowadays, optimal performance of organizations requires human resource development policies. Since human resources are considered as the most valuable factor of production, the most important capital and a major source of competitive advantage and essential competencies of organizations, the most effective means to achieve competitive advantage in the current environment is to improve efficiency of employees. Job rotation is the most important approach of job design as well as human capital development policies which has the potential to improve job satisfaction and increase capabilities in employees. Thus, present study attempts to study the effects of job rotation patterns on the performance of employees in Keshavarzi Banks of Gilan considering the skill diversity and job satisfaction and providing effective guidelines to enable managers to lead the organization toward a better future by desired policies. Statistical population consists of 218 employees from 30 branches of Keshavarzi Bank in three south, centre, east and west districts. The sample size was 137 estimated by Cochran formula. Hence, LISREL software was used to test the relationship between job rotation and performance by mediating the role of skill variation and job satisfaction by modelling structure equations. The results show that job performance is not directly influenced by job rotation. Job rotation positively influences job performance mediated by job satisfaction and skill variation.},
author_keywords={Job design;  Job performance;  Job rotation;  Job satisfaction;  Skill variation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Peng20131503,
author={Peng, J. and Wang, S. and Yang, L.},
title={The diversity of relationship between logistics and agricultural economy in the east China},
journal={Journal of Software},
year={2013},
volume={8},
number={6},
pages={1503-1510},
doi={10.4304/jsw.8.6.1503-1510},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878701787&doi=10.4304%2fjsw.8.6.1503-1510&partnerID=40&md5=fed259250492b64f01b5f2f32d240aa7},
affiliation={School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; Library Zhejiang Gongshang University, Hangzhou, China},
abstract={This paper aims at displaying the diversity between modern logistics of different districts and agricultural economy, and uses the Granger Causality Test to analyze the relationship between modern logistics and the local agricultural economy in four provinces in the east of China. The result shows that the agricultural economy of Jiangsu and Zhejiang province is more developed than AnHui and JiangXi province, but the logistics' promotion to the local agricultural economy of the former is not obvious as the later. For further study, finding this phenomenon is related to the economic structure of Jiangsu and Zhejiang, and these two also have large room to improve the mutual promotion between the logistics and the agricultural economy. The purpose of the essay is to draw lessons for China government to make policies about accelerating logistics and the areas' economy. © 2013 ACADEMY PUBLISHER.},
author_keywords={Agricultural economy;  Diversity;  Eviews software;  Granger causality test;  Logistics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen2013197,
author={Chen, Y. and Groce, A. and Zhang, C. and Wong, W.-K. and Fern, X. and Eide, E. and Regehr, J.},
title={Taming compiler fuzzers},
journal={ACM SIGPLAN Notices},
year={2013},
volume={48},
number={6},
pages={197-207},
doi={10.1145/2499370.2462173},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880106689&doi=10.1145%2f2499370.2462173&partnerID=40&md5=27ed148b0122c72e48e78dcb0cb19086},
affiliation={University of Utah, Salt Lake City, UT, United States; Oregon State University, Corvallis, OR, United States},
abstract={Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.},
author_keywords={Automated testing;  Bug reporting;  Compiler defect;  Compiler testing;  Fuzz testing;  Random testing;  Test-case reduction},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mateo2013315,
author={Mateo, P.R. and Usaola, M.P.},
title={Parallel mutation testing},
journal={Software Testing Verification and Reliability},
year={2013},
volume={23},
number={4},
pages={315-350},
doi={10.1002/stvr.1471},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877617433&doi=10.1002%2fstvr.1471&partnerID=40&md5=d57d187a48a0d89a7a79fbbe0b411b06},
affiliation={Tecnologías y Sistemas de Informaciõn, Universidad de Castilla-La Mancha, Ciudad Real, Spain},
abstract={Despite the existing techniques to reduce the costs of mutation analysis, the computational cost to apply mutation testing with large applications can be very high. One effective technique to improve the efficiency of mutation without losing effectiveness is parallel execution, where mutants and tests are executed in parallel processors, reducing the total time needed to perform mutation analysis. This paper presents a study of this technique adapted to current technologies. Five algorithms to execute mutants in parallel are analysed with three studies that use different network configurations and different number of processors with diverse characteristics. The experiments are performed with Bacterio P, a tool that is also presented. Unlike previous studies about parallel mutant execution, which date from the mid-1990s, in the studies in this paper, the communication time in parallel systems no longer acts as a bottleneck. Thus, dynamic strategies, which require more communication, combined with other mutant cost reduction techniques, are the best strategies to run mutants in parallel.Copyright © 2012 John Wiley &amp; Sons, Ltd.},
author_keywords={mutation testing;  parallel execution},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Borisov2013125,
author={Borisov, N. and Babu, S.},
title={Rapid experimentation for testing and tuning a production database deployment},
journal={ACM International Conference Proceeding Series},
year={2013},
pages={125-136},
doi={10.1145/2452376.2452392},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876803679&doi=10.1145%2f2452376.2452392&partnerID=40&md5=8b1664f248492bcaedbc7c58c245ac07},
affiliation={Duke University, United States},
abstract={The need to perform testing and tuning of database instances with production-like workloads (W), configurations (C), data (D), and resources (R) arises routinely. The further W, C, D, and R used in testing and tuning deviate from what is observed on the production database instance, the lower is the trustworthiness of the testing and tuning tasks done. For example, it is common to hear about performance degradation observed after the production database is upgraded from one software version to another. A typical cause of this problem is that the W, C, D, or R used during upgrade testing differed in some way from that on the production database. Performing testing and tuning tasks in principled and automated ways is very important, especially since - -spurred by innovations in cloud computing - -the number of database instances that a database administrator (DBA) has to manage is growing rapidly. We present Flex, a platform for trustworthy testing and tuning of production database instances. Flex gives DBAs a high-level language, called Slang, to specify definitions and objectives regarding running experiments for testing and tuning. Flex's orchestrator schedules and runs these experiments in an automated manner that meets the DBA-specified objectives. Flex has been fully prototyped. We present results from a comprehensive empirical evaluation that reveals the effectiveness of Flex on diverse problems such as upgrade testing, near-real-time testing to detect corruption of data, and server configuration tuning. We also report on our experiences taking some of the testing and tuning software described in the literature and porting them to run on the Flex platform. © 2013 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tao2013560,
author={Tao, C. and Li, B. and Gao, J.},
title={A systematic state-based approach to regression testing of component software},
journal={Journal of Software},
year={2013},
volume={8},
number={3},
pages={560-571},
doi={10.4304/jsw.8.3.560-571},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875035976&doi=10.4304%2fjsw.8.3.560-571&partnerID=40&md5=2a4cdaca6af257cba6648606693523ff},
affiliation={School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China; Key Laboratory of Computer Network and Information Integration (Southeast University), Ministry of Education, China; Department of Computer Engineering, San Jose State University, San Jose, CA, United States},
abstract={Today, component-based software engineering has been widely used in software construction to reduce project cost and speed up software development cycle. Due to software changes in new release or update of components, regression testing is needed to assure system quality. When changes made to a component, the component could be affected, moreover, the changes could bring impacts on the entire system. We firstly identify diverse changes made to components and system based on models, then perform change impact analysis, and finally refresh regression test suite using a state-based testing practice. Related existing research did not address the issue of systematic regression testing of component-based software, especially at system level. The paper also reports a case study based on a realistic component-based software system using a, which shows that the approach is feasible and effective. © 2013 ACADEMY PUBLISHER.},
author_keywords={Change and impact analysis;  Component-based software regression testing;  Re-test model;  State-based testing;  Test suite refreshment},
document_type={Article},
source={Scopus},
}

@BOOK{NoAuthor20131,
author={Division on Earth and Life Studies and Board on Atmospheric Sciences and Climate and Committee on a National Strategy for Advancing Climate Modeling},
title={A national strategy for advancing climate modeling},
journal={A National Strategy for Advancing Climate Modeling},
year={2013},
pages={1-280},
doi={10.17226/13430},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032851074&doi=10.17226%2f13430&partnerID=40&md5=96b4eaf23282b96820e48363b3130114},
abstract={As climate change has pushed climate patterns outside of historic norms, the need for detailed projections is growing across all sectors, including agriculture, insurance, and emergency preparedness planning. A National Strategy for Advancing Climate Modeling emphasizes the needs for climate models to evolve substantially in order to deliver climate projections at the scale and level of detail desired by decision makers, this report finds. Despite much recent progress in developing reliable climate models, there are still efficiencies to be gained across the large and diverse U.S. climate modeling community. Evolving to a more unified climate modeling enterprise-in particular by developing a common software infrastructure shared by all climate researchers and holding an annual climate modeling forum-could help speed progress. Throughout this report, several recommendations and guidelines are outlined to accelerate progress in climate modeling. The U.S. supports several climate models, each conceptually similar but with components assembled with slightly different software and data output standards. If all U.S. climate models employed a single software system, it could simplify testing and migration to new computing hardware, and allow scientists to compare and interchange climate model components, such as land surface or ocean models. A National Strategy for Advancing Climate Modeling recommends an annual U.S. climate modeling forum be held to help bring the nation's diverse modeling communities together with the users of climate data. This would provide climate model data users with an opportunity to learn more about the strengths and limitations of models and provide input to modelers on their needs and provide a venue for discussions of priorities for the national modeling enterprise, and bring disparate climate science communities together to design common modeling experiments. In addition, A National Strategy for Advancing Climate Modeling explains that U.S. climate modelers will need to address an expanding breadth of scientific problems while striving to make predictions and projections more accurate. Progress toward this goal can be made through a combination of increasing model resolution, advances in observations, improved model physics, and more complete representations of the Earth system. To address the computing needs of the climate modeling community, the report suggests a two-pronged approach that involves the continued use and upgrading of existing climate-dedicated computing resources at modeling centers, together with research on how to effectively exploit the more complex computer hardware systems expected over the next 10 to 20 years. © 2012 by the National Academy of Sciences. All rights reserved.},
document_type={Book},
source={Scopus},
}

@ARTICLE{Chen2013829,
author={Chen, X.P. and Wang, Y.D. and Li, J.T. and Roskilly, A.P.},
title={Hybrid electrical storage and power system for household tri-generation application},
journal={Advanced Materials Research},
year={2013},
volume={614-615},
pages={829-836},
doi={10.4028/www.scientific.net/AMR.614-615.829},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871789685&doi=10.4028%2fwww.scientific.net%2fAMR.614-615.829&partnerID=40&md5=eb7be98999f1d38930e248f7fef934a7},
affiliation={Newcastle Institute for Research on Sustainability, Newcastle University, Newcastle Upon Tyne, United Kingdom; Electric Engineering College, Guizhou University, Guiyang 550003, China; School of Energy, Power and Mechanical Engineering, North China Electric Power University, Beijing 102206, China},
abstract={As a crucial constituent in tri-generation application, electric energy storage and power system plays an important role regarding efficient utilization of electrical energy in tri-generation. This paper presents the results showing that the optimization of electrical energy storage is able to promote the performance of tri-generation. Initial investigation, including laboratory tests and computational simulation using Dymola software, have been carried out. A case study exemplifies how diverse hybrid systems accommodate domestic power demands. The outcomes validate that the hybrid electric system consisting of generator, batteries and super capacitor can satisfy the electricity requirements for the household. it is also found that the hybrid system can supply the peak electricity demands where the integration of super capacitor can alleviate the overcharge of batteries in this application. © (2013) Trans Tech Publications, Switzerland.},
author_keywords={Dymola simulation;  Energy management;  Hybrid electric energy storage;  Supercapacitor;  Tri-generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Borowski2013,
author={Borowski, S.K. and McCurdy, D.R. and Packard, T.W.},
title={Nuclear Thermal Propulsion (NTP): A proven, growth technology for "fast transit" human missions to Mars},
journal={AIAA SPACE 2013 Conference and Exposition},
year={2013},
doi={10.2514/6.2013-5354},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088757881&doi=10.2514%2f6.2013-5354&partnerID=40&md5=ec1c8d5d796764e9382f3030ac55b6ee},
affiliation={NASA Glenn Research Center, 21000 Brookpark Road, MS: 86-4, Cleveland, OH, 44135, United States; NASA Glenn Research Center, LLC at Glenn Research Center, 3000 Aerospace Parkway, Cleveland, OH, 44135, United States},
abstract={The "fast conjunction" long surface stay mission option was selected for NASA's recent Mars Design Reference Architecture (DRA) 5.0 study because it provided adequate time at Mars (~540 days) for the crew to explore the planet's geological diversity while also reducing the "1-way" transit times to and from Mars to ~6 months. Short transit times are desirable in order to reduce the debilitating physiological effects on the human body that can result from prolonged exposure to the zero-gravity (0-gE) and radiation environments of space. Recent measurements from the RAD detector attached to the Curiosity rover indicate that astronauts would receive a radiation dose of ~0.66 Sv (~66 rem) - the limiting value established by NASA - during their 1-year journey in deep space. Proven nuclear thermal rocket (NTR) technology, with its high thrust and high specific impulse (Isp~900 s), can cut 1-way transit times in half by increasing the propellant-carrying capacity of the Mars transfer vehicle (MTV). No large technology scale-ups in engine size are required for these short transit missions either since the smallest engine tested during the Rover program - the 25 klbf "Pewee" engine is sufficient when used in a clustered arrangement of 3 - 4 engines. The "Copernicus" crewed MTV developed for DRA 5.0 is a 0-gE design consisting of three basic components: (1) the NTP stage (NTPS); (2) the crewed payload element; and (3) an integrated "saddle truss" and LH2 propellant drop tank assembly that connects the two elements. With a propellant capacity of ~190 t, Copernicus could support 1-way transit times ranging from ~150 - 220 days over the 15-year synodic cycle. The paper examines the impact on vehicle design of decreasing transit times for the 2033 mission opportunity. With a fourth "upgraded" SLS / HLV launch, an "in-line" LH2 tank element can be added to Copernicus allowing 1-way transit times of 130 days. To achieve 100 - 120 day transit times, Copernicus' saddle truss / drop tank assembly is replaced by a "star truss" assembly with paired modular drop tanks to further increase the vehicle's propellant capacity. The HLV launch count increases (from ~ 5 - 7) and a fourth engine is added to reduce total mission burn time and gravity losses. Using a "split mission" approach, the NTPS, in-line tank and the saddle truss / LH2 drop tank elements can be configured as a pre-deployed Earth Return Vehicle / propellant tanker supporting 90-day crewed mission transits. The split mission approach also eliminates the need for on-orbit assembly. Mission scenario descriptions, key features and operational characteristics for five different vehicle configurations are presented.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Benameur2013,
author={Benameur, A. and Evans, N.S. and Elder, M.C.},
title={Minestrone: Testing the soup},
journal={6th Workshop on Cyber Security Experimentation and Test, CSET 2013},
year={2013},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084159847&partnerID=40&md5=8543c23b7edb7a1d30af65e3478c408c},
affiliation={Symantec Research Labs, Herndon, VA, United States},
abstract={Software development using type-unsafe languages (e.g., C and C++) is a challenging task for several reasons, security being one of the most important. Ensuring that a piece of code is bug or vulnerability free is one of the most critical aspects of software engineering. While most software development life cycle processes address security early on in the requirement analysis phase and refine it during testing, it is not always sufficient. Therefore the use of commercial security tools has been widely adopted by the software industry to help identify vulnerabilities, but they often have a high false-positive rate and have limited effectiveness. In this paper we present MINESTRONE, a novel architecture that integrates static analysis, dynamic confinement, and code diversification to identify, mitigate, and contain a broad class of software vulnerabilities in Software Of Uncertain Provenance (SOUP). MINESTRONE has been tested against an extensive test suite and showed promising results. MINESTRONE showed an improvement of 34.6% over the state-of-the art for memory corruption bugs that are commonly exploited. © 2013 USENIX Association. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Weber201395,
author={Weber, G. and Herranz, L.E. and Bendiab, M. and Fontanet, J. and Funke, F. and Gonfiotti, B. and Ivanov, I. and Krajewski, S. and Manfredini, A. and Paci, S. and Pelzer, M. and Sevón, T.},
title={Thermal-hydraulic-iodine chemistry coupling: Insights gained from the SARNET benchmark on the THAI experiments Iod-11 and Iod-12},
journal={Nuclear Engineering and Design},
year={2013},
volume={265},
pages={95-107},
doi={10.1016/j.nucengdes.2013.07.012},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975842802&doi=10.1016%2fj.nucengdes.2013.07.012&partnerID=40&md5=2403b88773af748bc0efd33070a9398f},
affiliation={Gesellschaft für Anlagen- und Reaktorsicherheit (GRS)mbH, Garching and Cologne, Germany; CIEMAT, Madrid, Spain; AREVA NP, Erlangen, Germany; TUS, Sofia, Bulgaria; FZ-Jülich, Jülich, Germany; Università di Pisa, Pisa, Italy; VTT, Espoo, Finland},
abstract={In the SARNET2 WP8.3 THAI Benchmark the capability of current accident codes to simulate the iodine transport and behavior in sub-divided containments has been assessed. In THAI test Iod-11 and Iod-12, made available for the benchmark, the distribution of molecular iodine (I2) in the five compartments of the 60 m3 vessel under stratified and well mixed conditions was measured. The main processes addressed are the I2 transport with the atmospheric flows and the interaction of I2 with the steel surface. During test Iod-11 the surfaces in contact with the containment atmosphere were dry. In Iod-12, steam was released, which condensed on the walls. Nine post-test calculations were conducted for Iod-11 and eight for Iod-12 by seven organizations using four different codes: ASTEC-IODE (CIEMAT, GRS and TUS), COCOSYS-AIM (AREVA, FZ-Jülich and GRS), ECART (Pisa University) and MELCOR (Pisa University and VTT). Different nodalizations of the THAI vessel with 20-65 zones were applied. Generally, for both tests the analytical thermal-hydraulic results are in a fairly good agreement with the measurements. Only the calculated local relative humidity deviates significantly from the measured values in all calculations. The results in Iod-11 for the local I2 concentration in the gaseous phase are quite diverse. Three calculations show only minor deviations from the measurement, whereas the others are substantially different from the measured I2 concentrations. For Iod-12, no calculation delivers a satisfactory evolution of the I2 concentration in all five compartments of the vessel. There are three mediocre results standing out in the Iod-11 exercise which are from the same user-code combinations. The discrepancies derive from various reasons which are discussed in the paper. In the benchmark a significant user effect was detected, i.e. results achieved with the same code differed considerably. This work highlights the need of a detailed iodine adsorption/desorption model and precise thermal-hydraulic modeling for an accurate simulation of I2 transport in a sub-divided containment, as well as experienced users or straight forward user guidelines. References H.-J.Allelein, S.Arndt, W.Klein-Heßling, S.Schwarz, C.Spengler, and G.WeberCOCOSYS: status of development and validation of the German containment code systemNucl. Eng. Des.2382008872889 J.Ball, and C.MarchandISP-41 - Follow-up Exercise Phase 2200416NEA/CSNI/R(2004) L.Bosland, L.Cantrel, N.Girault, and B.ClementModelling of iodine radiochemistry in the ASTEC severe accident code: description and application to FPT-2 PHEBUS testNucl. Technol.1711201088107 L.Bosland, G.Weber, W.Klein-Hessling, N.Girault, and B.ClementModelling and interpretation of iodine behaviour in PHEBUS FPT-1 containment with ASTEC and COCOSYS codesNucl. Technol.17720123662 B.Clément, and T.HasteComparison Report on International Standard Problem ISP-46 (PHEBUS FPT1)2003IRSN, Note Technique SEMAR 03/021 B.Clément, L.Cantrel, G.Ducros, F.Funke, L.Herranz, A.Rydl, G.Weber, and C.WrenState of the art report on iodine chemistry20071OECD NEA/CSNI/R(2007) S.Dickinson, F.Andreo, T.Karkela, J.Ball, L.Bosland, L.Cantrel, F.Funke, N.Girault, J.Holm, S.Guilbert, L.E.Herranz, C.Housiadas, G.Ducros, C.Mun, J.-C.Sabroux, and G.WeberRecent advances on containment iodine chemistryProg. Nucl. Energy522010128135 F.Firnhaber, T.F.Kanzleiter, S.Schwarz, and G.WeberVANAM M3 - A Multi Compartment Aerosol Depletion Test with Hygroscopic Aerosol Material199626NEA/CSNI/R(96) R.FontanaECART User's Manual, vol. 2, Code Structure and Theory2010ERSE6267 F.Funke, G.Weber, H.-J.Allelein, T.Kanzleiter, M.Morell, and G.PossMulti-compartment iodine tests in the THAI facilityEurosafe 2004 ForumBerlin, 8 and 9 November 20042004 F.Funke, G.Langrock, T.Kanzleiter, G.Poß, K.Fischer, G.Weber, and H.-J.AlleleinFinal report - test facility and program to investigate open questions on fission product behaviour in the containment - ThAI phase II - Part 2. Iodine tests2006Reactor Safety Research - Project No. 150 1272 (Translation of Excerpts), Report No. NTR-G/2007/de/0233A R.O.Gauntt, J.E.Cash, R.K.Cole, C.M.Erickson, L.L.Humphries, S.B.Rodriguez, and M.F.YoungMELCOR Computer Code Manuals, vol. 1, Primer and Users' Guidevol. 12005Sandia National LaboratoriesNUREG/CR-6119, Rev. 3 N.Girault, L.Bosland, S.Dickinson, F.Funke, S.Güntay, L.E.Herranz, and D.PowersLWR sever accident simulation: Iodine behaviour in FPT2 experiment and advances on containment iodine chemistryNucl. Eng. Des.2432012371392 T.Kanzleiter, K.Fischer, W.Häfner, H.-J.Allelein, and S.SchwarzTHAI multi-compartment containment experiments with atmosphere stratificationNURETH-11Avignon, France, October 2-6, 20052005 T.NugrahaIodine retention on stainless steel sampling linesPhD Thesis Presented at the Department of Chemical Engineering and Applied ChemistryUniversity of Toronto1997 OECDOECD/NEA behaviour of iodine project (BIP)2011http://www.oecd-nea.org/jointproj/bip.html J.P.Van DorsselaereThe ASTEC integral code for severe accident simulationNucl. Technol.1652009293307 G.WeberSpecification of the SARNET-2 WP8 THAI benchmarkSARNET2-ST-D8.22012 G.Weber, and F.FunkeDescription of the Iodine Model AIM-3 in COCOSYS2009GRS-A-3508 G.Weber, F.Funke, and G.PossIodine transport and behaviour in large-scale THAI tests4th European Review Meeting on Severe Accident Research (ERMSAR-2010)ENEA Bologna, Italy, 11-12 May 20102010 G.Weber, L.E.Herranz, M.Bendiab, J.Fontanet, F.Funke, B.Gonfiotti, I.Ivanov, S.Krajewski, A.Manfredini, S.Paci, M.Pelzer, and T.SevónSARNET2 WP8 benchmark on the THAI iodine multi-compartment tests Iod-11 and Iod-12SARNET2-ST-D8.42012 J.C.Wren, and G.A.GlowaKinetics of gaseous iodine uptake onto stainless steel during iodine-assisted corrosionNucl. Technol.1332001. © 2013 Elsevier B.V. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ferreira2013232,
author={Ferreira, J.M. and Vergilio, S.R. and Quinaia, M.A.},
title={A mutation approach to feature testing of software product lines},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2013},
volume={2013-January},
number={January},
pages={232-237},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937699438&partnerID=40&md5=e621408d32787baf56f3fc83c05f5f0c},
affiliation={DInf, UFPR - Federal University of ParanaCEP: 81531-970, Brazil; UNICENTRO, State University of Central West GuarapuavaCEP: 85040-080, Brazil},
abstract={Diverse development methodologies use the feature model (FM) to represent common and variable features of a software product line (SPL). This model has also been used to derive products for testing. However, the test of all combinations of features (products) is infeasible in practice, due to the growing complexity of the applications, and only a subset of products is usually selected. Existing selection methods do not consider faults in the FM. The application of a fault-based approach can increase the probability of finding faults and the confidence that the SPL products match the requirements. Considering that, this paper introduces a mutation based approach to help in the selection of products for feature testing of SPLs. Mutation operators are introduced and a testing process is also proposed. Results from a case study are reported, and a comparison with pair-wise testing shows that other kind of faults can be revealed by the introduced approach. Copyright © 2013 by Knowledge Systems Institute Graduate School.},
author_keywords={Feature model;  SPL;  Testing criteria},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tao2013356,
author={Tao, C. and Li, B. and Gao, J.},
title={Testing configurable architectures for component-based software using an incremental approach},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2013},
volume={2013-January},
number={January},
pages={356-361},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937680808&partnerID=40&md5=5238e51e86685b98f170c25c476f5954},
affiliation={School of Computer Science and Engineering, Southeast University, Nanjing, Jiangsu, China; School of Computer Engineering, San Jose State University, San Jose, CA, United States},
abstract={Configurable software lets users customize applications in many ways, such as different configurable environments, diverse functions, and various configurable architectures. As the advance of software component technology, engineers encountered different issues and challenges in testing and automation of configurable components and component-based programs. In previous work, we proposed an approach to configuration testing based on a semantic tree, to model, present, and analyze diverse composite components and configurable software. Various configurations are modeled based on the spanning tree, which is a subtree of semantic tree. For realistic component-based software, there might exist a number of configurable combinations, i.e., the number of subtrees can be unimaginably high. Thus configuration testing becomes very complex and not cost-effective. In this paper, we propose an incremental approach to testing configurable architectures of component-based software based on the semantic tree model. Compared to our existing work, the new approach could reduce the testing complexity significantly. The initial study results indicate the proposed approach is feasible and effective in testing configurable architectures for component-based software. Copyright © 2013 by Knowledge Systems Institute Graduate School.},
author_keywords={Component-based software;  Configurable testing;  Test complexity;  Test modeling and analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Niemeyer2013287,
author={Niemeyer, F. and Schima, R. and Grenzdörffer, G.},
title={Relative and absolute calibration of a multihead camera system with oblique and nadir looking cameras for a UAS},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2013},
volume={40},
number={1W2},
pages={287-291},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922602080&partnerID=40&md5=d4a4a41801d006abcc4dc79907bbeeb1},
affiliation={Depart. for Geodesy and Geoinformatics, Rostock University, Rostock, 18059, Germany},
abstract={Numerous unmanned aerial systems (UAS) are currently flooding the market. For the most diverse applications UAVs are special designed and used. Micro and mini UAS (maximum take-off weight up to 5 kg) are of particular interest, because legal restrictions are still manageable but also the payload capacities are sufficient for many imaging sensors. Currently a camera system with four oblique and one nadir looking cameras is under development at the Chair for Geodesy and Geoinformatics. The so-called "Four Vision" camera system was successfully built and tested in the air. A MD4-1000 UAS from microdrones is used as a carrier system. Light weight industrial cameras are used and controlled by a central computer. For further photogrammetric image processing, each individual camera, as well as all the cameras together have to be calibrated. This paper focuses on the determination of the relative orientation between the cameras with the "Australis" software and will give an overview of the results and experiences of test flights.},
author_keywords={"Australis";  Calibration;  Multi camera system;  Oblique camera;  UAS},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tailor2013167,
author={Tailor, M. and Phaithoonbuathong, P. and Petzing, J. and Jackson, M. and Parkin, R.},
title={Real-time surface defect detection and the traceable measurement of defect depth in 3D},
journal={Laser Metrology and Machine Performance X - 10th International Conference and Exhibition on Laser Metrology, Machine Tool, CMM and Robotic Performance, LAMDAMAP 2013},
year={2013},
pages={167-176},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908264644&partnerID=40&md5=9e4a89fe9b6cae15cc504f325298768c},
affiliation={EPSRC Centre for Innovative Manufacturing in Intelligent Automation, Wolfson School of Mechanical and Manufacturing Engineering, Loughborough University, United Kingdom},
abstract={Three-dimensional non-contact optical surface topography and coordinate geometry measurement are of significant interest to many industrial sectors. The current requirement is to speed up inspection processes, increase sensor resolution, and sensor accuracy. This is not necessarily the case for online surface defect inspection. Human visual analysis of surface defects (pits, scratches, etc) is qualitative and subjective. Automated non-contact analysis should provide a robust and systematic quantitative route for defect assessment. However, different optical transducers use disparate physical principles, interact with surfaces and defects in diverse ways, leading to variation in measurement data from one instrument to the next. Instrument "black box" software is often non-traceable, leading to significant uncertainty about data manipulation. This is compounded by a lack of traceable surface defect standards and defect soft gauges with which to test the instruments and software respectively. This work reports the development of novel traceable surface defect artefacts (typically 100 μm wide by 10 μm deep) produced using Vickers equipment on flat metal plates with varying scales of surface roughness, and the development of a novel traceable, repeatable, mathematical solution for automatic defect detection and absolute depth measurement. Comparative results show that the new surface defect detection/quantification is more efficient, repeatable and robust than alternative measurement processes using a range of instrumentation based (and third party) software solutions.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sinnott201339,
author={Sinnott, R.O. and Bayliss, C. and Morandini, L. and Tomko, M.},
title={Tools and processes to support the development of a national platform for urban research: Lessons (being) learnt from the AURIN project},
journal={Conferences in Research and Practice in Information Technology Series},
year={2013},
volume={140},
pages={39-48},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907002165&partnerID=40&md5=9fba0c124c4842c73da8d8d9dc169557},
affiliation={Australian Urban Research Infrastructure Network (AURIN), University of Melbourne, VIC, 3052, Australia},
abstract={The development of large-scale software systems remains a non-trivial endeavour. This is especially so when the software systems comprise services and resources coming from multiple distributed software groups, and where they are required to interoperate with heterogeneous, independent (and autonomous) distributed data providers. The use of software development and management tools to support this process is highly desirable. In this paper we focus on the software development and management systems that have been adopted within the national Australian Urban Research Infrastructure Network (AURIN - www.aurin.org.au) project. AURIN is tasked with developing a software platform to support research into the urban and built environment - a domain with many diverse software system and data needs. In particular, given that AURIN is tasked with integrating a large portfolio of sub-projects offering both software and data that needs to be integrated, deployed and managed by a core team at the University of Melbourne, we illustrate how tooling and support processes are used to manage the software development lifecycle and code/data integration from the distributed teams and data providers that are involved. Results from the project demonstrating the ongoing status are presented. © 2013 Australian Computer Society, Inc.},
author_keywords={Code management;  Collaborative development environment;  Software testing;  Urban research},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Soares2013242,
author={Soares, C.A.O. and Batista, L.S. and Campelo, F. and Guimaraes, F.G.},
title={Computation of mixed strategy non-dominated nash equilibria in game theory},
journal={Proceedings - 1st BRICS Countries Congress on Computational Intelligence, BRICS-CCI 2013},
year={2013},
pages={242-247},
doi={10.1109/BRICS-CCI-CBIC.2013.47},
art_number={6855856},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905396247&doi=10.1109%2fBRICS-CCI-CBIC.2013.47&partnerID=40&md5=b8daeabc85f5ef8eb653377703abd65e},
affiliation={Graduate Program in Electrical Engineering, Universidade Federal de Minas Gerais, Belo Horizonte, Minas Gerais, Brazil; Department of Electrical Engineering, Universidade Federal de Minas Gerais, Belo Horizonte, Minas Gerais, Brazil},
abstract={Finding Nash equilibria has been one of the early objectives of research in game theory, and still represents a challenge to this day. We introduce a multiobjective formulation for computing Pareto-optimal sets of mixed Nash equilibria in normal form games. Computing these sets can be notably useful in decision making, because it focuses the analysis on solutions with greater outcome and hence more stable and desirable ones. While the formulation is suitable for any multiobjective optimization algorithm, we employ a method known as the cone-epsilon MOEA, due to its good convergence and diversity characteristics when solving multiobjective optimization problems. The adequacy of the proposed formulation is tested on most normal form games provided by the GAMBIT software test suite. The results show that the cone-epsilon MOEA working on the proposed formulation correctly finds the Pareto-optimal Nash equilibra in most games. © 2013 IEEE.},
author_keywords={Evolutionary algorithm;  Multiobjective;  Nash;  Pareto},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Brooks20131313,
author={Brooks, L. and Wu, J. and Teegarden, D.},
title={Early verification of complex distributed systems using model driven development and virtual engineering},
journal={Lecture Notes in Electrical Engineering},
year={2013},
volume={196 LNEE},
number={VOL. 8},
pages={1313-1325},
doi={10.1007/978-3-642-33738-3_32},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903650198&doi=10.1007%2f978-3-642-33738-3_32&partnerID=40&md5=5e66a3eda58e6b84196e4d351a2c5657},
affiliation={Mentor Graphics Corporation, Wilsonville, OR 97070, United States},
abstract={Research and/or Engineering Questions/Objectives: System-level modeling accelerates the development of distributed mechatronic systems by automating tasks and maintaining the integrity of validated executable specifications and test benches. This paper presents a systematic distributed embedded system development methodology that provides abstraction of hardware and software concerns, facilitates communication via highly accessible models, and promotes reliable early development of both hardware and software. The focus is on the development of complex mechatronic systems with specific emphasis placed on early development and subsequent reuse of hardware-dependent software components and on the concurrent and independent development of embedded electronic hardware, software, and physical plants. Methodology: Complex interactions between physical plants and distributed embedded computing components make developing mechatronic systems very difficult-even when everything is fully specified. New technology options add further complications and require diversified skills from system developers. Designers must deal with embedded software, electronic control units (ECUs), electro-mechanical subsystems (mechatronics), and the networks through which they communicate. In order to manage change and accelerate development, collaboration within disparate groups of people is essential; various cross-sections of engineering disciplines must be allowed to work independently from each other without incurring huge integration costs in subsequent development phases. Model Driven Development (MDD) has been a term used in our industry for some time, but it has had a hard time achieving widespread adoption and respect as the most effective way to drive a design process. It accelerates the creation, verification, and validation of embedded software using models as the primary engineering deliverable. Results: Domain experts use MDD techniques to complete application-specific tasks early, using highly accurate mechatronic models. As projects progress through architectural design, functional partitioning, and detailed component design, the coherence of a system's model is maintained via communication abstractions, reuse of standard system-level architectures, and automatic C/C++ code generation. These capabilities decouple the decision points and ease regression testing to accelerate development as early confirmed strategies are successively verified when networks, actuators and sensors, real-time operating systems, embedded processors, and other sub-systems change. Models are captured using standard modeling languages such as xtUML and can be transformed, automatically, into production-ready C/C++ embedded software. Virtual Engineering augments MDD to provide a realistic modeling alternative-using modeling standards such as VHDL-AMS-to the physical electronics, mechanical devices, and other hardware that make up the typical environment that surrounds software under development. Limitations of this Study: The technology boundaries inherent in such systems pose two main problems for design teams: compatibility between tools and communication between specialists. Conventional simulation tools cannot adequately deal with diverse modeling requirements; also, technology specialists speak a unique design language that is tailored to his/her specialty. As systems become more complex, contractors who once specialized in narrow technical areas are being forced to act as systems integrators and in turn, are contracting subsystems to a global network of subcontractors. It adds up to significant potential for misunderstandings, errors, and omissions due to communication challenges across language and cultural boundaries. What does the paper offer that is new in the field in comparison to other works of the author: MDD provides an approach to the challenge of technology change by separating the application portions of a system from underlying platform technology. This technology promotes early, independent, and concurrent development by empowering designers to focus on application models without regard to platform-specific details. This is the difference between MDD and model-based development. MDD completely preserves early application modeling artefacts; the latter repeats modeling efforts for each platform variant. Conclusions: This paper describes the capabilities of a virtual system integration environment that supports conceptual system development used early in the design cycle, as well as complete distributed system development on final embedded computing hardware. It describes how MDD improves productivity in the design cycle, automatically generating parts of a design, thus improving quality by bringing in repeatability and standards compliance. It shows how design team members working on complex projects in disparate locations can effectively collaborate using a common modeling and analysis environment, allowing a common modeling and analysis environment to act as a communications vehicle for the entire team. © Springer-Verlag 2013.},
author_keywords={AUTOSAR;  Distributed systems;  Model driven development;  Verification;  Virtual engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Larner20137,
author={Larner, M. and Davila, L.P.},
title={The mechanical properties of porous aluminum using finite element method simulations and compression experiments},
journal={Materials Research Society Symposium Proceedings},
year={2013},
volume={1580},
pages={7-15},
doi={10.1557/opl.2013.663},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900338962&doi=10.1557%2fopl.2013.663&partnerID=40&md5=97e3f975c20b4178449d6d5a0d8173e6},
affiliation={Materials Science and Engineering, School of Engineering, University of California Merced, 5200 N. Lake Road, Merced, CA 95343, United States},
abstract={Lightweight porous metallic materials are generally created through specialized processing techniques. Their unique structure gives these materials interesting properties which allow them to be used in diverse structural and insulation applications. In particular, highly porous Al structures (Al foams) have been used in aircraft components and sound insulation; however due to the difficulty in processing and random nature of the foams, they are not well understood and thus they have not yet been utilized to their full potential. The objective of this project was to determine whether a relationship exists between the relative density (porous density/bulk density) and the mechanical properties of porous Al structures. For this purpose, a combination of computer simulations and experiments was pursued to better understand possible relationships. A Finite Element Method (FEM)-based software, COMSOL Multiphysics 4.3, was used to model the structure and to simulate the mechanical behavior of porous Al structures under compressive loads ranging from 1-100 MPa. From these simulated structures, the maximum von Mises stress, volumetric strain, and other properties were calculated. These simulation results were compared against data from compression experiments performed using the Instron Universal Testing Machine (IUTM) on porous Al specimens created via a computer-numerically- controlled (CNC) mill. CES EduPack software, a materials design program, was also used to estimate the mechanical properties of porous Al and open cell foams for values not available experimentally, and for comparison purposes. This program allowed for accurate prediction of the mechanical properties for a given percent density foam, and also provided a baseline for the solid Al samples tested. The main results from experiments were that the Young's moduli (E) for porous Al samples (55.8% relative density) were 15.9-16.6 GPa depending on pore diameter, which is in good agreement with the CES EduPack predictions; while the compressive strengths (σc) were 155-185 MPa, higher than those predicted by CES EduPack. The results from the FEM simulations using 3D models (55.8% relative density) revealed the onset of yielding at 13.5-14.0 MPa, which correlates well with CES EduPack data. Overall results indicated that a combination of experiments and FEM simulations can be used to calculate structure-property relationships and to predict yielding and failure, which may help in the pursuit of simulation-based design of metallic foams. In the future, more robust modeling and simulation techniques will be explored, as well as investigating closed cell Al foams and different porous geometries (nm to micron). This study can help to improve the current methods of characterizing porous materials and enhance knowledge about their properties for alternative energy applications, while promoting their design through integrated approaches. © 2013 Materials Research Society.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Marculescu2013102,
author={Marculescu, B. and Feldt, R. and Torkar, R.},
title={Objective re-weighting to guide an interactive search based software testing system},
journal={Proceedings - 2013 12th International Conference on Machine Learning and Applications, ICMLA 2013},
year={2013},
volume={2},
pages={102-107},
doi={10.1109/ICMLA.2013.113},
art_number={6786089},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899433128&doi=10.1109%2fICMLA.2013.113&partnerID=40&md5=01b337a89bfd1eb60c51fec7ad902787},
affiliation={Blekinge Institute of Technology, School of Computing, Karlskrona, Sweden; Chalmers and University of Gothenburg, Dept. of Computer Science and Engineering, Gothenburg, Sweden},
abstract={Even hardware-focused industries today develop products where software is both a large and important component. Engineers tasked with developing and integrating these products do not always have a software engineering background. To ensure quality, tools are needed that automate and support software testing while allowing these domain specialists to leverage their knowledge and experience. Search-based testing could be a key aspect in creating an automated tool for supporting testing activities. However, domain specific quality criteria and trade-offs make it difficult to develop a general fitness function a priori, so interaction between domain specialists and such a tool would be critical to its success. In this paper we present a system for interactive search-based software testing and investigate a way for domain specialists to guide the search by dynamically re-weighting quality goals. Our empirical investigation shows that objective re-weighting can help a human domain specialist interactively guide the search, without requiring specialized knowledge of the system and without sacrificing population diversity. © 2013 IEEE.},
author_keywords={embedded software;  Industrial experience;  interactive search based software engineering;  search based software testing;  user centered},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bhuyan2013,
author={Bhuyan, P. and Kumar, A. and Mohapatra, D.P.},
title={SOA testing perspective model for regression testing},
journal={2013 Nirma University International Conference on Engineering, NUiCONE 2013},
year={2013},
doi={10.1109/NUiCONE.2013.6780096},
art_number={6780096},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899415666&doi=10.1109%2fNUiCONE.2013.6780096&partnerID=40&md5=af2b4695e5b817c5b093808cd323d3c3},
affiliation={School of Computer Engineering, KIIT University, Bhubaneswar, Odisha, India; Department of Computer Science and Engineering, NIT Rourkela, Odisha, India},
abstract={Service-Oriented Architecture (SOA) supports loose-coupling and interoperability, where services communicate with each-other through message exchanging protocol and interfaces. SOA supports vendor diversity. In order to full-fill the vendor need, service composition is considered as a key process. Regression testing is inevitable to assure the quality of SOA based applications during their evolution. This paper defines a regression testing process which helps us in regression testing of complex SOA based applications. We also propose an SOA testing perspective model. Here we divide SOA testing perspective model into three parts: Service developer perspective, Service tester perspective and Service provider perspective. The Proposed model also focuses on service validity when the service is going to register in the Universal Description and Discovery Integration (UDDI). © 2013 IEEE.},
author_keywords={regression testing;  SOA;  testing perspective model},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gupta201360,
author={Gupta, S. and Ganguly, S. and Sundari, R.T.},
title={Quantitative analysis on performance of software projects},
journal={CONSEG 2013 - Proceedings of the 7th CSI International Conference on Software Engineering},
year={2013},
pages={60-67},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898841043&partnerID=40&md5=6966238ef9ef3755f71cdfa3b0a3a335},
affiliation={CDAC, Noida, C-56/1 Sector 62,Institutional Area, Noida, 201301, India},
abstract={Software projects are more inclined towards their diverse nature of concepts and process of execution. Quality of the software product depends on standard processes. For monitoring the efficacy of processes - data collection, statistical analysis, derivation of Process Performance Baseline (PPB) and Process Performance Model (PPM) of data are essentially required. The paper emphasizes on tools like i) Test of Hypothesis for formation of different Buckets by performing statistical test ii) Statistical analysis of Bucket data and derivation of process performance baseline iii) Creation of Process Performance Models by conducting regression testing. Finally this paper discusses the use and benefit of process performance model in software projects for quantitatively managing the projects. The end result enables us to quantitatively measure the product quality, project timeliness and resource utilization. © Computer Society of India, 2013.},
author_keywords={F-tests;  Normality tests;  Process performance baseline;  Process performance model;  T-tests},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Do2013156,
author={Do, N.Q. and Ong, H.S. and Lai, L.C. and Che, Y.X. and Ong, X.J.},
title={Open-source testing tools for smart grid communication network},
journal={2013 IEEE Conference on Open Systems, ICOS 2013},
year={2013},
pages={156-161},
doi={10.1109/ICOS.2013.6735066},
art_number={6735066},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897696293&doi=10.1109%2fICOS.2013.6735066&partnerID=40&md5=b0aee1b7253bd605dd8903255aa43928},
affiliation={Department of Electronics and Communication Engineering, Universiti Tenaga Nasional (UNITEN), Kajang, Malaysia},
abstract={In any smart grid communication implementation, to determine the performance factor of the network, a testing of an end-to-end process flow is required. Therefore, an effective testing tool plays a crucial role in evaluating the performance of smart grid communications. Currently, there are a large number of tools and utilities that are available to study different performance parameters of a network. However, there have been a few studies investigating the testing tools for smart grid communication system. Moreover, most research works done previously focused on only one or two performance metrics such as delay and/or throughput. Plus, diverse communication technologies in smart grid creates major challenges for conventional testing software, for legacy testing programs are designed only for a single communication technology or standard in the network. Therefore, this paper discusses various open-source monitoring and testing tools and determines which set of tools is the most suitable for evaluating the performance of smart grid communication network. © 2013 IEEE.},
author_keywords={Network performance evaluation;  Network testing tools;  Open-source;  Performance metrics;  Smart grid},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Manavizadeh2013124,
author={Manavizadeh, N. and Tavakoli, L. and Rabbani, M. and Jolai, F.},
title={A multi-objective mixed-model assembly line sequencing problem in order to minimize total costs in a Make-To-Order environment, considering order priority},
journal={Journal of Manufacturing Systems},
year={2013},
volume={32},
number={1},
pages={124},
doi={10.1016/j.jmsy.2012.09.001},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895064397&doi=10.1016%2fj.jmsy.2012.09.001&partnerID=40&md5=d171ededd509e6a85e291cc0182d1d2b},
affiliation={Department of Industrial Engineering, College of Engineering, University of Tehran, P.O. Box 11155/4563, Tehran, Iran},
abstract={Nowadays, mixed-model assembly line is used increasingly as a result of customers' demand diversification. An important problem in this field is determining the sequence of products for entering the line. Before determining the best sequence of products, a new procedure is introduced to choose important orders for entering the shop floor. Thus the orders are sorted using an analytical hierarchy process (AHP) approach based on three criteria: critical ratio of each order (CRo), Significance degree of customer and innovation in a product, while the last one is presented for the first time. In this research, six objective functions are presented: minimizing total utility work cost, total setup cost and total production rate variation cost are the objectives which were presented previously, another objective is minimizing total idle cost, meanwhile two other new objectives regarding minimizing total operator error cost and total tardiness cost are presented for the first time. The total tardiness cost tries to choose a sequence of products that minimizes the tardiness cost for customers with high priority. First, to check the feasibility of the model, GAMS software is used. In this case, GAMS software could not search all of the solution space, so it is tried in two stages and because this problem is NP-hard, particle swarm optimization (PSO) and simulated annealing (SA) algorithms are used. For small sized problems, to compare exact method with proposed algorithms, the problem must be solved using meta-heuristic algorithms in two stages as GAMS software, whereas for large sized problems, the problem can be solved in two ways (one stage and two stages) by using proposed algorithms; the computational results and pairwise comparisons (based on sign test) show GAMS is a proper software to solve small sized problems, whereas for a large sized problem the objective function is better when solved in one stage than two stages; therefore it is proposed to solve the problem in one stage for large sized problems. Also PSO algorithm is better than SA algorithm based on objective function and pairwise comparisons. © 2012 The Society of Manufacturing Engineers. Published by Elsevier Ltd. All rights reserved.},
author_keywords={Mixed-model assembly line sequencing problem;  Multi-objective;  Order priority;  Particle swarm optimization (PSO);  Simulated annealing (SA)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Jiang2013190,
author={Jiang, B. and Chan, W.K.},
title={Bypassing code coverage approximation limitations via effective input-based randomized test case prioritization},
journal={Proceedings - International Computer Software and Applications Conference},
year={2013},
pages={190-199},
doi={10.1109/COMPSAC.2013.33},
art_number={6649820},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891327461&doi=10.1109%2fCOMPSAC.2013.33&partnerID=40&md5=3a49a8ca6dc9b19711afd608c85b73ef},
affiliation={School of Computer Science and Engineering, Beihang University, Beijing, China; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong, Hong Kong},
abstract={Test case prioritization assigns the execution priorities of the test cases in a given test suite with the aim of achieving certain goals. Many existing test case prioritization techniques however assume the full-fledged availability of code coverage data, fault history, or test specification, which are seldom well-maintained in many software development projects. This paper proposes a novel family of LBS techniques. They make adaptive tree-based randomized explorations with an adaptive randomized candidate test set strategy to diversify the explorations among the branches of the exploration trees constructed by the test inputs in the test suite. They get rid of the assumption on the historical correlation of code coverage between program versions. Our techniques can be applied to programs with or without any previous versions, and hence are more general than many existing test case prioritization techniques. The empirical study on four popular UNIX utility benchmarks shows that, in terms of APFD, our LBS techniques can be as effective as some of the best code coverage-based greedy prioritization techniques ever proposed. We also show that they are significantly more efficient and scalable than the latter techniques. © 2013 IEEE.},
author_keywords={Adaptive test case prioritization;  Regression testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Haramy2013592,
author={Haramy, K.Y. and Zhang, R. and Rock, A.},
title={CRSP-3D application for remediating a rockfall at Yosemite National Park},
journal={Geotechnical Special Publication},
year={2013},
number={231 GSP},
pages={592-603},
doi={10.1061/9780784412787.062},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887458967&doi=10.1061%2f9780784412787.062&partnerID=40&md5=9ad0fa5bfafff3aa5b02202935a276d3},
affiliation={Central Federal Lands - FHWA, Brazil; Metropolitan State College of Denver, United States; Summit Peak Technologies, United States},
abstract={Understanding the mechanisms of rockfalls is complex since falling rocks can bounce, slide, roll, and launch. Numerous mechanical and mathematical models have been developed to accurately simulate rockfalls and to gain a fundamental understanding of their behavior. The biggest challenge of numerical models has been validating the parameters of the models to implement the different geo-materials properties and slope conditions, such as slope material hardness, size and shapes of falling rocks, slope roughness and geometry, geological conditions, and presence of vegetation or snow. Efforts have been made to calibrate the variables through full scale field testing. However, due to the diversity of various scenarios, engineers face the difficulty of selecting the right parameters for local conditions to replicate field test results. A newly developed rockfall simulation program, CRSP-3D, based on the discrete element modeling technique, provides an advanced technical approach that allows engineers to back-calculate and calibrate rock and slope parameters using available data from field investigations, site explorations, and field test results. Once the slope model is calibrated, it can be used to accurately simulate future rockfalls and to design rockfall barriers, attenuators, and catchment areas in 3D. This rockfall program was recently used to simulate a rockfall hazard event that occurred on January, 2012 at Yosemite National Park, CA. During this event, a large granite boulder estimated to be over 500 tons dislodged from the top of a steep cliff onto the roadway creating a deep failure path through both traffic lanes, forcing a roadway closure. The data from field investigations were used to perform the CRSP-3D parameter calibration and numerical simulation calculations. The results successfully reproduced the rockfall event and provided reliable data for project remediation. This paper presents the results of the engineering case study to demonstrate the use of an efficient and accurate numerical tool for rockfall hazard mitigation. © 2013 American Society of Civil Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Meyer20133843,
author={Meyer, G.E. and Coffman, G.F. and Conroy, K. and Young, S.L.},
title={An advanced real-time plant species identification system},
journal={American Society of Agricultural and Biological Engineers Annual International Meeting 2013, ASABE 2013},
year={2013},
volume={5},
pages={3843-3853},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881642165&partnerID=40&md5=5c284d6fe78117065566019b179c7591},
affiliation={Biological Systems Engineering, University of Nebraska, Lincoln, NE, United States; Agronomy and Horticulture, University of Nebraska, Lincoln, NE, United States},
abstract={Undesirable plants are negatively impacting many ecosystems and causing significant losses both economically and environmentally. Examples of losses include altered stream flow in riparian areas, increased frequency of fires in rangelands and forests, fewer habitats with high species diversity and increased management for aesthetics and yields in natural areas and production systems, respectively. The services provided by the terrestrial systems in which undesirable plants have invaded or are established have yet to be completely quantified, but estimates are in the billions of dollars. An improved plant species identification program is being developed and is currently being tested as potential "ap" for identifying selected plant species found in Nebraska and fields of the Great Plains. The identification process takes place through an acquired digital leaf image. The identification algorithm performs a classical analysis using a Fast Fourier Transform but also concentrates on detailed leaf venation and surface texture. Leaf species samples were initially supplied from plants grown in an environmental chamber, but will extend to spring/summer/fall field specimens. Plants will eventually include various herbaceous species that are native and non-native (e.g., velvet leaf, pigweed, downy brome, phragmites, common reed, leafy spurge, cheat grass). Specimens were imaged using a high-resolution digital camera and a special portable lighting system. The impact from the development of a real-time plant species identification system (APSIS) to identify desired and undesirable plants would revolutionize the way plant populations are studied by researchers, ecologists, and monitored by crop and land managers.},
author_keywords={Identification;  Machine;  Plants;  Species;  Vision},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Moon20133548,
author={Moon, J.W. and Chang, J.D. and Kim, S.},
title={Determining adaptability performance of artificial neural network-based thermal control logics for envelope conditions in residential buildings},
journal={Energies},
year={2013},
volume={6},
number={7},
pages={3548-3570},
doi={10.3390/en6073548},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881496039&doi=10.3390%2fen6073548&partnerID=40&md5=179b27bf7aeeb130926fdd45637f5d3e},
affiliation={Department of Building and Plant Engineering, Hanbat National University, Daejeon 305-719, South Korea; School of Architecture and Design and Planning, University of Kansas, Lawrence, KS 66045, United States; Department of Interior Architecture and Built Environment, Yonsei University, Seoul 120-749, South Korea},
abstract={This study examines the performance and adaptability of Artificial Neural Network (ANN)-based thermal control strategies for diverse thermal properties of building envelope conditions applied to residential buildings. The thermal performance using two non-ANN-based control logics and two predictive ANN-based control logics was numerically tested using simulation software after validation. The performance tests were conducted for a two-story single-family house for various envelope insulation levels and window-to-wall ratios on the envelopes. The percentages of the period within the targeted ranges for air temperature, humidity and PMV, and the magnitudes of the overshoots and undershoots outside of the targeted comfort range were analyzed for each control logic scheme. The results revealed that the two predictive control logics that employed thermal predictions of the ANN models achieved longer periods of thermal comfort than the non-ANN-based models in terms of the comfort periods and the reductions of the magnitudes of the overshoots and undershoots. The ANN-based models proved their adaptability through accurate control of the thermal conditions in buildings with various architectural variables. The ANN-based predictive control methods demonstrated their potential to create more comfortable thermal conditions in single-family homes compared to non-ANN based control logics.},
author_keywords={Artificial neural network;  Envelope insulation;  Ratio of window to wall;  Thermal condition;  Thermal control logic;  Thermal performance},
document_type={Article},
source={Scopus},
}

@ARTICLE{deLacerda201315,
author={de Lacerda, A.E.B. and Nimmo, E.R. and Sebbenn, A.M.},
title={Modeling the long-term impacts of logging on genetic diversity and demography of hymenaea courbaril},
journal={Forest Science},
year={2013},
volume={59},
number={1},
pages={15-26},
doi={10.5849/forsci.10-118},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874444797&doi=10.5849%2fforsci.10-118&partnerID=40&md5=71c0350b0b93063cb0cbd3641565fd8c},
affiliation={EMBRAPA, DFEE, São Paulo, Brazil; University of Manitoba, DFEE, São Paulo, Canada; Instituto Florestal de São Paulo, DFEE, São Paulo, Brazil},
abstract={Although selective logging is a common practice for timber production in the Brazilian Amazon, very little is known about its impacts on genetic diversity and demography of the harvested species. This study explores the sustainability of current forest management systems in the Brazilian Amazon by modeling harvesting cycles and examining the impacts on the genetic diversity and demography of the highly valued species Hymenaea courbaril. Using extensive field data, we introduced a two-step modeling procedure for EcoGene software that allowed us to identify optimal felling cycles that were later used for testing and defining sustainable logging parameters. The results show that logging cycles for H. courbaril should be approximately of 110 years, as opposed to the 30-year cycle currently used in Brazil, and harvesting levels should consider a combination of larger minimum cutting diameters (75-100 cm) and lower logging intensities (10-50%). We conclude that current practices in Brazil (30-year cycle, logging intensities of 90%, and minimum cutting diameters of 50 cm) are unsustainable for H. courbaril and that the current practice of using general logging prescriptions for all species does not deliver sustainable forest management in the Amazon. Brazilian forest harvesting regulations need to move toward species-specific prescriptions to ensure real sustainability in the long term. © 2013 by the Society of American Foresters.},
author_keywords={Genetic diversity;  Hymenaea courbaril;  Microsatellite loci;  Neotropics;  Selective logging;  Tropical tree species},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rodes2013308,
author={Rodes, B.D. and Nguyen-Tuong, A. and Hiser, J.D. and Knight, J.C. and Co, M. and Davidson, J.W.},
title={Defense against stack-based attacks using speculative stack layout transformation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2013},
volume={7687 LNCS},
pages={308-313},
doi={10.1007/978-3-642-35632-2_29},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872848278&doi=10.1007%2f978-3-642-35632-2_29&partnerID=40&md5=10ea7484e27b6365c306e70147a77413},
affiliation={Department of Computer Science, University of Virginia, P.O. Box 400740, 85 Engineer's Way, Charlottesville, VA 22904, United States},
abstract={This paper describes a novel technique to defend binaries against intra-frame stack-based attacks, including overflows into local variables, when source code is unavailable. The technique infers a specification of a function's stack layout, i.e., variable locations and boundaries, and then seeks to apply a combination of transformations, including variable reordering, random-sized padding between variables, and placement of canaries. To overcome the imprecision of static binary analysis, yet be as aggressive as possible in the transformations applied to the stack layout, the technique is speculative. A stack frame is aggressively transformed based on static analysis, and the validity of inferred stack layout is assessed through regression testing. If a transformation changes a program's semantics because of imprecision in the inference of the stack layout, a less aggressive layout is inferred until the transformed program passes the supplied regression tests. We present an overview of the technique and preliminary results of its feasibility and security effectiveness. © 2013 Springer-Verlag Berlin Heidelberg.},
author_keywords={artificial diversity;  buffer overflow;  non-control-data attacks;  run-time verification;  security attacks;  stack layout transformation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mansour2013463,
author={Mansour, N. and Zeitunlian, H. and Tarhini, A.},
title={Optimization metaheuristic for software testing},
journal={Advances in Intelligent Systems and Computing},
year={2013},
volume={175 ADVANCES},
pages={463-474},
doi={10.1007/978-3-642-31519-0_30},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872523115&doi=10.1007%2f978-3-642-31519-0_30&partnerID=40&md5=26b488589b84d1027f59dab9dc885d4a},
affiliation={Department of Computer Science and Mathematics, Lebanese American University, Beirut, Lebanon},
abstract={This paper presents an evolutionary method for testing web applications. Although state-based testing has been reported, few papers have addressed modern web applications. In our work, we model web applications by associating features or web pages with states; state transition diagrams are based on events representing state transitions. We formulate the web application testing problem as an optimization problem and use a simulated annealing (SA) metaheuristic algorithm to generate test cases as sequences of events while keeping the test suite size reasonable. SA evolves a solution by minimizing a function that is based on the contradictory objectives of coverage of events, diversity of events covered, and definite continuity of events. Our experimental results show that the proposed simultaneous-operation SA gives better results than an incremental SA version and significantly better than a greedy algorithm. © Springer-Verlag Berlin Heidelberg 2013.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Suri2012,
author={Suri, B. and Singhal, S.},
title={Literature survey of Ant Colony Optimization in software testing},
journal={2012 CSI 6th International Conference on Software Engineering, CONSEG 2012},
year={2012},
doi={10.1109/CONSEG.2012.6349501},
art_number={6349501},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870840701&doi=10.1109%2fCONSEG.2012.6349501&partnerID=40&md5=9e4c3af703367ee6f3db17b3206e8512},
affiliation={Department of Comp Sc, USIT, GGSIPU, India; Department of IT, JIMS, GGSIPU, Rohini, India},
abstract={Ant Colony Optimization (ACO) is a well known and rapidly evolving meta-heuristic technique. A large number of optimization problems have already taken advantage of the ACO technique while countless others are on their way. A copious amount of effort has also been put in by the researchers for applying ACO in solving various software testing problems. This paper presents a survey of twenty-one such studies, identified as relating to the use of ACO in diverse software testing concepts. To the best of our knowledge, no literature survey could be found published in the same context till date. Consequently, the twenty one studies have been rigorously analyzed to find some common parameters which can be grouped together or compared in order to provide a useful insight into the field. © 2012 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tieliu201251,
author={Tieliu, W. and Kui, L. and Yang, D. and Feixue, F.},
title={A new type design of the lighting current strength detection sensor},
journal={Applied Mechanics and Materials},
year={2012},
volume={201-202},
pages={51-55},
doi={10.4028/www.scientific.net/AMM.201-202.51},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870180447&doi=10.4028%2fwww.scientific.net%2fAMM.201-202.51&partnerID=40&md5=f2d33df1c95675348bcab9a095237e65},
affiliation={Beijing University of Technology, College of Electronic Information and Control Engineering, Beijing, 100124, China},
abstract={The lightning current parameters are very important for the lightning-line monitoring, they can provide important raw data for the construction of power grids and lightning protection engineering, conducive to the maintenance of high voltage transmission lines. But also provide the objective basis of the lightning accident analysis and statistics of lightning law. In this paper, we have Investigated parts of the complex and diverse terrain characteristics of Guizhou Province, we plan to design a simple, economic, effective data acquisition system for lightning current, timely and accurate grasp of the local lightning. we have done some research on the measurement mechanisms of lightning current strength. First, a novel lightning current strength detection program is proposed, and then we have done a lot of tests on the program and depicted the related parametric curve. Last, we verify the linearity of this sensor. © (2012) Trans Tech Publications, Switzerland.},
author_keywords={Impulse current;  Lightning current strength;  Rogowski coil;  Sensor},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hollis2012431,
author={Hollis, R. and Skutley, P. and Ortiz, C. and Varkey, V. and LePage, D. and Brown, B. and Davies, D. and Harris, M.},
title={Oxy-fuel turbomachinery development for energy intensive industrial applications},
journal={Proceedings of the ASME Turbo Expo},
year={2012},
volume={3},
pages={431-439},
doi={10.1115/GT2012-69988},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881278330&doi=10.1115%2fGT2012-69988&partnerID=40&md5=357771bc88028c108fc73f5f3dc01548},
affiliation={Clean Energy Systems, Inc., Rancho Cordova, CA, United States; Siemens Energy, Inc., Houston, TX, United States; TurboCare, Inc., Houston, TX, United States; Florida Turbine Technologies, Inc., Jupiter, FL, United States},
abstract={Future fossil-fueled power generation systems will require emission control technologies such as carbon capture and sequestration (CCS) to comply with government greenhouse gas regulations. The three prime candidate technologies which permit carbon dioxide (CO2) to be captured and safely stored include pre-combustion, post-combustion capture and oxy-fuel (O-F) combustion. For more than a decade Clean Energy Systems, Inc. (CES) has been designing and demonstrating enabling technologies for oxy-fuel power generation; specifically steam generators, hot gas expanders and reheat combustors. Recently CES has partnered with Florida Turbine Technologies, Inc. (FTT) and Siemens Energy, Inc. to develop and demonstrate turbomachinery systems compatible with the unique characteristics of oxy-fuel working fluids. The team has adopted an aggressive, but economically viable development approach to advance turbine technology towards early product realization. Goals include short-term, incremental advances in power plant efficiency and output while minimizing capital costs and cost of electricity. Phase 2 of this development work has been greatly enhanced by a cooperative agreement with the U.S. Department of Energy (DOE). Under this program the team will design, manufacture and test a commercial-scale intermediate-pressure turbine (IPT) to be used in industrial O-F power plants. These plants will use diverse fuels and be capable of capturing 99% of the produced CO2 at competitive cycle efficiencies and cost of electricity. Initial plants will burn natural gas and generate more than 200MWe with near-zero emissions. To reduce development cost and schedule an existing gas turbine engine will be adapted for use as a high-temperature OF IPT. The necessary modifications include the replacement of the engine's air compressor with a thrust balance system and altering the engine's air-breathing combustion system into a steam reheating system using direct fuel and oxygen injection. Excellent progress has been made to date. FTT has completed the detailed design and issued manufacturing drawings to convert a Siemens SGT-900 to an oxy-fuel turbine (OFT). Siemens has received, disassembled and inspected an SGT-900 B12 and ordered all necessary new components for engine changeover. Meanwhile CES has been working to upgrade an existing test facility to support demonstration of a "simple" oxy-fuel power cycle. Low-power demonstration testing of the newly assembled OFT-900 is expected to commence in late 2012. Copyright © 2012 by ASME and Siemens Energy, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gutiérrez-Madroñal201221,
author={Gutiérrez-Madroñal, L. and Shahriar, H. and Zulkernine, M. and Domínguez-Jiménez, J.J. and Medina-Bulo, I.},
title={Mutation testing of event processing queries},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2012},
pages={21-30},
doi={10.1109/ISSRE.2012.20},
art_number={6405401},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876367758&doi=10.1109%2fISSRE.2012.20&partnerID=40&md5=0680b5730e6de05fde23d5e6979b76a0},
affiliation={Department of Computer Languages and Systems, University of Cádiz, Cádiz, Spain; School of Computing, Queen's University, Kingston, Canada; Department of Computer Science, Kennesaw State University, Kennesaw, GA 30144, United States},
abstract={Event processing queries are intended to process continuous event streams. These queries are partially similar to traditional SQL queries, but provide the facilities to express rich features (e.g., pattern expression, sliding window of length and time). An error while implementing a query may result in abnormal program behaviors and lost business opportunities. Moreover, queries can be generated with unsanitized inputs and the structure of intended queries might be altered. Thus, a tester needs to test the behavior of queries in presence of malicious inputs. Mutation testing has been found to be effective to assess test suites quality and generating new test cases. Unfortunately, there is no effort to perform mutation testing of event processing queries. In this work, we propose mutation-based testing of event processing queries. We choose Event Processing Language (EPL) as our case study and develop necessary mutation operators and killing criteria to generate high quality event streams and malicious inputs. Our proposed operators modify different features of EPL queries (pattern expression, windows of length and time, batch processing of events). We develop an architecture to generate mutants for EPL and perform mutation analysis. We evaluate our proposed EPL mutation testing approach with a set of developed benchmark containing diverse types EPL queries. The evaluation results indicate that the proposed operators and mutant killing criteria are effective to generate test cases capable of revealing anomalous program behaviors (e.g., event notification failure, delay of event reporting, unexpected event), and SQL injection attacks. Moreover, the approach incurs less manual effort and can complement other testing approach such as random testing. © 2012 IEEE.},
author_keywords={Event processing language;  Event stream;  Mutation testing;  Test suite adequacy},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Choi2012,
author={Choi, S.-H. and Kim, J.-Y. and Kim, S.-H. and Kim, C.-H.},
title={Nondestructive evaluation of composite applying ultrasound-infrared thermography technique and finite element analysis},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2012},
volume={8409},
doi={10.1117/12.923341},
art_number={84092X},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874918120&doi=10.1117%2f12.923341&partnerID=40&md5=eb6509efa67bfcd3de9a45e2229d07b2},
affiliation={Dept. of Advanced Parts and Materials Engineering, Chosun University, 375 Seosuk-Dong, Donggu, Gwangju, 501-759, South Korea; Dept. of Mechatronics Engineering, Chosun University, 375 Seosuk-Dong, Dong-gu, Gwangju, 501-759, South Korea; Korea Nondestructive Examination Co., Ltd., Myeongii-dong, Gangseo-gu, Busan 3173-32, South Korea},
abstract={The infrared thermographic nondestructive inspection technique is a quality inspection and stability assessment method used to diagnose the physical characteristics and defects by detecting the infrared ray radiated from the object without destructing it. Recently, the nondestructive inspection and assessment that use the ultrasound-infrared thermography technique are widely adopted in diverse areas. The ultrasound-infrared thermography technique uses the phenomenon that the ultrasound wave incidence to an object with cracks or defects on its mating surface generates local heat on the surface. Aiming to establish a convergence non-destructive evaluation system, this study used the control technology of an ultrasound exciter as an association technology that satisfies various required conditions in order to control the ultrasound excitation time and output, and performed basic research for establishing a convergence non-destructive system by applying the Lock-in technology that improves the infrared thermography detectability. Also it compared and analyzed the test result by using COMSOL Multiphysic, a finite element analysis program. © 2012 SPIE.},
author_keywords={Finite element analysis;  Horn;  Lock-in Thermography;  Non-Destructive Evaluation;  Ultrasound Excitation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Campos2012,
author={Campos, D.S. and Mendes, A.J. and Marcelino, M.J. and Ferreira, D.J. and Alves, L.M.},
title={A multinational case study on using diverse feedback types applied to introductory programming learning},
journal={Proceedings - Frontiers in Education Conference, FIE},
year={2012},
doi={10.1109/FIE.2012.6462412},
art_number={6462412},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874709405&doi=10.1109%2fFIE.2012.6462412&partnerID=40&md5=70c7923276b299e3ba33fb3ec89e0991},
affiliation={Center for Informatics and Systems, University of Coimbra, Coimbra, Portugal; Federal University of Goiás, Institute of Informatics, Goiânia, Brazil},
abstract={Building written feedback, pedagogically sound, standardized and flexible enough to accommodate students who may be in different stages and learning curves is a complex and laborious task. In this paper, we describe a multinational case study involving diverse types of pedagogical feedback provided to Portuguese and Brazilian novice programming students. Programming errors, especially logical ones, can be used as a consistent metric for assessing learning. The research done looks for an innovative form to define content of several types of feedback. It also aims to create an efficient method for the discovery and mapping of students' logical programming errors. The results obtained so far using this approach are presented and analyzed. © 2012 IEEE.},
author_keywords={feedback;  logical errors;  pedagogical rubric;  programming learning;  software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Goerke20124212,
author={Goerke, U.-J. and Wang, W. and Kolditz, O.},
title={Towards a European modelling framework analysing coupled physico-chemical processes in porous media for geotechnical applications},
journal={ECCOMAS 2012 - European Congress on Computational Methods in Applied Sciences and Engineering, e-Book Full Papers},
year={2012},
pages={4212-4227},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871639415&partnerID=40&md5=efdbd538f4f778c514141ac3b29a3a24},
affiliation={Department of Environmental Informatics, Helmholtz Centre for Environmental Research, UFZ, Permoserstr. 15, D-04318 Leipzig, Germany; University of Technology, Dresden, Germany},
abstract={In this paper, the motivation and the scientific concept of a number of European modelling groups is presented sharing the idea of the common development of a generic modelling framework for coupled physico-chemical processes characterising energy-related geoscientific applications. Within the context of the European policy of transformation of energy technologies, the geological underground becomes a specific focus of attention in terms of: i) energy source, ii) energy storage, and iii) energy waste disposal. All these geoscientific applications have the observation of very similar complex multiphysics porous media processes in the subsurface formations in common. Thus, assessing the behaviour of geo-reservoirs under different operating conditions requires the analysis of complex coupled processes at the reservoir scale. The full length and time scales for which the performance of geological sites must be assessed are not amenable to direct investigation by laboratory studies or field investigations. The only practical option for predicting the long-term behaviour of geo-reservoirs is numerical analysis, constrained by the understanding gained from the relatively short-term laboratory and field-scale experiments available, and natural analogue systems. Modelling and simulation of thermo-hydro-mechanical-chemical processes in the subsurface require the development of sophisticated mathematical models, numerical algorithms and software tools that have to be adapted to the needs of real world applications. As the development of scientific software for complex geoscientific problems is beyond the capabilities of individual groups, the main innovation of the presented cooperative initiative would be the development of an integrated framework, capable of simulating all known relevant processes involved in the geotechnical use of the subsurface. This facilitates substantial progress in understanding of long-term fate of the considered formations, particularly directed to large scale deployment of relevant technologies. Integrated model and software concepts for the simulation of energy-related coupled processes have to be approved by benchmark and test site simulations, and provide the opportunity of predictive in silico analyses of diverse mechanisms on reservoir scale.},
author_keywords={Coupled processes;  Finite element method;  Geotechnology;  Porous media mechanics;  Software platform},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kim2012158,
author={Kim, H.-K.},
title={Mobile applications software testing methodology},
journal={Communications in Computer and Information Science},
year={2012},
volume={342 CCIS},
pages={158-166},
doi={10.1007/978-3-642-35270-6_22},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869803721&doi=10.1007%2f978-3-642-35270-6_22&partnerID=40&md5=5421f4bd3b687e9afcd88eca93fdc661},
affiliation={School of Information Technology, Catholic University of Daegu, South Korea},
abstract={Today's Mobile Applications deliver complex functionality on platforms that have limited resources for computing. Yet, unlike the PC-based environment, the Mobile environment comprises a number of devices with diverse hardware and software configurations and communication intricacies. This diversity in mobile computing environments presents unique challenges in mobile application development, quality assurance, and deployment, requiring unique testing strategies. Many enterprise applications that were deployed as desktop/web applications are now being ported to Mobile devices. In this paper, we have constructed the Mobile Applications Quality Assurance Tool(MAQAT) by integrating tools and prototype systems that we built for program analysis and testing for mobile applications software. MAQAS provides a architecture of program analysis and testing for mobile, and supports many program-analysis-based techniques, including automated mobile applications software inspection, software visualization, testing coverage analysis, performance evaluation, concurrent program debugging, software measurement, etc. The paper briefly describes the overall architecture of MAQAS, and introduces the implementation of its tools and components. © 2012 Springer-Verlag Berlin Heidelberg.},
author_keywords={Mobile applications quality assurance;  Mobile applications testing;  Software testing tools and methodology},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Johnson2012126,
author={Johnson, E. and Verret, R.},
title={Advancing RF test with open FPGAs},
journal={AUTOTESTCON (Proceedings)},
year={2012},
pages={126-129},
doi={10.1109/AUTEST.2012.6334567},
art_number={6334567},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869741779&doi=10.1109%2fAUTEST.2012.6334567&partnerID=40&md5=028a4611924e33ea4aa7210f1845bff2},
affiliation={National Instruments, Austin, TX, United States},
abstract={The number of wireless devices, diversity of communication standards, and sophistication of modulation schemes are increasing dramatically each year. With each subsequent generation of technology, the cost of testing wireless devices using traditional techniques also has increased. One way to minimize hardware costs and reduce test time is to use virtual or synthetic instruments along with modular I/O; however, a new approach, software-designed instrumentation, not only provides microprocessor software flexibility but an open, user-programmable FPGA for further customization. This approach gives RF test engineers the ability to reduce test times orders of magnitude beyond what was previously possible without custom or standard-specific instrumentation. In this work, we demonstrate how a software-designed RF instrument can include an architecture that facilitates the record-based model of typical virtual or synthetic instruments. We show how this architecture can be extended with simple FPGA modifications to digitally control the device under test (DUT), reducing capital equipment costs by eliminating unnecessary instruments. We achieve a test time reduction of three orders of magnitude in a power leveling algorithm, common in RF power amplifier test. We also show how a software-designed RF instrument can be completely re-architected to implement a real-time RF channel emulator by including complex mathematical fading models on the FPGA. Using this approach, we demonstrate a 2×2 real-time MIMO channel emulator with up to 36 taps per fading filter. © 2012 IEEE.},
author_keywords={channel;  device;  DUT;  emulator;  flexibility;  FPGA;  instruments;  MIMO;  modular;  PA;  real-time;  RF;  software-designed;  test;  virtual},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Watson2012295,
author={Watson, R.},
title={Development and application of a heuristic to assess trends in API documentation},
journal={SIGDOC'12 - Proceedings of the 30th ACM International Conference on Design of Communication},
year={2012},
pages={295-302},
doi={10.1145/2379057.2379112},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869074991&doi=10.1145%2f2379057.2379112&partnerID=40&md5=256bf47c05dbcfe5a731923de1a7fff1},
affiliation={University of Washington, Seattle, WA, United States},
abstract={Computer technology has made amazing advances in the past few decades; however, the software documentation of today still looks strikingly similar to the software documentation used 30 years ago. If this continues into the 21st century, more and more soft-ware developers could be using 20 th-century-style documentation to solve 21 st-century problems with 21 st-century technologies. Is 20 th-century- style documentation up to the challenge? How can that be measured? This paper seeks to answer those questions by developing a heuristic to identify whether the documentation set for an application programming interface (API) contains the key elements of API reference documentation that help software developers learn an API. The resulting heuristic was tested on a collection of software documentation that was chosen to provide a diverse set of examples with which to validate the heuristic. In the course of testing the heuristic, interesting patterns in the API documentation were observed. For example, twenty-five percent of the documentation sets studied did not have any overview information, which, according to studies, is one of the most basic elements an API documentation set needs to help software developers learn to use the API. The heuristic produced by this research can be used to evaluate large sets of API documentation, track trends in API documentation, and facilitate additional research. Copyright © 2012 ACM.},
author_keywords={API;  API reference documentation;  Application programming interface;  Software documentation;  Software libraries},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shan201213,
author={Shan, Q.-Y. and Cao, G. and Cai, H. and Cong, X.-D. and Cai, B.-C.},
title={Novel software-based method to classify structurally similar compounds combined with high performance liquid chromatography-quadrupole time of flight mass spectrometry to identify complex components of herbal medicines},
journal={Journal of Chromatography A},
year={2012},
volume={1264},
pages={13-21},
doi={10.1016/j.chroma.2012.09.045},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867897743&doi=10.1016%2fj.chroma.2012.09.045&partnerID=40&md5=1fd7fb17fa987874d253c7c7bc0d4060},
affiliation={Research Center of TCM Processing Technology, Zhejiang Chinese Medical University, Hangzhou 310053, China; National First-Class Key Discipline for Science of Chinese Materia Medica, Nanjing University of Chinese Medicine, Nanjing 210046, China; Engineering Center of State Ministry of Education for Standardization of Chinese Medicine Processing, Nanjing University of Chinese Medicine, Nanjing 210029, China; Nanjing Haichang Chinese Medicine Group Corporation, Nanjing, China},
abstract={The components of herbal medicines (HMs) are usually extremely complex, belonging to hundreds of compound classes with diverse chemical and physical properties. Full characterization of HMs is hugely important in order to identify the individual chemical constituents and provide a first step toward determining which components are responsible for the therapeutic effects of a particular medical plant. In this study, a novel software-based approach was developed to classify structurally similar compounds, and this was combined with high performance liquid chromatography/quadrupole time-of-flight mass spectrometry (HPLC-QTOF-MS) to identify the individual components in an extract of Mentha haplocalyx. A total of 33 compounds were tentatively identified in samples of M. haplocalyx extract, including 9 new minor constituents reported for the first time. Semi-quantitative analysis of the extract sample was also carried out. Software validation and robustness tests were performed. The results of this study demonstrate the enormous potential of this strategy, using classification based on structural similarity together with HPLC-QTOF-MS, for the identification and quantification of complex components in HMs and related products. © 2012 Elsevier B.V.},
author_keywords={Herbal medicines;  HPLC-QTOF-MS;  Mentha haplocalyx;  Structural similarity classification},
document_type={Article},
source={Scopus},
}

@ARTICLE{Groce2012572,
author={Groce, A. and Fern, A. and Erwig, M. and Pinto, J. and Bauer, T. and Alipour, A.},
title={Learning-based test programming for programmers},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7609 LNCS},
number={PART 1},
pages={572-586},
doi={10.1007/978-3-642-34026-0_42},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868277694&doi=10.1007%2f978-3-642-34026-0_42&partnerID=40&md5=deb45ed833576f847dcba467cb98f36a},
affiliation={School of Electrical Engineering and Computer Science, Oregon State University, Corvalis, OR, United States},
abstract={While a diverse array of approaches to applying machine learning to testing has appeared in recent years, many efforts share three central challenges, two of which are not always obvious. First, learning-based testing relies on adapting the tests generated to the program being tested, based on the results of observed executions. This is the heart of a machine learning approach to test generation. A less obvious challenge in many approaches is that the learning techniques used may have been devised for problems that do not share all the assumptions and goals of software testing. Finally, the usability of approaches by programmers is a challenge that has often been neglected. Programmers may wish to maintain more control of test generation than a "push button" tool generally provides, without becoming experts in software testing theory or machine learning algorithms, and with access to the full power of the language in which the tested system is written. In this paper we consider these issues, in light of our experience with adaptation-based programming as a method for automated test generation. © 2012 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lei2012,
author={Lei, H. and Han, X.},
title={Software test data generation method using hill climbing algorithm combined with a modified ARPSO},
journal={Dianzi Keji Daxue Xuebao/Journal of the University of Electronic Science and Technology of China},
year={2012},
volume={41},
number={6},
pages={885-889+898},
doi={10.3969/j.issn.1001-0548.2012.06.014},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871834141&doi=10.3969%2fj.issn.1001-0548.2012.06.014&partnerID=40&md5=fa1c20e30696bbd60fa7a2a2c7d6dd6a},
affiliation={School of Computer Science and Engineering, University of Electronic and Science Technology of China, Chengdu 611731, China},
abstract={A modified attractive and repulsive particle swarm optimization (MARPSO) algorithm is proposed, which is based on the attractive and repulsive particle swarm optimization (ARPSO) algorithm, by employing new diversity-measure and repulsive operator. Combining both the local convergence ability of hill climbing(HC) algorithm and the characteristic avoid precocious of MARPSO, the way of automatic generation of the software test data based on hill climbing algorithm combined with MARPSO(HC-MARPSO) is proposed. Finally, the results of experiment show that this new algorithm can generate test data more effective than other algorithms, such as genetic algorithm and particle swarm optimization algorithm.},
author_keywords={Automatic test data generation;  Hill climbing algorithm;  Particle swarm optimization algorithm (PSO);  Software test;  The attractive and repulsive particle swarm optimization algorithm (ARPSO)},
document_type={Article},
source={Scopus},
}

@ARTICLE{Banzi201212131,
author={Banzi, A.S. and Nobre, T. and Pinheiro, G.B. and Árias, J.C.G. and Pozo, A. and Vergilio, S.R.},
title={Selecting mutation operators with a multiobjective approach},
journal={Expert Systems with Applications},
year={2012},
volume={39},
number={15},
pages={12131-12142},
doi={10.1016/j.eswa.2012.04.041},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863099345&doi=10.1016%2fj.eswa.2012.04.041&partnerID=40&md5=05376de2ca8ea672d66a07fadfa4203a},
affiliation={Computer Science Department, Federal University of Paraná (UFPR), CP 19:081, CEP: 81531-970, Curitiba, Brazil},
abstract={The mutation score is an important measure to evaluate the quality of the test cases. It is obtained by executing a lot of mutant programs generated by a set of operators. A common problem, however, is that some operators can generate unnecessary and redundant mutants. Because of this, different strategies were proposed to find a set of operators that generates a reduced number of mutants without decreasing the mutation score. However, the operator selection, in practice, may include real constraints and is dependent on diverse factors besides the number of mutants and score, such as: number of test data, execution time, number of revealed faults, number of equivalent mutants, etc. In fact this is a multi-objective problem, which does not have a single solution. Different set of operators exist for multiple objectives to be satisfied, and some restrictions can be used to choose among the existing sets. To make this choice possible, in this paper, we introduce a multi-objective strategy. We investigate three multi-objective algorithms and introduce a procedure to establish a set of operators to prioritize mutation score. Better results are obtained in comparison with traditional strategies. © 2012 Elsevier Ltd. All rights reserved.},
author_keywords={Essential mutation set;  Multiobjective metaheuristics;  Software test},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ali2012211,
author={Ali, N.B. and Petersen, K. and Mäntylä, M.V.},
title={Testing highly complex system of systems: An industrial case study},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2012},
pages={211-220},
doi={10.1145/2372251.2372290},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867555453&doi=10.1145%2f2372251.2372290&partnerID=40&md5=8fa469697b0c5f38203024794ce2cdeb},
affiliation={School of Computing, Blekinge Institute of Technology, 37179 Karlskrona, Sweden; Department of Computer Science, Lund University, 22100 Lund, Sweden},
abstract={Context: Systems of systems (SoS) are highly complex and are integrated on multiple levels (unit, component, system, system of systems). Many of the characteristics of SoS (such as operational and managerial independence, integration of system into system of systems, SoS comprised of complex systems) make their development and testing challenging. Contribution: This paper provides an understanding of SoS testing in large-scale industry settings with respect to challenges and how to address them. Method: The research method used is case study research. As data collection methods we used interviews, documentation, and fault slippage data. Results: We identified challenges related to SoS with respect to fault slippage, test turn-around time, and test maintainability. We also classified the testing challenges to general testing challenges, challenges amplified by SoS, and challenges that are SoS specific. Interestingly, the interviewees agreed on the challenges, even though we sampled them with diversity in mind, which meant that the number of interviews conducted was sufficient to answer our research questions. We also identified solution proposals to the challenges that were categorized under four classes of developer quality assurance, function test, testing in all levels, and requirements engineering and communication. Conclusion: We conclude that although over half of the challenges we identified can be categorized as general testing challenges still SoS systems have their unique and amplified challenges stemming from SoS characteristics. Furthermore, it was found that interviews and fault slippage data indicated that different areas in the software process should be improved, which indicates that using only one of these methods would have led to an incomplete picture of the challenges in the case company. Copyright 2012 ACM.},
author_keywords={Case study;  Software test;  System of systems},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ding2012670,
author={Ding, R. and Feng, X. and Li, S. and Dong, H.},
title={Automatic generation of software test data based on hybrid particle swarm genetic algorithm},
journal={Proceedings - 2012 IEEE Symposium on Electrical and Electronics Engineering, EEESYM 2012},
year={2012},
pages={670-673},
doi={10.1109/EEESym.2012.6258748},
art_number={6258748},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866934160&doi=10.1109%2fEEESym.2012.6258748&partnerID=40&md5=d28c3e6a6c443f6c19d3f0d22794374f},
affiliation={Computer Department, Mudanjiang Normal University, Mudanjiang, China; National Science Park, Harbin Engineering University, Harbin, China},
abstract={A hybrid particle swarm genetic algorithm is purposed to apply in software testing using case automated generations. On the basis of classical genetic algorithm, the algorithm divided the population into families, influencing the convergence efficiency by crossover in family, keeping the diversity of the population by crossover between families; meanwhile, enhancing the speed of convergence by the PSO crossover (commixed the thought of PSO in genetic algorithm) According to the characteristics of software testing problems, we designed the corresponding fitness function and the encoding method. The results of data experiment were given to illustrate the effectiveness of the algorithm. © 2012 IEEE.},
author_keywords={Case Automatically Generates;  Genetic Algorithm;  Particle Swarm Optimization;  Software Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gong201230,
author={Gong, L. and Lo, D. and Jiang, L. and Zhang, H.},
title={Diversity maximization speedup for fault localization},
journal={2012 27th IEEE/ACM International Conference on Automated Software Engineering, ASE 2012 - Proceedings},
year={2012},
pages={30-39},
doi={10.1145/2351676.2351682},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866908869&doi=10.1145%2f2351676.2351682&partnerID=40&md5=1460ccec902fdc5937fcf3dcea95da37},
affiliation={School of Software, Tsinghua University, Beijing 100084, China; Tsinghua National Laboratory for Information Science and Technology (TNList), China; School of Information Systems, Singapore Management University, Singapore},
abstract={Fault localization is useful for reducing debugging effort. However, many fault localization techniques require nontrivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (Dms). Dms orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that Dms can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), Dms can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant. Copyright 2012 ACM.},
author_keywords={Fault localization;  Test case prioritization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yuan2012170,
author={Yuan, B.-X. and Chen, W.-W. and Teng, J. and Chen, R.},
title={Mechanism research on fracturing-toppling pattern of cliff body in Jiaohe Ruins},
journal={Yantu Lixue/Rock and Soil Mechanics},
year={2012},
volume={33},
number={SUPPL. 1},
pages={170-174},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867663810&partnerID=40&md5=525ad43f15fcd001e07abaa4969a6265},
affiliation={Shenzhen Graduate School, Harbin Institute of Technology, Shenzhen, Guangdong 518055, China; Key Laboratory of Mechanics on Disaster and Environment in Western China of Education Ministry, Lanzhou University, Lanzhou 730000, China; College of Civil Engineering and Mechanics, Lanzhou University, Lanzhou 730000, China},
abstract={The cliff body has been severely damaged by diverse wind erosion and secondary relaxed fissures of cliff bodies in the Jiaohe Ruins. It is important to study the influence of diverse wind erosion and secondary relaxed fissures on cliff bodies and damaged pattern under both of them. The finite element software ANSYS is used to simulate cliff body damage under diverse wind erosion and secondary relaxed fissures. In order to verify the result of finite element software simulation, the basal contact friction test is carried out to simulate the cliff body damage process. The result of finite element software simulation illuminates that the maximum displacement of the cliff body is 487.326 mm, the maximum plastic strain is at the root of the relaxed fissure, and the cliff body is fracturing and toppling. The result of the basal contact friction test shows that the open width of the fissure is 20 mm after 2.7 h, and then the soil body is falling. The fracturing-toppling progress is that the bottom of silty clay layer incised by secondary relaxed fissure is empty; the root of relaxed fissure is pulled apart; the incised soil body is inclined deformation; and then the root would appear tensile failure and the soil body would topple and fall. For the impacts of diverse wind erosion and secondary relaxed fissures on damage of cliff bodies, the reinforcement program of cliff bodies is brought forward.},
author_keywords={ANSYS;  Basal contact friction test;  Diversity of wind erosion;  Fracturing-toppling pattern;  Secondary relaxed fissures},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Groce201278,
author={Groce, A. and Zhang, C. and Eide, E. and Chen, Y. and Regehr, J.},
title={Swarm testing},
journal={2012 International Symposium on Software Testing and Analysis, ISSTA 2012 - Proceedings},
year={2012},
pages={78-88},
doi={10.1145/04000800.2336763},
note={cited By 63},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865280752&doi=10.1145%2f04000800.2336763&partnerID=40&md5=794be43405f5322b0c4ecf4ebb402c74},
affiliation={School of Electrical Engineering and Computer Science, Oregon State University, Corvallis, OR, United States; University of Utah, School of Computing, Salt Lake City, UT, United States},
abstract={Swarm testing is a novel and inexpensive way to improve the diversity of test cases generated during random testing. Increased diversity leads to improved coverage and fault detection. In swarm testing, the usual practice of potentially including all features in every test case is abandoned. Rather, a large "swarm" of randomly generated configurations, each of which omits some features, is used, with configurations receiving equal resources. We have identified two mechanisms by which feature omission leads to better exploration of a system's state space. First, some features actively prevent the system from executing interesting behaviors; e.g., "pop" calls may prevent a stack data structure from executing a bug in its overflow detection logic. Second, even when there is no active suppression of behaviors, test features compete for space in each test, limiting the depth to which logic driven by features can be explored. Experimental results show that swarm testing increases coverage and can improve fault detection dramatically; for example, in a week of testing it found 42% more distinct ways to crash a collection of C compilers than did the heavily hand-tuned default configuration of a random tester. © 2012 ACM.},
author_keywords={configuration diversity;  Random testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stefan2012302,
author={Stefan, I. and Miclea, L.},
title={The usage of contextual information to develop data test vectors},
journal={2012 IEEE International Conference on Automation, Quality and Testing, Robotics, AQTR 2012 - Proceedings},
year={2012},
pages={302-306},
doi={10.1109/AQTR.2012.6237721},
art_number={6237721},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865015506&doi=10.1109%2fAQTR.2012.6237721&partnerID=40&md5=87ea0d98a179d15f0811080053c5d867},
affiliation={Automation Department, Technical University of Cluj-Napoca, Romania},
abstract={In real-time systems/hybrid systems the quality of the controlling software represents one of the major aspects. Many of these systems allow interaction with the users by a graphical interface having inputs to select or type in. Even where the software does not control an industrial plant, machinery, a car or robot, it is possible to control financial transactions, personal details or electronic patient information. Large amount of financial, time, human resources are allocated toward testing the software. The paper proposes a method to use contextual data from the GUI of applications to create test cases for functional testing. The objective is to enhance the automation in testing by reducing the time allocated to generate the controllable input values. The DOM architecture for Web and.NET resource file for Windows OS applications will be considered as starting points in the development of the method. The discussion remains if the tester inspection is needed in order to choose between the diversity of test cases automatically generated or the tests will be all executed without exception. It will be taken in consideration the required system resources to repeatedly run all the tests in regression testing. In case of applying this method to several user interfaces, by saving the properties extracted and the generated test cases and results, statistical data regarding effective templates to use would emerge. © 2012 IEEE.},
author_keywords={automated test vector generator;  code combination;  genetic combination;  test vector},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Truntzler2012731,
author={Truntzler, M. and Ranc, N. and Sawkins, M.C. and Nicolas, S. and Manicacci, D. and Lespinasse, D. and Ribière, V. and Galaup, P. and Servant, F. and Muller, C. and Madur, D. and Betran, J. and Charcosset, A. and Moreau, L.},
title={Diversity and linkage disequilibrium features in a composite public/private dent maize panel: Consequences for association genetics as evaluated from a case study using flowering time},
journal={Theoretical and Applied Genetics},
year={2012},
volume={125},
number={4},
pages={731-747},
doi={10.1007/s00122-012-1866-y},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866335238&doi=10.1007%2fs00122-012-1866-y&partnerID=40&md5=1b72512dcb46b895d007eaee4f34f804},
affiliation={INRA, UMR de Genetique Vegetale INRA/Université Paris-Sud/CNRS, Gif-sur-Yvette, France; Molecular Breeding, Syngenta Seeds, Saint-Sauveur, France; Université d'Orsay, UMR de Genetique Vegetale INRA/Université Paris-Sud/CNRS, Gif-sur-Yvette, France; Markers laboratory, Syngenta Seeds, Saint-Sauveur, France; Bioinformatics, Syngenta Seeds, Saint-Sauveur, France},
abstract={Recent progress in genotyping and resequencing techniques have opened new opportunities for deciphering quantitative trait variation by looking for associations between traits of interest and polymorphisms in panels of diverse inbred lines. Association mapping raises specific issues related to the choice of appropriate (i) panels and marker-densities and (ii) statistical methods to capture associations. In this study, we used a panel of 314 maize inbred lines from the dent pool, composed of inbred material from public institutes (113 inbred lines) and a private company (201 inbred lines). We showed that local LD was higher and genetic diversity lower in the material of private origin than in the public material. We compared the results obtained by different software for identifying population structure and computing relatedness among lines, and ran association tests for earliness related traits. Our results confirmed the importance of the mite polymorphism of Vgt1 on flowering time, but also showed that its effect can be captured by zmRap2. 7 polymorphisms located 70 kb apart. We also highlighted associations with polymorphisms within genes putatively involved in lignin biosynthesis pathway, which deserve further investigations. © 2012 Springer-Verlag.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Petrenko2012383,
author={Petrenko, A. and Simao, A. and Maldonado, J.C.},
title={Model-based testing of software and systems: Recent advances and challenges},
journal={International Journal on Software Tools for Technology Transfer},
year={2012},
volume={14},
number={4},
pages={383-386},
doi={10.1007/s10009-012-0240-3},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864346696&doi=10.1007%2fs10009-012-0240-3&partnerID=40&md5=c7b745853dc3e48b5efce5026f6b139e},
affiliation={Centre de recherche informatique de Montreal (CRIM), Montreal, QC, Canada; São Paulo University, São Carlos, São Paulo, Brazil},
abstract={Model-based testing is focused on testing techniques which rely on the use of models. The diversity of systems and software to be tested implies the need for research on a variety of models and methods for test automation. We briefly review this research area and introduce several papers selected from the 22nd International Conference on Testing Software and Systems (ICTSS). © 2012 Springer-Verlag.},
author_keywords={Model-based testing;  Software testing},
document_type={Review},
source={Scopus},
}

@ARTICLE{Chan2012967,
author={Chan, E.Y.K. and Chan, W.K. and Poon, P.-L. and Yu, Y.T.},
title={An empirical evaluation of several test-a-few strategies for testing particular conditions},
journal={Software - Practice and Experience},
year={2012},
volume={42},
number={8},
pages={967-994},
doi={10.1002/spe.1098},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862492771&doi=10.1002%2fspe.1098&partnerID=40&md5=6ade1a8cb63ccac4e52392b479a69a0d},
affiliation={Department of Computer Science, City University of Hong Kong, Kowloon Tong, Hong Kong; School of Accounting and Finance, Hong Kong Polytechnic University, Hung Hom, Hong Kong},
abstract={Existing specification-based testing techniques often generate comprehensive test suites to cover diverse combinations of test-relevant aspects. Such a test suite can be prohibitively expensive to execute exhaustively because of its large size. A pragmatic strategy often adopted in practice, called test-once strategy, is to identify certain particular conditions from the specification and to test each such condition once only. This strategy is implicitly based on the uniformity assumption that the implementation will process a particular condition uniformly, regardless of other parameters or inputs. As the decision of adopting the test-once strategy is often based on the specification, whether the uniformity assumption actually holds in the implementation needs to be critically assessed, or else the risk of inadequate testing could be non-negligible. As viable alternatives to reduce such a risk, a family of test-a-few strategies for the testing of particular conditions is proposed in this paper. Two rounds of experiments that evaluate the effectiveness of the test-a-few strategies as compared with the test-once strategy are further reported. Our experiments do the following: (1) provide clear evidence that the uniformity assumption often, but not always, holds and that the assumption usually fails to hold when the implementation is faulty; (2) demonstrate that all our proposed test-a-few strategies are statistically more reliable than the test-once strategy in revealing faulty programs; (3) show that random sampling is already substantially more effective than the test-once strategy; and (4) indicate that, compared with other test-a-few strategies under study, choice coverage seems to achieve a better trade-off between test effort and effectiveness. Copyright © 2011 John Wiley & Sons, Ltd. Copyright © 2011 John Wiley & Sons, Ltd.},
author_keywords={particular condition;  software testing;  software validation;  test case selection;  test-a-few strategy;  test-once strategy},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ma20122528,
author={Ma, Y. and Choi, E.M.},
title={Hook-based mobile software testing by using aspect-oriented programming},
journal={2012 International Conference on Systems and Informatics, ICSAI 2012},
year={2012},
pages={2528-2532},
doi={10.1109/ICSAI.2012.6223568},
art_number={6223568},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864272330&doi=10.1109%2fICSAI.2012.6223568&partnerID=40&md5=22dc6561f62e635dada78d642c92ba64},
affiliation={Dept. of Computer Engineering, Dongguk University, Seoul, 100-715, South Korea},
abstract={The debugging for mobile software has been became supported easily and strongly by diversified technology such as performance analysis, tracing, and memory debugging. However, testers who want to find and locate knotty defects between the various components of embedded software feel that just using debugging tools is not enough. Although, using testing tools can achieve the target, but not all the testing tools can be deployed on the real device. So in this paper, we describe a new user-friendly Hook-based testing approach for embedded software. Besides introducing the concept of Hook-based testing, we show how to use AOP to implement components testing on the target easily. Meanwhile, we will provide the testing result by process tracing, and memory checking. Finally we compare with famous debugging tools to prove our approach much better. © 2012 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeLucia2012145,
author={De Lucia, A. and Di Penta, M. and Oliveto, R. and Panichella, A.},
title={On the role of diversity measures for multi-objective test case selection},
journal={2012 7th International Workshop on Automation of Software Test, AST 2012 - Proceedings},
year={2012},
pages={145-151},
doi={10.1109/IWAST.2012.6228983},
art_number={6228983},
note={cited By 21},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864183517&doi=10.1109%2fIWAST.2012.6228983&partnerID=40&md5=0b2062fafd7a5a33197c788748dca41e},
affiliation={University of Salerno, via Ponte don Melillo, Fisciano (SA), 84084, Italy; University of Sannio, Palazzo Ex Poste, Via Traiano, 82100 Benevento, Italy; University of Molise, Contrada Fonte Lappone, 86090 Pesche (IS), Italy},
abstract={Test case selection has been recently formulated as multi-objective optimization problem trying to satisfy conflicting goals, such as code coverage and computational cost. This paper introduces the concept of asymmetric distance preserving, useful to improve the diversity of non-dominated solutions produced by multi-objective Pareto efficient genetic algorithms, and proposes two techniques to achieve this objective. Results of an empirical study conducted over four programs from the SIR benchmark show how the proposed technique (i) obtains non-dominated solutions having a higher diversity than the previously proposed multi-objective Pareto genetic algorithms; and (ii) improves the convergence speed of the genetic algorithms. © 2012 IEEE.},
author_keywords={Empirical Studies;  Niched Genetic Algorithms;  Search-based Software Testing;  Test Case Selection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Uzun2012177,
author={Uzun, E. and Atluri, V. and Sural, S. and Vaidya, J. and Parlato, G. and Ferrara, A.L. and Madhusudan, P.},
title={Analyzing temporal role based access control models},
journal={Proceedings of ACM Symposium on Access Control Models and Technologies, SACMAT},
year={2012},
pages={177-186},
doi={10.1145/2295136.2295169},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864067251&doi=10.1145%2f2295136.2295169&partnerID=40&md5=173c25ec77260db716ed271b0bcc9a9d},
affiliation={Rutgers University, United States; Indian Institute of Technology, India; University of Southampton, United Kingdom; University of Bristol, United Kingdom; University of Illinois at Urbana-Champaign, United States},
abstract={Today, Role Based Access Control (RBAC) is the de facto model used for advanced access control, and is widely deployed in diverse enterprises of all sizes. Several extensions to the authorization as well as the administrative models for RBAC have been adopted in recent years. In this paper, we consider the temporal extension of RBAC (TRBAC), and develop safety analysis techniques for it. Safety analysis is essential for understanding the implications of security policies both at the stage of specification and modification. Towards this end, in this paper, we first define an administrative model for TRBAC. Our strategy for performing safety analysis is to appropriately decompose the TRBAC analysis problem into multiple subproblems similar to RBAC. Along with making the analysis simpler, this enables us to leverage and adapt existing analysis techniques developed for traditional RBAC. We have adapted and experimented with employing two state of the art analysis approaches developed for RBAC as well as tools developed for software testing. Our results show that our approach is both feasible and flexible. Copyright 2012 ACM.},
author_keywords={Access control;  Safety analysis;  Temporal RBAC},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fairbairn2012193,
author={Fairbairn, M.L. and Burns, A.},
title={Implementing and verifying EDF preemption-level resource control},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2012},
volume={7308 LNCS},
pages={193-206},
doi={10.1007/978-3-642-30598-6_14},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862506165&doi=10.1007%2f978-3-642-30598-6_14&partnerID=40&md5=6e04426b3729b0e133401767575a46ed},
affiliation={Department of Computer Science, University of York, United Kingdom},
abstract={To support Earliest Deadline First (EDF) dispatching of application tasks the Ada language has had to incorporate Baker's Stack Resource Protocol (SRP). This protocol has proved problematic both in terms of its language definition and implementation. This paper proposes a means of verifying the implementation of complex language features. It describes a prototype tool that allows a comparison to be made between the output of an executing program and a diverse simulator that directly implements EDF+SRP. The tool creates a collection of cases (scenarios); for each of which a program is automatically generated (and executed) and a separate simulation script produced. Tests on an existing run-time for Ada has shown that in certain circumstances an Ada program and its corresponding simulation diverge. © 2012 Springer-Verlag.},
author_keywords={EDF;  real-time;  resource control},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{McMinn2012141,
author={McMinn, P. and Shahbaz, M. and Stevenson, M.},
title={Search-based test input generation for string data types using the results of web queries},
journal={Proceedings - IEEE 5th International Conference on Software Testing, Verification and Validation, ICST 2012},
year={2012},
pages={141-150},
doi={10.1109/ICST.2012.94},
art_number={6200106},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862325220&doi=10.1109%2fICST.2012.94&partnerID=40&md5=640a1c494253a832adc595981bae6268},
affiliation={University of Sheffield, Regent Court, 211 Portobello, Sheffield, S1 4DP, United Kingdom},
abstract={Generating realistic, branch-covering string inputs is a challenging problem, due to the diverse and complex types of real-world data that are naturally encodable as strings, for example resource locators, dates of different localised formats, international banking codes, and national identity numbers. This paper presents an approach in which examples of inputs are sought from the Internet by reformulating program identifiers into web queries. The resultant URLs are downloaded, split into tokens, and used to augment and seed a search-based test data generation technique. The use of the Internet as part of test input generation has two key advantages. Firstly, web pages are a rich source of valid inputs for various types of string data that may be used to improve test coverage. Secondly, the web pages tend to contain realistic, human-readable values, which are invaluable when test cases need manual confirmation due to the lack of an automated oracle. An empirical evaluation of the approach is presented, involving string input validation code from 10 open source projects. Well-formed, valid string inputs were retrieved from the web for 96% of the different string types analysed. Using the approach, coverage was improved for 75% of the Java classes studied by an average increase of 14%. © 2012 IEEE.},
author_keywords={Automatic test data generation;  search based testing;  string inputs;  web queries},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ni2012,
author={Ni, H.},
title={PJM advanced technology pilots for system frequency control},
journal={2012 IEEE PES Innovative Smart Grid Technologies, ISGT 2012},
year={2012},
doi={10.1109/ISGT.2012.6175661},
art_number={6175661},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860859510&doi=10.1109%2fISGT.2012.6175661&partnerID=40&md5=59ee761b4bfa78cf4af92562556d43c1},
affiliation={PJM Interconnection LLC, 955 Jefferson Avenue, Norristown, PA 19403-2497, United States},
abstract={PJM Advanced Technology Pilot Program sets up a platform to smart-grid technologies to demonstrate their technical viability and commercial values through running pilot project. This paper focuses on those pilot projects that target to provide high-quality regulation services to grid. Piloted technologies are diversified, from batteries, smart building, electric vehicles, to electric thermal storages. This paper will share program status, testing procedure and key observations that PJM staff obtained from the pilots. This paper also highlights the challenges that must be overcome to support moving those successfully piloted technologies into production environment and bring tangible benefits to all market members. © 2012 IEEE.},
author_keywords={Demand Response;  Electric Vehicles;  Electricity Market;  Energy Storage System;  Frequency Control;  Smart Grid},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhu2012116,
author={Zhu, H. and Zhang, Y.},
title={Collaborative testing of web services},
journal={IEEE Transactions on Services Computing},
year={2012},
volume={5},
number={1},
pages={116-130},
doi={10.1109/TSC.2010.54},
art_number={4629386},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863231745&doi=10.1109%2fTSC.2010.54&partnerID=40&md5=1798508b80cc8f5cd59695360f602228},
affiliation={Department of Computing and Electronics, Wheatley Campus, Oxford Brookes University, Oxford OX33 1HX, United Kingdom; National Laboratory for Parallel and Distributed Processing, School of Computer Science, National University of Defense Technology, No. 47 Yan Wachi Street, ChangSha, Hunan Province, China},
abstract={Software testers are confronted with great challenges in testing Web Services (WS) especially when integrating to services owned by other vendors. They must deal with the diversity of implementation techniques used by the other services and to meet a wide range of test requirements. However, they are in lack of software artifacts, the means of control over test executions and observation on the internal behavior of the other services. An automated testing technique must be developed to be capable of testing on-the-fly nonintrusively and nondisruptively. Addressing these problems, this paper proposes a framework of collaborative testing in which test tasks are completed through the collaboration of various test services that are registered, discovered, and invoked at runtime using the ontology of software testing STOWS. The composition of test services is realized by using test brokers, which are also test services but specialized in the coordination of other test services. The ontology can be extended and updated through an ontology management service so that it can support a wide open range of test activities, methods, techniques, and types of software artifacts. The paper presents a prototype implementation of the framework in semantic WS and demonstrates the feasibility of the framework by running examples of building a testing tool as a test service, developing a service for test executions of a WS, and composing existing test services for more complicated testing tasks. Experimental evaluation of the framework has also demonstrated its scalability. © 2008 IEEE.},
author_keywords={distributed/internet based software engineering tools and techniques;  ontology;  semantic web services;  service composition;  Software engineering;  software testing;  testing tools;  web services},
document_type={Article},
source={Scopus},
}

@ARTICLE{Uzun2012189,
author={Uzun, A.},
title={A parametric study for specific fuel consumption of an intercooled diesel engine using a neural network},
journal={Fuel},
year={2012},
volume={93},
pages={189-199},
doi={10.1016/j.fuel.2011.11.004},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855947150&doi=10.1016%2fj.fuel.2011.11.004&partnerID=40&md5=35b1304e58468972315814a41c7fd34b},
affiliation={Sakarya Vocational School, Automotive Programming, Sakarya University, 54187 Sakarya, Turkey},
abstract={Turbocharging is a process wherein the amount of oxygen used in a combustion reaction is increased to raise output and decrease specific fuel consumption. On account of this, fuel economy and thermal efficiency are more important for all engines. The use of an intercooler reduces the temperature of intake air to the engine, and this cooler and denser air increases thermal and volumetric efficiency. Most research projects on engineering problems usually take the form of experimental studies. However, experimental research is relatively expensive and time consuming. In recent years, Neural Networks (NNs) have increasingly been used in a diverse range of engineering applications. In this study, various parametric studies are executed to investigate the interrelationship between a single variable and two steadies and two constant parameters on the brake specific fuel consumption (BSFC, g/kW h). The variables selected are engine speed, load and Crankshaft Angel (CA). The data used in the present study were obtained from previous experimental research by the author. These data were used to enhance, train and test a NN model using a MATLAB-based program. The results of the NN based model were found to be convincing and were consistent with the experimental results. The trained NN based model was then used to perform the parametric studies. The performance of the NN based model and the results of parametric studies are presented in graphical form and evaluated. © 2011 Elsevier Ltd. All rights reserved.},
author_keywords={Diesel engine;  Intercooling;  Neural networks;  Scaled conjugate gradient algorithm;  Specific fuel consumption},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ryssel201283,
author={Ryssel, U. and Ploennigs, J. and Kabitzsch, K.},
title={Automatic library migration for the generation of hardware-in-the-loop models},
journal={Science of Computer Programming},
year={2012},
volume={77},
number={2},
pages={83-95},
doi={10.1016/j.scico.2010.06.005},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855203457&doi=10.1016%2fj.scico.2010.06.005&partnerID=40&md5=2248a469cb26dae39db633f20ae0a806},
affiliation={Institute of Applied Computer Science, Department of Computer Science, Dresden University of Technology, Dresden, Germany},
abstract={Embedded systems are widely used in several applications nowadays. As they integrate hard- and software elements, their functionality and reliability are often tested by hardware-in-the-loop methods, in which the system under test runs in a simulated environment. Due to the rising complexity of the embedded functions, performance limitations and practicability reasons, the simulations are often specialized to test specific aspects of the embedded system and develop a high diversity by themselves. This diversity is difficult to manage for a user and results in erroneously selected test components and compatibility problems in the test configuration. This paper presents a generative programming approach that handles the diversity of test libraries. Compatibility issues are explicitly evaluated by a new interface concept. Furthermore, a novel model analyzer facilitates the efficient application in practice by migrating existing libraries. The approach is evaluated for an example from the automotive domain using MATLAB/Simulink. © 2010 Elsevier B.V. All rights reserved.},
author_keywords={Function-block-based design;  Generative programming;  Library migration;  Structural comparison},
document_type={Article},
source={Scopus},
}

@ARTICLE{Amaral2012,
author={Amaral, J.C. and De Oliveira E Souza, M.L.},
title={Application of methods to smooth the transition between control submodes in the nominal mode of the multimission platform},
journal={SAE Technical Papers},
year={2012},
doi={10.4271/2012-36-0378},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072360478&doi=10.4271%2f2012-36-0378&partnerID=40&md5=cb9949c20a67d5945ae4912a7706ba9a},
affiliation={National Institute for Space Research, INPE, Brazil},
abstract={The Multimission Platform (MMP) is a generic service module currently in Project at INPE. In the 2001 version, its control system can be switched between nine main Operation Modes and other submodes, according to information from satellite sensors and ground commands. The Nominal Mode stabilizes the MMP in three axes and takes it to a nominal attitude, using three reaction wheels. Each wheel has coarse and fine acquisition submodes. The use of multiple modes of control for specific situations frequently is simpler than projecting a single controller for all cases. However, besides being harder to warrant its general stability, the mere switching between these submodes generates bumps, which can reduce the performance and even damage the actuator or plant. In this work, we present an application of diverse methods to smooth the transition between control submodes of the Nominal Mode of the MMP. We will use techniques including, but not limited to, output tracking, anti-windup and crossfading to compare and identify the combinations which produce the more satisfactory results. The tests are based in simulations with the software MatrixX/Systembuild, of National Instruments, which helps developers with tools to model, analyze and test control systems. The tests focus on the worst cases the satellite may face. Being able to withstand these worst cases, the control system is considered apt to simpler situations. The tests show that many of the adopted strategies could smooth the transition and improve the performance of the system, and it was possible to identify advantages and disadvantages of each one. Copyright © 2012 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Garfinkel2012S80,
author={Garfinkel, S.},
title={Lessons learned writing digital forensics tools and managing a 30TB digital evidence corpus},
journal={Proceedings of the Digital Forensic Research Conference, DFRWS 2012 USA},
year={2012},
pages={S80-S89},
doi={10.1016/j.diin.2012.05.002},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068669082&doi=10.1016%2fj.diin.2012.05.002&partnerID=40&md5=2608d518d2e9e1816dfae25dd238dd56},
affiliation={Naval Postgraduate School, Computer Science, 900 N Glebe Rd, Arlington, VA  2203, United States},
abstract={Writing digital forensics (DF) tools is difficult because of the diversity of data types that needs to be processed, the need for high performance, the skill set of most users, and the requirement that the software run without crashing. Developing this software is dramatically easier when one possesses a few hundred disks of other people’s data for testing purposes. This paper presents some of the lessons learned by the author over the past 14 years developing DF tools and maintaining several research corpora that currently total roughly 30TB. © 2012 Digital Forensic Research Workshop. All rights reserved.},
author_keywords={Digital corpora;  Digital forensics;  Lessons learned},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kang2012,
author={Kang, D. and Zhang, D. and Fleming, W. and Fron, J. and Jaikamal, V.},
title={Establishing an efficient test framework for embedded software verification via hardware-in-the-loop testing},
journal={SAE Technical Papers},
year={2012},
volume={8},
doi={10.4271/2012-01-2029},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881200696&doi=10.4271%2f2012-01-2029&partnerID=40&md5=9ceef3da42b731cae0bcd2420d319800},
affiliation={Navistar Inc, Lisle, IL, United States; ETAS Inc, Ann Arbor, MI, United States},
abstract={The heavy-duty diesel industry continues to expand the use of sophisticated electronic controls to reduce greenhouse gas emissions and improve fuel economy in the transportation sector. This inevitably leads to a need for increased knowhow in all aspects of embedded software development, e.g. in writing model-based specifications, rapid controls prototyping, automatic code generation, and hardware-in-the-loop (HiL) testing. In addition, the software development organization has to recognize the need to establish maturity in its verification and validation test framework, which includes, among other things, a proper definition of testing process and workflow, greater organizational focus, and a robust tool architecture for software testing. Navistar has already undertaken a model-based approach to software development and continues to improve their processes, technology, and tooling in this area. Another focus area in the past year has been HiL-based test framework for increased software testing in the lab environment. In this paper, we will outline a HiL-based verification and validation (V&V) framework-developed jointly by Navistar and ETAS-that is built upon open-architecture tools, standard interfaces, and established industry best-practices. The primary goal of the HiL-based test framework was to create a foundation for a mature test process based on an environment where software testing is tightly coupled with software requirements. Further, we will demonstrate how Navistar, through the adoption of this test framework, is now able to fully automate functional testing of embedded control unit (ECU) software, re-use test cases across ECU families and test platforms, improve regression test coverage, and automatically test the diagnostics software in an engine ECU. Thanks to this framework, Navistar software engineering teams in diverse geographies are now able to increase collaboration, resulting in improved software quality with lower overall V&V test effort. Copyright © 2012 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tsang2012363,
author={Tsang, Y.},
title={Effects of coupled processes on a proposed high-level radioactive waste repository at Yucca Mountain, Nevada},
journal={Memoir of the Geological Society of America},
year={2012},
volume={209},
pages={363-393},
doi={10.1130/2012.1209(07)},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876795668&doi=10.1130%2f2012.1209%2807%29&partnerID=40&md5=8db8f9981b12f9a5b1172a9576751bbd},
affiliation={Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720, United States},
abstract={Heat from radionuclide decay causes coupled thermal (T), hydrological (H), chemical (C), and mechanical (M) processes in the rock mass. These coupled processes impact the ability of a repository to isolate waste by affecting water seepage into wasteemplacement drifts and by affecting radionuclide transport. The U.S. Department of Energy's Thermal Testing Program at the proposed high-level radioactive waste repository at Yucca Mountain, Nevada, began in the mid-1990s and consisted of three large-scale in situ thermal tests. Although in 2010, the U.S. government decided to pursue alternative solutions to geologic disposal of radioactive waste at Yucca Mountain, the work reported throughout this volume refers to "the proposed repository" at Yucca Mountain, which was the status at the time the chapters were written (2009). The main objective of these thermal tests was to gain an in-depth understanding of the coupled THCM processes that would occur in the repository rock. Numerical models that capture coupled processes were constructed for the respective thermal tests, and the predictions from these numerical models, when compared to measured data, enabled the evaluation of processes occurring in the thermal tests. In turn, analysis of the thermal tests, particularly of the drift-scale test (the largest of these tests), has provided information on THCM processes that were incorporated in drift-scale and mountain-scale numerical models for the proposed repository at Yucca Mountain to predict repository performance during thermal loading. Such coupled-processes models for the proposed repository show that TH processes would produce a vaporization barrier, which would prevent water from seeping into the drifts when the temperature near the drifts rises above boiling. THC and THM processes cause permeability changes that modify fl ow paths near the drifts and, in turn, seepage of water into drifts. The impact of thermally driven coupled processes is largest near the drifts, where the increase in temperature is the greatest. Further away (tens of meters) from the drifts, the impact of THCM processes on radionuclide transport is insignificant. The detailed THCM studies at Yucca Mountain indicate that, overall, the effects of heating due to radioactive decay would not degrade the long-term ability of the proposed repository to isolate waste. On the contrary, the THCM coupled processes lead to more diversion of water around and less seepage into the waste-emplacement drifts than that at ambient conditions, thus making Yucca Mountain a more effective natural barrier to potential release of radionuclides to the biosphere. © 2012 The Geological Society of America. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Estivill-Castro2012428,
author={Estivill-Castro, V. and Hexel, R. and Rosenblueth, D.A.},
title={Efficient modelling of embedded software systems and their formal verification},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2012},
volume={1},
pages={428-433},
doi={10.1109/APSEC.2012.21},
art_number={6462690},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874629026&doi=10.1109%2fAPSEC.2012.21&partnerID=40&md5=c29601848999c05ab25cec10f7b5f125},
affiliation={Vladimir Estivill-Castro, School of ICT, Griffith University, Nathan 4111, Australia; IIMAS, UNAM, 01000 Mexico D.F, Mexico},
abstract={We propose vectors of finite-state machines whose transitions are labeled by formulas of a common-sense logic as the modeling tool for embedded systems software. We have previously shown that this methodology is very efficient in producing succinct and clear models (e.g., in contrast to plain finite-state machines, Petri nets, or Behavior Trees). We show that we can capture requirements precisely and that we can simulate and validate the models. We can, therefore, directly apply Model-Driven Engineering and deploy the models into software for diverse platforms with full tractability of requirements. Moreover, the sequential semantics of our vector of finite-state machines enables model-checking, formally establishing the correctness of the model. Finally, our approach facilitates systematic Failure Modes and Effects Analysis (FMEA) for diverse target platforms. We demonstrate the effectiveness of our methodology with several examples widely discussed in the software engineering literature and compare this with other approaches, showing that we can prove more properties, and that some claims about verification in such approaches have been exaggerated or are incomplete. © 2012 IEEE.},
author_keywords={formal methods in software engineering;  model-driven engineering;  software requirements engineering;  Software testing;  verification and validation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Garfinkel2012,
author={Garfinkel, S.},
title={Lessons learned writing digital forensics tools and managing a 30TB digital evidence corpus},
journal={Digital Investigation},
year={2012},
volume={9},
number={SUPPL.},
pages={S80-S89},
doi={10.1016/j.diin.2012.05.002},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864614764&doi=10.1016%2fj.diin.2012.05.002&partnerID=40&md5=59b332d8285ceca35e713dd3e26f55db},
affiliation={Naval Postgraduate School, Computer Science, 900 N Glebe Rd, Arlington, VA 2203, United States},
abstract={Writing digital forensics (DF) tools is difficult because of the diversity of data types that needs to be processed, the need for high performance, the skill set of most users, and the requirement that the software run without crashing. Developing this software is dramatically easier when one possesses a few hundred disks of other people's data for testing purposes. This paper presents some of the lessons learned by the author over the past 14 years developing DF tools and maintaining several research corpora that currently total roughly 30TB. © 2012 Dykstra & Sherman. Published by Elsevier Ltd. All rights reserved.},
author_keywords={Digital corpora;  Digital forensics;  Lessons learned},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bogdan201289,
author={Bogdan, A. and Zwolińska, M.},
title={Future Trends in the development of thermal manikins applied for the design of clothing thermal insulation},
journal={Fibres and Textiles in Eastern Europe},
year={2012},
volume={93},
number={4},
pages={89-95},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863477409&partnerID=40&md5=859f7406abf75697886dff399a992a18},
affiliation={Central Institute for Labour Protection, National Research Institute, Department of Ergonomics, ul. Czerniakowska 16, 00-701 Warsaw, Poland},
abstract={Thermal manikins were created with the intention to design and model protective clothing insulation for specific conditions, e.g. military clothing and clothing for divers. At present thermal manikins have a broader use, i.e. to assess the effect of clothing on a human body, to assess its influence on thermal comfort during work in a given clothing ensemble and to test innovative solutions bringing about a reduction in the thermal heat load. When using thermal manikins it should be borne in mind that heat exchange through clothing is, to a large extent, determined by the temperature distribution on human skin. The aim of the study was to find out to what extent thermal manikins can be used to represent the correct distribution of temperature on human skin. To this end, comparative measurements of the temperature distribution on the surface of a standard thermal manikin with a structure generally used and on the surface of the skin of volunteers were performed. The tests conducted showed that for the further development of thermal manikins computer software should be developed which would help to predict the temperature distribution on the manikin surface corresponding to the skin temperature of volunteers. Such software should allow for the simulation of thermal regulatory mechanisms in a human, i.e. an increase in skin temperature caused by vasoconstriction and shivering, as well as a decrease in skin temperature due to vasodilatation and sweating. In a thermally neutral environment, the methods which are currently used to control the thermal manikin seem to be sufficient.},
author_keywords={Protective clothing;  Skin temperature;  Thermal insulation;  Thermal manikin},
document_type={Article},
source={Scopus},
}

@ARTICLE{Fernandes2012337,
author={Fernandes, I. and Pericão, M. and Hagelia, P. and Noronha, F. and Ribeiro, M.A. and Maia, J.},
title={Identification of acid attack on concrete of a sewage system},
journal={Materials and Structures/Materiaux et Constructions},
year={2012},
volume={45},
number={3},
pages={337-350},
doi={10.1617/s11527-011-9769-y},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861636220&doi=10.1617%2fs11527-011-9769-y&partnerID=40&md5=bb6daec3ab998338ee323f26e39e3616},
affiliation={Centro de Geologia/DGAOT, University of Porto, Porto, Portugal; Instituto de Soldadura e Qualidade, Porto, Portugal; Technology Department, Norwegian Public Roads Administration, Oslo, Norway},
abstract={A sewage system, 300 km long, showed superficial deterioration of concrete just 2 years after construction. In order to re-habilitate the structure, it was necessary to identify the main mechanism of deterioration and to understand the heterogeneous distribution of the damage observed. The study was performed in a three stepped program: site investigation and sampling, laboratory tests and concrete petrography. During the site inspection it was recognized that there was dissolution of the concrete in some sectors of the structure, with the aggregate particles protruding in relation to the undulated surface. In some places a white to yellowish putty-like product could be excavated by hand. The main deterioration was observed above the water level. The composition of the atmosphere inside the sewer was assessed and a high content of hydrogen sulfide was detected. Sampling was performed in different structural elements which showed diverse exposition to the aggressive environment. Impregnated thin sections of concrete with fluorescent yellow dye were analyzed by optical microscope. Concrete petrography showed to be crucial for the diagnoses. The study showed that the putty-like product was composed by gypsum with small residual particles of siliceous sand which resisted to the acid attack. SEM/EDS was used to evaluate the content of sulfur in different sections of the concrete cores and also to characterize the putty-like product at the surface of the concrete. This study led to the confirmation of the presence of an extensive sulfuric acid attack with rather minor sulfate attack within the concrete due to the exposition to aggressive environment. It also showed that behind the superficial deteriorated level, the concrete was sound with no signs of internal deleterious reactions. Ammonium content in residual water might have also contributed to the superficial deterioration of the concrete sewer. © RILEM 2011.},
author_keywords={Acid attack;  Concrete petrography;  Gypsum;  Popcorn calcite deposition},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mäntylä2012145,
author={Mäntylä, M.V. and Itkonen, J. and Iivonen, J.},
title={Who tested my software? Testing as an organizationally cross-cutting activity},
journal={Software Quality Journal},
year={2012},
volume={20},
number={1},
pages={145-172},
doi={10.1007/s11219-011-9157-4},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855358438&doi=10.1007%2fs11219-011-9157-4&partnerID=40&md5=39c7f78bc16ba32ff684441cc1542e6f},
affiliation={Department of Computer Science and Engineering, School of Science, Aalto University, P.O. Box 19210, 00076 Aalto, Finland},
abstract={There is a recognized disconnect between testing research and industry practice, and more studies are needed on understanding how testing is conducted in real-world circumstances instead of demonstrating the superiority of specific methods. Recent literature indicates that testing is a cross-cutting activity that involves various organizational roles rather than the sole involvement of specialized testers. This research empirically investigates how testing involves employees in varying organizational roles in software product companies. We studied the organization and values of testing using an exploratory case study methodology through interviews, defect database analysis, workshops, analyses of documentation, and informal communications at three software product companies. We analyzed which employee groups test software in the case companies, and how many defects they find. Two companies organized testing as a team effort, and one company had a specialized testing group because of its different development model. We found evidence that testing was not an action conducted only by testing specialists. Testing by individuals with customer contact and domain expertise was an important validation method. We discovered that defects found by developers had the highest fix rates while those revealed by specialized testers had the lowest. The defect importance was susceptible to organizational competition of resources (i. e., overvaluing defects of reporter's own products or projects). We conclude that it is important to understand the diversity of individuals participating in software testing and the relevance of validation from the end users' viewpoint. Future research is required to evaluate testing approaches for diverse organizational roles. Finally, to improve defect information, we suggest increasing automation in defect data collection. © 2011 The Author(s).},
author_keywords={Defect data analysis;  Defect fix rate;  Defect reporters;  Industrial case study;  Interviews;  Roles;  Testers;  Testing;  Values},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ko2012489,
author={Ko, J.-W. and Han, J.-S. and Song, Y.-J.},
title={Pattern based model transformation using mapping patterns for model transformation verification},
journal={Lecture Notes in Electrical Engineering},
year={2012},
volume={120 LNEE},
pages={489-493},
doi={10.1007/978-94-007-2911-7_46},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84255187423&doi=10.1007%2f978-94-007-2911-7_46&partnerID=40&md5=2a22ffe0b1340b9096534beebaa8e9b2},
affiliation={Department of Computer Engineering, Kyung Hee University, Seocheon-dong, Giheung-gu, Yongin-si, Gyeonggi-do 446-701, South Korea; Division of Information and Communication, BaekSeok University, 115, Anseo-dong, Dongnam-gu, Cheonan-si, Chungnam-do 234-567, South Korea},
abstract={In order to easily port mobile applications suitable for each platform, that have been developed under diverse development environment for individual wireless communication service providers, or redevelop them on a specific platform, it is required to reuse them at software model level that is a software development paradigm for Model Driven Architecture (MDA). Verification of existing design models and test models for the study, mainly checking (Model Checking) with a code-based software designed to define in the abstract syntax tree or on the models generated using refactoring on design models for refinery operations and define how to perform. The problem with these traditional research methods, but the first model, design model for checking the information with the model by defining a formal representation in the form of an abstract syntax tree, as you've shown how to perform verification of the model. Additional steps need to define more complex due to a software problem that is not the way to the model suitable for model transformation verification. In this paper, as defined in the pattern-based model transformation studies of a reusable mapping patterns, given the pattern information automatically through the software model offers a way to perform model transformation verification.},
author_keywords={MDA;  Model transformation verification;  Pattern based model transformation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ko2012597,
author={Ko, J.-W. and Song, Y.-J.},
title={Test driven development of model transformation with reusable patterns},
journal={Lecture Notes in Electrical Engineering},
year={2012},
volume={114 LNEE},
pages={597-605},
doi={10.1007/978-94-007-2792-2_57},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84255178197&doi=10.1007%2f978-94-007-2792-2_57&partnerID=40&md5=e72680e96524f876f257fe9b789ea84c},
affiliation={Department of Computer Engineering, Kyung Hee University, Yong-in 446701, South Korea},
abstract={In order to easily port mobile applications suitable for each platform, that have been developed under diverse development environment for individual wireless communication service providers, or redevelop them on a specific platform, it is required to reuse them at software model level that is a software development paradigm for MDA (Model Driven Architecture). Verification of existing design models and test models for the study, mainly checking (Model Checking) with a code-based software designed to define in the abstract syntax tree or on the models generated using refactoring on design models for refinery operations and define how to perform. The problem with these traditional research methods, but the first model, design model for checking the information with the model by defining a formal representation in the form of an abstract syntax tree, as you've shown how to perform verification of the model to perform refactoring. Additional steps need to define more complex due to a software problem that is not the way to the model suitable for optimization refactoring. In this paper, as defined in the MDA-based model transformation studies of a reusable model transformation patterns, abstract factory pattern and the bridge pattern given the pattern information automatically through the software model offers a way to perform refactoring. © 2012 Springer Science+Business Media B.V.},
author_keywords={Graph model transformation;  Model comparison algorithms;  Model transformation verification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ko20122476,
author={Ko, J.-W. and Song, Y.-J.},
title={A study on model transformation mechanism using graph comparison algorithm, abstract factory pattern and bridge pattern},
journal={Applied Mechanics and Materials},
year={2012},
volume={121-126},
pages={2476-2481},
doi={10.4028/www.scientific.net/AMM.121-126.2476},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81255136413&doi=10.4028%2fwww.scientific.net%2fAMM.121-126.2476&partnerID=40&md5=c42aabade1e7092426880ccd577a7358},
affiliation={Dept. Computer Engineering, Kyung Hee University, 446701 Yongin, South Korea},
abstract={In order to easily port mobile applications suitable for each platform, that have been developed under diverse development environment for individual wireless communication service providers, or redevelop them on a specific platform, it is required to reuse them at software model level that is a software development paradigm for MDA (Model Driven Architecture). Verification of existing design models and test models for the study, mainly checking (Model Checking) with a code-based software designed to define in the abstract syntax tree or on the models generated using refactoring on design models for refinery operations and define how to perform. The problem with these traditional research methods, but the first model, design model for checking the information with the model by defining a formal representation in the form of an abstract syntax tree, as you've shown how to perform verification of the model to perform refactoring. Additional steps need to define more complex due to a software problem that is not the way to the model suitable for optimization refactoring. In this paper, as defined in the MDA-based model transformation studies of a reusable model transformation patterns, abstract factory pattern and the bridge pattern given the pattern information automatically through the software model offers a way to perform refactoring.},
author_keywords={Graph model transformation;  Model comparison algorithms;  Model transformation verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Szczepanowska2011265,
author={Szczepanowska, H. and Mathia, T.G.},
title={Space heritage: The Apollo heat shield; atmospheric reentry imprint on materials' surface},
journal={Materials Research Society Symposium Proceedings},
year={2011},
volume={1319},
pages={265-274},
doi={10.1557/opl.2011.780},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84455190081&doi=10.1557%2fopl.2011.780&partnerID=40&md5=f4bfff8be1debb30a2e710e4fdab90de},
affiliation={Museum Conservation Institute, Smithsonian Institution, 4210 Silver Hill Road, Suitland, MD 20746-2863, United States; C.N.R.S., Ecole Centrale de Lyon, Laboratoire de Tribologie et Dynamique des Systèmes, 36 Av. Guy de Collongue, 69134 Ecully, France},
abstract={The heat shield is part of a thermal protective system (TPS) essential in shielding the cargo of a spacecraft during reentry to the earth's atmosphere. The ablated surface of the heat shield is a testimony to the harsh reentry environment, evidenced in melting and charring among other phenomena that occur during reentry at velocity of 9-11 km/sec. The aim of this study was to extrapolate information about atmospheric reentry from the surface of the ablated material. A sample of the heat shield from the test vehicle of the Apollo Program, AS-202, was the subject of the analysis. For the preliminary studies, selected investigation modes from the Global Optimal Strategy model, developed to identify wear of engineering surfaces, were applied: examination of structure, optical observation, physico-chemical characterization and surface morphology. Instrumentation used included: microscopic surface analysis with Extended Depth of Field composite images (EDF), Fourier transform infrared spectroscopy (FTIR), attenuated total reflectance (ATR), confocal scanning laser microscopy and laser scanning microscopy. The Apollo Program testing vehicle AS-202 (1966) ablated specimen sample was obtained from the collection of the National Air and Space Museum (NASM), Smithsonian Institution, Washington DC. The authors combine their diverse experiences in tribology and in artifacts' museum conservation so as to contribute to the space heritage material science. This study represents one of the building blocks of a larger project, the Fundamental Model of public outreach and perception (FAM-pop) of complex aerospace technologies. © 2011 Materials Research Society.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Isabel2011129,
author={Isabel, S.S. and Travassos, G.H.},
title={Features of software testing techniques for use in projects web [Características de técnicas de teste de software para uso em projetos web]},
journal={14th Ibero-American Conference on Software Engineering and 14th Workshop on Requirements Engineering, CIbSE 2011},
year={2011},
pages={129-142},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886695842&partnerID=40&md5=24b0827ed5b9537dcee842f5869dc5ad},
affiliation={Universidade Federal do Rio de Janeiro, COPPE/Sistemas, Caixa Postal 68.511, CEP 21.941-972 - Rio de Janeiro - RJ, Brazil},
abstract={Small and large scale Web applications have been built along the years influencing the conventional software engineering approaches and demanding care with additional quality issues such as verification, validation and testing. Regarding software testing, the results of a secondary study have pointed out at least 71 different testing techniques specifically applicable to Web software projects. Hence, the diversity of Web applications and development methodologies combined with the great number of available software testing techniques make hard the decision about which technique should be used in the project. Therefore, the understanding of testing techniques characteristics can support the decision making and represents an interesting research challenge. Based on this, this paper presents a set of Web software testing techniques characteristics synthesized from the results of a secondary study and evaluated through a survey with specialists in the field. It represents an initial result towards the organization of a Web application testing techniques characterization schema, which intends to support the choosing of suitable software testing techniques for a Web software project.},
author_keywords={Experimental software engineering;  Selection of technologies;  Software testing;  Survey;  Systematic review;  Web application},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yan20112171,
author={Yan, W. and Jishou, Z. and Lei, Z.},
title={Integration of solar buildign technologies in an office building, Changzhou, China},
journal={30th ISES Biennial Solar World Congress 2011, SWC 2011},
year={2011},
volume={3},
pages={2171-2180},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873826932&partnerID=40&md5=7edf107311e15f0a452fc11d57351b28},
affiliation={China Architecture Design and Research Group, Beijing, China; Special Committee of Solar Buildings, Chinese Renewable Energy Society, Beijing, China},
abstract={This paper presents a demonstration project concerning the solar building integration design idea for an office building located in south part of China. The project client is a manufacturer of PV products, with relevant system design and installation. Thus, it has strong desire to lay out the PV products via the office building facade. On the other hand, the local climate is humid through the whole year and sweltering in the summer, which prefers passive solar technologies. How to effectively integrate the active and passive solar technologies into the office building is the emphasis of the project design. Based on the weather characteristics and client requirements, the project develops solar building integration approaches to improve the solar technology diversity, efficiency and to reduce the CO2 emission for the buildings. The solar energy technologies are integrated and applied includes PV, natural ventilation, atrium lighting, respiration façade and roof planting. The software simulation and on site testing by instrument are adopted and compared to validate the design ideas. The results show that via the application of these technologies, the used approaches and technologies adapt well to the local climate and satisfy the requirements of the enterprise's image as well.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Marwaha20111293,
author={Marwaha, S. and Saha, A. and Yadav, V.K.},
title={Ontology based expert system for varietal selection of maize},
journal={Proceedings of the 5th Indian International Conference on Artificial Intelligence, IICAI 2011},
year={2011},
pages={1293-1313},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872197020&partnerID=40&md5=70684f1a929f8cdb554513a59f7f4d2e},
affiliation={Division of Computer Applications, Indian Agricultural Statistics Research Institute, Pusa, Delhi, India; Directorate of Maize Research, Pusa, Delhi, India},
abstract={Immemorial Indian farmers were growing land races/local cultivars having low productivity which was a matter of concern at national level. To address the issue of low productivity systematic maize research in India was initiated in 1957 with the launch of All India Coordinated Maize Improvement Project (AICMIP). A coordinated emphasis has been laid on various aspects of maize breeding activities throughout the country viz., development of genetically diverse, productive, inbred lines with good per se performance and resistance/tolerance to biotic and abiotic stresses, testing of large number of 2-parent crosses at different locations under multi-location program, easy, economical and efficient commercial seed production technology, and demonstrations especially on farmers' fields. This expert system is designed to help farmers to select variety or varieties for a particular area based on maturity type, grain color and grain type, which is neither feasible nor practical by conventional system of extension. In conventional architecture of expert system, knowledge engineers along with domain experts build the knowledgebase in terms of facts and rules. This knowledgebase resides in the working memory of the inference engine and only accessible to one system. This knowledgebase is not portable and cannot be made accessible to other applications concurrently. Moreover, traditional rule-base systems are not inherently based on Unicode and thus lacks support for internationalization or for regional languages. The presented system has a portable knowledgebase and is based on OWL ontology. The system acts as a tool for transferring the site and crop specific knowledge of various domain experts to the farmers.},
author_keywords={Maize;  Ontology;  OWL;  Protégé;  Semantic web;  SPARQL;  Variety selection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bhattacharyya201138,
author={Bhattacharyya, S.S. and Plishker, W. and Shen, C.-C. and Gupta, A.},
title={Teaching cross-platform design and testing methods for embedded systems using DICE},
journal={Proceedings - 2011 Workshop on Embedded Systems Education, WESE 2011},
year={2011},
pages={38-45},
doi={10.1145/2077370.2077376},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863417177&doi=10.1145%2f2077370.2077376&partnerID=40&md5=fecdc9b84cb43896dac867d6a27fae1f},
affiliation={Dept. of ECE, UMIACS, University of Maryland, College Park, United States; Dept. of Physics, University of Maryland, College Park, United States},
abstract={DICE (the DSPCAD Integrative Command Line Environment) is a package of utilities that facilitates efficient management of software projects. Key areas of emphasis in DICE are cross-platform operation, support for projects that integrate heterogeneous programming languages, and support for applying and integrating different kinds of design and testing methodologies. The package is being developed at the University of Maryland to facilitate the research and teaching of methods for implementation, testing, evolution, and revision of engineering software. The platform- and language-independent focus of DICE makes it an effective vehicle for teaching high-productivity, high-reliability methods for design and implementation of embedded systems for a variety of courses. In this paper, we provide an overview of features of DICE - particularly as they relate to testing driven design practices - that are useful in embedded systems education, and discuss examples and experiences of applying the tool in courses at the University of Maryland aimed at diverse groups of students - undergraduate programming concepts for engineers, graduate VLSI architectures (aimed at research-oriented students), and graduate FPGA system design (aimed at professional Master's students). Copyright 2011 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wu201187,
author={Wu, J. and Wang, C. and Liu, Y. and Zhang, L.},
title={AGARIC - A hybrid cloud based testing platform},
journal={Proceedings - 2011 International Conference on Cloud and Service Computing, CSC 2011},
year={2011},
pages={87-94},
doi={10.1109/CSC.2011.6138558},
art_number={6138558},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863125924&doi=10.1109%2fCSC.2011.6138558&partnerID=40&md5=f6b164d0499546a3277363c7c7188035},
affiliation={School of Computer Science and Engineering, Beihang Univeristy (BUAA), Beijing, China},
abstract={Cloud computing technology enables developers spend much more time on application quality without considering computing resource constraint, load balancing and performance tuning, etc. It raises challenges along with the benefits it offers to software testing. This paper is motivated with the concerns on how to test the online web applications in a scalable and diverse way. The resources occupied in the proposed cloud testing are diverse in computing ability, network performance, and software configuration (including operating system and browser, etc.). This paper at first identifies the eight unique features to define cloud testing. Then, this paper introduces the design of Agaric - hybrid cloud based testing platform. In Agaric we use both the diverse internet distributed and user owned computing resources, and the centered computing resources. The key to organize the test network is the proposed Test Flow Control Protocol (TFCP). TFCP is verified formally with Colored Petri Net and the platform is evaluated with both controlled and uncontrolled experiments. © 2011 IEEE.},
author_keywords={cloud computing;  cloud testing;  software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vasta20112047,
author={Vasta, R. and Watson, M. and LeBlanc, K.},
title={NAVFEST: A cost effective solution to GPS NAVWAR testing},
journal={24th International Technical Meeting of the Satellite Division of the Institute of Navigation 2011, ION GNSS 2011},
year={2011},
volume={3},
pages={2047-2052},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861416730&partnerID=40&md5=58daa713aca87590b128d3ccbed2a94a},
affiliation={746th Test Squadron, United States},
abstract={From 7 to 10 February 2011, the 746th Test Squadron (746 TS), located at Holloman Air Force Base (AFB), New Mexico (NM), planned and executed an innovative Global Positioning System (GPS) jamming program at White Sands Missile Range (WSMR) NM. This program, known as NAVFEST, was aimed at providing low-cost, realistic, GPS jamming scenarios for testing GPS-based navigation systems, as well as training personnel in unique GPS-denied environments. Through sponsorship from the GPS Directorate and the 46th Test Group (46 TG), the 746 TS was able to provide this diverse testing and training opportunity at a significantly reduced cost to each participant. During NAVFEST, the 746 TS hosted multiple simultaneous, yet very dissimilar customers, including multi-service Department of Defense (DoD) agencies, several defense contractors, civil organizations, and two foreign nations. Their objectives ranged from training personnel on the effects of GPS jamming to characterizing the performance of prototype advanced anti-jam technologies against operationally realistic threats. To accomplish these goals, participants drove, flew, or walked through diverse jamming scenarios specifically tailored to stress the systems under evaluation. This paper details NAVFEST strategies and conduct, as well as participant objectives and future NAVFEST activities.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Roberts20112767,
author={Roberts, W. and Bavaro, M. and Tijero, E.D. and Legrand, F. and Vaccaro, S. and Gigandet, C. and Sage, A. and Thornton, C. and Hill, C. and Hernández, I.F.},
title={PRECISIO - Design, development and testing of a multi-constellation, multi-frequency software receiver},
journal={24th International Technical Meeting of the Satellite Division of the Institute of Navigation 2011, ION GNSS 2011},
year={2011},
volume={4},
pages={2767-2773},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861396435&partnerID=40&md5=a37a97c133168fe78b4bd8bd6c56eb58},
affiliation={Nottingham Scientific Ltd., Nottingham, United Kingdom; GMV Aerospace and Defence SA, Madrid, Spain; M3 Systems, 26 Lavernose, France; JAST SA, Lausanne, Switzerland; Helios, Farnborough, United Kingdom; IESSG, University of Nottingham, United Kingdom; European Commission, Brussels, Belgium},
abstract={GNSS is entering into a period of intense change over the next 5-10 years which will impact on users and operators of high-end GNSS equipment. For many years, GPS and GLONASS existed as the only systems. However, all is set to change with the advent of new global and regional systems (Galileo, Compass, IRNSS, GINS and QZSS), augmentation services and test satellites offering combined new and improved performance. Furthermore, GPS and GLONASS are also going through their own phases of modernisation with the GPS III programme and the GLONASS M and K programmes. With such a diverse range of developments and future possibilities it is difficult to see how the traditional hardware receiver solutions can keep pace with the speed of changes demanded by users and operators. The PRECISIO project is aiming to address this challenge by developing a high-end prototype multi-constellation, multi-frequency receiver based on a Software Defined Radio (SDR) approach. This paper provides a high level description of the user requirements, gap analysis, design considerations, development and initial testing of a professional grade GNSS receiver. The individual components that have been designed and developed are the antenna, RF front end, and software receiver.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2011,
author={Nguyen, M.H. and Saha, T.K. and Eghbal, M.},
title={Impact of high level of renewable energy penetration on inter-area oscillation},
journal={2011 21st Australasian Universities Power Engineering Conference, AUPEC 2011},
year={2011},
art_number={6102548},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855751877&partnerID=40&md5=d31b2c4f76663c7a4fa173ef2ded981a},
affiliation={Queensland Geothermal Energy Centre of Excellence, University of Queensland, Brisbane, QLD, Australia; School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, QLD, Australia},
abstract={Many research projects have been focused on the impacts of each type of wind turbines on the stability of the power system. Due to the rapid increase in the penetration level of renewable energy and diverse power generation portfolios, it is vital to study the performance of the power system in presence of different renewable sources of energy. The focus of this paper is to study the small signal stability of a power system with high penetration level of wind and geothermal energy. The models of three commercially available wind turbines are used to investigate inter-area mode oscillations of a power system with renewable generators located in remote areas. Moreover, the performance of the power system with HVDC and HVAC interconnections is demonstrated. Simulations are carried out on a test power system using PSS/E software. © 2011 Queensland Univ of Tech.},
author_keywords={Damping;  geothermal;  HVAC;  HVDC;  interarea oscillation;  renewable energy;  small signal stability;  wind turbine},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2011495,
author={Gao, J. and Guan, J. and Ma, A. and Tao, C. and Bai, X. and Kung, D.C.},
title={Testing configurable component-based software - Configuration test modeling and complexity analysis},
journal={SEKE 2011 - Proceedings of the 23rd International Conference on Software Engineering and Knowledge Engineering},
year={2011},
pages={495-502},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855544090&partnerID=40&md5=1271c14357a1bf03e2144545e8faef24},
affiliation={San Jose State University, United States; Tsinghua University, China; University of Texas, Arlington, United States},
abstract={As the advance of software component technology, engineers encountered different issues and challenges in testing and automation of configurable components and component-based programs. One of them is how to validate configurable components and programs to achieve adequate test criteria and support test automation. This paper uses a test model, known as a semantic tree, to assist engineers to model and analyze diverse composite components and configurable software in terms of configurable environments, organization structures and functions. Based on this model, well-defined test criteria are presented to address the adequate testing issues. In addition, the paper discusses two test complexity evaluation methods for configurable components and software. Furthermore, some case study results are reported to demonstrate the testing complexity of diverse configurations.},
author_keywords={Configurable software testing;  Configuration testing;  Test complexity;  Test criteria;  Test modeling and analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang201143,
author={Zhang, P. and Elbaum, S. and Dwyer, M.B.},
title={Automatic generation of load tests},
journal={2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings},
year={2011},
pages={43-52},
doi={10.1109/ASE.2011.6100093},
art_number={6100093},
note={cited By 57},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855462010&doi=10.1109%2fASE.2011.6100093&partnerID=40&md5=a3e53d3d0e1df5969c6236974cce59e1},
affiliation={University of Nebraska, Lincoln, NE, United States},
abstract={Load tests aim to validate whether system performance is acceptable under peak conditions. Existing test generation techniques induce load by increasing the size or rate of the input. Ignoring the particular input values, however, may lead to test suites that grossly mischaracterize a system's performance. To address this limitation we introduce a mixed symbolic execution based approach that is unique in how it 1) favors program paths associated with a performance measure of interest, 2) operates in an iterative-deepening beam-search fashion to discard paths that are unlikely to lead to high-load tests, and 3) generates a test suite of a given size and level of diversity. An assessment of the approach shows it generates test suites that induce program response times and memory consumption several times worse than the compared alternatives, it scales to large and complex inputs, and it exposes a diversity of resource consuming program behavior. © 2011 IEEE.},
author_keywords={Load testing;  symbolic execution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wei2011440,
author={Wei, Y. and Roth, H. and Furia, C.A. and Pei, Y. and Horton, A. and Steindorfer, M. and Nordio, M. and Meyer, B.},
title={Stateful testing: Finding more errors in code and contracts},
journal={2011 26th IEEE/ACM International Conference on Automated Software Engineering, ASE 2011, Proceedings},
year={2011},
pages={440-443},
doi={10.1109/ASE.2011.6100094},
art_number={6100094},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855454244&doi=10.1109%2fASE.2011.6100094&partnerID=40&md5=1204cc22b518de687156184915c1a6b8},
affiliation={Department of Software Engineering, ETH Zurich, Switzerland},
abstract={Automated random testing has shown to be an effective approach to finding faults but still faces a major unsolved issue: how to generate test inputs diverse enough to find many faults and find them quickly. Stateful testing, the automated testing technique introduced in this article, generates new test cases that improve an existing test suite. The generated test cases are designed to violate the dynamically inferred contracts (invariants) characterizing the existing test suite. As a consequence, they are in a good position to detect new faults, and also to improve the accuracy of the inferred contracts by discovering those that are unsound. Experiments on 13 data structure classes totalling over 28,000 lines of code demonstrate the effectiveness of stateful testing in improving over the results of long sessions of random testing: stateful testing found 68.4% new faults and improved the accuracy of automatically inferred contracts to over 99%, with just a 7% time overhead. © 2011 IEEE.},
author_keywords={automation;  dynamic analysis;  random testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Dhaliwal2011323,
author={Dhaliwal, J. and Onita, C.G. and Poston, R. and Zhang, X.P.},
title={Alignment within the software development unit: Assessing structural and relational dimensions between developers and testers},
journal={Journal of Strategic Information Systems},
year={2011},
volume={20},
number={4},
pages={323-342},
doi={10.1016/j.jsis.2011.03.001},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-82955198474&doi=10.1016%2fj.jsis.2011.03.001&partnerID=40&md5=8a66f244d6a37f2d7d4c2e8e8da6e3e7},
affiliation={Department of MIS, 426 Fogelman College Administration Building, University of Memphis, Memphis, TN 38152, United States; Department of MIS, 363 Fogelman College Administration Building, University of Memphis, Memphis, TN 38152, United States; Department of MIS, 300 Fogelman College Administration Building, University of Memphis, Memphis, TN 38152, United States; Computer Information Systems, College of Business, University of North Alabama, 251 One Harrison Plaza, Florence, AL 35632, United States},
abstract={Just as business-IT alignment has received significant focus as a strategic concern in the IS literature, it is also important to consider internal alignment between the diverse subunits within the IT organization. This study investigates alignment between developers and testers in software development to understand alignment within the IT unit. Prior evidence of tension between these sub-groups (and others as well) suggests that all is not necessarily well within the IT organization. Misalignment within the IT unit can certainly make it difficult for the IT unit to add strategic value to the organization. This study is an important initial step in investigating IT subunit alignment which can inform future research focusing on the alignment of other IT subunits such as architecture, operations, and customer-support. Using theoretical concepts from strategic business-IT alignment, we test a research model through a survey of professional software developers and testers. Results suggest that relational but not structural dimensions influence IT subunit alignment. © 2011 Elsevier B.V. All rights reserved.},
author_keywords={Alignment within the IT unit;  Developers and testers;  IT subunits;  Software development;  Structural and relational dimensions;  Survey research},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nakagawa201166,
author={Nakagawa, E.Y. and Maldonado, J.C.},
title={Contributions and perspectives in architectures of software testing environments},
journal={Proceedings - 25th Brazilian Symposium on Software Engineering, SBES 2011},
year={2011},
pages={66-71},
doi={10.1109/SBES.2011.42},
art_number={6065147},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-82255189646&doi=10.1109%2fSBES.2011.42&partnerID=40&md5=fe91f77d92d559612cf8b99cd60a247c},
affiliation={Dept. of Computer Systems, University of São Paulo - USP, PO Box 668, 13560-970, São Carlos, SP, Brazil},
abstract={Producing high quality software systems has been one of the most important software development concerns. In this perspective, Software Architecture and Software Testing are two important research areas that have contributed in that direction. The attention given to the software architecture has played a significant role in determining the success of software systems. Otherwise, software testing has been recognized as a fundamental activity for assuring the software quality; however, it is an expensive, error-prone, and time consuming activity. For this reason, a diversity of testing tools and environments has been developed; however, they have been almost always designed without an adequate attention to their evolution, maintenance, reuse, and mainly to their architectures. Thus, this paper presents our main contributions to systematize the development of testing tools and environments, aiming at improving their quality, reuse, and productivity. In particular, we have addressed architectures for software testing tools and environments and have also developed and made available testing tools. We also state perspectives of research in this area, including open research issues that must be treated, considering the unquestionable relevance of testing automation to the testing activity. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Joshi2011171,
author={Joshi, P. and Gunawi, H.S. and Sen, K.},
title={PREFAIL: A programmable tool for multiple-failure injection},
journal={Proceedings of the Conference on Object-Oriented Programming Systems, Languages, and Applications, OOPSLA},
year={2011},
pages={171-187},
doi={10.1145/2048066.2048082},
note={cited By 36},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81455154903&doi=10.1145%2f2048066.2048082&partnerID=40&md5=cf86497685281c59dd0e160e2b0e1725},
affiliation={EECS, UC Berkeley, United States},
abstract={As hardware failures are no longer rare in the era of cloud computing, cloud software systems must "prevail" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures.We integrate PreFail to three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X-200X less time than exhaustive testing. Copyright is held by the author / owner(s).},
author_keywords={Distributed systems;  Fault injection;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li-Ming2011186,
author={Li-Ming, P. and Chuan-Qi, P. and Wei-Tao, Z. and Pu, H. and Qiang, H.},
title={Design and development of scoring system of test of the actual competition ability of basketball under separate enrollment system in physical education},
journal={Proceedings of the 2011 International Conference on Future Computer Science and Education, ICFCSE 2011},
year={2011},
pages={186-191},
doi={10.1109/ICFCSE.2011.54},
art_number={6041641},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155175774&doi=10.1109%2fICFCSE.2011.54&partnerID=40&md5=ed3114707656cd5a9c56befee3a583b4},
affiliation={School of Transportation, Wuhan University of Technology, Wuhan, 430063, China; Department of Sports Engineering and Information Technology, Key Lab. of Sports Engineering of State Sport Bureau, Wuhan Institute of Physical Education, Wuhan, 430079, China},
abstract={The national Separate Enrollment in physical education is an independent examination set by the state general sport administration including tests of Extracurricular courses and tests of specialty. Test of Basketball, one of the special tests, consists of 60-metre running, approach run and touch basket board, and shooting in limited time, dribble and shoot from diverse directions, and actual competition ability. This system realizes referees scoring and electronic display scene shows in the five to five basketball test. ASP.NET language and SQL Server database are adopted to complete such functions as roll-calling and grouping, ratings and scoring, printing, and successfully applied in test of specialty. © 2011 IEEE.},
author_keywords={program design;  serial communication;  test of specialty;  the national Separate Enrollment in PE;  wireless device for scoring},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bai2011149,
author={Bai, X. and Lu, H. and Zhang, Y. and Zhang, R. and Hu, L. and Ye, H.},
title={Interface-based automated testing for open software architecture},
journal={Proceedings - International Computer Software and Applications Conference},
year={2011},
pages={149-154},
doi={10.1109/COMPSACW.2011.34},
art_number={6032229},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80055004463&doi=10.1109%2fCOMPSACW.2011.34&partnerID=40&md5=9ca200958b8ea04412e2726e475b8988},
affiliation={Department of Computer Science and Engineering, INLIST, Tsinghua University, Beijing, China; Aeronautical Computing Technique Research Institute, Xi'an, China},
abstract={Open Software Architecture (OSA) has been a prevalent design principle for integrating large, complex software systems from components. In OSA, interface specifications provide standard representations of the exposed software functionalities and constraints. Using an industry OSA system, the paper investigates the potential to extract domain model from standard interface specifications and to automate testing following the model driven approach. It focuses on modeling of service external behavior from the syntax and semantics defined by OSA interface standards. The domain model can be translated into test cases, either encoded in XML or specific programming languages, by predefined mapping mechanisms. The generated test scripts can be further compiled with target interfaces and executed under control. In this way, the domain models and test cases can be reused throughout system integration and regression testing, and for testing diversified component implementations following the same interface standards. © 2011 IEEE.},
author_keywords={Automated testing;  Interface-based testing;  Model-based testing;  Open Software Architecture},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bell2011,
author={Bell, R.M. and Ostrand, T.J. and Weyuker, E.J.},
title={Does measuring code change improve fault prediction?},
journal={ACM International Conference Proceeding Series},
year={2011},
doi={10.1145/2020390.2020392},
art_number={2020392},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054077408&doi=10.1145%2f2020390.2020392&partnerID=40&md5=b4e185d3901bd364a860d68613d2bc87},
affiliation={AT and T Labs - Research, 180 Park Avenue, Florham Park, NJ 07932, United States},
abstract={Background: Several studies have examined code churn as a variable for predicting faults in large software systems. High churn is usually associated with more faults appearing in code that has been changed frequently. Aims: We investigate the extent to which faults can be predicted by the degree of churn alone, whether other code characteristics occur together with churn, and which combinations of churn and other characteristics provide the best predictions. We also investigate different types of churn, including both additions to and deletions from code, as well as overall amount of change to code. Method: We have mined the version control database of a large software system to collect churn and other software measures from 18 successive releases of the system. We examine the frequency of faults plotted against various code characteristics, and evaluate a diverse set of prediction models based on many different combinations of independent variables, including both absolute and relative churn. Results: Churn measures based on counts of lines added, deleted, and modified are very effective for fault prediction. Individually, counts of adds and modifications outperform counts of deletes, while the sum of all three counts was most effective. However, these counts did not improve prediction accuracy relative to a model that included a simple count of the number of times that a file had been changed in the prior release. Conclusions: Including a measure of change in the prior release is an essential component of our fault prediction method. Various measures seem to work roughly equivalently. Categories and Subject Descriptors: D.2.5 [Software Engineering]: Testing and Debugging - Debugging aids General Terms: Experimentation Copyright © 2011 ACM.},
author_keywords={Code churn;  Empirical study;  Fault prediction;  Fault-percentile average;  Software faults},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ridene2011,
author={Ridene, Y. and Barbier, F.},
title={A model-driven approach for automating mobile applications testing},
journal={ACM International Conference Proceeding Series},
year={2011},
doi={10.1145/2031759.2031769},
art_number={9},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053907180&doi=10.1145%2f2031759.2031769&partnerID=40&md5=bb77b3c07b34b9fa2286ba05f9e61cff},
affiliation={LIUPPA, University of Pau, Avenue de l'université, 64013 Pau, France},
abstract={Software testing faces up several challenges. One out of these is the opposition between time-to-market software delivery and the excessive length of testing activities. The latter results from the growth of the application complexity along with the diversity of handheld devices. The economical competition, branding impose zero-defect products, putting forward testing as an even more crucial activity. In this paper, we describe a Domain-Specific Modeling Language (DSML) built upon an industrial platform (a test bed) which aims to automate mobile application checking. A key characteristic of this DSML is its ability to cope with variability in the spirit of software product line engineering. We discuss this DSML as part of a tool suite enabling the test of remote devices having variable features. Copyright © 2011 ACM.},
author_keywords={Mobile software;  Model-driven engineering;  Testing;  Variability management},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Srinivasan2011101,
author={Srinivasan, R. and Blough, D.M.},
title={Throughput optimization in MIMO mesh networks},
journal={Proceedings of the Annual International Conference on Mobile Computing and Networking, MOBICOM},
year={2011},
pages={101-102},
doi={10.1145/2030718.2030744},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053611048&doi=10.1145%2f2030718.2030744&partnerID=40&md5=0c30e1c5baaa75528b358ea2d61250fd},
affiliation={Electrical and Computer Engineering, Georgia Institute of Technology, Atlanta, GA, United States},
abstract={Enabling routers in wireless mesh networks (WMNs) with MIMO capability boosts throughput by allowing for spatial multiplexing(SM), interference cancellation(IC) and spatial diversity. Antenna elements can be divided arbitrarily in accordance with the diversity-multiplexing-interference suppression trade-off. To the best of our knowledge, we were the first to fully account for this three way trade-off in designing and modeling practical stream allocation algorithms over arbitrary MIMO mesh networks[1]. We did this by addressing the problem of verifying the feasibility of a given stream allocation over the network and developing efficient and accurate feasibility tests that enable us to develop our near-optimal stream allocation algorithms. Existing MIMO stream allocation/link scheduling algorithms operate by iteratively checking for feasibility. While this is tractable in non-MIMO systems, it becomes computationally hard in the MIMO case. In building, scalable and accurate allocation algorithm(s) which can be used as a building block(s) by existing link scheduling algorithms, our contribution[1] is unique. In this work, we propose improvements upon the scalability and accuracy of our stream allocation heuristics and demonstrate results for certain network scenarios. Further, we propose a plan to implement a software defined radio(SDR) based test bed to prototype a MIMO mesh network. Very little is known about the behavior of MIMO link rates when part of the link's resources are used for IC. Toward this end, we specify the experimental set-ups that we will use to characterize MIMO link behavior at the physical layer level and to develop important practical insights into the network-wide optimization. © 2011 Authors.},
author_keywords={channel state information(CSI);  conflict graph density(CGD);  software defined radio(SDR);  stream allocation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Joshi2011171,
author={Joshi, P. and Gunawi, H.S. and Sen, K.},
title={PREFAIL: A programmable tool for multiple-failure injection},
journal={ACM SIGPLAN Notices},
year={2011},
volume={46},
number={10},
pages={171-187},
doi={10.1145/2076021.2048082},
note={cited By 33},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858330582&doi=10.1145%2f2076021.2048082&partnerID=40&md5=ee20c14d3c55ac40474d09e5ea7b19fe},
affiliation={EECS, UC Berkeley, United States},
abstract={As hardware failures are no longer rare in the era of cloud computing, cloud software systems must "prevail" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures.We integrate PreFail to three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X-200X less time than exhaustive testing.. Copyright © 2011 ACM.},
author_keywords={Distributed systems;  Fault injection;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Sullivan2011210,
author={Sullivan, K.T.},
title={Quality management programs in the construction industry: Best value compared with other methodologies},
journal={Journal of Management in Engineering},
year={2011},
volume={27},
number={4},
pages={210-219},
doi={10.1061/(ASCE)ME.1943-5479.0000054},
note={cited By 55},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-81855184564&doi=10.1061%2f%28ASCE%29ME.1943-5479.0000054&partnerID=40&md5=c66557f627071cbf5820eb7e1759d63a},
affiliation={Arizona State Univ., Del E. Webb School of Construction, Tempe, AZ, United States},
abstract={The drive to maintain competitiveness by increasing performance has been an ever-present goal of industries within the global market. Although many industries have benefited from classical quality management programs such as total quality management (TQM), lean production, and six sigma, the construction industry has remained primarily unaffected. This paper analyzes these three popular programs, the basis for their success and failures, and their documented level of susceptibility in the construction industry. These programs are then contrasted to the best value system, an owner-driven quality program that has been tested recently in the construction industry and documented to produce encouraging results. On the basis of the findings, it is proposed that most quality management programs are designed to be instigated by the vendor, by improving the company's ability to deliver a quantifiable, replicable product or service. This is significant because it indicates that although the underlying principles of the classic quality management programs are relevant to all markets, the processes and methods of application may be inappropriate for an industry that dispenses highly diverse or integrated products or services, such as construction. © 2011 American Society of Civil Engineers.},
author_keywords={Best value;  Construction industry;  Lean;  Quality control;  Quality management;  Six sigma;  TQM},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang20111363,
author={Wang, J. and Cui, K. and Zhou, K. and Li, X.},
title={Dynamic detection of interrupt overloads in embedded software using a genetic algorithm},
journal={Qinghua Daxue Xuebao/Journal of Tsinghua University},
year={2011},
volume={51},
number={10},
pages={1363-1368},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80355141989&partnerID=40&md5=d4909923b76732d52f165a23b5195b06},
affiliation={Department of Embedded System Engineering, Dalian University of Technology, Dalian 116620, China},
abstract={Embedded software needs to deal with many interrupts, but random interrupts lead to test difficulties in embedded systems which are becoming more and more complicated. A method is given to enable dynamic detection of interrupt overloads based on a genetic algorithm. The algorithm handles diverse types of interrupts with non-uniform searches giving improved local searches to construct an interrupt handling sequence to generate the stack depth and detect stack overflow errors during interrupt execution. Tests on a SPARC V8 architecture Virtualsparc simulation platform show that this method quickly detects the loss rate of non-executed interrupts in the stack. Dynamic changes in the stack depth greatly reduce the interrupt overload loss rate with increased detection efficiency of interrupt faults in embedded software.},
author_keywords={Genetic algorithm;  Interrupt overload;  Stack overflow},
document_type={Article},
source={Scopus},
}

@ARTICLE{Furness2011132,
author={Furness, P.},
title={Applications of Monte Carlo Simulation in marketing analytics},
journal={Journal of Direct, Data and Digital Marketing Practice},
year={2011},
volume={13},
number={2},
pages={132-147},
doi={10.1057/dddmp.2011.25},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80155205411&doi=10.1057%2fdddmp.2011.25&partnerID=40&md5=9a08b40d9adfb94c5576c791fc8a370c},
affiliation={Peter Furness Limited, United States},
abstract={Monte Carlo Simulation (MCS), originally developed in the 1940s for use in nuclear weapons design, is playing an increasing role in commercial applications, including marketing and Customer Relationship Management (CRM). It provides an efficient way to simulate processes involving chance and uncertainty and can be applied in areas as diverse as market sizing, customer lifetime value measurement and customer service management. This paper examines the history of MCS and presents an illustrative example to explain the basic principles of the technique. Three case studies from marketing and CRM, which underline the importance of MCS to marketing analytics, are described. Some key issues, including the drawbacks and pitfalls of MCS, are covered. The paper also considers the future, with MCS applied in the digital world, and concludes with a review of relevant software tools. © 2011 MacMillan Publishers Ltd.},
author_keywords={CRM;  customer service management;  Monte Carlo Simulation;  predictive analytics;  software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Oliveira2011405,
author={Oliveira, L.B.R. and Nakagawa, E.Y.},
title={A service-oriented reference architecture for software testing tools},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6903 LNCS},
pages={405-421},
doi={10.1007/978-3-642-23798-0_42},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053209159&doi=10.1007%2f978-3-642-23798-0_42&partnerID=40&md5=eebc4c105646c6e3084d2b27d25d6212},
affiliation={Department of Computer Systems, University of São Paulo - USP, PO Box 668, 13560-970, São Carlos, SP, Brazil},
abstract={Software testing is recognized as a fundamental activity for assuring software quality. Aiming at supporting this activity, a diversity of testing tools has been developed, including tools based on SOA (Service-Oriented Architecture). In another perspective, reference architectures have played a significant role in aggregating knowledge of a given domain, contributing to the success in the development of systems for that domain. However, there exists no reference architecture for the testing domain that contribute to the development of testing tools based on SOA. Thus, the main contribution of this paper is to present a service-oriented reference architecture, named RefTEST-SOA (Reference Architecture for Software Testing Tools based on SOA), that comprises knowledge and experience about how to structure testing tools organized as services and pursues a better integration, scalability, and reuse provided by SOA to such tools. Results of our case studies have showed that RefTEST-SOA is a viable and reusable element to the development of service-oriented testing tools. © 2011 Springer-Verlag.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Herbold2011172,
author={Herbold, S. and Grabowski, J. and Waack, S.},
title={A model for usage-based testing of event-driven software},
journal={2011 5th International Conference on Secure Software Integration and Reliability Improvement - Companion, SSIRI-C 2011},
year={2011},
pages={172-178},
doi={10.1109/SSIRI-C.2011.32},
art_number={6004520},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053040736&doi=10.1109%2fSSIRI-C.2011.32&partnerID=40&md5=9602ce2463e0c3932655e4bd19d89763},
affiliation={Intitute of Computer Science, Georg-August-Universität Göttingen, Göttingen, Germany},
abstract={Event-driven software is very diverse, e.g., in form of Graphical User Interfaces (GUIs), Web applications, or embedded software. Regardless of the application, the challenges for testing event-driven software are similar. Most event-driven systems allow a huge number of possible event sequences, which makes exhaustive testing infeasible. As a possible solution, usage-based testing has been proposed for several types of event-driven software. However, previous work has always focused on one type of event-driven software. In this paper, we propose a usage-based testing model for event-driven software in general. The model is divided into three layers to provide a maximum of platform independence while allowing interoperability with existing platform dependent solutions. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ghanam2011139,
author={Ghanam, Y. and Maurer, F.},
title={Using acceptance tests for incremental elicitation of variability in requirements: An observational study},
journal={Proceedings - 2011 Agile Conference, Agile 2011},
year={2011},
pages={139-142},
doi={10.1109/AGILE.2011.21},
art_number={6005496},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052980639&doi=10.1109%2fAGILE.2011.21&partnerID=40&md5=368302e9429e35eed01353c7a01536f4},
affiliation={Department of Computer Science, University of Calgary, Calgary, AB, Canada},
abstract={Variability in software systems refers to the notion that the components constituting the software may vary due to a range of factors including diverse customer needs, technical constraints, and business strategies. Traditionally, variability has been treated proactively by investing in an upfront domain analysis phase. Such proactive treatment of requirements is not encouraged in agile environments. This paper provides an observational study examining a reactive approach to variability wherein acceptance tests are used to elicit variability from requirements in an incremental manner. The findings suggest the following: the approach does support the evolutionary nature of agile development; the approach is easy and quick to learn; using acceptance tests yields consistent variability interpretations; and acceptance tests-on their own-may be insufficient to reflect implicit variability constraints. © 2011 IEEE.},
author_keywords={Acceptance tests;  Feature models;  Reuse;  Software product lines;  Variability},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Peng201131,
author={Peng, K. and Wang, C. and Li, Y. and Wu, Z. and Sun, C.},
title={Design of a typical medium-low voltage microgrid network},
journal={Dianli Xitong Zidonghua/Automation of Electric Power Systems},
year={2011},
volume={35},
number={18},
pages={31-35},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053974590&partnerID=40&md5=7506f319d9ba3ca648d6ac6ac570e141},
affiliation={Key Laboratory of Power System Simulation and Control of Ministry of Education, Tianjin University, Tianjin 300072, China},
abstract={A typical medium voltage and low voltage microgrid is designed for the actual distribution system in China. Multiple distribution generation and energy storage systems are considered, including the photovoltaic system, wind power system and battery storage system. The flexible structure and diverse operation modes can meet various research demands for microgrid research. Finally, simulation tests are performed on a commercial simulation software DIgSILENT, and simulation results verify the rationality of the microgrid network proposed. © 2011 State Grid Electric Power Research Institute Press.},
author_keywords={Distributed generation;  Distribution network;  Example system;  Microgrid;  Simulation test},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gao2011213,
author={Gao, H. and Wang, Y. and Wang, L. and Liu, L. and Li, J. and Cheng, X.},
title={Trojan characteristics analysis based on stochastic petri nets},
journal={Proceedings of 2011 IEEE International Conference on Intelligence and Security Informatics, ISI 2011},
year={2011},
pages={213-215},
doi={10.1109/ISI.2011.5984084},
art_number={5984084},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052897203&doi=10.1109%2fISI.2011.5984084&partnerID=40&md5=3a647bcaef9852854a29586f0b41600b},
affiliation={Institute of Computing Technology, Chinese Academy of Sciences, Beijing 100190, China; University of Science and Technology Beijing, Beijing, 100083, China},
abstract={Trojan's attack behavior has become increasingly common and diversifiable. How to judge Trojan-like features of the softwares which the users download has become the problem that the users concern about. In this paper, we first capture the software's behavior and related parameters from our virtual software test bed, then a modeling method using Stochastic Petri Nets is proposed, which supports quantitative analysis for the application software's behaviors. Based on the model, the similarity degree between application software and Trojan software is analyzed quantitatively. This analysis show that the model can be used to design an effective anti-Trojan system. The paper concludes with an example to illustrate the effectiveness of the model and analysis method. © 2011 IEEE.},
author_keywords={attack model;  remote accesss;  security behavior;  Stochastic Petri Nets;  Trojan horse},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Song2011,
author={Song, Y.},
title={The path analysis of diversification strategy and capital structure in the listed companies},
journal={International Conference on Management and Service Science, MASS 2011},
year={2011},
doi={10.1109/ICMSS.2011.5999354},
art_number={5999354},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052892283&doi=10.1109%2fICMSS.2011.5999354&partnerID=40&md5=03c3f4e3fbcc29eaf31d3f5dd764ca6e},
affiliation={Economic and Management School, Wuhan University, Wuhan, Hubei, China},
abstract={Corporation diversification strategy is a trend. Will the past and existing diversification strategy choice affect the company's now and future decision? Moreover will the past and existing capital structure also affect the company's now and future strategic choice? This paper tests the causality of diversification strategy and capital structure by four structure equation models and AMOS6.0 software. The results reveal there is a significant causal relationship among capital structures and among company strategies. There exists a interaction between capital structure and company strategy. © 2011 IEEE.},
author_keywords={Capital structure;  Diversification strategy;  Related diversification;  Unrealted diversification},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Colanzi2011188,
author={Colanzi, T.E. and Assunção, W.K.G. and Vergilio, S.R. and Pozo, A.},
title={Integration test of classes and aspects with a multi-evolutionary and coupling-based approach},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6956 LNCS},
pages={188-203},
doi={10.1007/978-3-642-23716-4_18},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052872148&doi=10.1007%2f978-3-642-23716-4_18&partnerID=40&md5=0e14cb2b9afb18e3bf0e27048b39ce91},
affiliation={DInf - Federal University of Paraná, CP: 19081, CEP 19031-970, Curitiba, Brazil},
abstract={The integration test of aspect-oriented systems involves the determination of an order to integrate and test classes and aspects, which should be associated to a minimal possible stubbing cost. To determine such order is not trivial because different factors influence on the stubbing process. Many times these factors are in conflict and diverse good solutions are possible. Due to this, promising results have been obtained with multi-objective and evolutionary algorithms that generally optimize two coupling measures: number of attributes and methods. However, the problem can be more effectively addressed considering as many as coupling measures could be associated to the stubbing process. Therefore, this paper introduces MECBA, a Multi-Evolutionary and Coupling-Based Approach to the test and integration order problem, which includes the definition of models to represent the dependency between modules and to quantify the stubbing costs. The approach is instantiated and evaluated considering four AspectJ programs and four coupling measures. The results represent a good trade-off between the objectives and an example of use of the obtained results shows how they can be used to reduce test effort and costs. © 2011 Springer-Verlag.},
author_keywords={aspect-oriented software;  Integration testing;  multi-objective evolutionary algorithms},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Li2011834,
author={Li, D. and Huang, X. and Yang, X. and Jiao, X.},
title={Research on auto-composing test paper system based on improved genetic algorithm},
journal={ICSESS 2011 - Proceedings: 2011 IEEE 2nd International Conference on Software Engineering and Service Science},
year={2011},
pages={834-837},
doi={10.1109/ICSESS.2011.5982470},
art_number={5982470},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052469799&doi=10.1109%2fICSESS.2011.5982470&partnerID=40&md5=bb741b392fb7b158aa1bf71ccbf1092f},
affiliation={Department of Computer Science and Technology, Nanyang Institute of Technology, Nanyang, China; Information and Engineering Department, Henan School of Economics and Management, Nanyang, China},
abstract={Comosing test paper is an important part of examination system. Based on the researches on coding policy, fitness faction, genetic operation and control parameter, an improved genetic algorithm is advanced for the auto-composing test paper system. It is an efficient way to overcome the premature convergence and the genetic drifting, and at the same time,to prevent the colony coming into the partial optimal solution considering the colony diversity by scaling the fitness faction and building the adaptive crossover probability and mutation probability. Experiment shows that the improved genetic algorithm cold compose test paper more efficien. © 2011 IEEE.},
author_keywords={auto-composing test paper;  fitness faction;  genetic algorithm},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hou2011742,
author={Hou, C. and Wang, Q. and Ren, Z.},
title={One test case generation method for SW&HW reliability co-testing},
journal={ICRMS'2011 - Safety First, Reliability Primary: Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety},
year={2011},
pages={742-745},
doi={10.1109/ICRMS.2011.5979362},
art_number={5979362},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052444577&doi=10.1109%2fICRMS.2011.5979362&partnerID=40&md5=e52e822fb66365e3261dc10d91d50883},
affiliation={AVIC Aero-Polytechnology Establishment, Beijing, China},
abstract={Today, more and more software-intensive equipments are used in weapons, and their reliability is gradually being looked closely. For these equipments, software and hardware are coupling; their reliability can't be verified separately. Design testing profile is an important work to reliability verification. Hardware test profile design method is mature relatively. Software test profile design and test case generation method is developing. Although several operation profile construction methods have been proposed, they are not fit for hardware & software (SW&HW) co-testing. This paper combines testing profile of hardware, with utilized information of mission profile and environment profile, divides operation profile into three layers and model by diverse UML diagrams. This method considers both input parameters complexity and real-time requirements. It can be used in operation profile design and test case generation of SW&HW reliability co-testing. © 2011 IEEE.},
author_keywords={Reliability Co-testing;  SW&HW;  Test Case Generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ao2011700,
author={Ao, Q. and Ai, J. and Lu, M. and Zhong, F.},
title={Scenario-based software operational profile},
journal={ICRMS'2011 - Safety First, Reliability Primary: Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety},
year={2011},
pages={700-704},
doi={10.1109/ICRMS.2011.5979355},
art_number={5979355},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052442717&doi=10.1109%2fICRMS.2011.5979355&partnerID=40&md5=df48591e3134264ae76e2cd878afacdf},
affiliation={School of Reliability and System Engineering, Beihang University, Beijing, China},
abstract={As the diversity of software status and the uncertainty of software operation, the traditional methods to construct software reliability operational profile are difficult to be applied to reliability testing. This paper introduces an approach to generate operational profile based on scenario. It partitions the usage of software into various paths of software running according to the users and missions. With the proposal of scenario, a relation between user's activities and software's execution is established. Through the correlation which allows software inputs to be obtained from system usage, the generation of reliability test data could be implemented. The effect of this approach is illustrated with an example of missile control demonstration software (MCDS). © 2011 IEEE.},
author_keywords={operational profile;  scenario;  Software reliability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lai2011717,
author={Lai, J. and Zhang, H. and Huang, B.},
title={The object-FMA based test case generation approach for GUI software exception testing},
journal={ICRMS'2011 - Safety First, Reliability Primary: Proceedings of 2011 9th International Conference on Reliability, Maintainability and Safety},
year={2011},
pages={717-723},
doi={10.1109/ICRMS.2011.5979358},
art_number={5979358},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052434672&doi=10.1109%2fICRMS.2011.5979358&partnerID=40&md5=23b6077372e4ca4d7fb7c6e44e63ac8e},
affiliation={School of Reliability and System Engineering, BeiHang University, Beijing, China},
abstract={The traditional exception testing usually utilizes the error-guessing approach or the equivalence class-partition method to generate test cases, which heavily depend on the experience of testers and easily make the exception testing omissive. In order to solve this problem, this paper introduces the SFMEA (Software Failure Mode Effect Analysis) to generate the exception testing cases for the GUI software by analyzing the failure modes of the controls and the control sets of the GUI software and then translating those failure modes directly into the exception test cases. In order to make the failure mode analysis sufficient, we first propose an object-based approach for the failure mode analysis (i.e. Object-FMA), and then utilize this approach to analyze the failure modes of the common controls in the Windows, and generate the database of the failure modes of the controls for guiding to design the test cases. In an actual GUI software-testing project, a case study is presented through comparing four diverse exception test suites. Three test groups with different experience use the error-guessing approach to generate three exception test suites respectively. Then one group is selected from these three groups to use this proposed Object-FMA approach to generate one exception test suite. The comparison results show that the exception testing cases generated by the Object-FMA approach not only are more sufficient than the ones generated by the error-guessing approach, but also detect more exceptions. This proposed Object-FMA approach can avoid to overreliance on the experience of testers during designing the exception testing cases. Moreover, this approach can ensure the quality of the exception testing cases from the methodological viewpoint. Thus, the feasibility and validity of this proposed Object-FMA approach are validated consequently. © 2011 IEEE.},
author_keywords={exception testing;  GUI software;  Object-FMA;  SFMEA},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Doong2011401,
author={Doong, H.},
title={An evaluation of whether the psychological traits of managers are relevant to their intentions to purchase e-government software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6866 LNCS},
pages={401-408},
doi={10.1007/978-3-642-22961-9_31},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052348953&doi=10.1007%2f978-3-642-22961-9_31&partnerID=40&md5=406fd24f7db9777a8c7f17b327994af0},
affiliation={Department of Management Information Systems, National Chiayi University, 580 Sinmin Rd., 60054 Chiayi, Taiwan},
abstract={Managers have played a double identity of electronic government (e-government) software implementation within their organizations. On the one hand, they are potential sponsors of the software applications. On the other, they are also explicit users. However, past e-government software and information systems studies have overlooked this unique nature. While individual innovativeness has been asserted to be significantly related to innovation adoption behavior, an increasing number of studies have evidenced that innovativeness alone does not command innovation adoption behavior. In such cases, individual involvement may play a ruling effect on innovative behaviors. Therefore, this study empirically tested a model that was developed to assess whether the factors affecting the manager's decision for using or purchasing e-government software may be diverse. The experiment involved 56 managers from different functional departments in governments, and the present study proposes managerial implications according to the results. © 2011 Springer-Verlag Berlin Heidelberg.},
author_keywords={E-government;  Innovativeness;  Involvement},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Arcuri2011265,
author={Arcuri, A. and Briand, L.},
title={Adaptive random testing: An illusion of effectiveness?},
journal={2011 International Symposium on Software Testing and Analysis, ISSTA 2011 - Proceedings},
year={2011},
pages={265-275},
doi={10.1145/2001420.2001452},
note={cited By 92},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051947344&doi=10.1145%2f2001420.2001452&partnerID=40&md5=8bc9818701e9602cf91be2cafbd1bf09},
affiliation={Simula Research Laboratory, P.O. Box 134, 1325 Lysaker, Norway; University of Oslo, P.O. Box 134, 1325 Lysaker, Norway},
abstract={Adaptive Random Testing (ART) has been proposed as an enhancement to random testing, based on assumptions on how failing test cases are distributed in the input domain. The main assumption is that failing test cases are usually grouped into contiguous regions. Many papers have been published in which ART has been described as an effective alternative to random testing when using the average number of test case executions needed to find a failure (F-measure). But all the work in the literature is based either on simulations or case studies with unreasonably high failure rates. In this paper, we report on the largest empirical analysis of ART in the literature, in which 3727 mutated programs and nearly ten trillion test cases were used. Results show that ART is highly inefficient even on trivial problems when accounting for distance calculations among test cases, to an extent that probably prevents its practical use in most situations. For example, on the infamous Triangle Classification program, random testing finds failures in few milliseconds whereas ART execution time is prohibitive. Even when assuming a small, fixed size test set and looking at the probability of failure (P-measure), ART only fares slightly better than random testing, which is not sufficient to make it applicable in realistic conditions. We provide precise explanations of this phenomenon based on rigorous empirical analyses. For the simpler case of single-dimension input domains, we also perform formal analyses to support our claim that ART is of little use in most situations, unless drastic enhancements are developed. Such analyses help us explain some of the empirical results and identify the components of ART that need to be improved to make it a viable option in practice. © 2011 ACM.},
author_keywords={distance;  diversity;  F-measure;  faulty region;  P-measure;  random testing;  shape;  similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2011353,
author={Zhang, S. and Saff, D. and Bu, Y. and Ernst, M.D.},
title={Combined static and dynamic automated test generation},
journal={2011 International Symposium on Software Testing and Analysis, ISSTA 2011 - Proceedings},
year={2011},
pages={353-363},
doi={10.1145/2001420.2001463},
note={cited By 80},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051934401&doi=10.1145%2f2001420.2001463&partnerID=40&md5=1fcc102ae4d1c1c3859518a6b92b115f},
affiliation={University of Washington, United States; Google, Inc., United States; University of California, Irvine, CA, United States},
abstract={In an object-oriented program, a unit test often consists of a sequence of method calls that create and mutate objects, then use them as arguments to a method under test. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible. This paper proposes a combined static and dynamic automated test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests. Our Palus tool implements this testing approach. We compared its effectiveness with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on several popular open-source Java programs. Tests generated by Palus achieved higher structural coverage and found more bugs. Palus is also internally used in Google. It has found 22 previously-unknown bugs in four well-tested Google products. © 2011 ACM.},
author_keywords={automated test generation;  dynamic analyses;  static},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pinheiro2011779,
author={Pinheiro, E. and Postolache, O. and Girão, P.},
title={Robust heart rate estimation from cardiovascular signals unobtrusively acquired in a wheelchair},
journal={Conference Record - IEEE Instrumentation and Measurement Technology Conference},
year={2011},
pages={779-783},
doi={10.1109/IMTC.2011.5944037},
art_number={5944037},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051902356&doi=10.1109%2fIMTC.2011.5944037&partnerID=40&md5=4a8754c10961606ae60d65a106d3bc8c},
affiliation={Instituto de Telecomunicações, DEEC/Instituto Superior Técnico, Lisbon, Portugal; Instituto de Telecomunicações, Instituto Politécnico de Setúbal, Lisbon, Portugal},
abstract={Many wheelchair users have augmented cardiac risks, namely diabetics, and stroke victims in recovery. This paper describes the developments taken to assess a wheelchair user's heart rate from inconspicuous cardiac measurements. Unremarkably acquired cardiac signals are greatly affected by artifacts, generated by wheelchair motion, and the subject's daily actions. Furthermore, the impedance plethysmogram (IPG) and the ballistocardiogram (BCG), two signals with significant information on the cardiovascular system, suffer significant morphological modifications from patient to patient, and when his posture changes. To robustly estimate the heart rate in these difficult circumstances, it was applied a real-time sine fitting methodology, an approach never used in biological signals. This method applies a sliding power-window to abstract the cardiac signal morphology, and then uses 7 parameter sine fitting to obtain the fundamental frequency of the signal, the heart rate. Sensing hardware was embedded in a manual wheelchair to acquire the subject's IPG and backrest and seat BCG. This implementation allows continuous monitoring of cardiac activity without more mobility limitations. Validation tests of the hardware and software show that the developments made are able to cope with the signals' variations, and provide accurate estimates of heart rate for diverse subject's under various conditions. © 2011 IEEE.},
author_keywords={Ballistocardiogram;  biomedical monitoring;  heart rate;  impedance plethysmogram;  sine wave fitting;  wheelchairs},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Walter2011465,
author={Walter, B. and Martenka, P.},
title={Looking for patterns in code bad smells relations},
journal={Proceedings - 4th IEEE International Conference on Software Testing, Verification, and Validation Workshops, ICSTW 2011},
year={2011},
pages={465-466},
doi={10.1109/ICSTW.2011.89},
art_number={5954448},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051610198&doi=10.1109%2fICSTW.2011.89&partnerID=40&md5=150843e2b8090c60a72a269f1410c5b9},
affiliation={Institute of Computing Science, Poznań University of Technology, Poznań, Poland},
abstract={Code smells are the named design anomalies that indicate to a need for refactoring. Due to their diverse and ambiguous nature, their identification in code requires complex methods. In the paper we formulate a hypothesis that some smells make associations that are repeatable and can be treated as patterns. We present also early results of investigation of two Large Class-related patterns. © 2011 IEEE.},
author_keywords={Code smells;  Patterns;  Refactoring},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ko2011,
author={Ko, J.-W. and Sim, S.-H. and Song, Y.-J.},
title={Test based model transformation framework for mobile application},
journal={2011 International Conference on Information Science and Applications, ICISA 2011},
year={2011},
doi={10.1109/ICISA.2011.5772373},
art_number={5772373},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960230879&doi=10.1109%2fICISA.2011.5772373&partnerID=40&md5=24bca2661ef4005538c9a6511dcd17a2},
affiliation={Department of Computer Science, Kyung-Hee University, Yong-In, South Korea},
abstract={In order to easily port mobile applications suitable for each platform, that have been developed under diverse development environment for individual wireless communication service providers, or redevelop them on a specific platform, it is required to reuse them at software model level that is a software development paradigm for MDA (Model Driven Architecture). The existing model transformation tools such as UMT,MTL,ATL have focused on various transformation format supports, scalability or applicability of model transformation mechanism itself or how it is easy to understand transformation rules without verification of the transformation model generated through mainly model transformation engines. The test based model transformation framework proposed on this paper generates prediction model by defining test Oracle as model transformation rules that may conduct verification test of the generated transformation model, in order to support verification on verification of the converted model through MDA based model transformation mechanism. By comparing this prediction model with the target model, it is possible to execute verification test on the converted model. Therefore, by increasing reliability of model transformation and further applying test issues on the software development process to the software mode at software design phase, it is possible to reduce modification cost through advantage to predict or find out any system error earlier than test to be progressed after implementation of the existing source codes. © 2011 IEEE.},
author_keywords={MDA;  Model comprison algorithms;  Model transformation approach},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ko2011256,
author={Ko, J.-W. and Jeong, H.-Y. and Song, Y.-J.},
title={A study on model transformation mechanism using graph comparison algorithms and software model property information},
journal={Communications in Computer and Information Science},
year={2011},
volume={184 CCIS},
number={PART 1},
pages={256-264},
doi={10.1007/978-3-642-22333-4_33},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960119049&doi=10.1007%2f978-3-642-22333-4_33&partnerID=40&md5=246583daa6b07763ae506d67c3ef18e6},
affiliation={Dept. Computer Engineering, Kyung Hee University, 446701 Yong-in, South Korea; School of General Education, Kyung Hee University, Seoul, South Korea},
abstract={In order to easily port mobile applications suitable for each platform, that have been developed under diverse development environment for individual wireless communication service providers, or redevelop them on a specific platform, it is required to reuse them at software model level that is a software development paradigm for MDA (Model Driven Architecture). The existing model verification approaches have focused on using graph comparison between input model and target model or applying graph pattern by simple version tree types. The graph model transformation mechanism proposed on this paper generates prediction model by defining test Oracle as model transformation rules that may conduct verification test of the generated transformation model, in order to support verification on verification of the converted model through MDA based model transformation mechanism. By comparing this prediction model with the target model, it is possible to execute verification test using graph comparison algorithms on the converted model. we supported verification mechanism of transformation model with model property information,dynamic analysis in this paper. Therefore, by increasing reliability of model transformation and further applying test issues on the software development process to the software mode at software design phase. A case study in AGG tool is presented to illustrate the feasibility of the model transformation verification with model property information. © 2011 Springer-Verlag.},
author_keywords={graph model transformation;  model comparison algorithms;  model transformation verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2011391,
author={Chen, Z. and Zhang, J. and Luo, B.},
title={Teaching software testing methods based on diversity principles},
journal={2011 24th IEEE-CS Conference on Software Engineering Education and Training, CSEE and T 2011 - Proceedings},
year={2011},
pages={391-395},
doi={10.1109/CSEET.2011.5876111},
art_number={5876111},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959972443&doi=10.1109%2fCSEET.2011.5876111&partnerID=40&md5=2e4acacf1266597a4ed345aacfdb9e65},
affiliation={State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210093, China; Software Institute, Nanjing University, Nanjing 210093, China},
abstract={Software testing is the primary approach to support software quality assurance. Many novel software testing methods have been proposed to achieve various tasks in recent years. It is a challenge to teach these new testing methods and classical testing methods within limited time. This paper reports our work in progress on the new teaching approach to software testing methods based on diversity principles. © 2011 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang20111182,
author={Zhang, S.},
title={Palus: A hybrid automated test generation tool for java},
journal={Proceedings - International Conference on Software Engineering},
year={2011},
pages={1182-1184},
doi={10.1145/1985793.1986036},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959885144&doi=10.1145%2f1985793.1986036&partnerID=40&md5=829ffc4069db5ac8b5599bbde13a1e88},
affiliation={Department of Computer Science and Engineering, University of Washington, Seattle, WA, United States},
abstract={In object-oriented programs, a unit test often consists of a sequence of method calls that create and mutate objects. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible. This paper proposes a combined static and dynamic test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests. Our Palus tool implements this approach. We compared it with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on six popular open-source Java programs. Tests generated by Palus achieved 35% higher structural coverage on average. Palus is also internally used in Google, and has found 22 new bugs in four well-tested products. © 2011 ACM.},
author_keywords={automated test generation;  static and dynamic analyses},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lançon2011,
author={Lançon, F.},
title={Naval Electronic Warfare Simulation for effectiveness assessment and softkill programmability facility},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2011},
volume={8060},
doi={10.1117/12.883100},
art_number={80600N},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959540621&doi=10.1117%2f12.883100&partnerID=40&md5=af2856d6153b326a7014a49117e6d337},
affiliation={SAGEM, Division Optronique et Défense (Optronics and Defence Division), 100 Avenue de Paris, 91344 Massy, France},
abstract={The Anti-ship Missile (ASM) threat to be faced by ships will become more diverse and difficult. Intelligence, rules of engagement constraints, fast reaction-time for effective softkill solution require specific tools to design Electronic Warfare (EW) systems and to integrate it onboard ship. SAGEM Company provides decoy launcher system [1] and its associated Naval Electronic Warfare Simulation tool (NEWS) to permit softkill effectiveness analysis for anti-ship missile defence. NEWS tool generates virtual environment for missile-ship engagement and counter-measure simulator over a wide spectrum: RF, IR, EO. It integrates EW Command & Control (EWC2) process which is implemented in decoy launcher system and performs Monte-Carlo batch processing to evaluate softkill effectiveness in different engagement situations. NEWS is designed to allow immediate EWC2 process integration from simulation to real decoy launcher system. By design, it allows the final operator to be able to program, test and integrate its own EWC2 module and EW library onboard, so intelligence of each user is protected and evolution of threat can be taken into account through EW library update. The objectives of NEWS tool are also to define a methodology for trial definition and trial data reduction. Growth potential would permit to design new concept for EWC2 programmability and real time effectiveness estimation in EW system. This tool can also be used for operator training purpose. This paper presents the architecture design, the softkill programmability facility concept and the flexibility for onboard integration on ship. The concept of this operationally focused simulation, which is to use only one tool for design, development, trial validation and operational use, will be demonstrated. © 2011 SPIE.},
author_keywords={Command & control;  Decoy launcher;  EW;  Operationally focused simulation;  Performance analysis;  Softkill},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Li20111474,
author={Li, Y. and Zhao, Y. and Liu, Z. and Wang, R.},
title={Automatic tailoring and transplanting: A practical method that makes virtual screening more useful},
journal={Journal of Chemical Information and Modeling},
year={2011},
volume={51},
number={6},
pages={1474-1491},
doi={10.1021/ci200036m},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959688661&doi=10.1021%2fci200036m&partnerID=40&md5=5943c6bbeb2ee42b16e2be23af24b4ec},
affiliation={State Key Laboratory of Bioorganic Chemistry, Shanghai Institute of Organic Chemistry, Chinese Academy of Sciences, 345 Lingling Road, Shanghai 200032, China},
abstract={Docking-based virtual screening of large compound libraries has been widely applied to lead discovery in structure-based drug design. However, subsequent lead optimizations often rely on other types of computational methods, such as de novo design methods. We have developed an automatic method, namely automatic tailoring and transplanting (AutoT&T), which can effectively utilize the outcomes of virtual screening in lead optimization. This method detects suitable fragments on virtual screening hits and then transplants them onto a lead compound to generate new ligand molecules. Binding affinities, synthetic feasibilities, and drug-likeness properties are considered in the selection of final designs. In this study, our AutoT&T program was tested on three different target proteins, including p38 MAP kinase, PPAR-α, and Mcl-1. In the first two cases, AutoT&T was able to produce molecules identical or similar to known inhibitors with better potency than the given lead compound. In the third case, we demonstrated how to apply AutoT&T to design novel ligand molecules from scratch. Compared to the solutions generated by other two de novo design methods, i.e., LUDI and EA-Inventor, the solutions generated by AutoT&T were structurally more diverse and more promising in terms of binding scores in all three cases. AutoT&T also completed the assigned jobs more efficiently than LUDI and EA-Inventor by several folds. Our AutoT&T method has certain technical advantages over de novo design methods. Importantly, it expands the application of virtual screening from lead discovery to lead optimization and thus may serve as a valuable tool for many researchers. © 2011 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rotella2011113,
author={Rotella, P. and Chulani, S.},
title={Implementing quality metrics and goals at the corporate level},
journal={Proceedings - International Conference on Software Engineering},
year={2011},
pages={113-122},
doi={10.1145/1985441.1985459},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959263432&doi=10.1145%2f1985441.1985459&partnerID=40&md5=e235fcc43d803ee3fb3735aacdac4ce2},
affiliation={Cisco Systems, Inc., 7200 Kit Creek Road, Research Triangle Park, NC 27709, United States; Cisco Systems, Inc., 821 Alder Drive, Milpitas, CA 95035, United States},
abstract={Over the past eight years, Cisco Systems, Inc., has implemented software quality goals for most groups engaged in software development, including development for both customer and internal use. This corporate implementation has proven to be a long and difficult process for many reasons, including opposition from many groups, uncertainties as to how to proceed with key aspects of the goaling, and the many unanticipated modifications needed to adapt the program to a large and diverse development and test environment. This paper describes what has worked, what has not work so well, and what levels of improvement the Engineering organization has experienced in part as a result of these efforts. Key customer experience metrics have improved 30% to 70% over the past six years, partly as a result of metrics and process standardization, dashboarding, and goaling. As one would expect with such a large endeavor, some of the results shown are not statistically provable, but are nevertheless generally accepted within the corporation as valid. Other important results do have strong statistical substantiation, and we will also describe these. But whether or not the results are statistically provable, Cisco has in fact improved its software quality substantially over the past eight years, and the corporate goaling mechanism is generally recognized as a necessary (but of course not sufficient) part of this improvement effort. © 2011 ACM.},
author_keywords={bic (best-in-class);  bu (business unit);  cfd;  csat (customer satisfaction);  goaling;  mttr (mean time to repair)},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Svinkin20111686,
author={Svinkin, M.R.},
title={Engineering aspects in evaluation of pile capacity by dynamic testing},
journal={Structures Congress 2011 - Proceedings of the 2011 Structures Congress},
year={2011},
pages={1686-1697},
doi={10.1061/41171(401)147},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958079348&doi=10.1061%2f41171%28401%29147&partnerID=40&md5=5cee92eddb17cc66cd75ddcb12b5469b},
affiliation={Vibraconsult, 13821 Cedar Road 205, Cleveland, OH 44118-2376, United States},
abstract={High strain dynamic pile testing is widely spread in the pile driving industry. Evaluation of pile capacity from field testing is the primary application of the dynamic method, but the accuracy and the area of utilization (soil conditions and pile type) of this method are vague. The application of hardware and software is only partially established because different hardware and software produce substantially diverse pile capacities. Engineering evaluation of pile capacity is not established at all. Statistical analysis of pile capacities is unable to find out applicability of the method to specified site conditions. Uncertainties are available in signal matching technique and comparison of the results of static and dynamic tests. Quality of dynamic measurements together with the software features and the engineering factors affect pile capacity obtained by dynamic testing. Engineering research is needed to ascertain the accuracy and the area of application of high strain dynamic pile testing. © ASCE 2011.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hyser2011,
author={Hyser, C. and Gmach, D. and Ml, U. and Chen, Y. and Suryanarayana, V.},
title={Improving server power management in research and development data centers},
journal={Compute 2011 - 4th Annual ACM Bangalore Conference},
year={2011},
doi={10.1145/1980422.1980428},
art_number={6},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957998650&doi=10.1145%2f1980422.1980428&partnerID=40&md5=52d55235bea24293951b91adeca97a69},
affiliation={HP Labs, Palo Alto, CA, United States; HP India, Bangalore, India},
abstract={Research data centers are often composed of thousands of diverse computer systems used for ongoing research, development, software regression and hardware compatibility testing. The usage patterns of many of these systems result in periodic non-use and extended periods of idleness. Users routinely fail to ensure that idle machines are powered down prior to overnight or extended absence periods. The annual amount of wasted energy in the HP Bangalore development data center is estimated at 14400 MWh resulting in over 8600 tons of CO2 emissions per year. In this paper, we propose Idle Machine Power Savings (IMPS), which seeks to address potential power cost savings and minimize environmental impact. IMPS consists of a low overhead, highly scalable data acquisition framework enabling the development of algorithms (an artificial neural network is used in the initial prototype) for automatic "extended idle" notifications and optional automatic shutdown of unused computers in data centers. This paper describes our approach, the framework, a prototype implementation and provides preliminary results. The results show an enormous potential for energy savings that translate directly into financial savings and lowered greenhouse gas emissions. Copyright 2011 ACM.},
author_keywords={Data mining;  Machine learning algorithms;  Power management;  Sustainability;  Sustainable computing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zagordi2011,
author={Zagordi, O. and Bhattacharya, A. and Eriksson, N. and Beerenwinkel, N.},
title={ShoRAH: Estimating the genetic diversity of a mixed sample from next-generation sequencing data},
journal={BMC Bioinformatics},
year={2011},
volume={12},
doi={10.1186/1471-2105-12-119},
art_number={119},
note={cited By 182},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955125679&doi=10.1186%2f1471-2105-12-119&partnerID=40&md5=cc9f18fc90fe717878e087a7fae822bf},
affiliation={Department of Biosystems Science and Engineering, ETH Zurich, Mattenstrasse 26, 4058 Basel, Switzerland; SIB Swiss Institute of Bioinformatics, Switzerland; 23andMe, Mountain View, CA 94043, United States},
abstract={Background: With next-generation sequencing technologies, experiments that were considered prohibitive only a few years ago are now possible. However, while these technologies have the ability to produce enormous volumes of data, the sequence reads are prone to error. This poses fundamental hurdles when genetic diversity is investigated.Results: We developed ShoRAH, a computational method for quantifying genetic diversity in a mixed sample and for identifying the individual clones in the population, while accounting for sequencing errors. The software was run on simulated data and on real data obtained in wet lab experiments to assess its reliability.Conclusions: ShoRAH is implemented in C++, Python, and Perl and has been tested under Linux and Mac OS X. Source code is available under the GNU General Public License at http://www.cbg.ethz.ch/software/shorah. © 2011 Zagordi et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ioannides2011112,
author={Ioannides, C. and Barrett, G. and Eder, K.},
title={Feedback-based coverage directed test generation: An industrial evaluation},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6504 LNCS},
pages={112-128},
doi={10.1007/978-3-642-19583-9_13},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953036682&doi=10.1007%2f978-3-642-19583-9_13&partnerID=40&md5=8344541e0bc6b0e17c01e723b1958e6d},
affiliation={Industrial Doctorate Centre in Systems, University of Bristol, Queen's Building, Bristol BS8 1TR, United Kingdom; Broadcom Corporation, Broadcom BBE BU, 220 Bristol Business Park, Coldharbour Lane, Bristol BS16 1FJ, United Kingdom; Department of Computer Science, University of Bristol, MVB, Woodland Road, Bristol BS8 1UB, United Kingdom},
abstract={Although there are quite a few approaches to Coverage Directed test Generation aided by Machine Learning which have been applied successfully to small and medium size digital designs, it is not clear how they would scale on more elaborate industrial-level designs. This paper evaluates one of these techniques, called MicroGP, on a fully fledged industrial design. The results indicate relative success evidenced by a good level of code coverage achieved with reasonably compact tests when compared to traditional test generation approaches. However, there is scope for improvement especially with respect to the diversity of the tests evolved. © 2011 Springer-Verlag Berlin Heidelberg.},
author_keywords={Coverage Directed Test Generation;  Genetic Programming;  MicroGP;  Microprocessor Verification},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murman2011,
author={Murman, S.M.},
title={Evaluating modified diffusion coefficients for the SST turbulence model using benchmark tests},
journal={41st AIAA Fluid Dynamics Conference and Exhibit},
year={2011},
doi={10.2514/6.2011-3571},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088345189&doi=10.2514%2f6.2011-3571&partnerID=40&md5=48fe929e949ee32a9b403f1f795914c0},
affiliation={NASA Ames Research Center, Moffett Field, CA, United States},
abstract={Empirical modification of the diffusion coefficients for the shear-stress-transport (SST) turbulence model is presented against experimental data for a suite of model test problems. The modified form improves the prediction of mean flow and turbulent quantities at the edge of a free shear layer without significantly disrupting established correlations. The suite of benchmark cases illuminates turbulence modeling trends across a diverse range of flows, encompassing different physical mechanisms. This diversity and automation improves model validation, code regression testing, and the development of novel models.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bardelang2011,
author={Bardelang, T. and Gutmayer, H.-J. and Günes, M.},
title={Maturity level and variant validation of mechatronic systems in commercial vehicles},
journal={SAE Technical Papers},
year={2011},
doi={10.4271/2011-01-2263},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072507404&doi=10.4271%2f2011-01-2263&partnerID=40&md5=d42a88bcdb680af0d7769300582347e6},
affiliation={Daimler AG, Mercedesstrasse 137/HPC: B104, 70546 Stuttgart, Germany},
abstract={Driver assistance systems (e.g. the emergency brake assist Active Brake Assist2, or ABA2 for short, in the Mercedes-Benz Actros) are becoming increasingly common in heavy-duty commercial vehicles. Due to the close interconnection with drivetrain and suspension control systems, the integration and validation of the functions make the most exacting demands on processes and tools involved in mechatronics development. In addition to a multi-stage test process focusing on the functions of the driver assistance systems (software), the "electrical" aspects (hardware) also form part of holistic maturity level validation. The test process is supported by state-of-the-art, high-performance tools (e.g. automatable component test benches and overall vehicle HiL systems) which, in particular, allow quick and accurate configuration in line with different vehicle variants. Another challenge is the diversity of commercial vehicle variants, which entails numerous different control unit parameters that are custom-coded in the production process. This means that almost every vehicle is unique in terms of how its mechatronic systems are programmed. Copyright © 2011 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nikanjam2011,
author={Nikanjam, M. and Rutherford, J. and Morgan, P.},
title={Performance and emissions of diesel and alternative diesel fuels in modern light-duty diesel vehicles},
journal={SAE Technical Papers},
year={2011},
doi={10.4271/2011-24-0198},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072493271&doi=10.4271%2f2011-24-0198&partnerID=40&md5=19d55d06a5fd643abf74566c8daf85b5},
affiliation={Chevron Products Co.; Chevron Corp; Southwest Research Institute},
abstract={Conventional diesel fuel has been in the market for decades and used successfully to run diesel engines of all sizes in many applications. In order to reduce emissions and to foster energy source diversity, new fuels such as alternative and renewable, as well as new fuel formulations have entered the market. These include biodiesel, gas-to-liquid, and alternative formulations by states such as California. Performance variations in fuel economy, emissions, and compatibility for these fuels have been evaluated and debated. In some cases contradictory views have surfaced. "Sustainable", "Renewable", and "Clean" designations have been interchanged. Adding to the confusion, results from one fuel in one type of engine such as an older heavy-duty engine, is at times compared to that of another fuel in another type such as a modern light-duty engine. This study was an attempt to compare the performance of several fuels in identical environments, using the same engine, for direct comparison. Results of a large-scale fleet test and emissions test to evaluate the performance of several diesel fuels in a modern heavy-duty diesel (HDD) engine were presented in an earlier technical paper. That study was followed by a more recent article describing the results of emissions and performance of the same fuels in an older heavy-duty industry-standard engine. This article is the third and the final in this series and includes three modern light-duty diesel vehicles (BMW 335d, Volkswagen Jetta TDI, and Chevrolet Silverado) to evaluate emissions and fuel economy with a number of diesel fuels that cover a range of products being used in the North American market. EPA, California, Texas LED diesel, biodiesel, biodiesel blends, and gas-to-liquid fuel were tested in this program. Federal Test Procedure (FTP), Highway Fuel Economy Test (HwFET) Procedure, US06 Test Procedure, and 0-to-60 mph wide-open-throttle (WOT) were utilized for emissions, fuel economy, and power effects evaluation. This document will provide a detailed description of this project along with statistical analysis of test results for eight diesel fuels.1 Copyright © 2011 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{deMoura20115,
author={de Moura, L.},
title={Satisfiability at microsoft},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6959 LNCS},
pages={5},
doi={10.1007/978-3-642-24431-5_2},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038105755&doi=10.1007%2f978-3-642-24431-5_2&partnerID=40&md5=c39e6168f6dfdd0cc09695b3f0492b03},
affiliation={Microsoft Research, One Microsoft Way, Redmond, WA  98052, United States},
abstract={Constraint satisfaction problems arise in many diverse areas including software and hardware verification, type inference, static program analysis, test-case generation, scheduling, planning and graph problems. These areas share a common trait, they include a core component using logical formulas for describing states and transformations between them. The most well-known constraint satisfaction problem is propositional satisfiability, SAT, where the goal is to decide whether a formula over Boolean variables, formed using logical connectives can be made true by choosing true/false values for its variables. Some problems are more naturally described using richer languages, such as arithmetic. A supporting theory (of arithmetic) is then required to capture the meaning of these formulas. Solvers for such formulations are commonly called Satisfiability Modulo Theories (SMT) solvers. Modern software analysis and model-based tools are increasingly complex and multi-faceted software systems. However, at their core is invariably a component using logical formulas for describing states and transformations between system states. In a nutshell, symbolic logic is the calculus of computation. The state-of-the art SMT solver, Z3, developed at Microsoft Research, can be used to check the satisfiability of logical formulas over one or more theories. SMT solvers offer a compelling match for software tools, since several common software constructs map directly into supported theories. SMT solvers have been the focus of increased recent attention thanks to technological advances and an increasing number of applications. The Z3 solver from Microsoft Research is particularly prolific both concerning applications and technological advances. We describe several of the applications of Z3 within Microsoft, some are included as critical components in tools shipped with Windows 7, others are used internally and yet more are available for academic research. Z3 ranks as the premier SMT solver available today. © 2011, Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{deMoura20111,
author={de Moura, L.},
title={Orchestrating satisfiability engines},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2011},
volume={6876 LNCS},
pages={1},
doi={10.1007/978-3-642-23786-7_1},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020055625&doi=10.1007%2f978-3-642-23786-7_1&partnerID=40&md5=6415226809ec400a16f91795de64a45d},
affiliation={Microsoft Research, One Microsoft Way, Redmond, WA  98052, United States},
abstract={Constraint satisfaction problems arise in many diverse areas including software and hardware verification, type inference, static program analysis, test-case generation, scheduling, planning and graph problems. These areas share a common trait, they include a core component using logical formulas for describing states and transformations between them. The most well-known constraint satisfaction problem is propositional satisfiability, SAT, where the goal is to decide whether a formula over Boolean variables, formed using logical connectives can be made true by choosing true/false values for its variables. Some problems are more naturally described using richer languages, such as arithmetic. A supporting theory (of arithmetic) is then required to capture the meaning of these formulas. Solvers for such formulations are commonly called Satisfiability Modulo Theories (SMT) solvers. Software analysis and model-based tools are increasingly complex and multi-faceted software systems. However, at their core is invariably a component using logical formulas for describing states and transformations between system states. In a nutshell, symbolic logic is the calculus of computation. The state-of-the art SMT solver, Z3, developed at Microsoft Research, can be used to check the satisfiability of logical formulas over one or more theories. SMT solvers offer a compelling match for software tools, since several common software constructs map directly into supported theories. Z3 comprises of a collection of symbolic reasoning engines. These engines are combined to address the requirements of each application domain. In this talk, we describe the main challenges in orchestrating the different engines, and the main application domains within Microsoft. © 2011, Springer-Verlag Berlin Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brandl2011,
author={Brandl, A. and Bray, S. and Magelky, C. and Lant, K. and Martin, R. and St-Clergy, J.},
title={An innovative cement spacer with biodegradable components effectively sealing severe lost circulation zones},
journal={Offshore Mediterranean Conference and Exhibition 2011, OMC 2011},
year={2011},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969348885&partnerID=40&md5=097c555ac5bedf8b962040c798b7b47a},
affiliation={BJ Services Company, United States},
abstract={This study is about a cement spacer system containing a biopolymer additive package as a lost circulation material to reduce losses and minimize filtrate invasion into wellbore formations for improved wellbore integrity. A testing program was conducted in the lab to determine the spacer fluid performance (rheology, stability) as well as its sealing efficiency under critical conditions (highly permeable and unconsolidated zones, high pressures and temperatures) as well as diverse design features (addition of salts to protect formations or barite as weighting agent). The spacer's sealing efficiency was tested against 325 and/or 60 mesh screens, 20/40 frac sand and pea gravel pack at static differential pressures up to 1,000 psi (6.9 MPa) and temperatures up to 400° F (204° C). Regain permeability studies to oil on Berea sandstone cores after treatment with the spacer were performed and the sealing mechanism is described. Case histories are evaluated to support further benefits of this sealing spacer. It was concluded that the spacer can seal highly permeable formations, strengthens the wellbore wall while in place and is non-damaging to formation permeability. The sealing spacer helps to improve cementing success in particular for the growing number of wells drilled through depleted reservoirs and lost circulation zones. © 2011 Offshore Mediterranean Conference. All Rights Reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bahr201125,
author={Bahr, T.J. and Takahashi, S.G. and Chow, C.M. and Martin, D. and Kennedy, M.},
title={What do i want my doctor to know?: Integrating video simulation and multimedia for the learning and teaching of medical residents},
journal={Proceedings of the International Conference on e-Learning, ICEL},
year={2011},
pages={25-35},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904720782&partnerID=40&md5=4c2800d756f6a72196382648167d0e5a},
affiliation={University of Toronto, Canada},
abstract={The Postgraduate Medical Education Office (PGME) Core Curriculum Web Initiative - called PGCorEd™ was implemented in 2008 as a required foundational curriculum for all 1st and 2nd year resident physicians. This new curriculum was accelerated by new accreditation standards for residency training programs by the Royal College of Physicians and Surgeons of Canada in 2006. PGCorEd™ modules are web-based and interactive in nature, integrating several simulation approaches to learning and evaluation including: video for role model simulations, video narratives, content interactivity techniques, and electronic key feature case assessments. This paper reports on a case study in a multi-method curriculum project that included: action research, deliberative curriculum inquiry, development of a standardized online multimedia based curriculum, simulation approaches, learner assessment including pre module and post module testing of learners, a comprehensive program evaluation involving pilot testing of content, and the collection of learner satisfaction data. The results from the pilot testing and 2 years of data on pre module and post module testing of learners reveal that this fully online self-moderated curriculum is a viable forum for teaching learners in busy field-based training environments. Learner satisfaction surveys for the completed modules show that using multimedia and storytelling are effective approaches to teaching and learner engagement in an online environment. Additionally, the findings reveal a positive impact on learning and utility to medical residents across diverse training programs. This paper also illustrates some practices to integrate simulation in the effective learning and teaching of the foundational humanistic competencies for postgraduate medical residents. Deliberative efforts by the design teams to be attentive to the needs of learners, including the active participation of learners in constructing the simulations, has resulted in an authenticity of the simulations noted by learners and faculty involved with this project.},
author_keywords={Assessment;  ELearning;  Medical education;  Multimedia;  Residency;  Simulation;  Video simulation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Sokolov2011644,
author={Sokolov, O. and Molchanova, O.},
title={Fuzzy assessment of test results},
journal={Proceedings of the 7th Conference of the European Society for Fuzzy Logic and Technology, EUSFLAT 2011 and French Days on Fuzzy Logic and Applications, LFA 2011},
year={2011},
volume={1},
number={1},
pages={644-650},
doi={10.2991/eusflat.2011.42},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84871982264&doi=10.2991%2feusflat.2011.42&partnerID=40&md5=f85961b2b4e41a531e5e78ae61234591},
affiliation={National Aerospace University, Dept. of Informatics, Chkalov Street 17, 61070 Kharkov, Ukraine},
abstract={The objective of this research is to study various assessment methods used in modern test systems. Methods based on classical test theory and IRT (item response theory) have been considered in this paper. The approaches to improve the quality of assessment of the ability level as a function of the diversity of evaluation points have been proposed. Statistical analysis packages SPSS, the tests analysis WinSteps and MatLab packages have been applied as software tools. © 2011. The authors-Published by Atlantis Press.},
author_keywords={Ability;  Assessment scales;  Difficulty;  Item response theory;  Logit},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kanij2011359,
author={Kanij, T. and Merkel, R. and Grundy, J.},
title={A preliminary study on factors affecting software testing team performance},
journal={International Symposium on Empirical Software Engineering and Measurement},
year={2011},
pages={359-362},
doi={10.1109/esem.2011.48},
art_number={6092588},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858719614&doi=10.1109%2fesem.2011.48&partnerID=40&md5=afd4b2951302b7cbd80459c7fef81688},
affiliation={Swinburne University of Technology, Hawthorn, VIC, Australia; Monash University, Clayton, VIC, Australia},
abstract={With the growth of the software testing industry, many in-house testing groups and outsourcing testing companies have been established. Underlying the success of these testing groups and companies are team(s) of testers. This research investigates the importance of different factors, diversity and experience on building a successful testing team. We collected the opinions of testing practitioners on these factors via a survey. The outcome strongly indicates the relative importance of different factors and that diversity is helpful for a testing team. The results also support the importance of suitable team experience. © 2011 IEEE.},
author_keywords={Diversity;  Experience;  Rank;  Testing team},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hu201198,
author={Hu, S. and McNaughton, R.B.},
title={Online distribution of packaged software},
journal={Lecture Notes in Business Information Processing},
year={2011},
volume={80 LNBIP},
pages={98-109},
doi={10.1007/978-3-642-21544-5_9},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959995597&doi=10.1007%2f978-3-642-21544-5_9&partnerID=40&md5=829e00a5716bdf5ab19a12a545510f09},
affiliation={Conrad Business, Entrepreneurship and Technology Centre, University of Waterloo, 295 Hagey Blvd., Waterloo, ON N2L 6R5, Canada},
abstract={Increased bandwidth and user sophistication make it practical for software developers to sell and distribute packaged software to customers online. This paper develops a transaction cost model of the conditions under which software developers are more likely to distribute their packaged software online rather than through traditional channels. The model is tested using data from a survey of Canadian software firms. Almost three-quarters of the respondents use the Internet at least in part to distribute their products. Firms are more likely to distribute their packaged software online in less diverse markets and where channel volume is increasing rapidly. However, the results are not consistent with other conditions posited to be associated with online distribution. Implications for understanding how the Internet is changing the transaction costs of distributing digital products are discussed. © 2011 Springer-Verlag.},
author_keywords={distribution channels;  packaged software;  transaction cost analysis},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Delgado2011443,
author={Delgado, O.F. and Clark, N.N. and Thompson, G.J.},
title={Modeling Transit Bus Fuel Consumption on the Basis of Cycle Properties},
journal={Journal of the Air and Waste Management Association},
year={2011},
volume={61},
number={4},
pages={443-452},
doi={10.3155/1047-3289.61.4.443},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955681425&doi=10.3155%2f1047-3289.61.4.443&partnerID=40&md5=54eba37b0aa7c5a2672b890c1e5ed4e7},
affiliation={Center for Alternative Fuels, Engines and Emissions, Department of Mechanical and Aerospace Engineering, West Virginia University, Morgantown, WV, United States},
abstract={A method exists to predict heavy-duty vehicle fuel economy and emissions over an “unseen” cycle or during unseen on-road activity on the basis of fuel consumption and emissions data from measured chassis dynamometer test cycles and properties (statistical parameters) of those cycles. No regression is required for the method, which relies solely on the linear association of vehicle performance with cycle properties. This method has been advanced and examined using previously published heavy-duty truck data gathered using the West Virginia University heavy-duty chassis dynamometer with the trucks exercised over limited test cycles. In this study, data were available from a Washington Metropolitan Area Transit Authority emission testing program conducted in 2006. Chassis dynamometer data from two conventional diesel buses, two compressed natural gas buses, and one hybrid diesel bus were evaluated using an expanded driving cycle set of 16 or 17 different driving cycles. Cycle properties and vehicle fuel consumption measurements from three baseline cycles were selected to generate a linear model and then to predict unseen fuel consumption over the remaining 13 or 14 cycles. Average velocity, average positive acceleration, and number of stops per distance were found to be the desired cycle properties for use in the model. The methodology allowed for the prediction of fuel consumption with an average error of 8.5% from vehicles operating on a diverse set of chassis dynamometer cycles on the basis of relatively few experimental measurements. It was found that the data used for prediction should be acquired from a set that must include an idle cycle along with a relatively slow transient cycle and a relatively high speed cycle. The method was also applied to oxides of nitrogen prediction and was found to have less predictive capability than for fuel consumption with an average error of 20.4%. IMPLICATIONS It is unclear what direction the next major regulation will take, but greenhouse gas emissions are at the forefront of concern for mobile power sources. From a regulatory perspective for inventory purposes or from that of a fleet owner for reducing costs, a robust but simple model for estimation of fuel consumption is desired. This study developed a modeling methodology to predict the fuel consumption of a vehicle exercised over unseen cycles using limited chassis dynamometer data, reducing heavy-duty chassis dynamometer costs. This approach may also help to compare emissions or fuel economy between vehicles that were not tested over the same set of driving cycles. © 2011 Air & Waste Management Association.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Travillian20111,
author={Travillian, R.S. and Diatchka, K. and Judge, T.K. and Wilamowska, K. and Shapiro, L.G.},
title={An ontology-based comparative anatomy information system},
journal={Artificial Intelligence in Medicine},
year={2011},
volume={51},
number={1},
pages={1-15},
doi={10.1016/j.artmed.2010.10.001},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650930397&doi=10.1016%2fj.artmed.2010.10.001&partnerID=40&md5=21cc3e9b8f71a1f40e359bbd4e6e3e13},
affiliation={Functional Genomics Team, European Bioinformatics Institute, Wellcome Trust Genome Campus, Hinxton, Cambridgeshire, CB10 1SD, United Kingdom; Faculté Informatique et Communications, École Polytechnique Fédérale de Lausanne, EPFL IC-DO, BC 408 (Bâtiment BC), Station 14, CH-1015 Lausanne, Switzerland; Center for Human-Computer Interaction, Department of Computer Science, Virginia Polytechnic Institute and State University, 2202 Kraft Drive, KWII Building (0106), Blacksburg, VA 24061-0106, United States; Biomedical and Health Informatics Program, Department of Medical Education and Biomedical Informatics, University of Washington, Box 357240, 1959 NE Pacific Street, HSB I-264, Seattle, WA 98195-7240, United States; Department of Computer Science and Engineering, University of Washington, Box 352350, Seattle, WA 98195-2350, United States},
abstract={Introduction: This paper describes the design, implementation, and potential use of a comparative anatomy information system (CAIS) for querying on similarities and differences between homologous anatomical structures across species, the knowledge base it operates upon, the method it uses for determining the answers to the queries, and the user interface it employs to present the results. The relevant informatics contributions of our work include (1) the development and application of the structural difference method, a formalism for symbolically representing anatomical similarities and differences across species; (2) the design of the structure of a mapping between the anatomical models of two different species and its application to information about specific structures in humans, mice, and rats; and (3) the design of the internal syntax and semantics of the query language. These contributions provide the foundation for the development of a working system that allows users to submit queries about the similarities and differences between mouse, rat, and human anatomy; delivers result sets that describe those similarities and differences in symbolic terms; and serves as a prototype for the extension of the knowledge base to any number of species. Additionally, we expanded the domain knowledge by identifying medically relevant structural questions for the human, the mouse, and the rat, and made an initial foray into the validation of the application and its content by means of user questionnaires, software testing, and other feedback. Methods: The anatomical structures of the species to be compared, as well as the mappings between species, are modeled on templates from the Foundational Model of Anatomy knowledge base, and compared using graph-matching techniques. A graphical user interface allows users to issue queries that retrieve information concerning similarities and differences between structures in the species being examined. Queries from diverse information sources, including domain experts, peer-reviewed articles, and reference books, have been used to test the system and to illustrate its potential use in comparative anatomy studies. Results: 157 test queries were submitted to the CAIS system, and all of them were correctly answered. The interface was evaluated in terms of clarity and ease of use. This testing determined that the application works well, and is fairly intuitive to use, but users want to see more clarification of the meaning of the different types of possible queries. Some of the interface issues will naturally be resolved as we refine our conceptual model to deal with partial and complex homologies in the content. Conclusions: The CAIS system and its associated methods are expected to be useful to biologists and translational medicine researchers. Possible applications range from supporting theoretical work in clarifying and modeling ontogenetic, physiological, pathological, and evolutionary transformations, to concrete techniques for improving the analysis of genotype-phenotype relationships among various animal models in support of a wide array of clinical and scientific initiatives. © 2010 Elsevier B.V.},
author_keywords={Anatomy;  Comparative anatomy;  Foundational Model of Anatomy;  Graph matching;  Graph similarity;  Homology;  Isomorphism;  Knowledge base;  Ontology;  Protégé},
document_type={Article},
source={Scopus},
}

@ARTICLE{Stilman20101,
author={Stilman, B. and Yakhnis, V. and Umanskiy, O.},
title={Discovering role of linguistic geometry},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6438 LNAI},
number={PART 2},
pages={1-21},
doi={10.1007/978-3-642-16773-7_1},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650062950&doi=10.1007%2f978-3-642-16773-7_1&partnerID=40&md5=edb2a615a86ffd70aa5b8fc8775f9d77},
affiliation={University of Colorado Denver, Department of Computer Science and Engineering, Campus Box 109, Denver, CO 80217-3364, United States; STILMAN Advanced Strategies Denver, 1623 Blake Street, Denver, CO 20202, United States},
abstract={Linguistic Geometry (LG) is a type of game theory for extensive discrete games scalable to the level of real life defense systems. LG was developed by generalizing experiences of the advanced chess players. In this paper we summarize experiences of highly successful application of LG to a diverse set of board games and military operations. We believe that LG has a more fundamental nature than simply yet another mathematical theory of efficient wargaming. Every LG application generated new ideas that experts evaluated as brilliant. We suggest that LG is a mathematical model of human thinking about armed conflict, a mental reality that existed for thousands of years. The game of chess was invented 1.5-2 thousand years ago (following different accounts) as a formal gaming model of ancient wars. In our case, chess served as a means for discovering human methodology of efficient warfare. To test this hypothesis we would have to demonstrate power of LG software on wars happened at times when the game of chess had been unknown. In this paper, we present an approach to LG-based analysis of the battles of Alexander the Great demonstrating that after tuning the LG-based software will generate the same courses of action as those reported by the historians. © 2010 Springer-Verlag.},
author_keywords={ancient warfare;  Artificial Intelligence;  game theory;  Linguistic Geometry;  search},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hemmati201063,
author={Hemmati, H. and Arcuri, A. and Briand, L.},
title={Reducing the cost of model-based testing through test case diversity},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6435 LNCS},
pages={63-78},
doi={10.1007/978-3-642-16573-3_6},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649883267&doi=10.1007%2f978-3-642-16573-3_6&partnerID=40&md5=137e3a9955b680d6bced3ed362f76b69},
affiliation={Simula Research Laboratory, University of Oslo, Norway; Department of Informatics, University of Oslo, Norway},
abstract={Model-based testing (MBT) suffers from two main problems which in many real world systems make MBT impractical: scalability and automatic oracle generation. When no automated oracle is available, or when testing must be performed on actual hardware or a restricted-access network, for example, only a small set of test cases can be executed and evaluated. However, MBT techniques usually generate large sets of test cases when applied to real systems, regardless of the coverage criteria. Therefore, one needs to select a small enough subset of these test cases that have the highest possible fault revealing power. In this paper, we investigate and compare various techniques for rewarding diversity in the selected test cases as a way to increase the likelihood of fault detection. We use a similarity measure defined on the representation of the test cases and use it in several algorithms that aim at maximizing the diversity of test cases. Using an industrial system with actual faults, we found that rewarding diversity leads to higher fault detection compared to the techniques commonly reported in the literature: coverage-based and random selection. Among the investigated algorithms, diversification using Genetic Algorithms is the most cost-effective technique. © 2010 IFIP International Federation for Information Processing.},
author_keywords={Adaptive Random Testing;  Clustering algorithms;  Genetic Algorithms;  Jaccard Index;  Model-based testing;  Search-based testing;  Similarity measure;  Test case selection;  UML state machines},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kim-Park20105,
author={Kim-Park, D.S. and De La Riva, C. and Tuya, J.},
title={An automated test oracle for XML processing programs},
journal={1st International Workshop on Software Test Output Validation, STOV 2010, in Conjunction with the 2010 International Conference on Software Testing and Analysis, ISSTA 2010},
year={2010},
pages={5-12},
doi={10.1145/1868048.1868050},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649782400&doi=10.1145%2f1868048.1868050&partnerID=40&md5=e75b0b6286668e88029b552914392a66},
affiliation={Department of Computing, University of Oviedo, Campus de Viesques, s/n, 33204, Spain},
abstract={XML processing programs play an important role in the achievement of XML data querying, manipulation, and construction operations to compose XML data structures for very diverse purposes regarding information representation, storing and exchange on XML-based systems. Testing of XML processing programs is a challenging task since the test input and output data involved in the test executions may be complex and large in volume, which makes it difficult to determine the correctness of the execution results. However, existing approaches on XML-based testing pay scarce attention to the specification and automation of so-called test oracles in charge of judging the execution results from XML processing programs. This paper deals with the definition of an automated test oracle for XML processing programs which operates with differentiated levels of specification. The oracle automation is achieved by transforming these specification levels into program code, and the resulting oracle implementation is evaluated through an experimental study that reveals promising results.},
author_keywords={Software testing;  Test oracles;  XML queries;  XML-based testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vadeikis20101309,
author={Vadeikis, C. and Germain, M. and Raff aillac, E.},
title={Flow sheet development of three geographically distinct chromium-bearing ores},
journal={XXV International Mineral Processing Congress 2010, IMPC 2010},
year={2010},
volume={2},
pages={1309-1317},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874381335&partnerID=40&md5=9828147b101e217f75844ce8d2ff3765},
affiliation={DownerEdi Mining-Mineral Technologies, 11 Elysium Road, Carrara Qld 4211, Australia; Mineral Processing Specialist, DownerEdi Mining-Mineral Technologies, 11 Elysium Road, Carrara Qld 4211, Australia; Senior Metallurgist, DownerEdi Mining-Mineral Technologies, 11 Elysium Road, Carrara Qld 4211, Australia},
abstract={Chromium bearing minerals occur globally with a diverse range of characteristics that affect their amenability to benefi ciation. This paper presents the results of the mineralogical and metallurgical test work of four distinctly different chromite ores originating from Europe, Asia and Africa. The impact of the unique physical properties of each ore type on the laboratory test work program and on the resultant benefi ciation fl owsheets developed in light of these differences are examined. The fi ndings of an investigation into the development of a gravimetric, magnetic and electrostatic processing route for the production of high purity chromite concentrates containing less than 0.5 percent SiO2 are also presented.},
author_keywords={Characterisation;  Chromite;  Flow sheet design},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Cheng20101486,
author={Cheng, C.-C. and Chen, H.-M. and Lo, H.-C.},
title={Study on monitoring the landscape structure of leucaena leucocephala},
journal={31st Asian Conference on Remote Sensing 2010, ACRS 2010},
year={2010},
volume={2},
pages={1486-1491},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865630939&partnerID=40&md5=7e8c3a84e7d2a96ceb0e889a8e56f784},
affiliation={Department of Landscape Architecture, Chinese Culture University, No.55, Hwa Kang Road, Taipei, Taiwan; School of Forestry and Resource Conservation, National Taiwan University, No. 1, Sec. 4, Roosevelt Road, Taipei, 10617, Taipei, Taiwan},
abstract={This study focuses on applying remote sensing technique and landscape indices to analyze the landscape structure and the dispersal change of Leucaena leucocephala, and also monitor landscape change in Penghu islands and Henchun peninsula. The research processes include generating the land-use map and the spatial distribution map of Leucaena leucocephala using SPOT images; calculating the quantified landscape indices for analyzing the landscape structure and the dispersal change of Leucaena leucocephala using the FRAGSTATS program; and finally applying the Shannon diversity test to investigate if the landscape structure at both areas are significantly affected by the dispersal change of Leucaena leucocephala. The result is as follows. The land-use classification obtained from Penghu and Hengchun area includes 6 land-use types. The overall classification accuracy in Penghu area was 88.9% in 1993 and 86.8% in 2005, while Hengchun area was 88.41% in 1994 and 86.6% in 2006. The dispersal trend of Leucaena leucocephala within 12 years is in the direction of northeast and southwest, but a slight difference in Penghu and Henchun area. In addition, Leucaena leucocephala did not invade natural forests at high elevation area. As for monitoring landscape change based on Shannon diversity t-test, the result indicated that the landscape at both areas was affected by the dispersal change of Leucaena leucocephala. The result obtained from this study could be a reference for forest units to control the dispersal of Leucaena leucocephala and prevent an ecological crisis at both areas.},
author_keywords={Landscape indices;  Landscape structure;  Leucaena leucocephala;  Remote sensing;  Species invasion},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Andrews2010340,
author={Andrews, A. and Azghandi, S. and Pilskalns, O.},
title={Regression testing of web applications using FSMWeb},
journal={Proceedings of the IASTED International Conference on Software Engineering and Applications, SEA 2010},
year={2010},
pages={340-349},
doi={10.2316/P.2010.725-033},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862272689&doi=10.2316%2fP.2010.725-033&partnerID=40&md5=498bb207ad9820415261d378fb0daccc},
affiliation={Department of Computer Science, University Of Denver, 2360 S. Gaylord St., Denver, CO, United States; Department of Engineering and Computer Science, Washington State University, 14204 NE Salmon Creek Avenue, Vancouver, WA, United States},
abstract={Testing of web base-d applications presents a number of challenges posed by the heterogeneity of hardware, operating systems, software platforms, etc. The distributed nature of web-based applications increases behavioral diversity and contributes to complexity. Many web applications evolve rapidly and experience frequent changes. These characteristics also make regression testing a challenge. This paper proposes a set of rules to identify and classify black box regression testing needs depending on types of changes to web applications. The approach is based on the FSMWeb [1] test generation methodology.},
author_keywords={Regression testing;  Software testing;  Web applications},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lo2010,
author={Lo, J.Y.-C.},
title={The more, the merrier? Resource mobilization and field diversity in emerging fields},
journal={Academy of Management 2010 Annual Meeting - Dare to Care: Passion and Compassion in Management Practice and Research, AOM 2010},
year={2010},
page_count={6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858641352&partnerID=40&md5=02dd139fcf61da6095d6784974e07163},
affiliation={Department of Management and Organizations, University of Southern California, United States},
abstract={While scholars generally view resource mobilization as a factor that contributes to legitimation of new fields, research to date does little to explore the potential downside of being too successful at attracting resources. In this paper, I theorize a link between macro-level field outcomes and the micro-foundations of reactions to the intentional actions by field entrepreneurs. When field entrepreneurs in pursuit of resources frame their ideas broadly enough to connect to multiple audiences, they increase both the diversity of field participants and the ambiguity of the field. While moderate levels of diversity permit nascent fields to develop, too much diversity undermines development of either a clear identity or consistent evaluation criteria. I test these ideas with survey data from Advanced Technology Program (ATP) at the National Institute of Standards and Technology (NIST), and findings support the hypotheses of this model.},
author_keywords={Field diversity;  Field emergence;  Mobilization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Khan2010,
author={Khan, T.M. and Pezeshki, V. and Clear, F. and Al-Kaabi, A.},
title={Diverse virtual social networks: Implications for remote software testing teams},
journal={Proceedings of the European, Mediterranean and Middle Eastern Conference on Information Systems: Global Information Systems Challenges in Management, EMCIS 2010},
year={2010},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857561841&partnerID=40&md5=a01e890baac3f2bf224204dbd6f42be9},
affiliation={Brunel Business School, Brunel University, United Kingdom},
abstract={This paper evaluates offshore outsourcing in the IT testing industry and determines what conditions determine its success. There is particular focus on the influence of diversity in teams on group relationships. Two studies are described: the first, investigated the perceptions of professional software testers on the critical factors of offshore outsourcing; and the second study looked at the ability for diverse teams to form close working relationships through virtual networks. We find that overt diversity factors inhibit interaction across nationality boundaries. The limitations of virtual networks for fostering personal communications is apparent in preventing group members from overcoming the initial aversion to mix with out-group members, which could be achieved with closer and more personal communications between members with different diversity factors in normal face to face communications. Where software testing teams are outsourced globaly, and must rely on virtual communications, there seems potential for significant difficulties in developing close working relationships, which on the one hand, can be negative for group cohesion, but one the other hand, can be positive for encouraging imparitality.},
author_keywords={Culture;  Diversity;  Offshore;  Outsourcing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Turner20101713,
author={Turner, A.E.},
title={Porpoise floating launch sounding rocket},
journal={Advances in the Astronautical Sciences},
year={2010},
volume={136},
pages={1713-1728},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053398144&partnerID=40&md5=8fa0d1a6497c820164598fd9bffcec08},
affiliation={Advanced Program and Systems, Space Systems/Loral, MS G-54, 3825 Fabian Way, Palo Alto, CA 94303-4604, United States},
abstract={The flight test program planned for the Porpoise floating launch sounding rocket is summarized. Trajectory and Guidance & Control simulation output are shown to be consistent with achievement of the research goals. The means by which a rapid flight rate is supported are discussed as well as its diverse benefits to fields including upper atmospheric research, missile development and new launch vehicles, also the education of young aerospace engineers.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Feist2010100,
author={Feist, M.D. and Landau, M. and Harte, E.},
title={The effect of fuel composition on performance and Emissions of a Variety of Natural Gas Engines},
journal={SAE International Journal of Fuels and Lubricants},
year={2010},
volume={3},
number={2},
pages={100-117},
doi={10.4271/2010-01-1476},
note={cited By 19},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959530082&doi=10.4271%2f2010-01-1476&partnerID=40&md5=65e99fe3708e36bed933f49e4b67beca},
affiliation={Southwest Research Institute, United States; Southern California Gas Co, United States},
abstract={Work was performed to determine the feasibility of operating heavy-duty natural gas engines over a wide range of fuel compositions by evaluating engine performance and emission levels. Heavy-duty compressed natural gas engines from various engine manufacturers, spanning a range of model years and technologies, were evaluated using a diversity of fuel blends. Performance and regulated emission levels from these engines were evaluated using natural gas fuel blends with varying methane number (MN) and Wobbe Index in a dynamometer test cell. Eight natural gas blends were tested with each engine, and ranged from MN 75 to MN 100. Test engines included a 2007 model year Cummins ISL G, a 2006 model year Cummins C Gas Plus, a 2005 model year John Deere 6081H, a 1998 model year Cummins C Gas, and a 1999 model year Detroit Diesel Series 50G TK. All engines used lean-burn technology, except for the ISL G, which was a stoichiometric engine. Performance testing consisted of monitoring engine knock or auto-ignition, as well as engine power levels and overall engine operability. Emissions of total hydrocarbons (HC), non-methane hydrocarbons (NMHC), carbon monoxide (CO), oxides of nitrogen (NO x), nitrogen dioxide (NO 2), particulate matter (PM), and carbon dioxide (CO 2) were measured using procedures developed by the United States Environmental Protection Agency (EPA) for heavy-duty, onhighway engines. The engines showed no sign of auto ignition throughout the program, with slight differences in power levels with the various test fuels. All lean burn engines showed increased NO x and HC emission levels with decreased MN and increased Wobbe level, while the stoichiometric ISL G showed no clear trend in NO x or HC levels with the various fuels. © 2010 SAE International.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{EagleJr.201081,
author={Eagle Jr., E.O.},
title={Revision of branch technical position 7-19 regarding D3 evaluation of digital I&C},
journal={7th International Topical Meeting on Nuclear Plant Instrumentation, Control, and Human-Machine Interface Technologies 2010, NPIC and HMIT 2010},
year={2010},
volume={1},
pages={81-90},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958256715&partnerID=40&md5=81f8ba5e0c18d8039b83d8b30c8f7938},
affiliation={U. S. Nuclear Regulatory Commission, 11555 Rockville Pike, Rockville, MD 20852-2738, United States},
abstract={The U. S. Nuclear Regulatory Commission is in the process of issuing Revision 6 to Branch Technical Position (BTP) 7-19, "Guidance for Evaluation of Diversity and Defense-in-Depth in Digital Computer-Based Instrumentation and Control Systems" (NUREG-0800, Standard Review Plan (SRP), Chapter 7) that will incorporate the guidance from the Interim Staff Guidance (ISG) DI&C-ISG-02, Revision 2, "Diversity and Defense-in-Depth Issues" (June 5, 2009). Digital Instrumentation and Control (I&C) technology is being utilized to an ever greater extent in safety systems in new nuclear power plant (NPP) designs, and in current NPPs. Despite many good design features, quality software develop, and testing there is still a concern that an undetected software design error combined with an initiating event could disable significant portions of the multi-division automated safety systems. The ISG documents are the NRCs most recent method of rapidly developing additional guidance with industry input. The purpose of this paper is to highlight and identify the key questions or concerns from DI&C-ISG-02 and discuss the resulting guidance incorporated into this revised BTP 7-19. The leading issue addressed is when operator action may be used as part of the diverse actuation means to prevent or mitigate the effects of a postulated software common-cause failure (CCF).},
author_keywords={BTP 7-19;  Common-cause failure;  D3;  Diversity and defense-in-depth;  Standard review plan},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Choroś2010120,
author={Choroś, K. and Pawlaczyk, P.},
title={Content-based scene detection and analysis method for automatic classification of TV sports news},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2010},
volume={6086 LNAI},
pages={120-129},
doi={10.1007/978-3-642-13529-3_14},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956261007&doi=10.1007%2f978-3-642-13529-3_14&partnerID=40&md5=d32df5b117144d9ca4d57f6e7d9d43d3},
affiliation={Institute of Informatics, Wrocław University of Technology, Wybrzeze Wyspiańskiego 27, 50-370 Wrocław, Poland},
abstract={A large amount of digital video data is stored in local or network visual retrieval systems. The new technology advances in multimedia information processing as well as in network transmission have made video data publicly and relatively easy available. Users need the adequate tools to locate their desired video or video segments quickly and efficiently, for example in Internet video collections, TV shows archives, video-on-demand systems, personal video archives offered by many public Internet services, etc. Detection of scenes in TV videos is difficult because the diversity of effects used in video editing puts up a barrier to construct an appropriate model. The framework of automatic recognition and classification of scenes reporting the sport events in a given discipline in TV sports news have been proposed. Experimental results show good performance of the proposed scheme on detecting scenes on a given sport discipline in TV sports news. In the tests a special software called AVI - the Automatic Video Indexer has been used to detect shots and then scenes in tested TV news videos.+++++ © 2010 Springer-Verlag Berlin Heidelberg.},
author_keywords={Content-based video indexing;  Digital video segmentation;  Scene classification;  Scene detection;  Sport videos;  TV sport news},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nakagawa2010628,
author={Nakagawa, E.Y. and Trevisan, J.V.T. and Maldonado, J.C.},
title={Software Configuration Management as a crosscutting concern: An example on software testing},
journal={SEKE 2010 - Proceedings of the 22nd International Conference on Software Engineering and Knowledge Engineering},
year={2010},
pages={628-633},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952372480&partnerID=40&md5=f3c7dd07493d616322e3642fb1943e96},
affiliation={Dept. of Computer Systems, University of Sãao Paulo - USP, PO Box 668, 13560-970, Sãao Carlos, SP, Brazil},
abstract={SCM (Software Configuration Management) has substantially contributed as a mature, reliable and essential technology for successful software development, controlling changes and software evolution. In this scenario, tools that automate SCM have been proposed, developed and used in diverse software engineering activities and processes. However, each SCM tool has usually its particular architecture and data structures. Furthermore, there is a lack of work that investigate reuse and evolvability of such tools. In this paper we propose to see SCM as a crosscutting concern, i.e., as an activity that is spread across or tangled with other software engineering activities, in the same perspective of the AOSD (Aspect-Oriented Software Development). Based on this approach, we present a case study to the development of a SCM tool, named ATCMag, for software testing domain. Preliminary results show the viability of our approach and point out to the possibility of developing reusable and evolvable SCM tools.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Farzat201031,
author={Farzat, F.D.A.},
title={Test case selection method for emergency changes},
journal={Proceedings - 2nd International Symposium on Search Based Software Engineering, SSBSE 2010},
year={2010},
pages={31-35},
doi={10.1109/SSBSE.2010.13},
art_number={5635168},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952067358&doi=10.1109%2fSSBSE.2010.13&partnerID=40&md5=bc88661ca18f4e7f5fca166be454c7aa},
affiliation={Postgraduate Information Systems Program - UNIRIO, Av. Pasteur 458, Urca - Rio de Janeiro, RJ, Brazil},
abstract={Software testing is an expensive task that significantly contributes to the total cost of a software development project. Among the many strategies available to test a software project, the creation of automated test cases that can be enacted after building a release or resolving a defect is increasingly used in the industry. However, certain defects found in the system operation may block major business operations. These critical defects are sometimes resolved directly in the production environment under such a restricted deadline that there is not enough time to run the complete set of automated test cases upon the patched version of the software. Declining to run the test case suite allows a quicker release of the software to production, but also allows other defects to be introduced into the system. This paper presents a heuristic approach to select test cases that might support emergency changes aiming to maximize the coverage and diversity of the testing activity under a strict time constraint and given the priority of the features that were changed. © 2010 IEEE.},
author_keywords={Genetic algorithm;  Process development;  Risk management;  Software testing;  Time constraint},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yoo201019,
author={Yoo, S.},
title={A novel mask-coding representation for set cover problems with applications in test suite minimisation},
journal={Proceedings - 2nd International Symposium on Search Based Software Engineering, SSBSE 2010},
year={2010},
pages={19-28},
doi={10.1109/SSBSE.2010.12},
art_number={5635169},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952051900&doi=10.1109%2fSSBSE.2010.12&partnerID=40&md5=0b9d5f0694c9e637da94c1555e548f46},
affiliation={Centre for Research on Search, Evolution and Testing, King's College London, London, United Kingdom},
abstract={Multi-Objective Set Cover problem forms the basis of many optimisation problems in software testing because the concept of code coverage is based on the set theory. This paper presents Mask-Coding, a novel representation of solutions for set cover optimisation problems that explores the problem space rather than the solution space. The new representation is empirically evaluated with set cover problems formulated from real code coverage data. The results show that Mask-Coding representation can improve both the convergence and diversity of the Pareto-efficient solution set of the multi-objective set cover optimisation. © 2010 IEEE.},
author_keywords={Search-based software engineering;  Set-cover representation;  Test suite minimisation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nikiforidou2010537,
author={Nikiforidou, Z. and Pange, J.},
title={Teachers' evaluation of preschool educational software: The case of probabilistic thinking},
journal={Procedia - Social and Behavioral Sciences},
year={2010},
volume={9},
pages={537-541},
doi={10.1016/j.sbspro.2010.12.193},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951889865&doi=10.1016%2fj.sbspro.2010.12.193&partnerID=40&md5=2f3f7c5713950f6deae33b94a97da52b},
affiliation={Laboratory of New Technologies and Distance Learning, Department of Early Childhood Education, University of Ioannina, Ioannina, 45110, Greece},
abstract={Nowadays, children interact in formal and informal settings with technology from their very early age. As a result, a wide range of software has emerged with diverse purposes, usages, and applications. One such category is the software that is designed to assess children's prior knowledge and skills concerning a particular subject area. In this study, software testing children's abilities to make estimations and judgments based on probabilities was evaluated by preschool teachers. This pilot study took place in Greece, during 2009-2010, in a self selected group of in-service teachers (N = 45). After personal interaction with the software, educators filled in a questionnaire with their opinions. Further analysis was made in order to discriminate the criteria of great importance that comprise software as developmentally appropriate. The results revealed that not only the design but also the content, the process and the purpose of preschool software should be taken into consideration when implementing pedagogical software in Preschool Education. © 2010 Published by Elsevier Ltd.},
author_keywords={Preschooler education;  Probabilistic thinking;  Software evaluation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Brown2010,
author={Brown, M.K.},
title={Work in progress: A framework for parallel unit testing},
journal={Proceedings of the Annual Southeast Conference},
year={2010},
doi={10.1145/1900008.1900150},
art_number={110},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951834322&doi=10.1145%2f1900008.1900150&partnerID=40&md5=7b6f94ca9770591882928dd6ffc90d0d},
affiliation={Florida A and M University, Department of Computer and Information Sciences, Tallahassee, FL 32307-5100, United States},
abstract={The power of parallel computing needs to be exploited on software testing because of challenges that remain today, such improving test effectiveness and automating test input generation. There has been research on applying parallel computing to software testing, but there are still areas where the power of parallel computers can be exploited. Another challenge which has remained due to the lack of tool support is the oracle problem. We want to be relieved of tedious and manual processes in software testing. This paper proposes a framework for parallel unit testing. To alleviate the oracle problem, we will use the Java Modeling Language as the test oracle. We will take advantage of the power of parallel computing and apply it to random unit testing of Java classes to overcome some of today's challenges. We believe that random testing can be used to help us achieve this goal because it is effective and also cost-effective. We will show that we can overcome these challenges by generating and executing more test cases in the same amount of time, and by implementing data diversity. Our framework will be extensible to support additional programming languages and technique diversity. Copyright ©2010 ACM.},
author_keywords={Parallelization;  Random testing;  Unit testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Eltschlager2010,
author={Eltschlager, K.K.},
title={Federal blaster certificate program},
journal={Journal of Explosives Engineering},
year={2010},
volume={27},
number={6},
pages={32-38+46},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855389515&partnerID=40&md5=d9f05918bc2524adf5fe5523db7febc3},
affiliation={Office of Surface Mining Reclamation and Enforcement, Pittsburgh, PA, United States},
abstract={Blasting represents the highest risk of any surface coal mining activity that could result in injury and/or property damage off the permit area. In addition, blasting related issues e.g. flyrock, ground vibrations, air blast, etc., are responsible for a large percentage of citizen complaints. Thorough and consistent regulation of individuals certified to conduct blasting by the Office of Surface Mining Reclamation and Enforcement in Federal Program states and on Indian Lands under Federal jurisdiction is vital, as it serves to: facilitate coal production to meet the nation's energy needs; protect people and property on and adjacent to Federal mining permits; form a basis for states to grant reciprocity certificates to Federal blasters; and be a positive example to primacy states. OSM issues Federal blaster certificates for blasting on Federal surface coal mine permits. The certification program includes experience, training and testing components similar to many of the states. But where a state program may focus on the specific blasting applications within its boundary, the OSM certificate requires a very diverse test to cover all potential blasting applications and products across the nation. Therefore, the Federal certificate is a good surface mining credential for blasters anywhere in the United States.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2010151,
author={Wang, H. and Zhai, K. and Tse, T.H.},
title={Correlating context-awareness and mutation analysis for pervasive computing systems},
journal={Proceedings - International Conference on Quality Software},
year={2010},
pages={151-160},
doi={10.1109/QSIC.2010.57},
art_number={5562954},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958179258&doi=10.1109%2fQSIC.2010.57&partnerID=40&md5=ae706ce325353171d598dbd888d92510},
affiliation={Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong},
abstract={Pervasive computing systems often use middleware as a means to communicate with the changing environment. However, the interactions with the context-aware middleware as well as the interactions among applications sharing the same middleware may introduce faults that are difficult to reveal by existing testing techniques. Our previous work proposed the notion of context diversity as a metric to measure the degree of changes in test inputs for pervasive software. In this paper, we present a case study on how much context diversity for test cases relates to fault-based mutants in pervasive software. Our empirical results show that conventional mutation operators can generate sufficient candidate mutants to support test effectiveness evaluation of pervasive software, and test cases with higher context diversity values tend to have higher mean mutation scores. On the other hand, for test cases sharing the same context diversity, their mutation scores can vary significantly in terms of standard derivations © 2010 IEEE.},
author_keywords={Context diversity;  Mutation analysis;  Pervasive computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2010723,
author={Chen, T.Y.},
title={Fundamentals of test case selection: Diversity, diversity, diversity},
journal={2nd International Conference on Software Engineering and Data Mining, SEDM 2010},
year={2010},
pages={723-724},
art_number={5542825},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956529772&partnerID=40&md5=22b9321150dd5d8691fb02678e205b42},
affiliation={Centre for Software Analysis and Testing, Swinburne University of Technology, Hawthorn, VIC 3122, Australia},
abstract={Our recent investigations in software testing reveal that diversity constitutes the underlying foundation in many test case selection strategies. This talk attempts to provide an overview of the concept of diversity in test case selection through two families of test case selection strategies, namely, random testing and partition testing. We also present some areas of software testing where the application of data mining techniques shows great potential in identifying key aspects of diversity in various forms.},
author_keywords={Adaptive random testing;  Partition testing;  Proportional sampling strategy;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Godefroid20101,
author={Godefroid, P. and Kinder, J.},
title={Proving memory safety of floating-point computations by combining static and dynamic program analysis},
journal={ISSTA'10 - Proceedings of the 2010 International Symposium on Software Testing and Analysis},
year={2010},
pages={1-11},
doi={10.1145/1831708.1831710},
note={cited By 29},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955913455&doi=10.1145%2f1831708.1831710&partnerID=40&md5=4eec9abeebbe2d1000b1da8801404f28},
affiliation={Microsoft Research, Redmond, WA, United States; Technische Universität Darmstadt, Darmstadt, Germany},
abstract={Whitebox fuzzing is a novel form of security testing based on dynamic symbolic execution and constraint solving. Over the last couple of years, whitebox fuzzers have found many new security vulnerabilities (buffer overflows) in Windows and Linux applications, including codecs, image viewers and media players. Those types of applications tend to use floating-point instructions available on modern processors, yet existing whitebox fuzzers and SMT constraint solvers do not handle floating-point arithmetic. Are there new security vulnerabilities lurking in floating-point code? A naive solution would be to extend symbolic execution to floating-point (FP) instructions (months of work), extend SMT solvers to reason about FP constraints (months of work or more), and then face more complex constraints and an even worse path explosion problem. Instead, we propose an alternative approach, based on the rough intuition that FP code should only perform memory safe data-processing of the "payload" of an image or video file, while the non-FP part of the application should deal with buffer allocations and memory address computations, with only the latter being prone to buffer overflows and other security critical bugs. Our approach combines (1) a lightweight local path-insensitive "may" static analysis of FP instructions with (2) a high-precision whole-program path-sensitive "must" dynamic analysis of non-FP instructions. The aim of this combination is to prove memory safety of the FP part of each execution and a form of non-interference between the FP part and the non-FP part with respect to memory address computations. We have implemented our approach using two existing tools for, respectively, static and dynamic x86 binary analysis. We present preliminary results of experiments with standard JPEG, GIF and ANI Windows parsers. For a given test suite of diverse input files, our mixed static/dynamic analysis is able to prove memory safety of FP code in those parsers for a small upfront static analysis cost and a marginal runtime expense compared to regular dynamic symbolic execution. © 2010 ACM.},
author_keywords={Program verification;  Static and dynamic program analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheon20101020,
author={Cheon, Y. and Avila, C.},
title={Automating Java program testing using OCL and AspectJ},
journal={ITNG2010 - 7th International Conference on Information Technology: New Generations},
year={2010},
pages={1020-1025},
doi={10.1109/ITNG.2010.123},
art_number={5501499},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955293238&doi=10.1109%2fITNG.2010.123&partnerID=40&md5=0d11724d6fa122559d7e871b814aa267},
affiliation={Department of Computer Science, University of Texas at El Paso, El Paso, TX 79968, United States},
abstract={Random testing can eliminate subjectiveness in constructing test data and increase the diversity of test data. However, one difficult problem is to construct test oracles that decide test results - test failures or successes. Assertions can be used as test oracles and are most effective when they are derived from formal specifications such as OCL constraints. Random testing, if fully automated, can reduce the cost of testing dramatically. We propose an automated testing approach for Java programs by combining random testing and OCL. The key idea of our approach is to use OCL constraints as test oracles by translating them to runtime checks written in AspectJ. We implement our approach by adapting existing frameworks for translating OCL to AspectJ and assertion-based random testing. We evaluate the effectiveness of our approach through case studies and experiments. The results are encouraging in that our approach can detect errors in both implementations and OCL constraints and provide a practical means for using OCL. © 2010 IEEE.},
author_keywords={AspectJ;  Object constraint language;  Pre and postconditions;  Random testing;  Runtime assertion checking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Papadakis2010111,
author={Papadakis, M. and Malevris, N. and Kallia, M.},
title={Towards automating the generation of mutation tests},
journal={Proceedings - International Conference on Software Engineering},
year={2010},
pages={111-118},
doi={10.1145/1808266.1808283},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954949357&doi=10.1145%2f1808266.1808283&partnerID=40&md5=d3eb8f4626ac6d8a6ed1234b29cde965},
affiliation={Department of Informatics, Athens University of Economics and Business, Greece},
abstract={Automating software testing activities can increase the quality and drastically decrease the cost of software development. Towards this direction various automated test data generation tools have been developed. The majority of them aim at branch testing, while a quite limited number aim at a higher level of testing thoroughness such as mutation. In this paper an automated framework that makes a joint use of diverse techniques and tools is introduced in the context of automating mutation based test generation. The motivation behind this work is the use of existing techniques and tools such as symbolic execution and evolutionary testing towards automating the test input generation activity according to the weak mutation testing criterion. The proposed framework integrates existing automated tools for branch testing in order to effectively generate mutation test data. To fulfill this suggestion three automated tools are used for illustration purposes and preliminary results are obtained by applying the proposed framework to a set of java program units indicating the applicability and effectiveness of the proposed approach. Copyright 2010 ACM.},
author_keywords={Automated test case generation;  Concolic execution;  Genetic algorithms;  Mutation testing;  Symbolic execution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeLaRiva201067,
author={De La Riva, C. and Suárez-Cabal, M.J. and Tuya, J.},
title={Constraint-based test database generation for SQL queries},
journal={Proceedings - International Conference on Software Engineering},
year={2010},
pages={67-74},
doi={10.1145/1808266.1808276},
note={cited By 28},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954932788&doi=10.1145%2f1808266.1808276&partnerID=40&md5=403d4698e064bb9fdda02e77c59ac095},
affiliation={Computer Science Department, University of Oviedo, Campus Universitario, Gijón, Spain},
abstract={Populating test databases with meaningful test data is a difficult task as it involves generating data for manyjoined tables that must be diverse enough to be able to reveal faults and small enough to make the testing process efficient. This paper proposes an approach for the automatic generation of a test database for a set of SQL queries using a test criterion specifically tailored for the SQL language (SQLFpc). Given as input a schema database and a set of test requirements derived from the application of the test criterion to the target queries, the approach returns a database instance which satisfies the test requirements. Both the schema and the test requirements are modeled in the Alloy language, after which the analyzer generates the test database. The approach is evaluated on a real case study and the results show its feasibility, generating a test database of reduced size with an elevated coverage and mutation score. Copyright © 2010 ACM.},
author_keywords={Alloy toolset;  Database testing;  MCDC;  Software testing;  SQL coverage;  SQLFpc;  Test database generation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Nikanjam2010,
author={Nikanjam, M. and Rutherford, J. and Spreen, K.},
title={Performance and emissions of diesel and alternative diesel fuels in a heavy-duty industry-standard older engine},
journal={SAE Technical Papers},
year={2010},
doi={10.4271/2010-01-2281},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072365004&doi=10.4271%2f2010-01-2281&partnerID=40&md5=446fd5cb04f901109e33784f9be313dc},
affiliation={Chevron Products Co., United States; Chevron Corp, United States; Southwest Research Institute, United States},
abstract={Conventional diesel fuel has been in the market for decades and used successfully to run diesel engines of all sizes in many applications. In order to reduce emissions and to foster energy source diversity, new fuels such as alternative and renewable, as well as new fuel formulations have entered the market. These include biodiesel, gas-to-liquid, and alternative formulations by states such as California. Performance variations in fuel economy, emissions, and compatibility for these fuels have been evaluated and debated. In some cases contradictory views have surfaced. "Sustainable", "Renewable", and "Clean" designations have been interchanged. Adding to the confusion, results from one fuel in one type of engine such as an older heavy-duty engine, is at times compared to that of another type such as a modern light-duty. This study was an attempt to compare the performance of several fuels in an identical environment, using the same engine, for direct comparison. Results of a large-scale fleet test and emissions test to evaluate the performance of several diesel fuels in a modern heavy-duty diesel (HDD) engine were presented in a recent technical paper. This article is the second in this series and includes the use of a heavy-duty industry-standard older engine, DDC Series 60 on a stand, to evaluate emissions and fuel economy of a number of diesel fuels that cover a range of products being used in the North American market. EPA, California, Texas LED diesel, biodiesel, biodiesel blends, and gas-to-liquid fuel were tested in this program. Federal Test Procedure (FTP) for HDD and the 13-Mode European Stationary Cycle (ESC) portion of the Supplemental Emissions Test (SET) were used for emissions testing. This document will provide a detailed description of this project along with statistical analysis of test results for eight diesel fuels. 1 Copyright © 2010 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Feist2010100,
author={Feist, M.D. and Landau, M. and Harte, E.},
title={The effect of fuel composition on performance and emissions of a variety of natural gas engines},
journal={SAE Technical Papers},
year={2010},
pages={100-117},
doi={10.4271/2010-01-1476},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072361125&doi=10.4271%2f2010-01-1476&partnerID=40&md5=4f89d800cb1700acb740b8b0a90a13f8},
affiliation={Southwest Research Institute, United States; Southern California Gas Co., United States},
abstract={Work was performed to determine the feasibility of operating heavy-duty natural gas engines over a wide range of fuel compositions by evaluating engine performance and emission levels. Heavy-duty compressed natural gas engines from various engine manufacturers, spanning a range of model years and technologies, were evaluated using a diversity of fuel blends. Performance and regulated emission levels from these engines were evaluated using natural gas fuel blends with varying methane number (MN) and Wobbe Index in a dynamometer test cell. Eight natural gas blends were tested with each engine, and ranged from MN 75 to MN 100. Test engines included a 2007 model year Cummins ISL G, a 2006 model year Cummins C Gas Plus, a 2005 model year John Deere 6081H, a 1998 model year Cummins C Gas, and a 1999 model year Detroit Diesel Series 50G TK. All engines used lean-burn technology, except for the ISL G, which was a stoichiometric engine. Performance testing consisted of monitoring engine knock or auto-ignition, as well as engine power levels and overall engine operability. Emissions of total hydrocarbons (HC), non-methane hydrocarbons (NMHC), carbon monoxide (CO), oxides of nitrogen (NO x), nitrogen dioxide (NO 2), particulate matter (PM), and carbon dioxide (CO 2) were measured using procedures developed by the United States Environmental Protection Agency (EPA) for heavy-duty, on-highway engines. The engines showed no sign of auto ignition throughout the program, with slight differences in power levels with the various test fuels. All lean burn engines showed increased NO x and HC emission levels with decreased MN and increased Wobbe level, while the stoichiometric ISL G showed no clear trend in NO x or HC levels with the various fuels. Copyright © 2010 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hunley2010,
author={Hunley, S. and Whitman, J. and Baek, S. and Tan, X. and Kim, D.},
title={Incorporating the importance of interdisciplinary understanding in K-12 engineering outreach programs using a biomimetic device},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2010},
page_count={26},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029102934&partnerID=40&md5=5b07ea3bd24230ab22872175be0bacd9},
affiliation={Michigan State University, United States},
abstract={The project presented in this paper is designed to motivate interest in the engineering field for K-12 students, especially those who have previously viewed engineering as disconnected from biological sciences or the medical field. This idea is supported by recent trends in biomedical engineering, namely that the number of biomedical engineering bachelor's and master's degrees awarded throughout the United States has more than doubled since 2000, and that the demand for biomedical engineers will increase through 2010. However, to stimulate early interest in the biomedical engineering field, there is an apparent need for simple projects that clearly convey the relevance of engineering to biomedical contexts. This paper describes a novel educational program that seeks to achieve this connection at the K-12 understanding level using a build-and-test experimental device that incorporates physics, biology, teamwork, engineering analysis, and cutting edge technology into a single, integrative project. The build-and-test device used in this program is an actuator that simulates the action of sarcomeres (individual contractile units of muscle fibers) during muscle contraction, which demonstrates how creativity in engineering design may inspired by phenomenon found in nature. To build the device, a group of three or four students are assigned individual tasks that combine to produce a working device. The diversity of these specific tasks also allows students to identify areas of engineering that may pique their interest. Furthermore, the project implements new technology in the form of electroactive polymer (EAP), which produces a motion when subject to a voltage difference. After assembling the device and running the experiment, each student group gathers data from their test and determines basic engineering parameters (i.e., force, amount of work done) associated with the results of their experiment. Finally, the students are also given "challenge questions" to stimulate critical thinking skills by applying the same lessons used to complete their initial analysis in other contexts. We assess the quality of the program based on students' performance in building and testing the device within a given time frame, their answers to challenge questions and basic biological questions that form the basis of this project, and their feedback on the overall program. The authors finally suggest further improvements to the current project based on these assessments. © American Society for Engineering Education, 2010.},
document_type={Conference Paper},
source={Scopus},
}

@BOOK{Bui2010324,
author={Bui, N. and Castellani, A.P. and Casari, P. and Rossi, M. and Vangelista, L. and Zorzi, M.},
title={Implementation and performance evaluation of wireless sensor networks for smart grid},
journal={Smart Grid Communications and Networking},
year={2010},
volume={9781107014138},
pages={324-350},
doi={10.1017/CBO9781139013468.015},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924692258&doi=10.1017%2fCBO9781139013468.015&partnerID=40&md5=553a3e762b69a3781aa138967ae90e6e},
affiliation={University of Padova, Italy},
abstract={Introduction This chapter focuses on the usage of wireless sensor and actuator networks to provide data connectivity in smart grids. In particular, we discuss the configuration adopted for the implementation of the sensor network test-bed deployed at the Information Engineering Department of the University of Padova, Italy. The test-bed has been designed to reproduce typical deployment scenarios in an urban network by mimicking diverse contexts such as dense building networks, sparse environmental scenarios, and linear deployments along streets. The test-bed software has been realized taking full advantage of the most advanced solutions provided by the academic community and the standardization bodies by implementing a completely IP interoperable communication framework. Moreover, the latest solutions for the Internet of things [1] have been used to develop a lightweight modular architecture offering services and data sources through simple and efficient web services. All of this facilitates the integration of the test-bed functionalities into flexible web applications, capable of performing the needed monitoring and managing routines in the entire network as well as on single nodes. The Internet-like approach, coupled with a variety of network configurations, has been used to verify the advantages brought by the usage of constrained wireless communication for smart grids. In particular, we have been able to quantify useful performance metrics, such as maximum throughput, delivery delay, and transmission reliability, in typical smart grid network scenarios. Specifically, these performance metrics were determined for linearly shaped multihop configurations, to address networks deployed along streets, such as those controlling the street lights, as well as for dense single-and multihop configurations to address small-to-medium-sized building deployments. © Cambridge University Press 2012.},
document_type={Book Chapter},
source={Scopus},
}

@CONFERENCE{Realini2010,
author={Realini, E. and Yoshida, D. and Reguzzoni, M. and Raghavan, V.},
title={Testing goGPS low-cost RTK positioning with a web-based track log management system},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2010},
volume={38},
number={4W13},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923887090&partnerID=40&md5=10cef2970427131806d64040da525b9f},
affiliation={Graduate School for Creative Cities, Osaka City University, 3-3-138 Sugimoto, Sumiyoshi-ku, Osaka, 558-8585, Japan; Faculty of Liberal Arts, Tezukayama Gakuin University, 2-1823 Imakuma, Osaka Sayama City, Osaka, 589-8585, Japan; Dept. of Geophysics of the Lithosphere, OGS, Politecnico di Milano - Polo Regionale di Como, via Valleggio 11, Como, 22100, Italy},
abstract={Location-based online collaborative platforms are proving to be an effective and widely adopted solution for geospatial data collection, update and sharing. Popular collaborative projects like OpenStreetMap, Wikimapia and other services that collect and publish user-generated geographic contents have been fostered by the increasing availability of location-aware palmtop devices. These instruments include GPS-enabled mobile phones and low-cost GPS receivers, which are employed for quick field surveys at both professional and non-professional levels. Nevertheless, data collected with such devices are often not accurate enough to avoid heavy user intervention before using or sharing them. Providing tools for collecting and sharing accuracy-enhanced positioning data to a wide and diverse user base requires to integrate modern web technologies and online services with advanced satellite positioning techniques. A web-based prototype system for enhancing GPS tracks quality and managing track logs and points of interest (POI), originally developed using standard GPS devices, was tested by using goGPS software to apply kinematic relative positioning (RTK) with low-cost single-frequency receivers. The workflow consists of acquiring raw GPS measurements from the user receiver and from a network of permanent GPS stations, processing them by RTK positioning within goGPS Kalman filter algorithm, sending the accurate positioning data to the web-based system, performing further quality enhancements if needed, logging the data and displaying them. The whole system can work either in real-time or post-processing, the latter providing a solution to collect and publish enhanced location data without necessarily requiring mobile Internet connection on the field. Tests were performed in open areas and variously dense urban environments, comparing different indices for quality-based filtering. Results are promising and suggest that the integration of web technologies with advanced geodetic techniques applied to low-cost instruments can be an effective solution to collect, update and share accurate location data on collaborative platforms. © 2010 ISPRS Archives.},
author_keywords={Accuracy;  Cooperation;  GPS/INS;  Real-time;  Software;  Web based},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Smart2010,
author={Smart, J. and Davies, J. and Shirk, M. and Quinn, C. and Kurani, K.S.},
title={Electricity demand of PHEVs operated by private households and commercial fleets: Effects of driving and charging behavior},
journal={EVS 2010 - Sustainable Mobility Revolution: 25th World Battery, Hybrid and Fuel Cell Electric Vehicle Symposium and Exhibition},
year={2010},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907410919&partnerID=40&md5=1bd032f0eb35249f178a75a214948a97},
affiliation={Idaho National Laboratory, Energy Storage and Transportation Systems Department, 2351 N Boulevard, Idaho Falls, ID 83415, United States; Institute of Transportation Studies, University of California, Davis, One Shields Avenue, Davis, CA 95658, United States},
abstract={The U.S. Department of Energy's Advanced Vehicle Testing Activity - conducted by the Idaho National Laboratory for the U.S. Department of Energy's Vehicle Technologies Program - in partnership with the University of California, Davis's Institute for Transportation Studies, has collected data from a fleet of plug-in hybrid electric vehicle (PHEV) conversions, placed into diverse operating environments, to quantify the petroleum displacement potential of early PHEV models. This demonstration also provided an opportunity to assess the impact of PHEVs on the electric grid based on observed, rather than simulated, vehicle driving and charging behavior. This paper presents the electricity demand of PHEVs operating in undirected, real-world conditions. For personal-use vehicles on weekdays, peak power demand to charge the PHEVs and the period of greatest variability in demand across weekdays occurred during an evening peak that started around 4:00 p.m. and rose until 10:00 p.m. On weekends, peak demand also occurred at 10:00 p.m., but at lower magnitude than on weekdays because fewer vehicles were plugged-in and those that were typically had a higher battery state of charge. For the commercial-use group, peak demand occurred between 2:00 and 7:00 p.m. on weekdays, varying with the day of the week, and around 10:00 p.m. on the weekend. Weekend demand was significantly less than weekday demand. Driving and charging behaviors are examined for both commercial-use and personal-use vehicles. Underlying reasons for charging behavior, based on interviews and survey responses, are also presented. © EVS-25 Shenzhen, China.},
author_keywords={Electric grid impact;  Plug-in hybrid electric vehicle (PHEV) charging},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ivanova20102212,
author={Ivanova, T. and Fernex, F. and Kolbe, E. and Vasiliev, A. and Lee, G.S. and Woo, S.N. and Mennerdahl, D. and Nagaya, Y. and Neuber, J.-C. and Hoefer, A. and Rearden, B. and Mueller, D. and Rugama, Y. and Santamarina, A. and Venard, C. and Tsiboulia, A. and Golovko, Y.},
title={OECD/NEA expert group on uncertainty analysis for criticality safety assessment: Current activities},
journal={International Conference on the Physics of Reactors 2010, PHYSOR 2010},
year={2010},
volume={3},
pages={2212-2226},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952369053&partnerID=40&md5=da50d05e5e2b6ad479cbfa4da338f9cb},
affiliation={Institut de Radioprotection et de Sûreté Nucléaire, BP 17, 92262 Fontenay aux Roses, France; Paul Scherrer Institute, CH 5232 Villigen PSI, Switzerland; Korea Institute of Nuclear Safety, 34 Gwahak-ro, Yuseong-gu, Daejeon 305-338, South Korea; E Mennerdahl Systems, Starvägen 12, 18357 Täby, Sweden; Japan Atomic Energy Agency, Tokai-mura, Naka-gun, Ibaraki-ken, 319-1195, Japan; AREVA NP GmbH, Kaiserleistrasse 29, 63005 Offenbach am Main, Germany; Oak Ridge National Laboratory, M.S. 6170, P.O. Box 2008, Oak Ridge, TN 37831, United States; OECD/NEA, 12, Bd des Iles, 92130 Issy-les-Moulineaux, France; CEA/Cadarache, 13108 Saint-Paul-lez-Durance, France; Institute for Physics and Power Engineering, 1, Bondarenko sq., 249033 Obninsk, Russian Federation},
abstract={The expert group (EG) on Uncertainty Analysis for Criticality Safety Assessment (UACSA) was established within the OECD/NEA Working Party on Nuclear Criticality Safety in December 2007 to promote exchange of information on related topics; compare methods and software tools for uncertainty analysis; test their performance; and assist in selection/development of safe and efficient methodologies. At the current stage, the work of the group is focused on approaches for validation of criticality calculations. With the diversity of the approaches to validate criticality calculations, a thorough description of each approach and assessment of its performance is useful to the criticality safety community. Developers, existing and potential practitioners as well as reviewers of assessments using those approaches should benefit from this effort. Exercise Phase I was conducted in order to illustrate predictive capabilities of criticality validation approaches, which include similarity assessment, definition of keff bias and bias uncertainty, and selection of benchmarks. The approaches and results of the exercises will be thoroughly documented in a pending state-of-the-art report from the EG. This paper provides an overview of current and future activities for the EG, a summary of the participant-contributed validation approaches, and a synthesis of the results for the exercises.},
author_keywords={Benchmark;  Bias;  Criticality safety;  OECD/NEA;  Subcritical limit;  Uncertainty},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jiang2010,
author={Jiang, J. and Liu, L. and Jiang, T.},
title={Notice of Retraction: Study of high power motor intelligent protection controller based on PIC singlechip},
journal={2nd International Conference on Information Engineering and Computer Science - Proceedings, ICIECS 2010},
year={2010},
doi={10.1109/ICIECS.2010.5677716},
art_number={5677716},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951604354&doi=10.1109%2fICIECS.2010.5677716&partnerID=40&md5=b22986366397abd0e47ee26e33df0130},
affiliation={School of Electric and Electronic Engineering, Shandong University of Technology, Zibo, 255091, China; College of Automation Engineering, Qingdao University, Qingdao, 266071, China},
abstract={In response to industry power motor cannot directly starting and stopping issues, a hign power motor intelligent protection controller based on PIC singlechip is proposed in this paper. Intelligent soft starter design principles based on PIC singlechip is articulated. The hardware circuit design, software flow design and test data analysis are given in details. By producting in Zibo Galaxy high-technology development co., Ltd.,it shows that this smart soft starter has the characteristics which are flexible parameter setting, intuitive liquid crystal display, diverse starting way, precise current limiting protection, accurate protection of lack phase, low-cost. So it has broad application prospects. ©2010 IEEE.},
author_keywords={Liquid crystal display(LCD);  Motor;  PIC singlechip;  Soft start},
document_type={Retracted},
source={Scopus},
}

@CONFERENCE{Zhang201010,
author={Zhang, L. and Liu, Y. and Guo, W.},
title={Research on diversified designing methods and user evaluation of smartphone interface},
journal={Proceedings - 2010 International Symposium on Computational Intelligence and Design, ISCID 2010},
year={2010},
volume={2},
pages={10-13},
doi={10.1109/ISCID.2010.89},
art_number={5692721},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951601699&doi=10.1109%2fISCID.2010.89&partnerID=40&md5=4cfbc247684fbdb258ab9db92a0f6407},
affiliation={School of Design, Jiangnan University, Wuxi, China},
abstract={The smartphone featured with surpassing the traditional-functioned featurephones turns out to be the future trend. This paper discusses the differences between smartphones and featurephones, new features of the software interface of smartphones, such as functions, size of its screen, multi-touch and intelligent sensing technology, and some peculiarities of software interface of smartphone, such as its property of being sensed or understood, efficiency and artistic quality, etc.. It also discusses concrete methods of diverse design of the overall presentation of interface signs, visual style, and specific operation, etc.. Furthermore, this article conducts the user evaluation test of actual using results of software interface for three kinds of smart phone system and quantitative analysis. These are conducive for designers to master the evaluation of specific function of interface presenting effect, and making possible improvements. © 2010 IEEE.},
author_keywords={Diversify;  Smartphone;  Software interface;  User evaluation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Schneidewind2010294,
author={Schneidewind, N.},
title={Software testing and reliability strategies},
journal={Journal of Aerospace Computing, Information and Communication},
year={2010},
volume={7},
number={9},
pages={294-307},
doi={10.2514/1.49220},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957880907&doi=10.2514%2f1.49220&partnerID=40&md5=48525ca8ce561bea6a02a8ce6ca27e84},
affiliation={Naval Postgraduate School, Pebble Beach, CA 93953, United States},
abstract={Software testing strategies are developed based on fault removal-deterministic and random. Based on literature search, it is appears that this is the first research that has evaluated these strategies. Deterministic fault removal is determined by the ranking of deviations of the time of fault occurrence from statistical control chart upper limit. Random removal of faults is selected by random number generation. Counterintuitively, random fault removal proved to be the better strategy based on improvement in reliability metrics across a series of tests. Contrariwise, deterministic fault removal was better when the cost effectiveness of fault removal was considered. Fault and failure data from three diverse projects were used to make the strategy assessment: NASA Space Shuttle flight software, Japanese University application, and a database application. Copyright © 2010 by the American Institute of Aeronautics and Astronautics, Inc.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Korn2010602,
author={Korn, J.},
title={Statics and dynamics of hierarchy},
journal={Kybernetes},
year={2010},
volume={39},
number={4},
pages={602-624},
doi={10.1108/03684921011036826},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951991243&doi=10.1108%2f03684921011036826&partnerID=40&md5=416a3f7d97267c02abfed84ce438d7f3},
affiliation={Middlesex University, London, United Kingdom},
abstract={Purpose: The purpose of this paper is to describe how ordered pairs representing related objects in static state are used to create hierarchical structures yielding rapidly increasing choices of complex objects to be selected by objects in their environment and how "purposive systems" evolve for the production of such structures. Design/methodology/approach: Basic notions transcending discipline boundaries and natural language formalised into one- and two-place sentences are suggested as related constituents of complexity and hierarchy, the "systemic view". This leads into sets of ordered pairs and sequences of qualified predicate logic statements forming dynamics of systems. Findings: Hierarchies in general can be expressed as ordered pairs. An analytical method for showing how ordered pairs are organised into progressively more complex structures of objects and "products" with increased chances of being selected by environmental objects in evolution or design. Correspondingly, groups of purposive systems operating according to algorithms are needed for the production of products or their evolution is left to chance. Research limitations/implications: The approach uses natural language as the primary model transformed into a formal language for reasoning about outcomes of scenarios with inanimate and animate components with predominantly qualitative properties, emotions and will. The desirability of such an approach, although it matches the generality of the systemic view, needs to be debated. Practical implications: Once past the test of acceptability and software development, the approach can be used as part of "design methodology" for the design of "systems and products" in the context of human activity and technical scenarios. Originality/value: The formal language exhibits properties, relations and interactions or impressions of objects of great diversity and variety. It exhibits the effects of these constituents on the production of outcomes based on semantic and mathematical relationships; it is widely applicable and may facilitate the appreciation of how "related objects evolve". © Emerald Group Publishing Limited.},
author_keywords={Complexity theory;  Cybernetics;  Linguistics;  Modelling;  Statics},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chen201060,
author={Chen, T.Y. and Kuo, F.-C. and Merkel, R.G. and Tse, T.H.},
title={Adaptive Random Testing: The ART of test case diversity},
journal={Journal of Systems and Software},
year={2010},
volume={83},
number={1},
pages={60-66},
doi={10.1016/j.jss.2009.02.022},
note={cited By 251},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-71649102481&doi=10.1016%2fj.jss.2009.02.022&partnerID=40&md5=9627a98b1df3d81264badbd50acc6f84},
affiliation={Faculty of Information and Communication Technologies, Swinburne University of Technology, John St, Hawthorn, 3122, Australia; Department of Computer Science, The University of Hong Kong, Pokfulam, Hong Kong},
abstract={Random testing is not only a useful testing technique in itself, but also plays a core role in many other testing methods. Hence, any significant improvement to random testing has an impact throughout the software testing community. Recently, Adaptive Random Testing (ART) was proposed as an effective alternative to random testing. This paper presents a synthesis of the most important research results related to ART. In the course of our research and through further reflection, we have realised how the techniques and concepts of ART can be applied in a much broader context, which we present here. We believe such ideas can be applied in a variety of areas of software testing, and even beyond software testing. Amongst these ideas, we particularly note the fundamental role of diversity in test case selection strategies. We hope this paper serves to provoke further discussions and investigations of these ideas. © 2009 Elsevier Inc. All rights reserved.},
author_keywords={Adaptive random sequence;  Adaptive random testing;  Failure pattern;  Failure-based testing;  Random testing;  Software testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Craeye2009,
author={Craeye, B. and De Schutter, G. and Van Humbeeck, H. and Wacquier, W. and Van Cotthem, A.},
title={Finite element computation of early age thermal cracking in concrete supercontainers for radioactive waste disposal},
journal={Proceedings of the 12th International Conference on Civil, Structural and Environmental Engineering Computing},
year={2009},
page_count={19},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858414847&partnerID=40&md5=647c48af68a2451c3efec66679cc1827},
affiliation={Magnel Laboratory for Concrete Research, Ghent University, Belgium; ONDRAF/NIRAS, Belgian Agency for Radioactive Waste and Enriched Fissile Materials, Brussels, Belgium; Tractebel Development Engineering, Brussels, Belgium},
abstract={Worldwide thousands of m 3 of highly active radioactive waste is being produced each year originating from the most diverse sources. The Belgian reference disposal concept is founded on the use of a concrete Supercontainer deeply stored inside geological Clay layers. The container concept is based on a cylindrical multiple barrier system where the carbon steel heat-emitting waste canister is enclosed by a non-reinforced massive concrete buffer and an outer stainless steel liner. For the realization of the buffer two types of concrete are being tested and compared. In massive hardening concrete, the hydration heat has a considerable effect on the stress development inside the concrete structure due to the created thermal gradients. Therefore finite elements simulations are performed during the fabrication of the supercontainer. The thermal, mechanical and maturity related properties of the two types of concrete considered for the buffer are obtained via a laboratory characterization program and large scale tests and are implemented into the material database of the simulation program. This engineering tool yields the evolving temperature fields and the resulting stresses at any time during hardening. Through-going cracks inside the buffer, which will considerably diminish the durability and safety of the Belgian storage concept for high level waste, are not expected for both types of concrete. © Civil-Comp Press, 2009.},
author_keywords={Concrete;  High level waste (HLW);  Supercontainer;  Thermal cracking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Groce200922,
author={Groce, A.},
title={(Quickly) testing the tester via path coverage},
journal={WODA 2009 - Proceedings of the 7th International Workshop on Dynamic Analysis, Held in Conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA 2009},
year={2009},
pages={22-28},
doi={10.1145/2134243.2134249},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858019564&doi=10.1145%2f2134243.2134249&partnerID=40&md5=4eef05019463699557855b49cb1bc77c},
affiliation={Laboratory for Reliable Software, Jet Propulsion Laboratory, California Institute of Technology, Pasadena, CA 91109, United States; Oregon State University, School of Electrical Engineering and Computer Science, Corvallis, OR 97331, United States},
abstract={The configuration complexity and code size of an automated testing framework may grow to a point that the tester itself becomes a significant software artifact, prone to poor configuration and implementation errors. Unfortunately, testing the tester by using old versions of the software under test (SUT) may be impractical or impossible: test framework changes may have been motivated by interface changes in the tested system, or fault detection may become too expensive in terms of computing time to justify running until errors are detected on older versions of the software. We propose the use of path coverage measures as a "quick and dirty" method for detecting many faults in complex test frameworks. We also note the possibility of using techniques developed to diversify state-space searches in model checking to diversify test focus, and an associated classification of tester changes into focus-changing and non-focus-changing modifications. Copyright 2009 ACM.},
author_keywords={Evaluation of test systems;  Regression testing;  Test frameworks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Srivastava200930,
author={Srivastava, P.R. and Vijay, A. and Bariikha, B. and Senear, P.S. and Sharma, R.},
title={An optimized technique for test case generation and prioritization using "tabu" search and data clustering},
journal={Proceedings of the 4th Indian International Conference on Artificial Intelligence, IICAI 2009},
year={2009},
pages={30-46},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857607266&partnerID=40&md5=ceb4d216dcaf0ff6940dc985262649de},
affiliation={Computer Science and Information System Group, BITS PILANI, Pilani, 333031, India},
abstract={In practice, an available testing budget limits the number of test cases that can be executed over particular software. This paper presents a "Tabu" search algorithm for the automatic generation of software test cases and their prioritization through clustering technique of data mining. The developed test case generator has a cost function for intensifying the search and another for diversifying the search, used when the intensification is not successful. It also combines the use of memory with a backtracking process to avoid getting stuck in local minima. Test case prioritization technique schedules test cases in an order that increases their effectiveness in meeting some performance goal. Copyright © 2009 by IICAI.},
author_keywords={Clustering;  Dynamic test case generation;  Tabu search;  Test case prioritization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang2009610,
author={Wang, H. and Chan, W.K.},
title={Weaving context sensitivity into test suite construction},
journal={ASE2009 - 24th IEEE/ACM International Conference on Automated Software Engineering},
year={2009},
pages={610-614},
doi={10.1109/ASE.2009.79},
art_number={5431725},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952134081&doi=10.1109%2fASE.2009.79&partnerID=40&md5=f44a00c9776375ff3afc70d77a9f6e9f},
affiliation={Department of Computer Science, University of Hong Kong, Pokfulam, Hong Kong; Department of Computer Science, City University of Hong Kong, Tat Chee Avenue, Hong Kong},
abstract={Context-aware applications capture environmental changes as contexts and self-adapt their behaviors dynamically. Existing testing research has not explored context evolutions or their patterns inherent to individual test cases when constructing test suites. We propose the notation of context diversity as a metric to measure how many changes in contextual values of individual test cases. In this paper, we discuss how this notion can be incorporated in a test case generation process by pairing it with coverage-based test data selection criteria. © 2009 IEEE.},
author_keywords={Context diversity;  Context-aware programe;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tan2009344,
author={Tan, X.B. and Longxin, C. and Xiumei, X.},
title={Test data generation using annealing immune genetic algorithm},
journal={NCM 2009 - 5th International Joint Conference on INC, IMS, and IDC},
year={2009},
pages={344-348},
doi={10.1109/NCM.2009.56},
art_number={5331701},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-73549105014&doi=10.1109%2fNCM.2009.56&partnerID=40&md5=1eac4f808a40ce1d37dd29368f92409c},
affiliation={Institute of Computer, Foshan Vocational and Technical College, Guangzhou 510090, China; Institute of Policy and Management, Chinese Academy of Science, CAS, Beijing 100080, China; Computer and Information Technology, Beijing Jiaotong University, BJU, Beijing 100044, China},
abstract={With the development of software technology and the expansion of software project scale, software testing appears to be more crucial. And test data selection is one of the nodi during software structure testing because the suitability of test data may directly affect error detection. Notwithstanding existence of several methods to generate test data automatically, such an algorithm overcoming disadvantages of the existing methods in practice hasn't been brought out, that some errors still have to be detected by engineering experience. Therefore, this paper analyzes the characteristics and shortcomings of simple genetic algorithm, simulated annealing genetic algorithm as well as immune algorithm respectively. Aiming at solving the shortcomings in standard Genetic Algorithm on search efficiency, individual diversity and premature, the Annealing Immune Genetic Algorithm (AIGA) is presented as the core algorithm of test data generation by introducing the mechanism of reproduction rate adjustment of individual concentration of immune algorithm and annealing principium into genetic algorithm. Finally, AIGA mentioned above was applied and verified with a practical software testing example. © 2009 IEEE.},
author_keywords={Expectation of reproduction;  Genetic algorithm;  Software testing;  Test data generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tummeltshammer2009449,
author={Tummeltshammer, P. and Steininger, A.},
title={Power supply induced common cause faults - Experimental assessment of potential countermeasures},
journal={Proceedings of the International Conference on Dependable Systems and Networks},
year={2009},
pages={449-457},
doi={10.1109/DSN.2009.5270308},
art_number={5270308},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449849744&doi=10.1109%2fDSN.2009.5270308&partnerID=40&md5=0407dceb5dc305d1b254154aed3c837c},
affiliation={Vienna University of Technology, Embedded Computing Systems Group, Treitlstrasse 3, A-1040 Vienna, Austria},
abstract={Fault-tolerant architectures based on physical replication of components are vulnerable to faults that cause the same effect in all replica. Short outages in a power supply shared by all replica are a prominent example for such common cause faults. For systems in which the provision of a replicated power supply would cause prohibitive efforts the identification of reliable countermeasures against these effects is vital to maintain the required dependability level. In this paper we propose several of such countermeasures, namely parity protection, voltage monitoring and time diversity of the replica. We perform extensive fault injection experiments on three fault-tolerant dual core processor designs, one FPGA based and two commercial ASICs. These experiments provide evidence for the vulnerability of a completely unprotected dual core solution, while time diversity and voltage monitoring in combination with increased timing margins turn out particularly effective for eliminating common cause effects. ©2009 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lin2009711,
author={Lin, C.-T. and Huang, C.-Y.},
title={Staffing level and cost analyses for software debugging activities through rate-based simulation approaches},
journal={IEEE Transactions on Reliability},
year={2009},
volume={58},
number={4},
pages={711-724},
doi={10.1109/TR.2009.2019669},
art_number={4967914},
note={cited By 22},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957956368&doi=10.1109%2fTR.2009.2019669&partnerID=40&md5=b2afa3ab8629ebee7d19794253cd0691},
affiliation={Department of Computer Science, National Tsing Hua University, Hsinchu, Taiwan},
abstract={Research in the field of software reliability, dedicated to the analysis of software failure processes, is quite diverse. In recent years, several attractive rate-based simulation approaches have been proposed. Thus far, it appears that most existing simulation approaches do not take into account the number of available debuggers (or developers). In practice, the number of debuggers will be carefully controlled. If all debuggers are busy, they may not address newly detected faults for some time. Furthermore, practical experience shows that fault-removal time is not negligible, and the number of removed faults generally lags behind the total number of detected faults, because fault detection activities continue as faults are being removed. Given these facts, we apply the queueing theory to describe and explain possible debugging behavior during software development. Two simulation procedures are developed based on G/G/∞, and G/G/m queueing models, respectively. The proposed methods will be illustrated using real software failure data. The analysis conducted through the proposed framework can help project managers assess the appropriate staffing level for the debugging team from the standpoint of performance, and cost-effectiveness. © 2009 IEEE.},
author_keywords={Fault correction;  Non-homogeneous Poisson process (NHPP);  Software reliability growth model (SRGM);  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nachman2009,
author={Nachman, I. and Regev, A.},
title={BRNI: Modular analysis of transcriptional regulatory programs},
journal={BMC Bioinformatics},
year={2009},
volume={10},
doi={10.1186/1471-2105-10-155},
art_number={155},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-67650917123&doi=10.1186%2f1471-2105-10-155&partnerID=40&md5=0ed126a575ca600646e12215cb6377d7},
affiliation={FAS, Center for System Biology, Harvard University, Cambridge, MA 02138, United States; Broad Institute, MIT, Cambridge, MA 02142, United States; Department of Biology, Massachusetts Institute of Technology, Cambridge, MA 02142, United States},
abstract={Background: Transcriptional responses often consist of regulatory modules - sets of genes with a shared expression pattern that are controlled by the same regulatory mechanisms. Previous methods allow dissecting regulatory modules from genomics data, such as expression profiles, protein-DNA binding, and promoter sequences. In cases where physical protein-DNA data are lacking, such methods are essential for the analysis of the underlying regulatory program. Results: Here, we present a novel approach for the analysis of modular regulatory programs. Our method - Biochemical Regulatory Network Inference (BRNI) - is based on an algorithm that learns from expression data a biochemically-motivated regulatory program. It describes the expression profiles of gene modules consisting of hundreds of genes using a small number of regulators and affinity parameters. We developed an ensemble learning algorithm that ensures the robustness of the learned model. We then use the topology of the learned regulatory program to guide the discovery of a library of cis-regulatory motifs, and determined the motif compositions associated with each module. We test our method on the cell cycle regulatory program of the fission yeast. We discovered 16 coherent modules, covering diverse processes from cell division to metabolism and associated them with 18 learned regulatory elements, including both known cell-cycle regulatory elements (MCB, Ace2, PCB, ACCCT box) and novel ones, some of which are associated with G2 modules. We integrate the regulatory relations from the expression- and motif-based models into a single network, highlighting specific topologies that result in distinct dynamics of gene expression in the fission yeast cell cycle. Conclusion: Our approach provides a biologically-driven, principled way for deconstructing a set of genes into meaningful transcriptional modules and identifying their associated cis-regulatory programs. Our analysis sheds light on the architecture and function of the regulatory network controlling the fission yeast cell cycle, and a similar approach can be applied to the regulatory underpinnings of other modular transcriptional responses. © 2009 Nachman and Regev; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Crespi20093289,
author={Crespi, M. and de Vendictis, L.},
title={A procedure for high resolution satellite imagery quality assessment},
journal={Sensors},
year={2009},
volume={9},
number={5},
pages={3289-3313},
doi={10.3390/s90503289},
note={cited By 50},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77449127524&doi=10.3390%2fs90503289&partnerID=40&md5=3e01c8ce07876957c43b1789475948d9},
affiliation={DITS, Area di Geodesia e Geomatica, Sapienza Università di Roma, via Eudossiana 18, 00184 Rome, Italy; Telespazio S.p.A., via Tiburtina 956, 00156 Rome, Italy},
abstract={Data products generated from High Resolution Satellite Imagery (HRSI) are routinely evaluated during the so-called in-orbit test period, in order to verify if their quality fits the desired features and, if necessary, to obtain the image correction parameters to be used at the ground processing center. Nevertheless, it is often useful to have tools to evaluate image quality also at the final user level. Image quality is defined by some parameters, such as the radiometric resolution and its accuracy, represented by the noise level, and the geometric resolution and sharpness, described by the Modulation Transfer Function (MTF). This paper proposes a procedure to evaluate these image quality parameters; the procedure was implemented in a suitable software and tested on high resolution imagery acquired by the QuickBird, WorldView-1 and Cartosat-1 satellites. © 2009 by the authors; licensee Molecular Diversity Preservation International, Basel, Switzerland.},
author_keywords={Actual resolution;  High resolution satellite imagery quality;  Modulation transfer function;  Noise;  Radiometric analysis},
document_type={Article},
source={Scopus},
}

@ARTICLE{Blanco2009708,
author={Blanco, R. and Tuya, J. and Adenso-Díaz, B.},
title={Automated test data generation using a scatter search approach},
journal={Information and Software Technology},
year={2009},
volume={51},
number={4},
pages={708-720},
doi={10.1016/j.infsof.2008.11.001},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58549093345&doi=10.1016%2fj.infsof.2008.11.001&partnerID=40&md5=234d89dd733e243c479947120c0fc0cf},
affiliation={Department of Computer Science, University of Oviedo, Campus de Viesques, Gijón, Asturias 33204, Spain; Department of Management Science, University of Oviedo, Campus de Viesques, Gijón, Asturias 33204, Spain},
abstract={The techniques for the automatic generation of test cases try to efficiently find a small set of cases that allow a given adequacy criterion to be fulfilled, thus contributing to a reduction in the cost of software testing. In this paper we present and analyze two versions of an approach based on the scatter search metaheuristic technique for the automatic generation of software test cases using a branch coverage adequacy criterion. The first test case generator, called TCSS, uses a diversity property to extend the search of test cases to all branches of the program under test in order to generate test cases that cover these. The second, called TCSS-LS, is an extension of the previous test case generator which combines the diversity property with a local search method that allows the intensification of the search for test cases that cover the difficult branches. We present the results obtained by our generators and carry out a detailed comparison with many other generators, showing a good performance of our approach. © 2008 Elsevier B.V. All rights reserved.},
author_keywords={Automatic test case generation;  Branch coverage;  Local search;  Metaheuristic search techniques;  Scatter search;  Software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Catal20091040,
author={Catal, C. and Diri, B.},
title={Investigating the effect of dataset size, metrics sets, and feature selection techniques on software fault prediction problem},
journal={Information Sciences},
year={2009},
volume={179},
number={8},
pages={1040-1058},
doi={10.1016/j.ins.2008.12.001},
note={cited By 227},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-59149091560&doi=10.1016%2fj.ins.2008.12.001&partnerID=40&md5=f70bba2e18c98b24303556da7fae5ef4},
affiliation={TUBITAK-Marmara Research Center, Information Technologies Institute, Gebze, Kocaeli 41470, Turkey},
abstract={Software quality engineering comprises of several quality assurance activities such as testing, formal verification, inspection, fault tolerance, and software fault prediction. Until now, many researchers developed and validated several fault prediction models by using machine learning and statistical techniques. There have been used different kinds of software metrics and diverse feature reduction techniques in order to improve the models' performance. However, these studies did not investigate the effect of dataset size, metrics set, and feature selection techniques for software fault prediction. This study is focused on the high-performance fault predictors based on machine learning such as Random Forests and the algorithms based on a new computational intelligence approach called Artificial Immune Systems. We used public NASA datasets from the PROMISE repository to make our predictive models repeatable, refutable, and verifiable. The research questions were based on the effects of dataset size, metrics set, and feature selection techniques. In order to answer these questions, there were defined seven test groups. Additionally, nine classifiers were examined for each of the five public NASA datasets. According to this study, Random Forests provides the best prediction performance for large datasets and Naive Bayes is the best prediction algorithm for small datasets in terms of the Area Under Receiver Operating Characteristics Curve (AUC) evaluation parameter. The parallel implementation of Artificial Immune Recognition Systems (AIRS2Parallel) algorithm is the best Artificial Immune Systems paradigm-based algorithm when the method-level metrics are used. © 2008 Elsevier Inc. All rights reserved.},
author_keywords={Artificial Immune Systems;  J48;  Machine learning;  Naive Bayes;  Random Forests;  Software fault prediction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Chan2009422,
author={Chan, W.K. and Cheung, S.C. and Ho, J.C.F. and Tse, T.H.},
title={PAT: A pattern classification approach to automatic reference oracles for the testing of mesh simplification programs},
journal={Journal of Systems and Software},
year={2009},
volume={82},
number={3},
pages={422-434},
doi={10.1016/j.jss.2008.07.019},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-61349178878&doi=10.1016%2fj.jss.2008.07.019&partnerID=40&md5=31bfadf952d97ea2a6bf7fef6c7f8a20},
affiliation={City University of Hong Kong, Tat Chee Avenue, Hong Kong; The Hong Kong University of Science and Technology, Clear Water Bay, Hong Kong; University College London, Gower Street, London, United Kingdom; The University of Hong Kong, Pokfulam, Hong Kong},
abstract={Graphics applications often need to manipulate numerous graphical objects stored as polygonal models. Mesh simplification is an approach to vary the levels of visual details as appropriate, thereby improving on the overall performance of the applications. Different mesh simplification algorithms may cater for different needs, producing diversified types of simplified polygonal model as a result. Testing mesh simplification implementations is essential to assure the quality of the graphics applications. However, it is very difficult to determine the oracles (or expected outcomes) of mesh simplification for the verification of test results. A reference model is an implementation closely related to the program under test. Is it possible to use such reference models as pseudo-oracles for testing mesh simplification programs? If so, how effective are they? This paper presents a fault-based pattern classification methodology called PAT, to address the questions. In PAT, we train the C4.5 classifier using black-box features of samples from a reference model and its fault-based versions, in order to test samples from the subject program. We evaluate PAT using four implementations of mesh simplification algorithms as reference models applied to 44 open-source three-dimensional polygonal models. Empirical results reveal that the use of a reference model as a pseudo-oracle is effective for testing the implementations of resembling mesh simplification algorithms. However, the results also show a tradeoff: When compared with a simple reference model, the use of a resembling but sophisticated reference model is more effective and accurate but less robust. © 2008 Elsevier Inc. All rights reserved.},
author_keywords={Graphics rendering;  Mesh simplification;  Pattern classification reference models;  Software testing;  Test oracles},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nikanjam2009,
author={Nikanjam, M. and Rutherford, J. and Byrne, D. and Lyford-Pike, E.J. and Bartoli, Y.},
title={Performance and emissions of diesel and alternative diesel fuels in a modern heavy-duty vehicle},
journal={SAE Technical Papers},
year={2009},
doi={10.4271/2009-01-2649},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072457875&doi=10.4271%2f2009-01-2649&partnerID=40&md5=afa2e0a0a49728b103cd9811e8ce5827},
affiliation={Chevron Products Company; Alameda-Contra Costa Transit District; Cummins Inc},
abstract={Conventional diesel fuel (1) has been on the market for decades and used successfully to run diesel engines of all sizes in many applications.* In order to reduce emissions and to foster energy source diversity, new fuels such as alternative and renewable, as well as new formulations, have entered the market. These include biodiesel, gas-to-liquid, and alternative formulations by states such as California. Performance variations in fuel economy, emissions, and compatibility for these fuels have been evaluated and debated. In some cases, contradictory views have surfaced. "Renewable" and "clean" designations have been interchanged. Adding to the confusion, results from one fuel in one type of engine, such as an older heavy-duty engine, is at times compared to that of another type, such as a modern light-duty engine. Two fuel suppliers, an engine manufacturer, and a large transit district have worked in partnership to conduct a large-scale fleet and emissions test to evaluate the performance of several diesel fuels in a controlled and statistically designed research program. California Air Resources Board (CARB) diesel fuel (2), a 20% biodiesel blend (B20) (3), and gas-to-liquid (GTL) fuel were tested for 18,000 miles in several modern urban buses. Additional fuels were tested in a controlled environment in a heavy-duty chassis dynamometer facility, using the same engine type, to determine the emissions level and fuel economy. This document provides a detailed description of the program along with statistical analysis of test results for eight diesel and alternative diesel fuels that cover a range of products used in the North American market. Copyright © 2009 SAE International.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Andaloro2009,
author={Andaloro, F. and Ferraro, M. and Azzurro, E. and Mostarda, E. and Consoli, P.},
title={The use of underwater visual censuses to study fish diversity associated with off-shore platforms},
journal={Offshore Mediterranean Conference and Exhibition 2009, OMC 2009},
year={2009},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055024932&partnerID=40&md5=2a544eecee8cc3616c86e66caaf0dc05},
affiliation={ISPRA, Italy; Eni S.p.A. EandP Division, Italy},
abstract={Underwater visual census (UVC) techniques using SCUBA have been rarely employed to evaluate fish diversity associated with oil and gas offshore platforms. These techniques can be particularly useful since traditional methods such as fishing surveys are not able to give information about fishes strictly associated with high complex habitats such as off-shore platforms. Therefore ISPRA and Eni E&P Division, from 2003 to 2008, carried out a research program with the aim to test the appropriateness of UVC techniques using scuba in describing the fish diversity associated with off-shore platforms and to understand the ecosystem role of these artificial structures. UVC techniques, allowed an accurate description of the fish assemblage associated with off-shore platforms and the acquisition of a big amount of data if compared to the sampling effort. In particular, these underwater techniques allowed the exploration of the inner areas of the platforms where the cryptobenthonic species represent most of the total fish diversity: in Adriatic platforms blennids species represented 22% of the total fish diversity. These species hide inside the oil rigs internal structures and escape normally to captures. For these reason species richness resulted higher than that of previous studies carried out, in the Adriatic Sea, through traditional fishing surveys. The use, for the first time in Mediterranean Sea, of these techniques in off-shore platforms located in different areas allowed the introduction of an investigation protocol also replicable in other areas. However, underwater visual census techniques provided important information and hence should be employed in studies aimed at investigating fish assemblages associated with oil and gas platforms. © 2009 Offshore Mediterranean Conference. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Skokan2009,
author={Skokan, C.},
title={An introduction to energy choices: A multidisciplinary approach},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2009},
page_count={8},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029117162&partnerID=40&md5=a9d9c69ba3c95cc7c7ac9b4eab99d7e2},
affiliation={Colorado School of Mines, United States},
abstract={The Indian Affairs Office of Indian Energy and Economic Development (IEED) was established to assist in economic development on Indian lands, in part by assisting development of workforce capacity through education and facilitating partnerships between tribes and the private sector. Colorado School of Mines received a grant from IEED to develop an energy engineering program of study to be used by Tribal Colleges. After discussing the program with the leaders from two tribal college test locations, a curriculum was created. The test locations were chosen because of their geographic diversity and their technical thrust. The curriculum presently consists of six courses: Introduction to College Mathematics and Science (a systems course), Topographic Surveying, Introduction to Engineering, Design of a Wind Farm, Solar Design, and an Overview of Energy Resources. A key course in the curriculum is a review of energy resources. This course provides an overview of both traditional as well as alternative energy resources. A life-cycle approach is used to investigate each energy resource: oil and gas, coal, hydro, geothermal, nuclear, solar, wind, biomass, and synthetic fuels. The course is designed to inform and prepare students who could enter energy fields as engineers. Colorado School of Mines is responsible for preparing the curriculum at the lecture level and for training the college staff through a summer workshop to offer the courses. We are also available during the academic year as content consultants as well as visitors to present special topics to the college students. The process of curriculum development has resulted in challenges as well as successes. This paper will describe the overall IEED project and, specifically, the Overview of Energy Resources course, discuss the assessment of both the teachers and the students participating in the course, and will detail the challenges and successes of the program. © American Society for Engineering Education, 2009.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pickering-Reyna2009,
author={Pickering-Reyna, B.},
title={Beyond math enrichment : Applied practice with life and career skills intervention and retention applications matter in educating new minority freshmen},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2009},
page_count={12},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029099176&partnerID=40&md5=4b8ff3a0ce52880bd355176ea12ab569},
affiliation={University of Wisconsin, College of Engineering and Applied Science, UWM School of Information Studies, Milwaukee, United States},
abstract={STEM and urban education along with educational psychology scholarship established four critical research areas that needed judicious exploration to systematically increase the exercise of effective instructional programming for minorities: 1) Early access to and sustained engagement with salient concepts (e.g., logical reasoning, managing complexity) that practically apply classroom theories, 2) Curriculum that supports cognitive development in proportion to students' learning styles, 3) Peer and expert-model pedagogical agents as learning companions and social models, and 4) Considering sociocultural and gender issues in the other three areas. The research design for this study expected promising outcomes with broader applications for similar activities at other institutions based on the assumption that salient tenets of a statistically proven method, the Information Technology Life Skills Career Development (IT-LSCD) model, would transfer to a modified engineering and computer science (E/CS) learning system. Efforts in the University of Wisconsin-Milwaukee College of Engineering & Applied Science to infuse technical education with practical application, life skills counseling, career development instruction, and financial support made a difference in the preparation of new minority freshmen. Developing and testing the feasibility of the comprehensive quasi-bridge instructional system, Engineering & Computer Science Explorations III (ECSE III aka ex-see 3), through mixed methodology showed that the approach in this pilot significantly advanced all 10 participants (three mentors, seven mentees). Comparisons between pre- and post-program test scores showed significant aptitude improvement. Among mentees, five attained calculus with analytical geometry status and became scholarship worthy. Of the remaining three mentees, one tested out of trigonometry (not algebra) while the other two tested into both courses. Broad fundamentals helped expand students' experiences beyond math enrichment. All participants reported to have developed investigative responsiveness, increased their technical reflection abilities, formed presentation skills, established durable academic relationships, and gained networking confidence. Besides classroom and field instruction, the ECSE III strategy used multilevel mentoring, residential clustering in a Living Learning Community (LLC), professional associations, and enhancement activities during the ensuing academic semester. The goal was to improve student population diversity by increasing the number of minorities in E/CS disciplines through early access and preparation that concurrently addressed recruitment, matriculation, retention, and sustained support issues. This paper depicts results that derived from examining ECSE III through the abovementioned lenses. The paper informs about participants' traits, pre-program expectations, and post-program self-reported experiences. As well, the paper discloses gradations observed within ECSE III prepost- program test scores, outcomes of tracking participants' development during the ensuing fall semester, and suggestions for component modifications within ECSE III. © American Society for Engineering Education, 2009.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Smith2009,
author={Smith, R. and Hollebrands, K. and Parry, E. and Bottomley, L. and Smith, A. and Albers, L.},
title={The effects of a GK-12 program on students' achievement in and beliefs about mathematics},
journal={ASEE Annual Conference and Exposition, Conference Proceedings},
year={2009},
page_count={15},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029093589&partnerID=40&md5=4f320d18ddd9cffc37643455b7882077},
affiliation={North Carolina State University, United States},
abstract={To evaluate the effectiveness of a program whose goal is to increase the number and diversity of students enrolled in upper-level mathematics courses, an analysis was conducted comparing the standardized achievement test scores of program participants to similar non-participants. Results indicate that significant gains occur when students participate in the program for two years. In addition, program participants were surveyed to measure students' confidence about their abilities in mathematics, students' beliefs about mathematics as a male domain, and students' perceptions of their teacher's beliefs about their ability to learn mathematics. Analyses indicate that at least one significant mean difference occurred for all three between subject factors (gender, ethnicity, school type) for all three measures of attitudes and beliefs about mathematics. © American Society for Engineering Education, 2009.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hall200995,
author={Hall, R.E. and Easter, J. and Roth, E. and Kabana, L. and Mashio, K. and Takahashi, K. and Clouser, T.},
title={US-APWR human systems interface system verification & validation results: Application of the Mitsubishi advanced design to the US market},
journal={International Congress on Advances in Nuclear Power Plants 2009, ICAPP 2009},
year={2009},
volume={1},
pages={95-102},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908010427&partnerID=40&md5=4f603f01bfd077f81a6efb14a7742082},
affiliation={REH TECHNOLOGY SOLUTIONS, 2 Davids Way, Port Jefferson, NY  11777, United States; PREFERRED LICENSING SERVICES, PO Box 14431, Pittsburgh, PA  15239, United States; ROTH COGNITIVE ENGINEERING, 89 Raswson Rd, Brookline, MA  02445, United States; 122 Amber Woods Drive, Canonsburg, PA  15317, United States; MITSUBISHI HEAVY INDUSTRIES, 1-1 Wadaski-Cho 1-Chrome, Hyogo-Ku, Kobe, 652-8585, Japan; MITSUBISHI ELECTRIC CORPORATION, 1-1-2 Wadasaki-cho, Hyogo-ku, Kobe, 652-8555, Japan; LUMINANT POWER, Mail Code E15, 6322 N, FM 56, Glen Rose, TX  76043, United States},
abstract={The US-APWR, under Design Certification Review by the US Nuclear Regulatory Commission, is a four loop evolutionary pressurized water reactor with a four train active safety system by Mitsubishi Heavy Industries and Instrumentation and Control System (I&C) / Human Systems Interface (HSI) platform applied by Mitsubishi Electric Corporation. This design is currently being applied to the latest Japanese PWR plant under construction and to the nuclear power plant I&C modernization program in Japan. The US-APWR's fully digital I&C system and HSI platform utilizes computerized systems, including computer based procedures and alarm prioritization, relying principally on an HSI system with soft controls, console based video display units and a large overview wall display panel. Conventional hard controls are limited to Safety System level manual actions and a Diverse Actuation System. The overall design philosophy is based on the concept that operator performance will be enhanced through the integration of safety- and non-safety display and control systems in a robust digital environment. This philosophy is augmented, for diversity, by the application of independent safety-only soft displays and controls. As with all advanced designs, the digital systems resolve many long- standing issues of human and system performance while opening a number of new, less understood, questions. This paper discusses a testing program that begins to address these new questions and specifically explores the needs of moving a mature design into the US market with minimum changes from its original design. Details for the program took shape during 2007 and early 2008, resulting in an eight-week testing program during the months of July and August 2008. This extensive verification and validation program on the advanced design was undertaken with the objective of assessing United States operators' performance in this digital design environment. This testing program included analyses that identified and addressed the needs for successful application of the Japanese design and technology to the US market. Over the eight week period, two and three person operating crews were subjected to seven scenarios that were run on Mitsubishi's dynamic plant simulator over a four day period that included some introductory training on the use of the HSI. In all, twenty-two US licensed reactor operators and senior operators took part in the program. Subjective and objective data were collected on each crew for each scenario, and an extensive convergent measures analysis was performed that resulted in the identification of both specific and generic design suitability conclusions. These conclusions were then reviewed by an expert panel composed of both US and Japanese experts familiar with the design and the US testing program. Additionally, in support of the verification testing effort, a software package was developed enabling the independent evaluation of the effectiveness of the transfer of Japanese display screen designs to the US market. Any discrepancies and deficiencies in the screen designs were identified and included with the conclusions derived from the simulation tests in the test conclusions review by the expert panel. This paper presents a discussion of the HSI design as tested and that will be included in the USAPWR, the methodology adopted for this portion of the US-APWR HSI verification and validation testing program, and the test results supporting the transition of the MHI/MELCO design into the US market and, the associated regulatory review process.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Routray2009145,
author={Routray, R. and Nadgowda, S.},
title={CIMDIFF: Advanced difference tracking tool for CIM compliant devices},
journal={Proceedings of the 23rd Large Installation System Administration Conference, LISA 2009},
year={2009},
pages={145-157},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650156199&partnerID=40&md5=e28218c1fbea2085f7ca7259d5a1d408},
affiliation={IBM Almaden Research Center; IBM India Systems and Technology Lab},
abstract={Total Cost of Ownership (TCO) for any enterprise scale data center is significantly dependent upon the effectiveness of the system management solutions and procedures deployed. Complexity of managing a data center increases as various enterprise applications demand diverse sets of requirements, leading to a very heterogeneous environment often fueled by diverse emerging technologies. Emergence of the industry standard Common Information Model (CIM) has introduced uniformity and interoperability into this complex managed environment. In this paper, we describe a tool CIMDIFF that provides syntactic and semantic difference tracking for CIM compliant devices in both spatial and temporal flavors. Since this problem is NP-hard, in this paper we present an efficient technique that combines domain specific object oriented knowledge with hierarchical structure of CIM-XML to derive meaningful differences. We demonstrate the value of this tool for a) tracking difference in device characteristics b) verification of proper operation as well as automated validation of management software given the limited resources of testing infrastructure. An experimental evaluation of this tool in a complex data center is provided. © LISA 2009.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Benshoof2009,
author={Benshoof, P.H. and Uptain, N. and Trunzo, A.},
title={JAMFEST: A cost-effective solution to GPS vulnerability testing},
journal={U.S. Air Force T and E Days 2009},
year={2009},
doi={10.2514/6.2009-1715},
art_number={2009-1715},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958179285&doi=10.2514%2f6.2009-1715&partnerID=40&md5=447d7df459bf0e721a366b41eb21917e},
affiliation={746th Test Squadron, Holloman AFB, NM 88330, United States},
abstract={On January 12-15, 2009, the 746th Test Squadron (746 TS), located at Holloman Air Force Base (AFB), New Mexico (NM), planned and executed an innovative Global Positioning System (GPS) jamming program at White Sands Missile Range (WSMR), NM. This program, known as JAMFEST, was aimed at providing low-cost, realistic, GPS jamming scenarios for testing GPS-based navigation systems, as well as training personnel in unique GPS denied environments. Through sponsorship from the GPS Wing, the 746 TS was able to provide this diverse testing and training opportunity at a significantly reduced cost to each participant. During JAMFEST, the 746 TS hosted multiple simultaneous, yet very dissimilar customers, including multi-service Department of Defense (DoD) agencies, several defense contractors and other government organizations. Their objectives ranged from training personnel on the effects of GPS jamming to characterizing the performance of prototype advanced anti-jam technologies against operationally realistic threats. To accomplish these goals, participants drove, flew or walked through diverse jamming scenarios specifically tailored to stress the systems under evaluation. This paper details JAMFEST strategies and conduct, as well as participant objectives and future JAMFEST activities.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wilson20091839,
author={Wilson, D.P. and Malone, T.B. and Lockett-Reynolds, J. and Wilson, E.L.},
title={A vision for human systems integration in the U. S. Department of Homeland Security},
journal={Proceedings of the Human Factors and Ergonomics Society},
year={2009},
volume={3},
pages={1839-1843},
doi={10.1518/107118109x12524444082673},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77951596483&doi=10.1518%2f107118109x12524444082673&partnerID=40&md5=bae833869490eb8e48db3957aaf7c45c},
affiliation={Department of Homeland Security, United States; Carlow International Incorporated, United States; Caden James Enterprises, United States},
abstract={In the complex, technology-based, manpower-limited homeland security systems of the 21st century, a critical element of mission success is human performance, or the ability of the human operator, maintainer, manager, or public users to perform well under all operating conditions. The United States Department of Homeland Security (DHS) was formed just more than five years ago. The Human Factors / Behavior Sciences Division (HFD) was created 2 years ago within the DHS Science and Technology (S&T) directorate. HFD has since established the Human Systems Research and Engineering (HSRE) program and has given that program the responsibility for developing an approach for incorporating Human Systems Integration (HSI) into DHS research, development, and acquisitions. The 22 organizations that make up DHS were joined together under a headquarters organization that is still working on the logistics of operating as a unified agency (General Accountability Office, 2007). The integration of the various research, development, systems engineering, and acquisition processes of those legacy component organizations is ongoing and provides an opportunity to lay a foundation for successful HSI. Fundamental to the system life cycle of a given DHS program is the incorporation of user requirements and public perception input (Department of Homeland Security, 2008). Although similar to the Department of Defense (DoD) in the research and development of technology to enhance the safety and security of the nation (Department of Defense, 2008), the mission space of the DHS differs greatly in that the technologies developed and deployed by DHS are used within the United States and affect all citizens. The users of DHS technology represent a far more diverse population in terms of skills, anthropometry, age, training quantity and quality, intelligence, and readiness, than those in the military user community. Not only are the users and affected communities as diverse as the nation, but programs throughout the federal government have been phased out due to negative public outcry and media attention. Therefore, it is imperative for DHS to not only produce usable technologies, but also to comprehend the barriers and obstacles associated with technology acceptability, usability, supportability, reliability, affordability, safety, and survivability as these factors relate to the development of technology and systems. The HSRE program implements HSI in the DHS technology development process, and the integrated HSI analysis, design and test activities serve as the mechanism for addressing user requirements, and ensuring the design of the technology meets user needs. This paper describes a vision for the implementation of HSI in the Department of Homeland Security, focusing on both engineering and research efforts and strategies to accomplish goals in those areas.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Lang2009305,
author={Lang, C. and Meyer, D. and Niner, S. and McKay, J. and Lewis, S.},
title={The impact of gender and pedagogical factors on female pass rates},
journal={Communications of the Association for Information Systems},
year={2009},
volume={25},
number={1},
pages={305-320},
doi={10.17705/1cais.02528},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-73149116407&doi=10.17705%2f1cais.02528&partnerID=40&md5=50f7a244a4c85371b382ac816aebe9e0},
affiliation={Faculty of Information and Communication Technologies, Swinburne University of Technology, Australia; Faculty of Life and Social Sciences, Swinburne University of Technology, Australia; Faculty of Arts, Monash University, Australia},
abstract={An assessment of student achievement according to gender in core units of study of a Faculty of Information and Communication Technology program tested four hypotheses. The first of these related to the role-model effect of female academics; the second related to the advantages of formal education qualifications of academics; the third to the application of contextualized curricula, and the fourth to the use of a variety of assessment modes. Correlation and regression analysis on the data set indicated that the presence of two of these factors can significantly improve the pass rate of female students while having a benign effect on the pass rate of male students. It is suggested that information systems faculties pay close attention to gender diversity of their teaching faculty, particularly if their female student cohort is less than one in five in a unit of study. It also gives substance to the need or preference for university lecturers having education qualifications. This study needs to be replicated in other information systems faculties and schools to verify this finding. © 2009 by the authors.},
author_keywords={Gender diversity;  IS pedagogy;  Role models;  Student achievement},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Quimper20081567,
author={Quimper, C.-G. and Walsh, T.},
title={Decompositions of grammar constraints},
journal={Proceedings of the National Conference on Artificial Intelligence},
year={2008},
volume={3},
pages={1567-1570},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749107433&partnerID=40&md5=59b4b3e91151b15c83f9ae8b847e1f3f},
affiliation={Ecole Polytechnique de Montréal, Montréal, Canada; NICTA, UNSW, Sydney, Australia},
abstract={A wide range of constraints can be compactly specified using automata or formal languages. In a sequence of recent papers, we have shown that an effective means to reason with such specifications is to decompose them into primitive constraints (Quimper & Walsh 2006; 2007). We can then, for instance, use state of the art SAT solvers and profit from their advanced features like fast unit propagation, clause learning, and conflict-based search heuristics. This approach holds promise for solving combinatorial problems in scheduling, rostering, and configuration, as well as problems in more diverse areas like bioinformatics, software testing and natural language processing. In addition, decomposition may be an effective method to propagate other global constraints. Copyright © 2008.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheng20081067,
author={Cheng, L.-T. and De Souza, C. and Dittrich, Y. and Hazzan, O. and John, M. and Maurer, F. and Sharp, H. and Sillito, J. and Sim, S.E. and Singer, J. and Storey, M.A. and Tessem, B. and Venolia, G.},
title={Cooperative and human aspects of software engineering (CHASE 2008)},
journal={Proceedings - International Conference on Software Engineering},
year={2008},
pages={1067-1068},
doi={10.1145/1370175.1370250},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349194515&doi=10.1145%2f1370175.1370250&partnerID=40&md5=2124045b192521b3be3da38be78216e0},
affiliation={IBM, United States; UFPA, Brazil; IT University of Copenhagen, Denmark; Technion, Israel; Fraunhofer, Germany; University of Calgary, Canada; Open University, United Kingdom; University of CA, Irvine, United States; NRC, Canada; University of Victoria, Canada; University of Bergen, Norway; Microsoft Research, United States},
abstract={The CHASE 2008 workshop is concerned with exploring the cooperative and human aspects of software engineering, and providing a forum for discussing high-quality resarch. Accepted papers reflect the diversity of the field of software engineering-ranging from requirements to testing, and from ethnographic research to experiments. Moreover, the background of attendees reflects the diversity of researchers in this domain, ranging from sociology to psychology, from informatics to software engineering. CHASE 2008 met its goals in presenting high-quality research and building community through a mixture of presentations, discussions, posters, and social activities. Copyright 2008 ACM.},
author_keywords={Collaboration;  Design;  Distributed teams;  Organizations;  Process;  Social networks},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gao2008631,
author={Gao, J. and Kwok, K. and Fitch, T.},
title={Model-based test complexity analysis for software installation testing},
journal={20th International Conference on Software Engineering and Knowledge Engineering, SEKE 2008},
year={2008},
pages={631-637},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886942959&partnerID=40&md5=ad97520deff3c8751342536c862890dc},
affiliation={San Jose State University, United States; Intuit Corp., United States},
abstract={Software testing is the last critical phase in software quality control. Software installation testing is one of the most important and complex tasks in system testing. However, in past years, researchers have not paid much attention to the related issues and challenges in software installation testing. One of them is test complexity analysis and planning. The paper uses a test model, known as a semantic tree, to assist engineers in modeling test complexity of software installation in terms of diverse system environments and configurations, various running conditions, and system functional features. The paper presents one systematic method based on the model to compute and analyze test complexity of software installation testing in three perspectives: system configurations, system running conditions, and system installation functions. The related application examples and experimental results are reported.},
author_keywords={Model-based testing complexity analysis;  Software installation testing;  Software modeling and analysis for testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cheon2008861,
author={Cheon, Y. and Ceberio, M. and Cortes, A. and Leavens, G.T.},
title={Integrating random testing with constraints for improved efficiency and diversity},
journal={20th International Conference on Software Engineering and Knowledge Engineering, SEKE 2008},
year={2008},
pages={861-866},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886878256&partnerID=40&md5=16731dca2f70cbfaab02b210bd6799ae},
affiliation={Department of Computer Science, University of Texas at El Paso, El Paso, TX 79968, United States; School of Electrical Engineering and Computer Science, University of Central Florida, Orlando, FL 32816, United States},
abstract={Random testing can be fully automated, eliminates subjectiveness in constructing test data, and increases the diversity of test data. However, randomly generated tests may not satisfy program's assumptions such as method preconditions. While constraint solving can satisfy such assumptions, it does not necessarily generate diverse tests and is hard to apply to large programs. We blend these techniques by extending random testing with constraint solving, improving the efficiency of generating valid test data while preserving diversity. For domains such as objects, we generate input values randomly; however, for values of finite domains such as integers, we represent test data generation as a constraint satisfaction problem by solving constraints extracted from the precondition of the method under test. We also increase the diversity of constraint-based solutions by incorporating randomness into the solver's enumeration process. In our experimental evaluation we observed an average improvement of 80 times without decreasing test data diversity, measured in terms of the time needed to generate a given number of valid test cases.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DaCruz2008133,
author={Da Cruz, J.L. and Jino, M. and Crespo, A.N. and Argollo, M.},
title={PROMETEU - A tool to support documents generation and traceability in the test process},
journal={7th Jornadas Iberoamericanas de Ingenieria de Software e Ingenieria del Conocimiento 2008, JIISIC 2008},
year={2008},
pages={133-138},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883076826&partnerID=40&md5=bede7e5160ccb8d05cfe3006377dc0d9},
affiliation={Centro de Pesquisas Renato Archer (CenPRA), Campinas - SP, Brazil; UNICAMP, Campinas - SP, Brazil},
abstract={Systematic software testing isn't trivial. Among other things, testing must be well documented and based on up to date and consistent information to avoid troubles such as incomplete or inappropriate tests. This involves the definition of what must be recorded and the amount of documents, in addition to establishing and controlling diverse links among the many information elements that compose the documents. Documenting and tracing involve the manipulation of a very great amount of information, demanding automated support. This paper presents PROMETEU, a tool to support documents generation and traceability among artifacts and information that comprises the test process documents.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pohjolainen2008271,
author={Pohjolainen, P.A.},
title={The Development of Software Testing in Finland 1950-2000},
journal={IFIP Advances in Information and Communication Technology},
year={2008},
volume={303},
pages={271-282},
doi={10.1007/978-3-642-03757-3_28},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883006715&doi=10.1007%2f978-3-642-03757-3_28&partnerID=40&md5=c65da3c123e2ee32de2b77da8720acf6},
affiliation={Department of Computer Science, University of Kuopio, P.O. Box 1627, FI, 70211 Kuopio, Finland},
abstract={The paper presents the development of software testing in Finland. This topic has received little academic attention and it is frequently forgotten. The existing publications concentrate more on the history of machines and programming languages than on the history of the development of testing. The analysis made so far proves that the problems in the early times were very different from nowadays. For example, during the 1950s and 1960s, it was difficult to get computation time for testing. Meanwhile, during the 1990s, and after that, the greatest source of problems has been the complexity and the massiveness of programs. On the other hand, it seems that the education of testing has not been sufficient until the end of 1990s. Hence, the knowledge of diverse testing methods, test automation, and outsourcing are now better than in the past. In our research, we have interviewed over fifty persons. The interviewees vary from pioneers of Finnish computing, having tens of years career, to young professionals of testing. Their selection is from Finnish universities and over twenty companies in Finland. © IFIP International Federation for Information Processing 2009.},
author_keywords={Development of testing;  growth of testing;  testing methods},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ribeiro200885,
author={Ribeiro, J.C.B. and Rela, M.Z. and De Vega, F.F.},
title={A strategy for evaluating feasible and unfeasible test cases for the evolutionary testing of object-oriented software},
journal={Proceedings - International Conference on Software Engineering},
year={2008},
pages={85-92},
doi={10.1145/1370042.1370061},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960616092&doi=10.1145%2f1370042.1370061&partnerID=40&md5=e33affabed9ece2aed753e232b332059},
affiliation={Polytechnic Institute of Leiria, Morro do Lena, Alto do Vieiro Leiria, Portugal; University of Coimbra, CISUC, DEI, 3030-290, Coimbra, Portugal; University of Extremadura, C/ Sta Teresa de Jornet, 38, Mérida, Spain},
abstract={Evolutionary Testing is an emerging methodology for automatically producing high quality test data. The focus of our on-going work is precisely on generating test data for the structural unit-testing of object-oriented Java programs. The primary objective is that of efficiently guiding the search process towards the definition of a test set that achieves full structural coverage of the test object. However, the state problem of object-oriented programs requires specifying carefully fine-tuned methodologies that promote the traversal of problematic structures and difficult control-flow paths - which often involves the generation of complex and intricate test cases, that define elaborate state scenarios. This paper proposes a methodology for evaluating the quality of both feasible and unfeasible test cases - i.e., those that are effectively completed and terminate with a call to the method under test, and those that abort prematurely because a runtime exception is thrown during test case execution. With our approach, unfeasible test cases are considered at certain stages of the evolutionary search, promoting diversity and enhancing the possibility of achieving full coverage. Copyright 2008 ACM.},
author_keywords={evolutionary testing;  object-orientation;  search-based test case generation;  strongly-typed genetic programming},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cottam200857,
author={Cottam, J.A. and Hursey, J. and Lumsdaine, A.},
title={Representing unit test data for large scale software development},
journal={SOFTVIS 2008 - Proceedings of the 4th ACM Symposium on Software Visualization},
year={2008},
pages={57-66},
doi={10.1145/1409720.1409730},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-63149092982&doi=10.1145%2f1409720.1409730&partnerID=40&md5=fd041aeab90281dc24c5f66e1e1546b7},
affiliation={Indiana University, Open System Laboratory, Bloomington, IN, United States},
abstract={Large scale software projects rely on routine, automated, testing to gauge progress towards its goals. The diversity and quantity of these tests grow as time and project scope increase. This is as a consequence of both experience and expanding audience. It becomes increasingly difficult to interpret testing results as the testing suites multiply and diversify. If interpretation becomes too difficult, testings results could become ignored all together. Visualization has proven to be an effective tool to aid the interpretation of large amounts of data. We have adapted visualization techniques based on small multiples to communicate the health of the software project across several levels of abstraction. The collective set of techniques we refer to as the SeeTest visualization schema. We applied this visualization technique to the Open MPI test results in order to assist developers in the software release cycle. Through the visualizations, developers found a variety of surprising mismatches between their data and their intuitions. This exploration did not involve collecting any data not already being collected, merely presenting it in manner that better supported their needs. In this paper, we detail, the development of the representation we used and give more particular analysis of the insights gained by the Open MPI community. The techniques presented in this paper can be applied to other software projects. © 2008 ACM.},
author_keywords={MPI;  Project management;  Testing;  Visualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Kirisci2008248,
author={Kirisci, P.T. and Thoben, K.-D.},
title={The role of context for specifying Wearable computers},
journal={Proceedings of the 3rd IASTED International Conference on Human-Computer Interaction, HCI 2008},
year={2008},
pages={248-253},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-62949164456&partnerID=40&md5=2cdcde29d2e4481738a2efd09e7d9da8},
affiliation={Bremen Institute for Production and Logistics GmbH (BIB A), Hochschulring-20, 28359 Bremen, Germany},
abstract={Wearable computing is transcending the realms of laboratory environments. Increasing applied research in this area is boosting the introduction of wearable computers in a diversity of business and leisure areas. One major challenge is to guarantee that wearable computers are highly customized devices. Experience has shown that the physical properties and features of a wearable computer must be in line with the context of the user and his environment, in order to ensure an optimum on usability and acceptance. While the efficient use of context has always been a key issue for the development of context-aware applications in the area of mobile computing, it is of particular interest to wearable computing. This is because of the additional features that wearable computers comprise [1]. Additionally, context is extremely dynamic in wearable computing settings. This paper discusses the usage of context for the specification of wearable computers. Thus, the aim is not only to gain a better understanding of context and its use in the specification process of wearable computers, but also to provide some significant input in formalizing the design approaches for physical user interface design in Human-Computer-Interaction (HCI).},
author_keywords={Context models;  Formal methods in hci;  User interface development;  Wearable computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bueno2008809,
author={Bueno, P.M.S. and Wong, W.E. and Jino, M.},
title={Automatic test data generation using particle systems},
journal={Proceedings of the ACM Symposium on Applied Computing},
year={2008},
pages={809-814},
doi={10.1145/1363686.1363871},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749175773&doi=10.1145%2f1363686.1363871&partnerID=40&md5=a153f61a510dd71a45d6ef15be8b654a},
affiliation={Renato Archer Research Center, Rodovia Dom Pedro I, km 143, 6, Campinas, Säo Paulo, Brazil; University of Texas at Dallas, Richardson, TX 75083, United States; State University of Campinas, Av. Albert Einstein,400, Campinas, São Paulo, Brazil},
abstract={The simulated repulsion algorithm, which is based on particle systems, is used for the automatic generation of diversity oriented test sets (DOTS). These test sets are generated by taking randomly generated test sets and iteratively improving their diversity (the level of variability among values for the test data) towards DOTS. The results of a simulation performed to evaluate characteristics of DOTS indicate improvement, with respect to fault detection, of these test sets over the standard random test sets. Copyright 2008 ACM.},
author_keywords={Diversity oriented test data generation;  Genetic algorithms;  Random testing;  Self-organization;  Simulated annealing;  Simulated repulsion;  Software testing;  Test data generation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Díaz20083052,
author={Díaz, E. and Tuya, J. and Blanco, R. and Javier Dolado, J.},
title={A tabu search algorithm for structural software testing},
journal={Computers and Operations Research},
year={2008},
volume={35},
number={10},
pages={3052-3072},
doi={10.1016/j.cor.2007.01.009},
note={cited By 69},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-40249118430&doi=10.1016%2fj.cor.2007.01.009&partnerID=40&md5=2bd827b6c16a4abc2bad5c77f0aaf943},
affiliation={Department of Computer Science, University of Oviedo, Campus de Viesques, Gijón, Asturias 33204, Spain; Department of Computer Science and Languages, University of the Basque Country, Spain},
abstract={This paper presents a tabu search metaheuristic algorithm for the automatic generation of structural software tests. It is a novel work since tabu search is applied to the automation of the test generation task, whereas previous works have used other techniques such as genetic algorithms. The developed test generator has a cost function for intensifying the search and another for diversifying the search that is used when the intensification is not successful. It also combines the use of memory with a backtracking process to avoid getting stuck in local minima. Evaluation of the generator was performed using complex programs under test and large ranges for input variables. Results show that the developed generator is both effective and efficient. © 2007 Elsevier Ltd. All rights reserved.},
author_keywords={Automatic test generation;  Software testing;  Structural testing;  Tabu search},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alba20083161,
author={Alba, E. and Chicano, F.},
title={Observations in using parallel and sequential evolutionary algorithms for automatic software testing},
journal={Computers and Operations Research},
year={2008},
volume={35},
number={10},
pages={3161-3183},
doi={10.1016/j.cor.2007.01.016},
note={cited By 47},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-40249088314&doi=10.1016%2fj.cor.2007.01.016&partnerID=40&md5=fec4ef7da5bd1122651e63e2cd70f40b},
affiliation={Grupo GISUM, Dept. de Lenguajes y Ciencias de la Computación, University of Málaga, Spain},
abstract={In this paper we analyze the application of parallel and sequential evolutionary algorithms (EAs) to the automatic test data generation problem. The problem consists of automatically creating a set of input data to test a program. This is a fundamental step in software development and a time consuming task in existing software companies. Canonical sequential EAs have been used in the past for this task. We explore here the use of parallel EAs. Evidence of greater efficiency, larger diversity maintenance, additional availability of memory/CPU, and multi-solution capabilities of the parallel approach, reinforce the importance of the advances in research with these algorithms. We describe in this work how canonical genetic algorithms (GAs) and evolutionary strategies (ESs) can help in software testing, and what the advantages are (if any) of using decentralized populations in these techniques. In addition, we study the influence of some parameters of the proposed test data generator in the results. For the experiments we use a large benchmark composed of twelve programs that includes fundamental algorithms in computer science. © 2007 Elsevier Ltd. All rights reserved.},
author_keywords={Evolutionary algorithms;  Evolutionary testing;  Parallel evolutionary algorithms;  Software testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Feldt2008178,
author={Feldt, R. and Torkar, R. and Gorschek, T. and Afzal, W.},
title={Searching for cognitively diverse tests: Towards universal test diversity metrics},
journal={2008 IEEE International Conference on Software Testing Verification and Validation Workshop, ICSTW'08},
year={2008},
pages={178-186},
doi={10.1109/ICSTW.2008.36},
art_number={4567005},
note={cited By 41},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249109609&doi=10.1109%2fICSTW.2008.36&partnerID=40&md5=de0c1b6882069c2346170a3be9fcdc62},
affiliation={Dept. of Systems and Software Engineering, Blekinge Institute of Technology, SE-372 25 Ronneby, Sweden},
abstract={Search-based software testing (SBST) has shown a potential to decrease cost and increase quality of testing-related software development activities. Research in SBST has so far mainly focused on the search for isolated tests that are optimal according to a fitness function that guides the search. In this paper we make the case for fitness functions that measure test fitness in relation to existing or previously found tests; a test is good if it is diverse from other tests. We present a model for test variability and propose the use of a theoretically optimal diversity metric at variation points in the model. We then describe how to apply a practically useful approximation to the theoretically optimal metric. The metric is simple and powerful and can be adapted to a multitude of different test diversity measurement scenarios. We present initial results from an experiment to compare how similar to human subjects, the metric can cluster a set of test cases. To carry out the experiment we have extended an existing framework for test automation in an object-oriented, dynamic programming language. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen2008170,
author={Chen, S. and Moreland, D. and Nepal, S. and Zic, J.},
title={Yet another performance testing framework},
journal={Proceedings of the Australian Software Engineering Conference, ASWEC},
year={2008},
pages={170-179},
doi={10.1109/ASWEC.2008.4483205},
art_number={4483205},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249114000&doi=10.1109%2fASWEC.2008.4483205&partnerID=40&md5=733487df1ca62e9667818b224ca14671},
affiliation={Networking Technologies Laboratory, CSIROICT Centre, Australia},
abstract={Performance testing is one of the vital activities spanning the whole life cycle of software engineering. While there are a consideruble number of performance testing products and open source tools available, they are either too expensive and complicated for small projects, or too specific and simple for diverse performance tests. This paper presents a general-purpose testing framework for both simple and small, and complicated and large-scale performance testing. Our framework proposes an abstraction to facilitate performance testing by separating the application logic from the common performance testing functionalities. This leads to a set of general-purpose data models and components, which form the core of the framework. The framework has been prototyped on both.NET and Java platforms and was used for a number of performance-related projects. © 2008 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2008,
title={Proceedings of SPIE: Sensors and Systems for Space Applications II},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={2008},
volume={6958},
page_count={231},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949185327&partnerID=40&md5=7552f274eaac5dd2f7b8a96480a531b6},
abstract={The proceedings contain 21 papers. The topics discussed include: orbital express program summary and mission overview; orbital express mission operations planning and resource management using ASPEN; orbital express fluid transfer demonstration system; autonomous robotic operations for on-orbit satellite servicing; modeling, simulation, testing, and verification of the orbital express autonomous rendezvous and capture sensor system (ARCSS); comparison of navigation solutions for autonomous spacecraft from multiple sensor systems; on-orbit performance of the orbital express capture system; ULTOR passive pose and position engine for spacecraft relative navigation; optical SAR processor for space applications; phase retrieval in sparse aperture systems with phase diversity: a trade space study; optical detection of formaldehyde; and a radiation-hardened high-resolution optical encoder for use in aerospace application.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Erdim2008537,
author={Erdim, H. and Iliȩ, H.T.},
title={A point membership classification for sweeping solids},
journal={2007 Proceedings of the ASME International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, DETC2007},
year={2008},
volume={6 PART A},
pages={537-546},
doi={10.1115/DETC2007-34827},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44849123789&doi=10.1115%2fDETC2007-34827&partnerID=40&md5=cf9a9565c600ff8aaee7197135215fd5},
affiliation={Computational Design Laboratory, Department of Mechanical Engineering, University of Connecticut, Storrs, CT 06269, United States},
abstract={Sweeps are considered to be one of the basic representation schemes in solid modeling, and have numerous applications in very diverse fields ranging from engineering design and manufacturing to computer graphics. Despite their prevalence, many properties of the general sweeps are not well understood. Furthermore, boundary evaluation algorithms for 3-dimensional solid objects currently exist only for reasonably simple objects and motions. One of the main reasons for this state of affairs is the lack of a generic point membership test for sweeps. In this paper we describe a point membership classification (PMC) for sweeping solids of arbitrary complexity moving according to one parameter affine motions such that the initial and final configurations of the moving object do not intersect. Our PMC test is defined in terms of inverted trajectory tests against the original geometric representation of the generator object. This PMC test provides complete geometric information about the set swept by the 3-dimensional moving object, and can play a fundamental role in sweep boundary evaluation and trimming algorithms, as well in a number of practical applications such as contact analysis of higher pairs in design and manufacturing. Since our PMC is formulated in terms of intersections between inverted trajectories and the generator, it can be implemented for any geometric representation that supports curve-solid intersections. Copyright © 2007 by ASME.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor2008,
title={Formal Methods and Testing - An Outcome of the FORTEST Network, Revised Selected Papers},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2008},
volume={4949 LNCS},
page_count={378},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43049126195&partnerID=40&md5=4d5dbecf46c7fcd903ac29a51fda475f},
abstract={The proceeding contains 12 papers. The topics discussed include: model based testing with labelled transition systems; model-based testing of object-oriented reactive systems with spec explorer; testing real-time systems using UPPAAL; coverage criteria for state based specifications; testing in the distribution test architecture; testing data types implementations from algebraic specifications; from MC/DC to RC/DC: Formalization and analysis of control-flow testing criteria; comparing the effectiveness of testing techniques; testability transformation; and modelling the effects of combining diverse software fault detection techniques.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Belz2008,
author={Belz, M. and Eckhardt, H.-S. and Gonschior, C.P. and Nelson, G. and Klein, K.-F.},
title={Quality control of UV resistant fibers for 200-300 nm spectroscopic applications},
journal={Progress in Biomedical Optics and Imaging - Proceedings of SPIE},
year={2008},
volume={6852},
doi={10.1117/12.782447},
art_number={685209},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-42149195269&doi=10.1117%2f12.782447&partnerID=40&md5=e56e67a3fd9e2e4d6cd51bc66659f630},
affiliation={World Precision Instruments Inc., 175 Sarasota Center Blvd., Sarasota, FL 34240, United States; University of Applied Sciences Giessen-Friedberg, W.-Leuschner-Str.13, D-61169 Friedberg, Germany; Polymicro Technologies, 18019 N. 25th Avenue, Phoenix, AZ 85023-1200, United States},
abstract={UV solarization resistance of synthetic silica/silica fibers has been researched over many years. Fiber optic probes for applications as diverse as protein analysis, dissolution testing or high pressure liquid chromatography have been developed and successfully commercialized. Although fabrication technology for optical fibers has improved significantly and optical losses due to solarization effects have been minimized in synthetic silica fibers, the generation of UV induced defects in silica fibers due to the generation of E'centers visible in the 215 nm region is still present and can interfere with sensitive spectroscopic absorbance measurements. This work presents methodology to determine the transient response of optical fibers in the 200 nm to 300 nm region during the warm up period and during measurement as a function of light power coupled into the fiber, fiber length and fiber diameter.},
author_keywords={Defects;  Recovery of defect;  Silica-based fibers;  Solarization resistant fibers;  UV fibers;  UV-applications},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Maag2008827,
author={Maag, S. and Grepet, C. and Cavalli, A.},
title={A formal validation methodology for MANET routing protocols based on nodes' self similarity},
journal={Computer Communications},
year={2008},
volume={31},
number={4},
pages={827-841},
doi={10.1016/j.comcom.2007.10.031},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-39049095632&doi=10.1016%2fj.comcom.2007.10.031&partnerID=40&md5=1e150f55de2756481d8da202742fb0ad},
affiliation={National Institute of Telecommunications, CNRS SAMOVAR, 9, rue Charles Fourier, 91011 EVRY Cedex, France; CEA/Saclay, SOL, Bat. 451, 91191 Gif-sur-Yvette Cedex, France},
abstract={Conformance testing for ad hoc routing protocols is crucial to the reliability of Wireless Mobile Ad Hoc Networks. Most of the works related to the validation of such protocols are based on simulation and emulation analysis of simulated/emulated implementations without taking into account formal specifications. However, for many reasons, it has often been shown that the results obtained from simulator studies are far from the real case studies. In this paper, we propose a new conformance testing methodology dedicated to the validation of MANET routing protocols, especially Dynamic Source Routing. After generating test sequences from the formal model, the main issue is to execute them. There exist many techniques in wired systems, but several MANET inherent constraints lead to cope with the diverse mobility patterns and the topological changes. Therefore, a nodes' self similarity approach is introduced as well a specific testing architecture dealing with unexpected and unpredictable topology and messages. Interesting results have been provided as illustrated through experiments. © 2007 Elsevier B.V. All rights reserved.},
author_keywords={Conformance testing;  DSR;  Formal model;  MANET;  Self similarity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yang2008479,
author={Yang, D. and Yan, X.-T. and Clarke, D.},
title={Selection of a simulation approach for saturation diving decompression chamber control and monitoring system},
journal={Global Design to Gain a Competitive Edge: An Holistic and Collaborative Design Approach Based on Computational Tools},
year={2008},
pages={479-488},
doi={10.1007/978-1-84800-239-5_47},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957335005&doi=10.1007%2f978-1-84800-239-5_47&partnerID=40&md5=5f603af276576d94554048f990fbd5b0},
affiliation={University of Strathclyde, Glasgow G1, United Kingdom; Divex Ltd., Westhill, Aberdeen AB32 6TQ, United Kingdom},
abstract={Saturation diving decompression chamber control and monitoring system involves a large number of Inputs/Outputs (I/O) channels. Due to the large number of I/O channels and the bulkiness of the saturation diving decompression chamber systems, a simulator, which is capable of mimic physical I/O and physical processes, is highly desirable to support software development and test of the control and monitoring system. This can also be used for divers' training purpose. There are quite a number of options for I/O simulation. This paper describes five options studied first and then explains how a selection is made from the options.},
author_keywords={Control and monitoring system;  I/O simulation;  Saturation diving support},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gilat2008,
author={Gilat, A. and Seidt, J.D. and Pereira, J.M.},
title={Characterization of 2024-T351 aluminum for dynamic loading applications},
journal={Earth and Space Conference 2008: Proceedings of the 11th Aerospace Division International Conference on Engineering, Science, Construction, and Operations in Challenging Environments},
year={2008},
volume={323},
doi={10.1061/40988(323)75},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349389838&doi=10.1061%2f40988%28323%2975&partnerID=40&md5=396efc43b1a24b5b29d2d9b8e12748d8},
affiliation={Ohio State University, Columbus, OH 43210, United States; NASA Glenn Research Center, Cleveland, OH 44135, United States},
abstract={The deformation (plastic) and failure of 2024-T351 is studied under a wide range of loading conditions. Uniaxial tension and compression tests and pure shear tests have been conducted over a wide range of strain rates. The results show no significant strain rate effect on plastic deformation up to strain rate of about 2000s-1. Surface finish in tensile test has a noticeable effect on the strain to failure. Specimens with a rougher finish failed at a smaller strain than specimens with a smooth finish. Equivalent stress strain plots from tension, compression and shear tests do not coincide, which indicate that a plasticity model (either time independent or time dependent) that is based on J-2 flow theory will not be suitable for modeling the plastic deformation of the material. All the specimens that were tested were machined from plates. Specimens for tensile tests were machined in different orientations relative to the rolling direction of the plates. The results show that the plates have anisotropic properties with regard to plasticity and failure. Initially round cylindrical specimens tested in compression ended up with an elliptical cross sectional area. The objective of the testing program is to generate accurate and diverse data that will be used for developing accurate plasticity and failure models that will be incorporated in numerical codes that are used for simulations of impact events.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen20081773,
author={Chen, I.-J. and Foloppe, N.},
title={Conformational sampling of druglike molecules with MOE and catalyst: Implications for pharmacophore modeling and virtual screening},
journal={Journal of Chemical Information and Modeling},
year={2008},
volume={48},
number={9},
pages={1773-1791},
doi={10.1021/ci800130k},
note={cited By 84},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-54249090791&doi=10.1021%2fci800130k&partnerID=40&md5=eac40adcb457d11dabbeb4bf05a84ff9},
affiliation={Vernalis (R and D) Ltd., Granta Park, Abington, Cambridge CB21 6GB, United Kingdom},
abstract={Computational conformational sampling is integral to small molecule pharmaceutical research, for detailed conformational analysis and high-throughput 3D library enumeration. These two regimes were tested in details for the general-purpose modeling program MOE, using its three conformational sampling methods, i.e. systematic search, stochastic search, and Conformation Import. The tests include i) identification of the global energy minimum, ii) reproduction of the bioactive conformation, iii) measures of conformational coverage with 3D descriptors, and iv) compute times. The bioactive conformers are from a new set of 256 diverse, druglike, protein-bound ligands compiled and analyzed with particular care. The MOE results are compared to those obtained from the established program Catalyst. Key parameters controlling the conformational coverage were varied systematically. Coverage and diversity of the conformational space were characterized with unique pharmacophore triplets or quadruplets. Overall, the protocols in both MOE and Catalyst performed well for their intended tasks. MOE performed at least as well as Catalyst for high-throughput library generation and detailed conformational modeling. This work provides a guide and specific recommendations regarding the usage of conformational sampling tools in MOE. © 2008 American Chemical Society.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gao2007111,
author={Gao, J. and Tirumalasetti, S. and Hsu, C.-P. and Cheong, Y. and Colendich, A. and Fitch, T.},
title={Toward modeling and analysis for software installation testing},
journal={19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007},
year={2007},
pages={111-116},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84886922571&partnerID=40&md5=7a6e7c1e6e5168f14da8fb4aa0b2607e},
affiliation={San Jose State University, San Jose, CA 95192, United States; Intuit, Mountain View, United States},
abstract={Software testing is the last critical phase in software quality control. Software installation testing is one of the most important and complex tasks in system testing. However, in the past years, researchers have not paid much attention to the related issues and challenges in software installation testing. Today test and QA engineers have lacked systematic processes, test models, methods and tools to help them in software installation testing and patch testing. This paper addresses the test modeling and analysis issues in software installation testing. The paper proposes a new model, known as a semantic tree, to assist engineers to model and present diverse system environments and configurations, various running conditions, and system functional features. It is very useful to assist engineers to automatically identify, generate software installation test items and test cases. Copyright © (2007) by Knowledge Systems Institute (KSI).},
author_keywords={Software installation criteria and standards;  Software installation testing;  Software test modeling and analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Stewart2007151,
author={Stewart, A.D. and Zhang, X.},
title={Building a disordered protein database: A case study in managing biological data},
journal={Conferences in Research and Practice in Information Technology Series},
year={2007},
volume={63},
pages={151-159},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872844077&partnerID=40&md5=04d2c7d85fe877266a30d65bcb1dc9ea},
affiliation={School of Computer Science and IT, RMIT University, Australia},
abstract={A huge diversity of biological databases is available via the Internet, but many of these databases have been developed in an ad hoc manner rather than in accordance with any data management principles. In addition, in the area of disordered protein databases, many of the databases have not been made publicly available. This poses challenges to researchers, since reliable protein databases are required in order to test and measure the accuracy of protein structure prediction software. In this paper, we describe our work developing a disordered protein database using data from the protein secondary structure database DSSP-cont. In particular, we discuss the way in which we have addressed the issues of data cleaning, query processing and interoperability. This research is a pilot study in managing biological data. © 2007, Australian Computer Society, Inc.},
author_keywords={Biological data management;  Disordered proteins},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ezell2007584,
author={Ezell, E.A. and Felton, L.E. and La Roche, P. and Fox, M.},
title={Greenkit: A Modular Variable Application cooling system},
journal={American Solar Energy Society - Solar 2007: 36th ASES Annual Conf., 32nd National Passive Solar Conf., 2nd Renewable Energy Policy and Marketing Conference: Sustainable Energy Puts America to Work},
year={2007},
volume={2},
pages={584-591},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867880117&partnerID=40&md5=a9ba17cc0618e626f52dabe72b300305},
affiliation={Cal Poly Pomona, 3801 West Temple Ave., Pomona, CA 91768, United States},
abstract={A team of Cal Poly Pomona Faculty and students received a P3 Grant from the Environmental Protection Agency (EPA) for a student driven research project titled the "GREENKIT: A Modular, Variable Application System for Sustainable Cooling". The GreenKit project is being developed in a joint effort of architecture and engineering students working with local organizations in several locations in the USA, Uganda, Mexico, Venezuela and China. Climate change is the biggest environmental threat facing our planet. To address the problems of energy consumption, thermal comfort and CO2 emissions, that affect climate change, while increasing student awareness of our environmental problems, we have designed the "GreenKit". The GreenKit combines a vegetated component with a smart window and a distributed sensing and control system. The system, enables dynamic realtime management of environmental variables. During the 2006/2007 school year, students gained an understanding of passive cooling methods used in building design and the functions of a green roof while developing the smart window prototypes. Research focused on seven sites around the world, representative of diverse climate types and a range of economies. The most successful cooling strategies, as determined by climate modeling software and experimental tests, were incorporated into the development of a smart window component that could be distributed as a universal product used to moderate solar radiation, daylight, and air movement, thereby maintaining occupant comfort within a building.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lee2007,
author={Lee, G. and Xia, W.},
title={Relationships among software team flexibility, autonomy, diversity, and project performance},
journal={Academy of Management 2007 Annual Meeting: Doing Well by Doing Good, AOM 2007},
year={2007},
page_count={6},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858384000&partnerID=40&md5=f1ac28c333250c00a346ebe99f227a12},
affiliation={Kogod School of Business, American University, Washington, DC 20016, United States; Florida International University, Miami, United States},
abstract={As both business and technology environments change at a fast pace, software team flexibility in responding to requirement changes has become a critical software project success factor. However, little research has empirically examined software team flexibility in terms of its key dimensions, its determinants, and its impacts on software project performance. In this paper, we define software team flexibility in terms of its response extensiveness and response efficiency to system requirement change requests. Using survey data of 505 software development projects, we tested a research model and hypotheses regarding relationships among the dimensions of software team flexibility, team autonomy, team diversity, and project performance. The results revealed a negative relationship between the two dimensions of software project team flexibility. Team response efficiency positively affected both project process performance and software functionality, whereas team response extensiveness positively impacted only software functionality. While increased team autonomy helped enhance software team response efficiency, it reduced software team response extensiveness. In contrast, increased team diversity helped improve software team response extensiveness. The results suggest that, when it comes to issues related to software team flexibility, project managers must be aware of and effectively manage a complex set of tradeoffs among the various variables.},
author_keywords={Software team flexibility;  Team autonomy;  Team diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nakagawa2007157,
author={Nakagawa, E.Y. and Da Silva Sim̃ao, A. and Ferrari, F. and Maldonado, J.C.},
title={Towards a reference architecture for software testing tools},
journal={19th International Conference on Software Engineering and Knowledge Engineering, SEKE 2007},
year={2007},
pages={157-162},
note={cited By 15},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-77954889167&partnerID=40&md5=a93b35e4dcbd2d3ff04ddb4a6bae300b},
affiliation={Dept. of Computer Systems USP, University of São Paulo, São Carlos, SP, Brazil; Dept. of Administrative Science and Technology, Uniara - Araraquara University Center, Araraquara, SP, Brazil},
abstract={Producing high quality software systems has been one of the most important software development concerns. Software testing is recognized as a fundamental activity for assuring software quality; however, it is an expensive, errorprone, and time consuming activity. For this reason, a diversity of testing tools has been developed, however, they have been almost always designed without an adequate attention to their evolution, maintenance, and reuse. In this paper, we propose an aspect-based software architecture, named RefTEST (Reference Architecture for Software Testing Tools), that comprises the knowledge to develop testing tools. This architecture is strongly based on separation of concerns and aspects, aiming at evolving, maintaining and reusing efforts to develop these tools. Our experimental results have pointed out that RefTEST can contribute to the development and reengineering of testing tools. Copyright © (2007) by Knowledge Systems Institute (KSI).},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bueno200710,
author={Bueno, P.M.S. and Wong, W.E. and Jino, M.},
title={Improving random test sets using the diversity oriented test data generation},
journal={Proceedings of the 2nd International Workshop on Random Testing, RT 2007, Co-located with the 22nd IEEE/ACM International Conference on Automated Software Engineering, ASE 2007},
year={2007},
pages={10-17},
doi={10.1145/1292414.1292419},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-52249117352&doi=10.1145%2f1292414.1292419&partnerID=40&md5=a7b43857ccf5ffa061dfefa977f591e4},
affiliation={Renato Archer Research Center, Rodovia Dom Pedro I, km 143,6, Campinas, São Paulo, Brazil; University of Texas at Dallas, Richardson, TX 75083, United States; State University of Campinas, Av. Albert Einstein, 400, Campinas, São Paulo, Brazil},
abstract={We present a measure that characterizes the diversity of a test set from the perspective of the input domain of the program under test. By using a metaheuristic algorithm, randomly generated test sets (RTS) are evolved towards Diversity Oriented Test Sets (DOTS), which thoroughly cover the input domain. DOTS are evaluated using a Monte Carlo simulation to assess how testing factors influence their effectiveness and also by the values of data flow coverage and mutation scores attained on simple programs. Results provide understanding on possible gains of using DOTS and on circumstances where RTS can be more effective.},
author_keywords={data flow testing;  diversity oriented test data generation;  genetic algorithms;  mutation testing;  random testing;  simulated annealing;  simulated repulsion;  software testing;  test data generation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schardt2007117,
author={Schardt, J.},
title={GNF2 operating experience},
journal={American Nuclear Society - 2007 LWR Fuel Performance/Top Fuel},
year={2007},
pages={117-123},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-44949182647&partnerID=40&md5=d796d2ff2d7d0e45a7a090b28f8fa373},
affiliation={GE Energy, Nuclear, 3901 Castle Hayne Road, Wilmington, NC 28402, United States},
abstract={GNF's latest generation fuel product, GNF2, is designed to deliver improved nuclear efficiency, higher bundle and cycle energy capability, and more operational flexibility. But along with high performance, our customers face a growing need for absolute fuel reliability. This is driven by a general sense in the industry that LWR fuel reliability has plateaued. Too many plants are operating with fuel leakers, and the impact on plant operations and operator focus is unacceptable. The industry has responded by implementing an INPO-coordinated program aimed at achieving leaker-free reliability by 2010. One focus area of the program is the relationship between fuel performance (i.e., duty) and reliability. The industry recognizes that the right balance between performance and problem-free fuel reliability is critical. In the development of GNF2, GNF understood the requirement for a balanced solution and utilized a product development and introduction strategy that specifically addressed reliability: evolutionary design features supported by an extensive experience base; thoroughly tested components; and defense-in-depth mitigation of all identified failure mechanisms. The final proof test that the balance has been achieved is the application of the design, initially through lead use assemblies (LUAs), in a variety of plants that reflect the diversity of the BWR fleet. Regular detailed surveillance of these bundles provides the verification that the proper balance between performance and reliability has been achieved. GNF currently has GNF2 lead use assemblies operating in five plants. Included are plants that have implemented extended power uprates, plants on one and two-year operating cycles, and plants with and without NobleChem™ and zinc injection. The leading plant has undergone three poolside inspections outages to date. This paper reviews the actions taken to insure GNF2 's reliability, and the lead use assembly surveillance data accumulated to date to validate the adequacy/expected behavior of the design.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xu2007569,
author={Xu, C. and Cheung, S.C. and Chan, W.K. and Ye, C.},
title={On impact-oriented automatic resolution of pervasive context inconsistency},
journal={6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering, ESEC/FSE 2007},
year={2007},
pages={569-572},
doi={10.1145/1287624.1287712},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-37849008012&doi=10.1145%2f1287624.1287712&partnerID=40&md5=97b6ce1ab4903f1c3bc29642d3e7b0ba},
affiliation={Department of Computer Science and Engineering, Hong Kong University of Science and Technology, Kowloon, Hong Kong; Department of Computer Science, City University of Hong Kong, Kowloon, Hong Kong},
abstract={Context-awareness is a capability that allows applications in pervasive computing to adapt themselves continuously to changing contexts of their environments. However, contexts from physical environments may be inconsistent. It affects the correctness of these applications. Existing resolution strategies for context inconsistency have diverse adverse impacts on the context awareness of applications, such as feeding different amounts of contexts to the applications. In this paper, we examine the impacts of inconsistency resolution and study the extent to which their effects on context-awareness can be reduced. We conduct simulation experiments of two pervasive computing applications. The experimental results show that existing inconsistency resolution strategies adversely affect the context-awareness of applications. This motivates the importance of deploying an impact-oriented approach to respect context-awareness in inconsistency resolution. Copyright 2007 ACM.},
author_keywords={Inconsistency resolution;  Pervasive computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Koster2007108,
author={Koster, K.},
title={Using portfolio theory for better and more consistent quality},
journal={2007 ACM International Symposium on Software Testing and Analysis, ISSTA'07},
year={2007},
pages={108-117},
doi={10.1145/1273463.1273479},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34548234900&doi=10.1145%2f1273463.1273479&partnerID=40&md5=112bd2bfe1585ec2996042dc6099630d},
affiliation={Agitar Software Laboratories},
abstract={The effectiveness of software quality techniques varies. Many uncertain or unpredictable factors influence effectiveness, including human factors, the types of defects in the program, and luck. Compared to using a single quality technique, a diversified portfolio of techniques will typically be more effective and less variable. This work postulates a simple model, adapted from financial Modern Portfolio Theory, for the variability and effectiveness of techniques, singly and in portfolios. Proofs and simulations analyze the model to evaluate factors influencing the success of diversification; the model is checked against data sets from previous work.},
author_keywords={Diversification;  Economic models;  Effectiveness;  Portfolio software quality;  Testing;  Variability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Voas200748,
author={Voas, J.},
title={A baker's dozen: 13 software engineering challenges},
journal={IT Professional},
year={2007},
volume={9},
number={2},
pages={48-53},
doi={10.1109/MITP.2007.24},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34047265186&doi=10.1109%2fMITP.2007.24&partnerID=40&md5=be7e6e055b02ca50b2824472655c7452},
affiliation={Systems Assurance, SAIC and an SAIC Technical},
abstract={Software Engineering that has developed into a discipline of many diverse areas of interest, including testing, programming, maintenance, and metrics, faces several challenges. The 13 main challenges faced by it are software quality, return on investment, process improvement, metrics and measurement, standards confusion, standards interoperability, legacy software, testing stoppage criteria, interoperability and composability, operational profiles, designing in, product certification, and services. The software engineering community is working to develop ROI metrics using field data that is based on statistics, and not explanations. Metrics and models should be developed that define when testing should stop, based on criteria other than money and time constraints. The sustainability of the systems should also be better understood before they are developed, while design-for-maintainability paradigms should also be created to predict what life support should be rendered and at what ROI.},
document_type={Review},
source={Scopus},
}

@ARTICLE{Polo20073,
author={Polo, M. and Tendero, S. and Piattini, M.},
title={Integrating techniques and tools for testing automation},
journal={Software Testing Verification and Reliability},
year={2007},
volume={17},
number={1},
pages={3-39},
doi={10.1002/stvr.348},
note={cited By 26},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847372433&doi=10.1002%2fstvr.348&partnerID=40&md5=6b1e2d16ce66e3128b3cdc145b43c41c},
affiliation={University of Castilla-La Mancha, Paseo de la Universidad, 4, E-13071 Ciudad Real, Spain},
abstract={This article presents two tools to generate test cases, one for Java programs and one for .NET programs, as well as a structured testing process whereby such tools can be used in order to help in process automation. The main innovation in this work is the joint use of diverse techniques and technologies, which have been separately applied to test automation: reflection to extract the class structure; regular expressions to describe test templates and test cases; JUnit and NUnit as test case execution frameworks; mutation and MuJava as test case quality measurers; serialization to deal with the parameters of complex data types; and once more, reflection, to facilitate the test engineer in the task of writing the oracle. Finally, the article presents an experiment carried out to validate the work. Copyright © 2006 John Wiley & Sons, Ltd.},
author_keywords={JUnit;  Mutation;  Testing automation;  Testing process},
document_type={Article},
source={Scopus},
}

@ARTICLE{Heinz2007580,
author={Heinz, M. and Grosch, K.A.},
title={A laboratory method to comprehensively evaluate abrasion, traction and rolling resistance of tire tread compounds},
journal={Rubber Chemistry and Technology},
year={2007},
volume={80},
number={4},
pages={580-607},
doi={10.5254/1.3548182},
note={cited By 23},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149523047&doi=10.5254%2f1.3548182&partnerID=40&md5=51d8192c9e9fe96559ee7913f369d142},
affiliation={Degussa GmbH, Cologne, Germany; VMI Holland BV, Epe, Netherlands},
abstract={A laboratory test method has been developed which allows the evaluation of diverse properties of tire tread compounds on the same sample. The laboratory test instrument consists of a rotating abrasive disk against which a rubber sample wheel runs under a given load, slip angle and speed. All three force components acting on the wheel during the tests are recorded. By changing the variable values over a wide range practically all severities encountered in tire wear are covered. The well-known fact that compound ratings depend on the road testing conditions is verified. Most compounds are only significantly distinguishable against a control over a limited range of testing conditions. Using a road test simulation computer program based on the laboratory data shows that not only ratings correspond to practical experience but also calculated absolute tire life times do. Tests on surfaces of different coarseness and sharpness indicate that sharp coarse surfaces give the best results with road tests, which of necessity are mostly carried out on public roads of differing constitution. The abrasive surface can be wetted with water at different temperatures and hence either the friction force at a locked wheel or the side force at a slipping wheel can be measured over a wide range of temperatures and speeds. At small slip angles the side force is dominated by dynamic cornering stiffness of the compound, at large slip angles by the friction coefficient. In this case, too, good correlations to road experience exist over a limited range of testing conditions. Low water temperatures and low slip speed settings in the laboratory produce side force ratings, which correlate closely with ABS braking on the road. High and higher slip speeds give ratings in close agreement with locked wheel braking on the road. A heatable/coolable disk enables traction measurements on ice and newly abrasion measurements on surfaces at elevated surface temperature. Ice surface temperatures between -5 °C and -25 °C are possible. Friction measurements show that the difference in compound rating between summer and winter compounds is maintained over the whole temperature range. New investigations show not only a differentiation between different winter tire treads qualities but also an excellent correlation between tire and laboratory results. As a new topic side force measurements on dry surfaces highlight the correlation to dry handling of tires. The tire tread compound contributes to this performance through its shear stiffness and its friction coefficient. The shear stiffness contributes to the response of the tire in directional changes. The friction coefficient determines the maximum force, which can be transmitted. A simple operation possibility for evaluation of determined side forces is demonstrated. In addition to antecedent investigations the rolling resistance of the rubber wheel can be measured over a range of loads and speeds with the slip angle set at zero. Again for these new results good correlations are achieved with practical, experience. In particular, the dependence of the rolling resistance on the velocity and loads are pointed out. Ultimately a good correlation between tire test and laboratory test results was demonstrated.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kapur2007219,
author={Kapur, P.K. and Gupta, A. and Jha, P.C.},
title={Reliability analysis of project and product type software in operational phase incorporating the effect of fault removal efficiency},
journal={International Journal of Reliability, Quality and Safety Engineering},
year={2007},
volume={14},
number={3},
pages={219-240},
doi={10.1142/S021853930700260X},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547473167&doi=10.1142%2fS021853930700260X&partnerID=40&md5=415aaca0d631cd3804d9318e64286657},
affiliation={Department of Operational Research, University of Delhi, Delhi - 110007, India},
abstract={Since the early 1970's numerous Software Reliability Growth Models (SRGM) have been proposed in the literature to estimate the software reliability measures such as the remaining number of faults, failure rate and reliability growth during the testing phase. These models are applied to the software testing data collected during the testing phase and then are often used to predict the software failures in operational phase. In practice simulating mirror image of the diverse testing environment representative of the operational environment is difficult in practice and hence the simulated testing environment during the testing phase may not be similar to the conditions that exist in the operational phase. During testing phase testing is performed under a controlled environment whereas during the operational phase failure phenomenon depends on the operational environment and usage of software. Therefore an SRGM developed for the testing phase is not suitable for estimating the reliability growth during the operational phase. In this paper, we propose a generalized Software Reliability Growth Model, which can be used to estimate number of faults during the testing phase and can be easily extended to the operation phase. In the testing phase, it is appropriate to estimate the reliability growth with respect to the amount of testing resources spent on testing whereas in the operational phase the amount of effort to be spent on removing a fault reported by a user is fixed by the developer. The number of failures detected and hence the reliability growth during the user phase depends on the usage of software. The proposed model appropriately incorporates these changes. Further we categorize the software into two-categories-(a) project and (b) product type software. Appropriate usage functions are linked to both project and product type software. To describe the fault removal phenomenon, imperfect debugging environment is incorporated into the model building. The paper highlights an interdisciplinary mathematical modeling approach in Software Reliability Engineering and Marketing. The proposed model is validated for both phases using the software failure data sets obtained from different sources. Model describes the failure phenomenon for these data sets fairly. © World Scientific Publishing Company.},
author_keywords={Error generation;  Imperfect debugging;  Operation phase;  Product type software;  Project type software;  Software Reliability Growth Model (SRGM);  Testing efficiency;  Testing efforts;  Testing phase;  Usage function},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kelly2006599,
author={Kelly, A. and Amidi, O. and Bode, M. and Happold, M. and Herman, H. and Pilarski, T. and Rander, P. and Stentz, A. and Vallidis, N. and Warner, R.},
title={Toward reliable off road autonomous vehicles operating in challenging environments},
journal={Springer Tracts in Advanced Robotics},
year={2006},
volume={21},
pages={599-608},
doi={10.1007/11552246_57},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845466945&doi=10.1007%2f11552246_57&partnerID=40&md5=3c1c0d50d9af3cca0feb2f5d9997d5ba},
affiliation={Robotics Institute, Carnegie Mellon University, Pittsburgh, PA, United States},
abstract={The DARPA PerceptOR program implements a rigorous evaluative test program which fosters the development of field relevant outdoor mobile robots. Autonomous ground vehicles are deployed on diverse test courses throughout the USA and quantitatively evaluated on such factors as autonomy level, waypoint acquisition, failure rate, speed, and communications bandwidth. Our efforts over the three year program have produced new approaches in planning, perception, localization, and control which have been driven by the quest for reliable operation in challenging environments. This paper focuses on some of the most unique aspects of the systems developed by the CMU PerceptOR team and the most immediate challenges that remain to be addressed. © Springer-Verlag Berlin/Heidelberg 2006.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sârbu2006120,
author={Sârbu, C. and Johansson, A. and Fraikin, F. and Suri, N.},
title={Improving robustness testing of COTS OS extensions},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4328 LNCS},
pages={120-139},
doi={10.1007/11955498_9},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887121935&doi=10.1007%2f11955498_9&partnerID=40&md5=29aff5c70c4ab8726b92c197afbf64e4},
affiliation={Computer Science Department, Technische Universität Darmstadt, Hochschulstr. 10, 64289 Darmstadt, Germany},
abstract={Operating systems (OS) are increasingly geared towards support of diverse peripheral components, both hardware (HW) and software (SW), rather than explicitly focused on increased reliability of delivered OS services. The interface between the OS and the HW devices is provided by device drivers. Furthermore, drivers have become add-on COTS components to support the OS's capabilities of widespread device support. Unfortunately, drivers constitute a major cause of system outages, impacting overall service reliability. Consequently, the testing of drivers becomes important. However, despite the efforts to develop appropriate testing methods, the multitude of possible system configurations and lack of detailed OS specifications makes the task difficult. Not requiring access to OS source code, this paper develops novel, non-intrusive support for test methods, based on ascertaining test progress from a driver's operational state model. This approach complements existing schemes, enhancing the level of accuracy of the test process by providing test location guidance. © Springer-Verlag Berlin Heidelberg 2006.},
author_keywords={COTS;  Device driver;  Operating system;  Robustness testing;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Alshraideh2006107,
author={Alshraideh, M. and Bottaci, L.},
title={Using program data-state diversity in test data search},
journal={Proceedings - Testing: Academic and Industrial Conference - Practice and Research Techniques, TAIC PART 2006},
year={2006},
pages={107-114},
doi={10.1109/TAIC-PART.2006.37},
art_number={1691676},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052339708&doi=10.1109%2fTAIC-PART.2006.37&partnerID=40&md5=ba348c744f1845db89d52a2724e0fead},
affiliation={Department of Computer Science, University of Hull, Hull, HU6 7RX, United Kingdom},
abstract={Search-based automatic software test data generation for structural testing depends on the instrumentation of the test goal to construct a many-valued function which is then optimised. The method encounters difficulty when the search is in a region in which the function is not able to discriminate between different candidate test cases because it returns a constant value. A typical example of this problem arises in the instrumentation of branch predicates that depend on the value of a boolean-valued (flag) variable. Existing transformation techniques can solve many cases of the problem but there are situations for which transformation techniques are inadequate. This paper presents a technique for directing the search when the function that instruments the test goal is not able to discriminate candidate test inputs. The new technique depends on introducing program data-state diversity as an additional search goal. The search is guided by a new evaluation (cost) function made up of two parts, one depends on the conventional instrumentation of the test goal, the other depends on the diversity of the data-states produced during execution of the program under test. The method is demonstrated for a number of example programs for which existing methods are inadequate. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor2006,
title={2nd Workshop on Mutation Analysis Mutation 2006 - ISSRE Workshops 2006 Mutation06},
journal={2nd Workshop on Mutation Analysis (Mutation 2006 - ISSRE Workshops 2006), MUTATION'06},
year={2006},
page_count={123},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-48749119634&partnerID=40&md5=d41d3c01acedf7ba462dafd63c5c4700},
abstract={The proceedings contain 14 papers. The topics discussed include: mutation testing implements grammar-based testing; mutation-based model synthesis in model driven engineering; SQLmutation: a tool to generate mutants of SQL database queries; mutation analysis for reactive system environment properties; efficient mutant generation for mutation testing of pointcuts in aspect-oriented programs; ExMAn: a generic and customizable framework for experimental mutation analysis; finding sufficient mutation operators via variable reduction; assessment of data diversity methods for software fault tolerance based on mutation analysis; the SESAME experience: from assembly languages to declarative models; fault-based interface testing between real-time operating system and application; and basic operations for generating behavioral mutants.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Green2006257,
author={Green, P.J. and Taylor, D.P.},
title={Implementation of a real-time multiple input multiple output channel estimator on the smart Antenna software radio test system platform using the Xilinx virtex 2 pro field programmable gate array},
journal={Proceedings - 2006 IEEE International Conference on Field Programmable Technology, FPT 2006},
year={2006},
pages={257-260},
doi={10.1109/FPT.2006.270322},
art_number={4042444},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-43749123892&doi=10.1109%2fFPT.2006.270322&partnerID=40&md5=eef387079274cd89141ad93f376589fb},
affiliation={Department of Electrical and Computer Engineering, University of Canterbury, Christchurch, New Zealand},
abstract={This paper describes the concept, architecture, development and demonstration of a real time, channel estimator system on a Xilinx Virtex 2 Pro Field Programmable Gate Array for a 4-transmit 4-receiver multiple input and multiple output (MIMO) wireless test platform. It is designed and developed for research into receiver diversity and MIMO wireless systems. Hardware, firmware, use of the Xilinx Core Generator Intellectual Property modules and experimental verification of the channel estimator are discussed. © 2006 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pareto200630,
author={Pareto, L. and Boquist, U.},
title={A quality model for design documentation in model-centric projects},
journal={Proceedings of the Third International Workshop on Software Quality Assurance, SOQUA 2006},
year={2006},
pages={30-37},
doi={10.1145/1188895.1188905},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547678378&doi=10.1145%2f1188895.1188905&partnerID=40&md5=5ab351206d84dfdc9d67e2227811b05d},
affiliation={Chalmers / IT University of Göteborg, Box 8718, Lindholmen, Gothenburg, Sweden; Ericsson Software Research},
abstract={Model-centric software processes, such as RUP, involve a rich set of artefacts (e.g., requirements specifications, design models, code) used for many activities (e.g., cost estimation, construction, communication, maintenance, archiving) by engineers in many roles (e.g., analysts, designers, developers, testers, managers). This diversity makes organisation and implementation of infrastructure for artefacts (such as document repositories, web portals, editors, indexes) a challenge: without analysis of involved tasks, and validation of the infrastructure's implementation, some tasks may not be properly supported. This paper presents a quality model for design documentation (i.e., for artefacts on the abstraction level between requirements specifications and code) intended to be used for requirements engineering of such. Twenty two qualities, most of which concern nonfunctional requirements on the design documentation are identified. The model is based on qualitative analysis of interviews with engineers and managers in a large software development organisation with 8 years experience of implementing RUP. Copyright 2006 ACM.},
author_keywords={Design documentation;  Modeling;  Quality model;  UML},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Yang2006,
author={Yang, H.-C. and Pan, C.-C. and Lin, C.-Y. and Fann, C.S.J.},
title={PDA: Pooled DNA analyzer},
journal={BMC Bioinformatics},
year={2006},
volume={7},
doi={10.1186/1471-2105-7-233},
art_number={233},
note={cited By 18},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247550812&doi=10.1186%2f1471-2105-7-233&partnerID=40&md5=7f83ef04a1e688f400e8bc8f35f0cca0},
affiliation={Institute of Biomedical Sciences, Academia Sinica, Nankang, Taipei 115, Taiwan},
abstract={Background: Association mapping using abundant single nucleotide polymorphisms is a powerful tool for identifying disease susceptibility genes for complex traits and exploring possible genetic diversity. Genotyping large numbers of SNPs individually is performed routinely but is cost prohibitive for large-scale genetic studies. DNA pooling is a reliable and cost-saving alternative genotyping method. However, no software has been developed for complete pooled-DNA analyses, including data standardization, allele frequency estimation, and single/multipoint DNA pooling association tests. This motivated the development of the software, 'PDA' (Pooled DNA Analyzer), to analyze pooled DNA data. Results: We develop the software, PDA, for the analysis of pooled-DNA data. PDA is originally implemented with the MATLAB® language, but it can also be executed on a Windows system without installing the MATLAB®. PDA provides estimates of the coefficient of preferential amplification and allele frequency. PDA considers an extended single-point association test, which can compare allele frequencies between two DNA pools constructed under different experimental conditions. Moreover, PDA also provides novel chromosome-wide multipoint association tests based on p-value combinations and a sliding-window concept. This new multipoint testing procedure overcomes a computational bottleneck of conventional haplotype-oriented multipoint methods in DNA pooling analyses and can handle data sets having a large pool size and/or large numbers of polymorphic markers. All of the PDA functions are illustrated in the four bona fide examples. Conclusion: PDA is simple to operate and does not require that users have a strong statistical background. The software is available at http://www.ibms.sinica.edu.tw/%7Ecsjfann/first%20flow/pda.htm. © 2006 Yang et al; licensee BioMed Central Ltd.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gibson20061,
author={Gibson, J.P.},
title={E-voting and the need for rigourous software engineering – The past, present and future},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4355 LNCS},
pages={1},
doi={10.1007/11955757_1},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881560453&doi=10.1007%2f11955757_1&partnerID=40&md5=f56deabc9c39a1cd8b8816724ca6125c},
affiliation={Department of Computer Science, National University of Ireland, Maynooth, Ireland},
abstract={In many jurisdictions around the world, the introduction of e-voting has been subject to wide-ranging debate amongst voters, politicians, political scientists, computer scientists and software engineers. A central issue is one of public trust and confidence: should voters be expected to put their faith in “closed” electronic systems where previously they trusted “open” manual systems? As the media continues to report on the “failure” of e-voting machines, electoral administrators and e-voting machine manufacturers have been required to review their policies and systems in order to meet a set of ever changing requirements. Such an unstable problem domain stretches their understanding of the electoral process and their ability to apply a diverse range of technologies in providing acceptable electronic solutions. The breadth and depth of the issues suggest that no electoral administration can justifiably claim to have implemented a “trustworthy” electronic replacement for a paper system. All e-voting systems rely substantially on the correct functioning of their software. It has been argued that such e-voting software is “critical” to its users, and so one would expect to see the highest standards being applied in the development of software in e-voting machines: this is certainly not the case for machines that have already been used. Furthermore, in jurisdictions where e-voting machines have just been procurred we shall see that the software in these machines is often of very poor “quality”, even though it has been independently tested and accredited for use. Throughout the presentation we will focus on the software engineering issues, and will consider the question of whether the formal methods community could have done more - and should do more - to help alleviate the costly problems that society is facing from badly developed software in a wide range of critical government information systems (and not just voting machines). © Springer-Verlag Berlin Heidelberg 2006.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Grieskamp20061,
author={Grieskamp, W.},
title={Multi-paradigmatic model-based testing},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2006},
volume={4262 LNCS},
pages={1-19},
doi={10.1007/11940197_1},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34748818242&doi=10.1007%2f11940197_1&partnerID=40&md5=ab72ad0e45498b384e715898c1c390f1},
affiliation={Microsoft Research, Redmond, WA, United States},
abstract={For half a decade model-based testing has been applied at Microsoft in the internal development process. Though a success story compared to other formal quality assurance approaches like verification, a break-through of the technology on a broader scale is not in sight. What are the obstacles? Some lessons can be learned from the past and will be discussed. An approach to MBT is described which is based on multi-paradigmatic modeling, which gives users the freedom to choose among programmatic and diagrammatic notations, as well as state-based and scenario-based (interaction-based) styles, reflecting the different concerns in the process. The diverse model styles can be combined by model composition in order to achieve an integrated and collaborative model-based testing process. The approach is realized in the successor of Microsoft Research's MBT tool Spec Explorer, and has a formal foundation in the framework of action machines. © 2006 Springer-Verlag Berlin/Heidelberg.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Pohl20061049,
author={Pohl, K. and Metzger, A.},
title={Variability management in software product line engineering},
journal={Proceedings - International Conference on Software Engineering},
year={2006},
volume={2006},
pages={1049-1050},
doi={10.1145/1134285.1134499},
note={cited By 38},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-34247141998&doi=10.1145%2f1134285.1134499&partnerID=40&md5=4de56d9adc6633f103497abc7c400412},
affiliation={Lero (The Irish Software Engineering Research Centre), University of Limerick, Limerick, Ireland; Software Systems Engineering, University of Duisburg-Essen, Schützenbahn 70, 45117 Essen, Germany},
abstract={By explicitly modeling and managing variability, software product line engineering provides a systematic approach for creating a diversity of similar products at low cost, in short time, and with high quality. This tutorial focuses on the two principle differences of software product line engineering when compared to single systems development: The differentiation of two key development processes (domain engineering and application engineering) and the explicit representation and management of variability. We characterize the two processes and their main activities and introduce the orthogonal variability modeling approach (OVM). We further illustrate the OVM approach in the product line requirements engineering and product line testing activities.},
author_keywords={Requirements engineering;  Software product lines;  Testing;  Variability management;  Variability modeling},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Tamura2006143,
author={Tamura, Y. and Yamada, S.},
title={A flexible stochastic differential equation model in distributed development environment},
journal={European Journal of Operational Research},
year={2006},
volume={168},
number={1},
pages={143-152},
doi={10.1016/j.ejor.2004.04.034},
note={cited By 46},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244401661&doi=10.1016%2fj.ejor.2004.04.034&partnerID=40&md5=d5a865d632fcf90ca32bf79e550637b0},
affiliation={Fac. of Environ. and Info. Studies, Department of Information Systems, Tottori Univ. of Environ. Studies, Kita 1-1-1, Wakabadai, Tottori-shi 689-1111, Japan; Faculty of Engineering, Dept. of Social Systems Engineering, Tottori University, Minami 4-101, Koyama, Tottori-shi 680-8552, Japan},
abstract={In recent years, the dependence on a computer system has become large in our social life. Especially, a software development environment has been changing into distributed development environment interconnected with work-stations. Therefore, it becomes more difficult for software developers to produce highly reliable software systems efficiently, because of the more diversified and complicated software requirements. Also, in the software development process, the software testing-cost occupies more than a half of the total development cost. In this paper, we derive a flexible stochastic differential equation model describing a fault-detection process during the system testing phase of the distributed development environment by applying a mathematical technique of stochastic differential equations of an Itô type. Moreover, we discuss optimal software release problems based on the reusable rate of software components minimizing the expected total software cost, and also minimizing the cost with satisfying a software reliability requirement based on the coefficient of variation. © 2004 Elsevier B.V. All rights reserved.},
author_keywords={Distributed development environment;  Optimal software release problems;  Reliability;  Stochastic differential equation;  Stochastic processes},
document_type={Article},
source={Scopus},
}

@BOOK{Gross20051,
author={Gross, H.-G.},
title={Component-based software testing with UML},
journal={Component-Based Software Testing with UML},
year={2005},
pages={1-316},
doi={10.1007/b138012},
note={cited By 78},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891403524&doi=10.1007%2fb138012&partnerID=40&md5=013d1957c7ef483c2a38b06fff572dcf},
affiliation={Fraunhofer Institute for Experimental Software Engineering, Sauerwiesen 6, 67661 Kaiserslautern, Germany},
abstract={Component-based software development regards software construction in terms of conventional engineering disciplines where the assembly of systems from readily-available prefabricated parts is the norm. Because both component-based systems themselves and the stakeholders in component-based development projects are different from traditional software systems, component-based testing also needs to deviate from traditional software testing approaches. Gross first describes the specific challenges related to component-based testing like the lack of internal knowledge of a component or the usage of a component in diverse contexts. He argues that only built-in contract testing, a test organization for component-based applications founded on building test artifacts directly into components, can prevent catastrophic failures like the one that caused the now famous ARIANE 5 crash in 1996. Since building testing into components has implications for component development, built-in contract testing is integrated with and made to complement a model-driven development method. Here UML models are used to derive the testing architecture for an application, the testing interfaces and the component testers. The method also provides a process and guidelines for modeling and developing these artifacts. This book is the first comprehensive treatment of the intricacies of testing component-based software systems. With its strong modeling background, it appeals to researchers and graduate students specializing in component-based software engineering. Professionals architecting and developing component-based systems will profit from the UML-based methodology and the implementation hints based on the XUnit and JUnit frameworks. © Springer-Verlag Berlin Heidelberg 2005. All rights are reserved.},
document_type={Book},
source={Scopus},
}

@CONFERENCE{O'neil2005,
author={O'neil, T.D.},
title={The effective use of web-based training and assessment in a computer literacy course},
journal={Proceedings of ISECON},
year={2005},
page_count={8},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865759662&partnerID=40&md5=444c2049a916412326475647a955a6ab},
affiliation={Computer Science Department, Indiana University of Pennsylvania, 319 Stright Hall, Indiana, PA 15705, United States},
abstract={With the onset of the millennium comes the diversity of students' knowledge in the field of computer literacy. High schools are now graduating a more computer literate student. This poses a challenge to the basic computer literacy course instructor. How to meet the needs of all students is a common quandary. It has been my experience as an educator that the answer lies in the implementation of a web-based training and assessment software package. Fifty percent of our course in computer literacy delves into computer concepts. This author does not endorse web-based training and assessment in that area. When discussing computer ethics or cybercrimes, for example, the traditional classroom is a much better venue to entice student interaction and critical thinking. This paper will explain how using training and assessment software in the computer applications portion of the course will effectively meet the needs of the non-traditional student as well as the traditional student. It will explain the use of this software in the classroom. The author does not endorse one specific software tool, only the concept of using web-based training and assessment for teaching a skill subject. The software package used for this paper is Course Technology's SAM 2003 Version 3.0. © 2005 EDSIG.},
author_keywords={Assessment;  Computer;  Literacy;  Online;  Software;  Testing;  Training},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2005568,
author={Xie, X. and Xu, B. and Shi, L. and Nie, C. and He, Y.},
title={A dynamic optimization strategy for evolutionary testing},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2005},
volume={2005},
pages={568-575},
doi={10.1109/APSEC.2005.6},
art_number={1607196},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33847299024&doi=10.1109%2fAPSEC.2005.6&partnerID=40&md5=641ac478748b75b9da2017da9f79e913},
affiliation={Southeast University, Department of Computer Science and Engineering, 210096 Nanjing, China; Key Laboratory of Software Engineering, Wuhan University, 430072 Wuhan, China; National University of Defense Technology, Department of Computer, Changsha 410073, China; Department of Computer Science and Engineering, Southeast University, 210096 Nanjing, China},
abstract={Evolutionary Testing (ET) is an efficient technique of automated test case generation. ET uses a kind of metaheuristic search technique, Genetic Algorithm (GA), to convert the task of test case generation into an optimal problem. The configuration strategies of GA will have notable influences upon the performance of ET. In this paper, we present a dynamic self-adaptation strategy for evolutionary structural testing. It monitors evolution process dynamically, detects the symptom of prematurity by analyzing the population, and adjusts the mutation possibility to recover the diversity of the population. The empirical results show that the strategy can greatly improve the performance of the ET in many cases. Besides, some valuable advices are provided for the configuration strategies of ET by the empirical study. © 2005 IEEE.},
author_keywords={Dynamic optimization;  Evolutionary testing;  Software testing;  Structural testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{White200418,
author={White, L. and Robinson, B.},
title={Industrial real-time regression testing and analysis using firewalls},
journal={IEEE International Conference on Software Maintenance, ICSM},
year={2004},
pages={18-27},
doi={10.1109/ICSM.2004.1357786},
note={cited By 32},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-18044389303&doi=10.1109%2fICSM.2004.1357786&partnerID=40&md5=fb9dbfe13424d0e5a1492379b14e3cc5},
affiliation={Department of EECS, Case Western Reserve University, United States; ABB Inc., Cleveland, OH, United States},
abstract={Industrial real-time systems are complex and need to be thoroughly tested before being released to the customer. We have found that last minute changes are often responsible for the introduction of defects, causing serious problems for the customer. In this paper, we will demonstrate that these defects can be introduced into real-time software in diverse ways, and there is no simple regression testing method that can deal with all of these defect sources. This paper will describe the application of a testing firewall for regression testing whose form will differ depending upon the defect. The idea of the testing firewall is to limit the regression testing to those potentially affected system elements directly dependent upon changed system elements, and then to thoroughly test these elements. This has resulted in substantial savings in regression testing costs, and yet has been effective in detecting critical defects with significant implication in terms of customer acceptance at ABB. Empirical studies will be reported for these experiences in an industrial setting. © 2004 IEEE.},
author_keywords={Deadlock;  Real-Time Software;  Regression Testing;  Software Defects;  Software Testing;  Testing Firewall},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Cai2004125,
author={Cai, X. and Lyu, M.R.},
title={An empirical study on reliability modeling for diverse software systems},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2004},
pages={125-136},
doi={10.1109/ISSRE.2004.6},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-16244407414&doi=10.1109%2fISSRE.2004.6&partnerID=40&md5=d805966dcb6b88905abf5be2fe87bf07},
affiliation={Dept. of Comp. Sci. and Engineering, Chinese University of Hong Kong, Hong Kong, Hong Kong},
abstract={Reliability and fault correlation are two main concerns for design diversity, yet empirical data are limited in investigating these two. In previous work, we conducted a software project with real-world application for investigation on software testing and fault tolerance for design diversity. Mutants were generated by injecting one single real fault recorded in the software development phase to the final versions. In this paper, we perform more analysis and experiments on these mutants to evaluate and investigate the reliability features in diverse software systems. We apply our project data on two different reliability models and estimate the reliability bounds for evaluation purpose. We also parameterize fault correlations to predict the reliability of various combinations of versions, and compare three different fault-tolerant software architectures. © 2004 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lagier2004,
author={Lagier, L.C.E. and Craig, C.D. and Benshoof, P.},
title={JAMFEST, a cost effective solution to GPS vulnerability testing},
journal={USAF Developmental Test and Evaluation Summit},
year={2004},
doi={10.5081/jgps.3.1.40},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087603352&doi=10.5081%2fjgps.3.1.40&partnerID=40&md5=8e7a81bc166d6228afa5d57d73aeba72},
affiliation={46th Test Group, United States; 746th Test Squadron (CIGTF), United States},
abstract={From May 24-28, 2004, the 746th Test Squadron, located at Holloman Air Force Base (AFB), New Mexico (NM), planned and executed an innovative Global Positioning System (GPS) jamming program at White Sands Missile Range, NM. This program, known as JAMFEST, was aimed at providing low to no cost, realistic, GPS jamming scenarios for testing GPS-based navigation systems, as well as, training personnel in unique GPS denied environments. Through sponsorship from the GPS Joint Program Office, White Sands Missile Range, and the 46th Test Group, the 746th Test Squadron was able to provide this opportunity at a significantly reduced cost to each participant. During JAMFEST, the 746th Test Squadron hosted twelve simultaneous, yet very diverse customers, including multi-service Department of Defense (DoD) organizations, several defense contractors, and civil organizations. Their objectives ranged from training personnel on the effects of GPS jamming to characterizing the performance of prototype advanced anti-jam technologies against operationally realistic threats. To accomplish these goals, participants drove, flew, or walked through 59 jamming scenarios specifically tailored to stress the systems under evaluation. These tests would have cost a total of $660,000 or more if conducted separately. However, JAMFEST achieved the same objectives for approximately $85,000 in available funds coupled with discounted or donated services totaling $175,000. This paper details overall test and participant objectives, strategies, conduct, and addresses future JAMFEST activities.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xia2004348,
author={Xia, Q. and Eremin, A. and Wallace, M.},
title={Problem decomposition for traffic diversions},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={3011},
pages={348-363},
doi={10.1007/978-3-540-24664-0_24},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750345270&doi=10.1007%2f978-3-540-24664-0_24&partnerID=40&md5=78043078afe6b8ad50751ea7b7c4fe61},
affiliation={IC-Parc, Imperial College London, London SW7 2AZ, United Kingdom; School of Business Systems, Monash University, Clayton, Vic. 3800, Australia},
abstract={When a major road traffic intersection is blocked, vehicles should be diverted from the incoming roads in such a way as to avoid the roads on the diversions from also becoming over-congested. Assuming different diversions may use partly the same roads, the challenge is to satisfy the following traffic flow constraint: ensure that even in the worst case scenario, the diversions can accommodate the same volume of traffic as the blocked intersection. The number of diversions increases quadratically with the number of roads at the intersection. Moreover any road may be used by any subset of the diversions - thus the number of worst cases can grow exponentially with the number of diversions. This paper investigates two different approaches to the problem, describes their implementation on the hybrid MIP/CP software platform ECLiPSe, and presents benchmark results on a set of test cases. © Springer-Verlag Berlin Heidelberg 2004.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Guo2004164,
author={Guo, Q. and Hierons, R.M. and Harman, M. and Derderian, K.},
title={Computing unique input/output sequences using genetic algorithms},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2004},
volume={2931},
pages={164-177},
doi={10.1007/978-3-540-24617-6_12},
note={cited By 30},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646858453&doi=10.1007%2f978-3-540-24617-6_12&partnerID=40&md5=8fa95021475e4209dfdbbc890dc2b350},
affiliation={Department of Information System and Computing, Brunel University, Uxbridge, Middlesex, UB8 3PH, United Kingdom},
abstract={The problem of computing Unique Input/Ouput sequences (UIOs) is NP-hard. Genetic algorithms (GAs) have been proven to be effective in providing good solutions for some NP-hard problems. In this work, we investigated the construction of UIOs using GAs. We defined a fitness function to guide the search of potential UIOs and introduce a DO NOT CARE character to improve the GA's diversity. Experimental results suggest that, in a small system, the performance of the GA based approaches is no worse than that of random search while, in a more complex system, the GA based approaches outperform random search. © Springer-Verlag Berlin Heidelberg 2004.},
author_keywords={Conformance Testing;  FSMs;  Genetic Algorithms;  Optimisation;  UIOs},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bowring2004195,
author={Bowring, J.F. and Rehg, J.M. and Harrold, M.J.},
title={Active learning for automatic classification of software behavior},
journal={ISSTA 2004 - Proceedings of the ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2004},
pages={195-205},
doi={10.1145/1013886.1007539},
note={cited By 143},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-22944435659&doi=10.1145%2f1013886.1007539&partnerID=40&md5=35e62de7eff8fb37af99ea50d8fe89e6},
affiliation={College of Computing, Georgia Institute of Technology, Atlanta, GA 30332-0280, United States},
abstract={A program's behavior is ultimately the collection of all its executions. This collection is diverse, unpredictable, and generally unbounded. Thus it is especially suited to statistical analysis and machine learning techniques. The primary focus of this paper is on the automatic classification of program behavior using execution data. Prior work on classifiers for software engineering adopts a classical batch-learning approach. In contrast, we explore an active-learning paradigm for behavior classification. In active learning, the classifier is trained incrementally on a series of labeled data elements. Secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the Markov property, and that the resultant Markov models of individual program executions can be automatically clustered into effective predictors of program behavior. We present a technique that models program executions as Markov models, and a clustering method for Markov models that aggregates multiple program executions into effective behavior classifiers. We evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation. Copyright 2004 ACM.},
author_keywords={Machine learning;  Markov models;  Software behavior;  Software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ramachandran200394,
author={Ramachandran, M.},
title={Testing software components using boundary value analysis},
journal={Conference Proceedings of the EUROMICRO},
year={2003},
pages={94-98},
doi={10.1109/EURMIC.2003.1231572},
art_number={1231572},
note={cited By 13},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889583607&doi=10.1109%2fEURMIC.2003.1231572&partnerID=40&md5=9483f2d2d785528b2537d8e376610eff},
affiliation={School of Computing, Beckett Park Campus, Leeds Metropolitan University, Leeds, United Kingdom},
abstract={Most consumer electronics products today contain complex embedded software. We believe a component-oriented approach is an ideal way to handle the diversity of software complexity. Our earlier work on reusable components has addressed the development of the Koala component model has been developed (as an outcome of the ESPRIT project ARES) to address reuse with concept of late binding. This has been carried out as a part of the research project on 'testing software components'. Our approach to testing components was based on the principles of testing from object models. Therefore we have decomposed a COM-like component into OO models so that various test techniques can be automated. Also we are able to generate a volume of key test cases to study boundary value testing and analysis on component interfaces, which is the key to achieve testability of a reusable software component. © 2003 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Robinson2003654,
author={Robinson, S.J. and Hesse, B.W. and Shaikh, A.R. and Coss, M. and Crawford, C.},
title={Using converging methods across disciplines to guide the redesign of a large, information-rich Web site},
journal={Conference on Human Factors in Computing Systems - Proceedings},
year={2003},
pages={654-655},
doi={10.1145/765891.765912},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-5044244966&doi=10.1145%2f765891.765912&partnerID=40&md5=dad5be4042e5d9444b7add32ae4bec41},
affiliation={Center for Disease Control and Prevention, Atlanta, GA 30333, United States; Westat, 1650 Research Blvd., Rockville, MD 20850, United States; College of Computing, GVU Center, Georgia Tech., Atlanta, GA 30322-0280, United States; Health Media Research Lab., University of Michigan, Ann Arbor, MI 48109, United States},
abstract={This paper summarizes how differing research methodologies were sequenced during formative evaluation of a large-scale government Web site in order to generate consensus for site redesign and a clear typology of users. Each method was selected by an interdisciplinary research team to bring to the study a convergence of approaches across the fields of human computer interaction, information science, health communication, and social marketing. Researchers can use the study framework in optimizing their own program of organizational and user research, particularly if they are designing and testing largescale information-rich sites with varied content accessed by a diverse set of users.},
author_keywords={Audience research;  Formative research;  Methodology;  Organizational research;  Strategic planning;  User segmentation;  User-centered design;  Web evaluation},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chung2003729,
author={Chung, H.-Y. and Kim, D.-W.},
title={Design of advanced power reactor (APR1400) i&c system},
journal={IFAC Proceedings Volumes (IFAC-PapersOnline)},
year={2003},
volume={36},
number={20},
pages={729-734},
doi={10.1016/S14746670(17)34557-3},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064465387&doi=10.1016%2fS14746670%2817%2934557-3&partnerID=40&md5=04fe7d477037902edf0095fa3e0a84d1},
affiliation={Korea Hydro and Nuclear Power Company Ltd., Nuclear Power Environmental Technology Institute, 103-16 Munji, Taejon, South Korea},
abstract={Advanced Power Reactor 1400 (APR1400) Man-Machine Interface Systems (MMIS) designs, which encompass the design of the Main Control Room (MCR) and Instrumentation and Control (I&C) systems, make use of advanced digital technologies which have been proven through industrial operating experience and extensive testing and development program. I&C system has been designed with the network-based distributed control architecture. Diversity between safety I&C systems and non-safety I&C systems along with hardwired switches are provided for the defense in depth against common mode failure of software in the safety I&C systems. There are some design improvement such as adoption of multi-loop controllers, multiplexing devices for both safety and non safety control, universal soft controller in main control room for both safety and nonsafety I&C equipment controls. © 2003 IFAC.},
author_keywords={Decentralized control;  Design systems;  Digital control;  Distributed control;  Nuclear plants},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Givois2003,
author={Givois, E. and Guern, S.L.E. and Ancel, P.},
title={Dynamic simulation for determining gas quality in the context of new regulatory constraints},
journal={PSIG Annual Meeting 2003},
year={2003},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058789306&partnerID=40&md5=228129697047b12051af7ad7dc53237d},
affiliation={Gaz de France, France},
abstract={• Mastering the knowledge of gas characteristics is of increasing importance and complexity for the Transmission Division of Gaz de France: Increasing importance, in view of greater constraints brought in by the new legislation for market deregulation: more precise knowledge of energy balances required, and over shorter time steps, • Increasing complexity as opening the gas market involves a diversification of supply sources and greater variability of supply configurations. Improving energy metering at the delivery point, which includes allocating a more precise Calorific Value (CV) to quantities delivered, is therefore a key priority for work and improvement. Several methods, alone or in combination, make it possible to maintain and improve the accuracy of the energy delivered at each output point: increasing the total number of Chromatographs; operating the network under several restrictions to keep a constant CV; or, using a dynamic simulation of gas movements. The dynamic simulation method is currently undergoing testing at the Transmission Division of Gaz de France to determine gas quality at delivery points. This is done by an a posteriori reconstruction of the evolution of the various physical quantities over a period, at all points in the network. Used with network simulation software (SIMONE), this method has been in the testing phase for over eighteen months. We will be presenting results obtained on several parts of the network of Gaz de France. Relating to this testing, the Research Division of Gaz de France is also interested, in a more theoretical way, in the impact of the different parameters having an influence on the precise determination of gas quality. © PSIG 2003.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hagan2003447,
author={Hagan, P.C.},
title={The effect of resin annulus on anchorage performance of fully encapsulated rockbolts},
journal={10th ISRM Congress},
year={2003},
pages={447-450},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018945052&partnerID=40&md5=8f8d072ea21420fd0c6b0778d45a3539},
affiliation={University of New South Wales (UNSW), Sydney, Australia},
abstract={A diverse selection of rockbolt designs and resin anchors are available for use in underground mines. Research in recent years at the UNSW Mining Research Centre led to the construction of a rockbolt pull-testing facility. This facility has subsequently been upgraded, commissioned and initial test work has been completed to verify the pull-test process. A test program has been completed with the objective to understand the load transfer mechanism and improve the general performance of rockbolts. This paper describes the results of this research. © 2003 10th ISRM Congress. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Lyu2003119,
author={Lyu, M.R. and Huang, Z. and Sze, S.K.S. and Cai, X.},
title={An empirical study on testing and fault tolerance for software reliability engineering},
journal={Proceedings - International Symposium on Software Reliability Engineering, ISSRE},
year={2003},
volume={2003-January},
pages={119-130},
doi={10.1109/ISSRE.2003.1251036},
art_number={1251036},
note={cited By 42},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954452028&doi=10.1109%2fISSRE.2003.1251036&partnerID=40&md5=f6a9b574aba0378f5e212d4a4b147d0a},
affiliation={Computer Science and Engineering Department, Chinese University of Hong Kong, Hong Kong},
abstract={Software testing and software fault tolerance are two major techniques for developing reliable software systems, yet limited empirical data are available in the literature to evaluate their effectiveness. We conducted a major experiment to engage 34 programming teams to independently develop multiple software versions for an industry-scale critical flight application, and collected faults detected in these program versions. To evaluate the effectiveness of software testing and software fault tolerance, mutants were created by injecting real faults occurred in the development stage. The nature, manifestation, detection, and correlation of these faults were carefully investigated. The results show that coverage testing is generally an effective means to detecting software faults, but the effectiveness of testing coverage is not equivalent to that of mutation coverage, which is a more truthful indicator of testing quality. We also found that exact faults found among versions are very limited. This result supports software fault tolerance by design diversity as a creditable approach for software reliability engineering. Finally we conducted domain analysis approach for test case generation, and concluded that it is a promising technique for software testing purpose. © 2003 IEEE.},
author_keywords={data flow coverage testing;  empirical study;  mutation testing;  software fault tolerance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Maldonado-Naude2003101,
author={Maldonado-Naude, M.F. and Sánchez, J.A. and Baeza-Yates, R.},
title={Using Hermes-F: Experiences with a framework for developing information retrieval applications},
journal={Proceedings of the Mexican International Conference on Computer Science},
year={2003},
volume={2003-January},
pages={101-108},
doi={10.1109/ENC.2003.1232882},
art_number={1232882},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-14244256322&doi=10.1109%2fENC.2003.1232882&partnerID=40&md5=738534b9eb2e61d18c3808015538401c},
affiliation={Center for Research in Information and Automation Technologies (CENTIA), Universidad de las Américas, Puebla, Mexico; Libraries Division, Center for Research in Information and Automation Technologies, Universidad de las Américas, Puebla, Mexico},
abstract={Hermes-F is a framework that enables the development of applications that perform information retrieval (IR) tasks. Its central component is an extensible server that provides access to a variety of IR models, query transformations and digital collections. We have successfully instantiated our proposed framework by implementing a fully operational IR server that is accessed from various digital library applications. We describe the Hermes-F environment and focus on our results of using and extending the available IR models for diverse collections. © 2003 IEEE.},
author_keywords={Automation;  Heart;  Information filtering;  Information filters;  Information retrieval;  Proposals;  Software libraries;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{DeAvila2002,
author={De Avila, F.R.},
title={Integrated generic architecture object-oriented to hardware test},
journal={SAE Technical Papers},
year={2002},
doi={10.4271/2002-01-3411},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072468685&doi=10.4271%2f2002-01-3411&partnerID=40&md5=4b83b310d89d2b8b441e546cb63a6f6e},
affiliation={Actia Do Brasil Industria e Comércio Ltda, Brazil},
abstract={The marketing globalization, in all productive sectors, incites the competition between companies. The ones that develops the best processes obtains competitive products regarding technology, offering, at the same time, superior quality products. The electronic equipment manufacturers for the automotive industry and other segments needs equipment to automate its tests processes and to guarantee the quality of your products without losing competitiveness. The automatic tests equipment available in the market has a prohibitive cost for the majority companies, leading them to use manual equipment for these activities. The objective of this paper is to present the modeling and implementation of an integrated generic system of software and hardware object-oriented for testing of multiple electronic devices in an automatic way, with only one equipment in substitution to manual test. In this context, a new concept of hardware was developed, assigning object-oriented hardware, applying the same paradigm used in software. The creation of this concept come into the main objectives of this paper's conception: flexibility to support the diversity of tests and equipments, costs reduction, development time of new tests, the speed and the reliability of the tests, the automatic identification of the product in test, the generation of registers for the resource, as well as necessary statistics in the production and assembly lines. Copyright © 2002 Society of Automotive Engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{John2002,
author={John, R.C. and Young, A.L.},
title={New understanding on corrosion of alloys in high-temperature sulfidizing gases},
journal={NACE - International Corrosion Conference Series},
year={2002},
volume={2002-April},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046815289&partnerID=40&md5=ab2ec6c5260b5a55dd7820c73303c401},
affiliation={Shell Global Solutions (US), Houston, TX  77251-1380, United States; Humberside Solutions Ltd., Toronto, ON  M6N 4X7, Canada},
abstract={Many alloys exposed in high-temperature process equipment corrode by sulfidation corrosion in the presence of H2-H2S gases. This paper discusses the latest results of an extensive testing program for a diverse group of about 15 commercial alloys exposed to temperatures of 300-900°C with exposure times up to 6, 000 hours. The new data combined with prior data now allow engineering sulfidation corrosion assessments and predictions to be made for up to 26 alloys in wide ranges of conditions. The gaseous exposures included 0.001-0.2 atma H2S and 0.05-1.0 atma H2. The effects of H2S partial pressure, H2 partial pressure, temperature, exposure time and alloy type have all been analyzed and compiled to allow prediction of sulfidation corrosion for wide ranges of conditions to allow engineering predictions of corrosion-limited lifetimes. Applications for this technology can be found in oil refining, petrochemicals production, pulp/paper production, and power generation. © 2002 by NACE International.},
author_keywords={ASSET;  Corrosion data compilation;  Engineering lifetime predictions;  High-temperature corrosion predictions;  Parabolic rate constant;  Sulfidation;  Weight change},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Houssos2002458,
author={Houssos, N. and Pantazis, S. and Alonistioti, A.},
title={Generic adaptation mechanism for the support of context-aware service provision in 3G networks},
journal={2002 4th International Workshop on Mobile and Wireless Communications Network, MWCN 2002},
year={2002},
pages={458-462},
doi={10.1109/MWCN.2002.1045807},
art_number={1045807},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962867049&doi=10.1109%2fMWCN.2002.1045807&partnerID=40&md5=8daa6c86c36e122acf9cd8d0c2c66457},
affiliation={Communication Networks Laboratory, Department of Informatics and Telecommunications, University of Athens, Athens, 157 84, Greece},
abstract={The forthcoming era of 3rd generation (3G) mobile communications is heralded to change the wireless telecommunication experience dramatically. User expectations are raised to a significantly higher level, towards the demand for terminal-, network- and location-aware provision of ubiquitous, personalized multimedia services. Under this perspective, the need for adaptability of services and systems to largely diverse contexts is clearly identified. Generic, dynamically extensible adaptation mechanisms that are independent of the subject and criteria of adaptation can be a significant step in this direction. We introduce a mechanism aiming to fulfill these requirements. The proposed adaptation system has been designed, implemented and successfully integrated and tested in a distributed software platform for provision of value added services to 3G mobile users. © 2002 IEEE.},
author_keywords={3G mobile services;  adaptability;  context-awareness;  reconfigurability;  service management platforms;  ubiquitous computing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Berson2002446,
author={Berson, S. and Dawson, S. and Braden, R.},
title={Evolution of an active networks testbed},
journal={Proceedings - DARPA Active Networks Conference and Exposition, DANCE 2002},
year={2002},
pages={446-465},
doi={10.1109/DANCE.2002.1003513},
art_number={1003513},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949656781&doi=10.1109%2fDANCE.2002.1003513&partnerID=40&md5=f0cc8d534c2c15df607c671fb32e3240},
affiliation={USC, Information Sciences Institute, 4676 Admiralty Way, Marina del Rey, CA, United States; SRI International, 333 Ravenswood Avenue, Menlo Park, CA  94025, United States},
abstract={This paper explores the requirements for a network testbed designed specifically to support research in active networking. It also describes the design of the wide-area active networks testbed named the ABone. The ABone provides a virtual and real network infrastructure for active network experiments, using a diverse set of OS platforms. Its design embodies a tradeoff among the testbed goals of scalability, availability, security, heterogeneity, and modularity. © 2002 IEEE.},
author_keywords={Application software;  Computational modeling;  Computer architecture;  Computer networks;  Life testing;  Linear particle accelerator;  Protocols;  Scalability;  Software testing;  System testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Murray2002232,
author={Murray, L. and Griffiths, A. and Strooper, P.},
title={OptoNet - A case study in using rigorous analysis techniques to justify a revised product assurance strategy},
journal={Proceedings of the IEEE International Conference on Engineering of Complex Computer Systems, ICECCS},
year={2002},
volume={2002-January},
pages={232-237},
doi={10.1109/ICECCS.2002.1181516},
art_number={1181516},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948733418&doi=10.1109%2fICECCS.2002.1181516&partnerID=40&md5=67b9afcc3902aa49e8b8682eb1779c6a},
affiliation={School of Information Technology and Electrical Engineering, University of Queensland, Brisbane, QLD  4072, Australia; Foxboro Australia, PO Box 4009, Eight Mile Plains, QLD  4113, Australia},
abstract={When upgrading software in mission-critical or safety-related industrial control systems, it is imperative to ensure that system integrity properties are preserved. Comprehensive system testing is one way to gain this assurance. This has limitations, however, in that the hardware may be too expensive to assemble a large test rig, or where a product upgrade is to be deployed in diversely configured systems. This paper describes a method that uses rigorous system analysis to justify the replacement of system testing with both static analysis of the system configuration and dynamic testing of the upgraded system components. The paper reports on industrial experience in applying this method to the OptoNet product, which is an embedded software product used in industrial control systems. System analysis techniques are used to develop a detailed understanding of how OptoNet components (RTUs) interact to realise OptoNet system behaviour. Based on this detailed understanding, recommendations for a revised assurance strategy are made. The lessons learnt in the trial application of this method to the OptoNet product are discussed, and possible extensions to the method are proposed. © 2002 IEEE.},
author_keywords={Application software;  Assembly systems;  Computer industry;  Electrical equipment industry;  Embedded software;  Hardware;  Industrial control;  Mission critical systems;  Software safety;  System testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zong2002,
author={Zong, J.},
title={Multi-image tie-point detection applied to multi-angle imagery from MISR},
journal={International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives},
year={2002},
volume={34},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924425738&partnerID=40&md5=9cf9a9292384c29e85fcecb118cdb1ba},
affiliation={Jet Propulsion Laboratory, California Institute of Technology, 4800 Oak Grove Dr., Pasadena, CA  91109, United States},
abstract={An automatic tie-point (TP) detection algorithm for multi-image triangulation and registration is described. The algorithm uses a hierarchical approach that leads from the initial extraction of local image patches to the final TP detection on multiple imagery. The distribution of the TP detection is automatically adjusted to meet the needs of the triangulation or registration. Candidate point features are extracted based on the information of local image space only. A relational-based matching scheme using consistent labelling is designed for matching conjugate point features over multiple imagery. The final TPs are refined using the traditional area-based matching. The algorithm was applied successfully to the in-flight georectification of global imagery from the nine pushbroom cameras of the Multi-angle Imaging SpectroRadiometor (MISR) instrument. TP match is accurate at 0.2 pixels and is closely successful on all cameras regardless a diverse range of geometric and radiometric distortions. The software was initially tested with simulated MISR data resampled from LandSat imagery and later applied to the production operation of the MISR in-flight georectification. The software was also applied to air-born MISR imagery. The results indicated the system could be adaptive to and effective on TP matching of space-born as well small scale air-born imagery. © 2002 International Society for Photogrammetry and Remote Sensing. All rights reserved.},
author_keywords={Area-based matching;  Multi-image registration;  Relational-based feature matching;  Tie-point},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Toro2002143,
author={Toro, J. and Karppinen, J. and Hintikka, J. and Pohjanheimo, L.},
title={Automatic configuration and diagnostics for fieldbus based automation},
journal={IEEE International Workshop on Factory Communication Systems - Proceedings, WFCS},
year={2002},
volume={2002-January},
pages={143-148},
doi={10.1109/WFCS.2002.1159711},
art_number={1159711},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890793721&doi=10.1109%2fWFCS.2002.1159711&partnerID=40&md5=5eed0f8015ed237ea060294daff7176a},
affiliation={VTT Electronics Kaitovsyli 1, P.O.Box 1100, Oulu, FIN-90571, Finland},
abstract={The paper represents an automatic configuration model and a general purpose diagnostics tool for diverse industrial fieldbus systems. The ideas and concepts presented in the paper were born during the Kedi project, in which the purpose was to build an on-line diagnostics tool for fieldbus systems and make the first prototype of automatic configuration application. In the automatic configuration device parameters are downloaded into a fieldbus device from a planning system and useful device data is transferred back to the planning system. The online diagnostics tool includes both device and segment diagnostics that have been used in FF, Profibus and LON fieldbus systems. The tool shows traffic light representation of device and segment status, which can also be monitored via a Web browser in the factory network or directly as standalone software. All concepts discussed in the paper can be integrated into a single software module on the factory net. © 2002 IEEE.},
author_keywords={Automation;  Databases;  Distributed control;  Electronic equipment testing;  Field buses;  Monitoring;  Open systems;  Production facilities;  Prototypes;  Software tools},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{McClean2002207,
author={McClean, S. and Páircéir, R. and Scotney, B. and Greer, K.},
title={A negotiation agent for distributed heterogeneous statistical databases},
journal={Proceedings of the International Conference on Scientific and Statistical Database Management, SSDBM},
year={2002},
volume={2002-January},
pages={207-216},
doi={10.1109/SSDM.2002.1029722},
art_number={1029722},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-5444234592&doi=10.1109%2fSSDM.2002.1029722&partnerID=40&md5=3bb375cc28a7adcd7d1332296b31493f},
affiliation={School of Information and Software Engineering, University of Ulster, United Kingdom},
abstract={The World-Wide Web provides an ever-increasing source of diverse information. We focus on query agents, in particular the matching and negotiation agents that are responsible for pre-integration where the matching agent decomposes the query into sub-queries, and then searches metadata to find datasets that match the query fragments. In the case of heterogeneous data, the matching agent utilises a negotiation agent to find datasets that match the query fragments, provides mappings from the data to the query, and constructs the appropriate (sub-)query re-writing rules. Such matching is done by generalising the data and testing if the (sub) query is matchable to the generalised (meta) data: we call this g-matchable; if it is then we can construct an operator stack to transform the data to match the (sub) query. Such an approach provides a capability of automating the process of executing queries on heterogeneous statistical databases that are distributed over the Internet. The novelty lies in the provision of automated methods for statistical aggregates, where the heterogeneity essentially resides in the classification schemes of categorical data, including both heterogeneity of nomenclature and heterogeneity of granularity. In addition, our solution permits queries to be specified in a goal-driven query-by-example format. Rather than impose an a priori global standard, the user can query through a unified interface where integration is done at run-time. © 2002 IEEE.},
author_keywords={Aggregates;  Artificial intelligence;  Distributed computing;  Distributed databases;  Intelligent agent;  Internet;  Runtime;  Software agents;  Statistics;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Liu200274,
author={Liu, Y. and Gorton, I. and Liu, A. and Chen, S.},
title={Evaluating the scalability of Enterprise JavaBeans technology},
journal={Proceedings - Asia-Pacific Software Engineering Conference, APSEC},
year={2002},
volume={2002-January},
pages={74-83},
doi={10.1109/APSEC.2002.1182977},
art_number={1182977},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-4944251976&doi=10.1109%2fAPSEC.2002.1182977&partnerID=40&md5=11bb7f645c8736640c12d16e74c9a256},
affiliation={School of Information Technologies, University of Sydney, Sydney, NSW, Australia; Pacific Northwest National Laboratory, Richland, WA  99352, United States; CSIRO Mathematical and Information Sciences, Sydney, NSW, Australia},
abstract={One of the major problems in building large-scale distributed systems is to anticipate the performance of the eventual solution before it has been built. This problem is especially germane to Internet-based e-business applications, where failure to provide high performance and scalability can lead to application and business failure. The fundamental software engineering problem is compounded by many factors, including individual application diversity, software architecture trade-offs, COTS component integration requirements, and differences in performance of various software and hardware infrastructures. We describe the results of an empirical investigation into the scalability of a widely used distributed component technology, Enterprise JavaBeans (EJB). A benchmark application is developed and tested to measure the performance of a system as both the client load and component infrastructure are scaled up. A scalability metric from the literature is then applied to analyze the scalability of the EJB component infrastructure under two different architectural solutions. © 2002 IEEE.},
author_keywords={Application software;  Benchmark testing;  Hardware;  Internet;  Java;  Large-scale systems;  Scalability;  Software architecture;  Software engineering;  Software performance},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mocio2001,
author={Mocio, C.M.},
title={Demonstrating low cost access to space for small satellites: The DOD space test program medium launch vehicle 2005 mission},
journal={AIAA Space 2001 Conference and Exposition},
year={2001},
doi={10.2514/6.2001-4582},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087194063&doi=10.2514%2f6.2001-4582&partnerID=40&md5=5ad6d8efa9c51fe0ff128e68ce58b2a2},
affiliation={Space and Missile Systems Center, Detachment 12 SMC/Det 12,Bldg413, Kirtland AFB, 3548 Aberdeen Ave SE, NM 87117-5778, United States},
abstract={This paper will outline the genesis and evolution of the most aggressive United States Department of Defense (DoD) Research and Development (R&D) mission ever attempted. The DoD Space Test Program (STP) is charged with providing spaceflight to R&D payloads on the Space Experiments Review Board (SERB) priority list. STP is dedicated to timely, cost-effective spaceflight opportunities. Often these opportunities result in innovative missions that maximize the amount of SERB payloads manifested per launch vehicle. In this spirit, STP has designed a multi-manifest mission that will deliver up to 7 separate spacecraft including 10 separate payloads to different earth orbits. This mission is called Medium Launch Vehicle - 2005 (MLV-05) Mission. Actually, there may be room for more spacecraft! This complex mission is a collaboration between NASA, the Air Force Research Laboratory (AFRL), and the Office of Naval Research (ONR). Other participants include the Naval Research Laboratory (NRL), the Naval Postgraduate School (NFS), and the Air Force Academy (AFA). STP is managing this mission utilizing a diverse Integrated Product Team (IPT). This IPT development overcame several unique challenges in balancing limited manpower resources with the requirement to manage all the disparate mission components. This paper will also review the decisions leading to the initiation of this mission, the payloads being considered for manifest, and the significant management and technical challenges facing the IPT.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Peres2001240,
author={Peres, L.M. and Vergilio, S.R. and Jino, M. and Maldonado, J.C.},
title={Path selection in the structural testing: Proposition, implementation and application of strategies},
journal={Proceedings - International Conference of the Chilean Computer Science Society, SCCC},
year={2001},
volume={2001-January},
pages={240-246},
doi={10.1109/SCCC.2001.972653},
art_number={972653},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054970970&doi=10.1109%2fSCCC.2001.972653&partnerID=40&md5=0c955f47e3568368ba86d57738bb05c8},
affiliation={DInf, UFPR, Brazil; DCA, FEEC, UNICAMP, Brazil; ICMC-USP, Brazil},
abstract={Structural testing criteria help the tester in the generation and evaluation of a test case set T. They are predicates to be satisfied to consider the testing activity ended and generally require the execution of a set P of paths, capable of exercising certain elements in the program under testing. Determining P is an important and hard task, and its automation is strongly desirable for easing the criteria application. This task can influence on the efficacy and on the testing effort and costs. This work explores the use of diverse programs characteristics to propose strategies for selection of testing paths. The work also describes a module that implements a framework for representation and automation of those strategies. Using this module, a testing procedure is presented and a strategy, that uses the number of predicates to select paths, is evaluated. The obtained results give some information about the main advantage of this strategy: to easy the automatic test data generation by reducing the number of selected infeasible paths. © 2001 IEEE.},
author_keywords={Selection of Test Paths;  Software Metrics;  Software Testing;  Structural Criteria},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ranaudo2000,
author={Ranaudo, R.J. and Reynolds, P.T.},
title={The bombardier flight test center - Meeting the challenge},
journal={SAE Technical Papers},
year={2000},
doi={10.4271/2000-01-5502},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072453351&doi=10.4271%2f2000-01-5502&partnerID=40&md5=c2b27f340dcff302dd34b634a607a9d1},
affiliation={Bombardier Aerospace, Canada},
abstract={In 1991, shortly after acquiring Learjet, Bombardier consolidated all flight testing of new aircraft at the Wichita, Kansas facility. Since then, nine new aircraft were certified, and the Flight Test Center grew from 20 dedicated flight test personnel, to nearly 500 dedicated flight test personnel. The Canadian based company in conjunction with several international risk sharing partners, has created a highly dynamic flight test environment, tasking the Flight Test Center with the challenge of bringing a new product to market each year. This rapid growth was centered on supporting three aircraft product lines; Learjet, Canadair, and DeHavilland. New hangars, telemetry, and ground support facilities were built to accommodate the increased flight test demands. The Bombardier Flight Test Center, otherwise known as BFTC, conducts flight test operations on a seven day per week schedule, and in 1999, flew over 5000 flight test hours in development and certification testing. In nine years, BFTC has become one of the world's largest and most active flight test centers. Harmonizing the highly diverse company cultures, and structuring the flight test organization to safely meet the aggressive development schedule driven by market demands, has presented unique challenges at all technical and managerial levels. Process improvement is an ongoing effort, whose goal is to achieve the completion of a certification flight test program within a twelve-month period of time. Data base systems to standardize engineering flight test requirements, and maintain configuration control have been implemented, and are being employed to improve efficiency, safety, and schedule completion milestones. Bombardier Aerospace is a world leader in the business and regional jet market, and the Flight Test Center continues to meet the challenges of testing aircraft safely and efficiently in a highly dynamic, schedule driven environment. © Copyright 2000 by Learjet, Inc. Published by SAE International, and the American Institute of Aeronautics and Astronautics, Inc. with permission.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Morse2000104,
author={Morse, E. and Potts Steves, M.},
title={CollabLogger: A tool for visualizing groups at work},
journal={Proceedings of the Workshop on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE},
year={2000},
volume={2000-January},
pages={104-109},
doi={10.1109/ENABL.2000.883712},
art_number={883712},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949634113&doi=10.1109%2fENABL.2000.883712&partnerID=40&md5=c05f1f1708ac879371118c86a0527000},
affiliation={National Institute of Standards and Technology, 100 Bureau Drive, Gaithersburg, MD  20899, United States},
abstract={The CollabLogger is a visual tool that has been developed to support usability analyses of human-computer interaction in a team environment. The participants in the computer-mediated activity were engaged in a small-scale manufacturing testbed project. Interactions of the group were mediated by Teamwave Workplace and the members performed both synchronous and asynchronous activities depending on their availability, project requirements, and due to chance meetings in the collaborative space. The software was instrumented to log users' interactions with the system and each other. The CollabLogger addresses the problem of helping investigators analyze the volumes of log data that groupware tools can generate. Visual tools are powerful when large amounts of diverse data present themselves. The place-based collaboration environment offered by Teamwave Workplace provided a level of organization that allowed us to create a visual interface with which to perform exploratory sequential data analysis. © 2000 IEEE.},
author_keywords={Availability;  Collaborative software;  Collaborative work;  Computer aided manufacturing;  Data analysis;  Employment;  Instruments;  Testing;  Usability;  Visualization},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hillson200013,
author={Hillson, R. and Iglewski, M.},
title={C++2MPI: A software tool for automatically generating MPI datatypes from C++ classes},
journal={Proceedings - International Conference on Parallel Computing in Electrical Engineering, PARELEC 2000},
year={2000},
pages={13-17},
doi={10.1109/PCEE.2000.873593},
art_number={873593},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-24644462486&doi=10.1109%2fPCEE.2000.873593&partnerID=40&md5=666082bcb341525659c0f17bdfd22c25},
affiliation={Naval Research Laboratory, Canada; University of Quebec, Hull, QC, Canada},
abstract={The Message Passing Interface I.I (MPI I.I) standard defines a library of message-passing functions for parallel and distributed computing. We have developed a new software tool called C++2MPI which can automatically generate MPI derived datatypes for a specified C++ class. C++2MPI can generate data types for derived classes, for partially and fully-specialized templated classes, and for classes with private data members. Given one or more user-provided classes as input, C++2MPI generates, compiles and archives a function for creating the MPI derived datatype. When the generated function is executed, it builds the derived MPI datatype if the datatype does not already exist, and returns the value of an MPI handle for referencing the datatype. PGMT (Processing Graph Method Tool) is a set of application program interfaces for porting the Processing Graph Method (PGM), a parallel programming method, to diverse networks of processors. C++2MPI was developed as a component of PGMT, but can be used as a stand-alone tool. © 2000 IEEE.},
author_keywords={Computer architecture;  Costs;  Data structures;  Distributed computing;  Laboratories;  Message passing;  Parallel programming;  Software libraries;  Software tools;  Testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen200063,
author={Chen, L. and May, J. and Hughes, G.},
title={A constant perturbation method for evaluation of structural diversity in multiversion software},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2000},
volume={1943},
pages={63-73},
doi={10.1007/3-540-40891-6_6},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542489729&doi=10.1007%2f3-540-40891-6_6&partnerID=40&md5=c0915559f34896a5cd8e6f89f2e2c3bc},
affiliation={Safety Systems Research Centre, Department of Computer Science, University of Bristol, Bristol, BS8 1UB, United Kingdom},
abstract={In this paper, fault simulation is discussed as a test method for diversity assessment of multiversion software and data flow perturbation is used as a main technique for implementation. More specifically, constant perturbation is introduced as a specific example of data-flow perturbation. Some quantitative metrics are proposed for the description of software diversity, and the parameters needed to calculate the metrics estimated by fault injection experiments. A case study is presented to illustrate that the diversity metrics are appropriate, and that constant perturbation is a practical fault injecting technique to estimate parameters necessary for assessing diversity. © Springer-Verlag Berlin Heidelberg 2000.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Marcolin200037,
author={Marcolin, B.L. and Compeau, D.R. and Munro, M.C. and Huff, S.L.},
title={Assessing User Competence: Conceptualization and Measurement},
journal={Information Systems Research},
year={2000},
volume={11},
number={1},
pages={37-60},
doi={10.1287/isre.11.1.37.11782},
note={cited By 153},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034381936&doi=10.1287%2fisre.11.1.37.11782&partnerID=40&md5=194f64826a5ac2442449e6d76cfe8065},
affiliation={Faculty of Management, University of Calgary, 2500 University Drive NW, Calgary, Alta. T2N 1N4, Canada; Fac. of Commerce and Administration, Victoria University of Wellington, Wellington, New Zealand},
abstract={Organizations today face great pressure to maximize the benefits from their investments in information technology (IT). They are challenged not just to use IT, but to use it as effectively as possible. Understanding how to assess the competence of users is critical in maximizing the effectiveness of IT use. Yet the user competence construct is largely absent from prominent technology acceptance and fit models, poorly conceptualized, and inconsistently measured. We begin by presenting a conceptual model of the assessment of user competence to organize and clarify the diverse literature regarding what user competence means and the problems of assessment. As an illustrative study, we then report the findings from an experiment involving 66 participants. The experiment was conducted to compare empirically two methods (paper and pencil tests versus self-report questionnaire), across two different types of software, or domains of knowledge (word processing versus spreadsheet packages), and two different conceptualizations of competence (software knowledge versus self-efficacy). The analysis shows statistical significance in all three main effects. How user competence is measured, what is measured, what measurement context is employed: all influence the measurement outcome. Furthermore, significant interaction effects indicate that different combinations of measurement methods, conceptualization, and knowledge domains produce different results. The concept of frame of reference, and its anchoring effect on subjects' responses, explains a number of these findings. The study demonstrates the need for clarity in both defining what type of competence is being assessed and in drawing conclusions regarding competence, based upon the types of measures used. Since the results suggest that definition and measurement of the user competence construct can change the ability score being captured, the existing information system (IS) models of usage must contain the concept of an ability rating. We conclude by discussing how user competence can be incorporated into the Task-Technology Fit model, as well as additional theoretical and practical implications of our research.},
author_keywords={Competence;  Empirical;  End-user computing;  Self-efficacy;  Software skills;  Theoretical framework},
document_type={Article},
source={Scopus},
}

@ARTICLE{Antoy200055,
author={Antoy, S. and Hamlet, D.},
title={Automatically checking an implementation against its formal specification},
journal={IEEE Transactions on Software Engineering},
year={2000},
volume={26},
number={1},
pages={55-69},
doi={10.1109/32.825766},
note={cited By 66},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033907702&doi=10.1109%2f32.825766&partnerID=40&md5=d88059153604b5b8fae50431343ce265},
affiliation={Department of Computer Science, Center for Software Quality Research, Portland State University, Portland, OR 97207, United States},
abstract={We propose checking the execution of an abstract data type's imperative implementation against its algebraic specification. An explicit mapping from implementation states to abstract values is added to the imperative code. The form of specification allows mechanical checking of desirable properties such as consistency and completeness, particularly when operations are added incrementally to the data type. During unit testing, the specification serves as a test oracle. Any variance between computed and specified values is automatically detected. When the module is made part of some application, the checking can be removed, or may remain in place for further validating the implementation. The specification, executed by rewriting, can be thought of as itself an implementation with maximum design diversity, and the validation as a form of multiversion-programming comparison.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pratt1999,
author={Pratt, T.G.},
title={Georgia Tech Software Radio Laboratory},
journal={54th ARFTG Conference Digest Fall 1999: Automatic RF Techniques Group: Characterization of Broadband Access Technologies, ARFTG Fall 1999},
year={1999},
doi={10.1109/ARFTG.1999.327381},
art_number={4120060},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035129481&doi=10.1109%2fARFTG.1999.327381&partnerID=40&md5=74c1cd8f00f6aa5a57f7dda09416cf68},
affiliation={Georgia Institute of Technology, Atlanta, GA  30332-0821, United States},
abstract={The Georgia Tech Broadband Institute of the Georgia Institute of Technology is establishing a Software Radio Laboratory in the Georgia Center for Advanced Telecommunications Technology (GCATT). The laboratory, which is planned to be operational in early 2000, has been designed as a testbed for research, development, test, and evaluation of software radio concepts. The laboratory consists of waveform generators and communications signal sources, local area networking transceivers, radio frequency (RF) channel emulators with branch diversity and smart antenna emulation capabilities, and a multichannel programmable VME-based software radio platform. The software radio platform incorporates a programmable RF front-end, digital down-converters, and multiple Quad TI-C6x DSP boards to facilitate algorithm development for intermediate frequency (IF), baseband, and bitstream processing. The laboratory promises to have capability for addressing a broad spectrum of problems including the development and test of communications modulation techniques, access methods, traffic types, channel distortion effects, transmitter diversity, receiver signal processing algorithms, coding, power control, and many other diverse topics of research. © 1999 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mahmoudi1999,
author={Mahmoudi, R. and Spirito, M. and Valk, P. and Tauritz, J.L.},
title={A novel load and source tuning system for balanced and unbalanced WCDMA power amplifiers},
journal={54th ARFTG Conference Digest Fall 1999: Automatic RF Techniques Group: Characterization of Broadband Access Technologies, ARFTG Fall 1999},
year={1999},
doi={10.1109/ARFTG.1999.327377},
art_number={4120056},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954261248&doi=10.1109%2fARFTG.1999.327377&partnerID=40&md5=efaca476dd2bddb2e35f823cea71d5a5},
affiliation={Microwave Components Group, Delft University of Technology, Netherlands; Electronic Engineering Department, Universita' di Napoli Federico II, Italy},
abstract={The recent large-scale introduction of spread spectrum techniques for personal communications (PCS) in the USA, Japan and Europe is an enormous stimulus for innovation [1]. The varieties and dynamics of spread spectrum based systems such as: frequency hopping, time domain hopping, multi carrier CDMA and the direct sequence spread spectrum technique have complicated and altered the design paradigm [2]. This diversity has confronted designers with the non-trivial task of rapidly responding to the vagaries of the mobile communication business. Complex digital communications signals with broad bandwidth, varying rectified envelope power and a relatively large crest factor have rendered inadequate traditional verification methods based on the use of CW signals [3]. The attendant demands on cost, efficiency and system constraints and the growing use of balanced circuits in broadband wireless applications has increased the need for accurate modeling and characterization of the nonlinear behavior of active devices. Since the input and output terminations of these devices dictate their large signal behavior, complete device characterization including the determination of constant output power contours requires the use of source and load tuning. Furthermore, proper verification entails simulation and test at both software and hardware levels using the same stimuli [2]. This paper offers a comprehensive, automatic tuner based solution suitable for balanced and unbalanced devices driven by WCDMA based signals. © 1999 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dalal1999285,
author={Dalal, S.R. and Jain, A. and Karunanithi, N. and Leaton, J.M. and Lott, C.M. and Patton, G.C. and Horowitz, B.M.},
title={Model-based testing in practice},
journal={Proceedings - International Conference on Software Engineering},
year={1999},
pages={285-294},
doi={10.1145/302405.302640},
note={cited By 378},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032660359&doi=10.1145%2f302405.302640&partnerID=40&md5=42ae803b12a2bbf458e07c6429719240},
affiliation={Bellcore, Morristown, NJ, United States},
abstract={Model-based testing is a new and evolving technique for generating a suite of test cases from requirements. Testers using this approach concentrate on a data model and generation infrastructure instead of hand-crafting individual tests. Several relatively small studies have demonstrated how combinatorial test generation techniques allow testers to achieve broad coverage of the input domain with a small number of tests. We have conducted several relatively large projects in which we applied these techniques to systems with millions of lines of code. Given the complexity of testing, the model-based testing approach was used in conjunction with test automation harnesses. Since no large empirical study has been conducted to measure efficacy of this new approach, we report on our experience with developing tools and methods in support of model-based testing. The four case studies presented here offer details and results of applying combinatorial test-generation techniques on a large scale to diverse applications. Based on the four projects, we offer our insights into what works in practice and our thoughts about obstacles to transferring this technology into testing organizations.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Weyuker199854,
author={Weyuker, E.J.},
title={Testing component-based software: A cautionary tale},
journal={IEEE Software},
year={1998},
volume={15},
number={5},
pages={54-59},
doi={10.1109/52.714817},
note={cited By 188},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032165751&doi=10.1109%2f52.714817&partnerID=40&md5=4e608176aab7287504aa72abe282c503},
affiliation={AT and T Labs., United States; Research Division, AT and T Labs., Florham Park, NJ, United States; Department of Computer Science, New York University, Courant Inst. of Math. Sciences, United States; Moore Sch. of Electrical Engineering, University of Pennsylvania, United States; Rutgers University, United States; ACM, United States; IEEE, United States; ACM SIGSOFT, IEEE Comp. Soc. Tech. Comm. S., United States; AT and T Labs.-Research, 180 Park Ave., Florham Park, NJ 07932, United States},
abstract={We need new ways to validate software components, specifically those deployed in diverse software environments. We must also consider the likelihood of realizing anticipated savings and whether component-based systems can meet reliability and availability requirements.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Karoly1998,
author={Karoly, D. and Copeland, T. and Gardner, D.},
title={WinACIF: A telecom IC support tool using Tcl/Tk},
journal={Proceedings of the 6th Annual Tcl/Tk Conference, TCL/TK 1998},
year={1998},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094647583&partnerID=40&md5=9bb35ce0444c7f5ae78fdf20ad90ad21},
affiliation={Advanced Micro Devices, Austin, TX, United States},
abstract={We discuss our use of Tcl/Tk to provide software support for telecommunications Integrated Circuits (ICs). Our Windows -based Advanced Computer Interface (WinACIF) program works in concert with reconfigurable hardware based on Field Programmable Gate Arrays (FPGAs) to provide essential coordination in laboratory data collection and analysis of a device under test. WinACIF replaces several MS-DOS based applications. Whereas the previous implementations suffered from the classic limitations of MS-DOS, WinACIF provides the flexibility and functionality of windowing applications by virtue of its Tcl/Tk roots. Tcl/Tk not only supplies more than ample power to create WinACIF, but also adds the benefit of saving valuable time otherwise spent learning a complex API. Run-time loaded Tcl extensions provide the flexibility to support various devices having diverse interfaces. A single Tcl/Tk script dynamically builds a Graphical User Interface (GUI) based on product configuration data retrieved from a data store. Additionally, we used canvas widgets to provide an intuitive interface. For the engineer who requires control beyond that afforded by our GUI, Tcl serves as WinACIF's command language. © Tcl/Tk Conference, TCL/TK 1998, TCL/TK 1996.All right reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Malin1998,
author={Malin, J.T.},
title={Some roles of models for monitoring and control in bio-plex},
journal={SAE Technical Papers},
year={1998},
doi={10.4271/981727},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072464617&doi=10.4271%2f981727&partnerID=40&md5=bda3b9ad42b97be21b32893514631f27},
affiliation={NASA Johnson Space Center, United States},
abstract={Diverse modeling information is needed for system modeling and simulation for the purposes of developing, testing and supporting intelligent layered software for planning/scheduling, monitoring, control and fault management for test articles in BIO-Plex. A framework of types of system management goals is used to provide perspective for integrated discussion of the diverse modeling and control approaches. This paper discusses model representations in the CONFIG discrete-event modeling and simulation tool, which are used to link diverse modeling styles, to support integrated use and reuse of diverse modeling information. © 1998 Society of Automotive Engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Malin1998,
author={Malin, J.T. and Fleming, L. and Hatfield, T.R.},
title={Interactive simulation-based testing of product gas transfer integrated monitoring and control software for the lunar mars life support phase III test},
journal={SAE Technical Papers},
year={1998},
doi={10.4271/981769},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072457884&doi=10.4271%2f981769&partnerID=40&md5=d614f86812b5c49755a7eaeeb5e8c384},
affiliation={NASA Johnson Space Center, United States; Hernandez Engineering, Inc., United States; International Business Machines, United States},
abstract={Gas transfer systems in a closed life support test were controlled by intelligent layered monitoring and control software. Interactive simulation-based testing was used for system-level validation of the discrete sequencer layer of the software. An advanced discrete event simulation tool was used to model diverse components and systems for processing gases in a plant growth chamber, crew chamber and incinerator, and transferring gases between chambers. Models included physico-chemical and biological gas processors, pumps, concentrators, chambers and tanks, and devices for configuring and controlling gas transfer. Several types of control were modeled. This paper describes the models, the testing approach, and some results of the testing. © 1998 Society of Automotive Engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wood1998336,
author={Wood, C.},
title={Meeting customer needs using participatory techniques},
journal={Proceedings - 1998 Australasian Computer Human Interaction Conference, OzCHI 1998},
year={1998},
volume={1998-November},
pages={336},
doi={10.1109/OZCHI.1998.732236},
art_number={732236},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051940251&doi=10.1109%2fOZCHI.1998.732236&partnerID=40&md5=8b353f032ded14d982fbdb68a69a95a5},
affiliation={DSTO, Department of Defence, Canberra, ACT  2600, Australia},
abstract={I have used participatory design techniques on a number of projects, including the design of a GUI for a group meeting support system and the design of a multimodal information management tool. This technique has been used to scope functionality, produce a project plan, design the system, create and usability-test the user documentation, and facilitate best-practice principles in code review and software engineering. Using participatory methods has also facilitated team development and cohesion in a team that is geographically dispersed and consists of members from diverse backgrounds. © 1998 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{NoAuthor1997,
title={ACM International Conference Proceeding Series},
journal={ACM International Conference Proceeding Series},
year={1997},
volume={Part F129322},
page_count={231},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029638797&partnerID=40&md5=8b3a2e53a9cc6e87a42d99f6e78d907b},
abstract={The proceedings contain 31 papers. The topics discussed include: the web with relevance; teaching Internet literacy to a large and diverse audience; teaching internet literacy to a large and diverse audience; social and ethical education in computing using virtual environments; teaching programming paradigms and languages for qualitative learning; a smorgasbord of pedagogical dishes; paradigms for educational research in computer science; cross-sectional case studies: integrating case studies and projects in I.S. management education; teaching software testing; and in search of the inverse curriculum.},
document_type={Conference Review},
source={Scopus},
}

@CONFERENCE{Neville1997172,
author={Neville, K.W.},
title={Industry initiative for revised training simulator validation process},
journal={1997 Modeling and Simulation Technologies Conference},
year={1997},
pages={172-180},
art_number={AIAA-97-3665},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978229872&partnerID=40&md5=1f1803bf1b4f36fb0032bb2d6e654703},
affiliation={The Boeing Company, Seattle, WA, United States},
abstract={The increasing cost of producing data and modeling information which meet today's standards for qualification of high-level civil transport training simulators is a major concern for airlines and training centers. The cost of high-fidelity data packages is a direct result of the need to acquire and analyze large quantities of flight-test data and to ensure a close quantitative match of simulation results to airplane data for a wide variety of maneuvers and flight conditions. A proposal to revise regulatory policy for simulator qualification would allow the controlled use of the aircraft manufacturer's engineering simulation data to partially validate a training simulator which differs in an incremental, well-defined manner from a fully flight-validated simulator. Such differences include modeling of simple geometric changes to the airplane configuration or revisions to software in onboard computers. The Boeing Next Generation 737-600/-700/-800 program was identified as a test case to verify the proposed process and its application to simple body length derivatives. This paper discusses the successful cooperative effort by airplane manufacturers, airlines, simulator manufacturers, and regulatory authorities to develop a proposed alternative qualification procedure which satisfies the sometimes diverse objectives of the training simulator industry. © 1997, American Institute of Aeronautics and Astronautics, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Paradkar1997133,
author={Paradkar, A. and Tai, K.C. and Vouk, M.A.},
title={Specification-based testing using cause-effect graphs},
journal={Annals of Software Engineering},
year={1997},
volume={4},
pages={133-157},
doi={10.1023/a:1018979130614},
note={cited By 16},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031498898&doi=10.1023%2fa%3a1018979130614&partnerID=40&md5=d088b037a4642642a82685436384dc1f},
affiliation={Department of Computer Science, North Carolina State University, Raleigh, NC 27695-8206, United States; T.J. Watson Research Center, IBM Corp., 30 Saw Mill River Road, Hawthorne, NY 10532, United States},
abstract={In this paper we discuss the advantages and limitations of a specification-based software testing technique we call CEG-BOR. There are two phases in this approach. First, informal software specifications are converted into cause-effect graphs (CEG). Then, the Boolean OperatoR (BOR) strategy is applied to design and select test cases. The conversion of an informal specification into a CEG helps detect ambiguities and inconsistencies in the specification and sets the stage for design of test cases. The number of test cases needed to satisfy the BOR strategy grows linearly with the number of Boolean operators in CEG, and BOR testing guarantees detection of certain classes of Boolean operator faults. But, what makes the approach especially attractive is that the BOR based test suites appear to be very effective in detecting other fault types. We have empirically evaluated this broader aspect of the CEG-BOR strategy on a simplified safety-related real-time control system, a set of N-version programs, and on elements of a commercial data-base system. In all cases, CEG-BOR testing required fewer test cases than those generated for the applications without the use of CEG-BOR. Furthermore, in all cases CEG-BOR testing detected all faults that the original, and independently generated, application test-suites did. In two instances CEG-BOR testing uncovered additional faults. Our results indicate that the CEG-BOR strategy is practical, scalable, and effective across diverse applications. We believe that it is a cost-effective methodology for the development of systematic specification-based software test-suites.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Bhagat1996259,
author={Bhagat, P.K. and Bessette, L. and Leonelli, F.},
title={Current and future inspection and maintenance challenges},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1996},
volume={2948},
pages={259-268},
doi={10.1117/12.259208},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58149252100&doi=10.1117%2f12.259208&partnerID=40&md5=ec657c81bb4faeb09c244640fef7a926},
affiliation={Aircraft Maintenance Division, Federal Aviation Administration, 800 Independence Ave, SW, Washington, DC 20591-0002, United States},
abstract={Ever-increasing diversity of civilian aircraft inventory and continued technological advancements in aircraft materials, aircraft flight control equipment, testing equipment, and software methodologies are impacting aircraft inspection and maintenance practices. Current procedures deal mainly with issues related to structural and electrical or electronic integrity to assure continued airworthiness of operational aircraft. Techniques and methodologies for these are widely available, and training needs are well defined. Advances in technology, however, are yielding new and different aircraft, which require more sophisticated electronic instruments for navigation and control.A major issue is the continued reliability and airworthiness of avionics and development of adequate safeguards for these aircraft. Built-in test equipment, maintenance across terminals, and data bases defining inspection needs that are based on operational data, and software integrity, are also rapidly becoming important considerations in aircraft maintenance. In this era of declining funds and personnel resources, a cost-effective approach requires a fresh look at all phases of the current inspection and maintenance practices, including oversight and management. This paper provides a perspective on issues and challenges facing a civilian regulatory agency, specifically, the aircraft maintenance division in the FAA. ©2005 Copyright SPIE - The International Society for Optical Engineering.},
author_keywords={Aviation regulations;  Avionics;  Civilian aircraft;  Composite materials;  Continued airworthiness;  Electronic integrity;  Inspection and maintenance;  Portable electronic devices;  Smart test equipment;  Structural integrity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Schwartz19961027,
author={Schwartz, M. and Schwartz, J. and Bergquist, J. and Keller, C. and Kelsey, P. and Stringer, T.},
title={Lunar neighborhoods: Architecture for extreme environments},
journal={Engineering, Construction, and Operations in Space V},
year={1996},
pages={1027-1031},
doi={10.1061/40177(207)137},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904795703&doi=10.1061%2f40177%28207%29137&partnerID=40&md5=18f000d84a1e6a317fec2a89c415d99b},
affiliation={School of Architecture and Planning, University of New Mexico, Albuquerque NM, United States; Schwartz Communications, Lafayette, CO, United States},
abstract={Students in a unique, graduate seminar program at the University of New Mexico that explored the role of architects in understanding and planning for the needs of humans in extreme environments present initial results of an engineering test case that pursed development of an international lunar space station housing 30 persons over a two-year period. Seminar participants, challenged with balancing both people and engineering factors, propose establishing a science station featuring modular neighborhoods of three 10-person groups sited under a radiation shielding dome in a near-side crater with Earth views. Central to the crater-based neighborhood design is an architectural and engineering philiosophy that incorporates creating familiar sights, sounds, and a shirt-sleeve lifestyle to enrich the crews' lunar experience as missions in astronomy, extravehicular research, geology, and pre-Mars base habitation are conducted. The neighborhood concept is essential in achieving the multiple goals of safe, cost effective, and rewarding exploration, while maintaining healthy and creative social and cultural interactions among multi-national and skill-diverse crews. Spaces for bathing, recreation, crew-arrival celebrations, and problem solving are desired while the introduction of a new culture to the Moon occurs concurrently with human adaptation to a foreign environment. "Neighborhooding" features regular events - such as meals, seminars, and a gathering place for descision-making - as required rituals. To address population spread, architectural eco-care considerations were viewed as critical. Engineering and construction objectives comprise eco-considerations to protect the lunar environment through techniques that maximize in-situ vitrification and dust mitigation building technologies, as well as air-, heat-, water-, plant-, and waste-matter recycling. Power needs are met by a combination of solar arrays and a stand-alone, inherently safe nuclear unit. Additional out-of-crater electrical needs, primarily for science experiments, are met via radio isotopic thermoelectric generator technology. Station access for crew rotations and resupply is available via a nearby landing site, and such events are celebrated. Emerging areas of concern remain, however, as this academic program, now in its second year, applies terrestrial, architectural and engineering principles and experience as an analog to the lunar environment. This one-of-its-kind seminar program is performed under the auspices of the University's School of Architecture and Planning. Active involvement of astronauts, architects, engineers, scientists, and space pioneers from U.S. industry, the national laboratory system, NASA, and academia has proven invaluable to this applied academic program. Students from ten countries attended, with significant Pacific Rim participation. Academic disciplines ranging from engineering to philosophy were represented, as were multi-national and multi-gender perspectives, and that of the physically challenged. The program's test-case application could serve as a template for future courses, encouraging a multi-disciplinary approach to science projects. Program results could have applications to architecture in extreme space and Earth environments, in multi-national and multi-cultural urban settings, and to sub-surface defense, intelligence, and science posts. © 1996 American Society of Civil Engineers.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhu1996467,
author={Zhu, D. and Hill, A.D.},
title={Field results demonstrate enhanced matrix acidizing through real-time monitoring},
journal={Society of Petroleum Engineers - Permian Basin Oil and Gas Recovery Conference, OGR 1996},
year={1996},
pages={467-476},
doi={10.2523/35197-ms},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-1242318705&doi=10.2523%2f35197-ms&partnerID=40&md5=0bfd38a357f917732ff48d53b55ba89a},
affiliation={U. of Texas at Austin, United States},
abstract={The use of an inverse injectivity versus superposition time plot to diagnose the changing skin factor in a matrix acidizing treatment has been presented previously by Hill and Zhu1. The model has been extended to calculate skin factor as a function of injection time or injected volume directly to help the operator monitor and optimize the treatment. A Windows program based on the theory has been developed to provide a pretreatment test to evaluate the permeability and the initial skin factor of the formation when they are not available before the acid treatment, to calculate and plot the evolving skin during the treatment in real-time, and to evaluate treatments afterwards. It converts surface pressure, when measured, to the bottomhole pressure for the calculation, and handles fluid density and viscosity changes in real time. Several field examples showed that the technique can be used conveniently to monitor skin changes and diversion effects during matrix acidizing treatments. The program is reliable and flexible in acquiring and processing data, calculating skin, and diagnosing matrix acidizing treatments. © 1996, Society of Petroleum Engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gan1996531,
author={Gan, K.-W. and Lua, K.-T. and Palmer, M.},
title={A Statistically Emergent Approach for Language Processing: Application to Modeling Context Effects in Ambiguous Chinese Word Boundary Perception},
journal={Computational Linguistics},
year={1996},
volume={22},
number={4},
pages={531-553},
note={cited By 14},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038216910&partnerID=40&md5=1429a4db78cfe3c92bbf0865b340de82},
affiliation={Hong Kong Univ. of Sci. and Technol.; University of Pennsylvania; National University of Singapore; Department of Computer Science, Hong Kong Univ. of Sci. and Technol., Clear Water Bay, Kowloon, Hong Kong; Dept. of Comp. Information Science, University of Pennsylvania, Philadelphia, PA 19104-6389, United States; Dept. of Info. Syst. and Comp. Sci., National University of Singapore, Lower Kent Ridge Road, Singapore 119260, Singapore},
abstract={This paper proposes that the process of language understanding can be modeled as a collective phenomenon that emerges from a myriad of microscopic and diverse activities. The process is analogous to the crystallization process in chemistry. The essential features of this model are: asynchronous parallelism; temperature-controlled randomness; and statistically emergent active symbols. A computer program that tests this model on the task of capturing the effect of context on the perception of ambiguous word boundaries in Chinese sentences is presented. The program adopts a holistic approach in which word identification forms an integral component of sentence analysis. Various types of knowledge, from statistics to linguistics, are seamlessly integrated for the tasks of word boundary disambiguation as well as sentential analysis. Our experimental results showed that the model is able to address the word boundary ambiguity problems effectively.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Fugerer1995295,
author={Fugerer, R.H. and Hervig, D.J. and Holt, L.L. and Banks, C.R. and Jennings, D.I. and Worley, T.J.},
title={The development of a focal plane array data system for component-level characterization and real-time mission simulation testing},
journal={Proceedings of SPIE - The International Society for Optical Engineering},
year={1995},
volume={2474},
pages={295-304},
doi={10.1117/12.210567.full},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-58749104663&doi=10.1117%2f12.210567.full&partnerID=40&md5=ac8284f69f377ed8b1051ed44bbb53bc},
affiliation={Micro Craft Technology, IAEDC Operations, MS 6400, Arnold AFB, TN  37389-6400, United States},
abstract={This paper will describe the USAF Arnold Engineering Development Center (AEDC) technology efforts that provide signal processing and data system support for infrared (IR) Focal Plane Array (FPA) testing. The requirements for AEDC space sensor testing range from component-level FPA characterization to advanced mission simulation. The technology efforts underway address these requirements by developing hardware and software that meet AEDC' s generic needs for FPA testing. Component-level EPA characterization places unique requirements on system fidelity and bandwidth performance. Diversity in sensor types being tested and levels of sensor integration creates the need for versatility in data handling and sensor interfaces. Mission simulation requirements emphasize the need for extended data storage, system throughput, and data display capabilities. A signal processing system will be presented which addresses AEDC's requirements for component-level sensor operation, data acquisition, and flexible interface architectures that can be modified quickly to accommodate different sensor interfaces and data formats. The system will also address the need for high-speed storage of very large data arrays during mission simulation testing. Techniques used to verify and validate system operation will also be presented. © 1995 SPIE.},
author_keywords={Analog;  Digital;  FPA;  Infrared;  Mission Simulation;  Radiometric Calibration;  Sensor;  Subsystem},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cannon1995,
author={Cannon, T.M. and Winters, B.A.},
title={International space station alpha external thermal control system overview},
journal={SAE Technical Papers},
year={1995},
doi={10.4271/951649},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072411736&doi=10.4271%2f951649&partnerID=40&md5=94e813bdf3385452512f092f768eb0cc},
affiliation={McDonnell Douglas Aerospace, Space Station Division, Huntington Beach, CA, United States},
abstract={An overview of the external Active Thermal Control System (ATCS) for the International Space Station Alpha (ISSA) is presented. Changes to ATCS architecture made during the transition from the Space Station Freedom (SSF) program have significantly affected system design and operational characteristics. ATCS architecture, described in a system schematic, shows the results of this evolution based on new requirements implemented for the ISSA program. Two independent, single phase ammonia thermal loops replace the three independent, two phase ammonia loops used for SSF. The concept of packaging system components in orbital replaceable units (ORUs) is maintained, however an additional module replaces the interface with the environmental control system previously used as the source for pressurant to the system. The functions and characteristics of the ORUs are summarized. Avionics, fluids, and structural interfaces have been simplified to minimize crew extra-vehicular activity time during system startup and maintenance. Concepts for providing on-orbit maintenance of ATCS hardware is discussed. The diverse test program discussed in the paper is being implemented to verify ATCS operation at component, assembly, and system levels. © Copyright 1995 Society of Automotive Engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cagan199535,
author={Cagan, M.},
title={Untangling configuration management: Mechanism and methodology in SCM systems},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1995},
volume={1005},
pages={35-52},
doi={10.1007/3-540-60578-9_2},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958610060&doi=10.1007%2f3-540-60578-9_2&partnerID=40&md5=a4136d69b952a089f1d078f81e523e10},
affiliation={Continuus Software Corporation, 108 Pacifica, Irvine, CA  92718, United States},
abstract={There is considerable diversity in the SCM methodological needs of today’s software teams. The SCM methods a team requires are a function of technical, social and corporate constraints that define how the project team must design, construct, test and deliver software. Most commercial and academic SCM systems created to date support particular SCM methodologies. Some created specific mechanisms to support their methodology, while others simply support the methods that work given the constraints and limitations of their mechanisms. If modern SCM systems are to be applicable to the broad spectrum of software development teams, the methodologies must be separated from the mechanisms, the mechanisms must be distilled into a flexible set of widely applicable capabilities, and the definition of methodologies using these mechanisms must be facilitated. The purpose of this paper is to explore the mechanisms needed to support advanced SCM methodologies, and process-based software configuration management in general. © Springer-Verlag Berlin Heidelberg 1995.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Miller19955,
author={Miller, J. and Roper, M. and Wood, M. and Brooks, A.},
title={Towards a benchmark for the evaluation of software testing techniques},
journal={Information and Software Technology},
year={1995},
volume={37},
number={1},
pages={5-13},
doi={10.1016/0950-5849(94)00456-3},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029214892&doi=10.1016%2f0950-5849%2894%2900456-3&partnerID=40&md5=0fe6d6d8c72f746225b3591976cf671b},
affiliation={Department of Computer Science, University of Strathclyde, Livingstone Tower, Richmond Street, Glasgow, G1 1XH, United Kingdom},
abstract={Despite the existence of a great number of software testing techniques we are largely ignorant of their respective powers as software engineering methods. It is argued that more experimental work in software testing is necessary in order to place testing techniques onto a scale of measurement, or classify them in such a way that is useful to the software engineer. Current experimental practices are examined using a parametric framework and are shown to contribute little towards a cohesive and useful body of knowledge. The idea of a benchmark repository of faulty and correct software is explored enabling unification of diverse experimental results. Such a unification should start the process of moving towards an evaluation taxonomy of testing methods. © 1995.},
author_keywords={benchmark repository;  experimentation;  software quality;  testing},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Rajhel1994307,
author={Rajhel, J.A.},
title={The application of an automated software tool for modeling test processes},
journal={AUTOTESTCON (Proceedings)},
year={1994},
pages={307-310},
doi={10.1109/AUTEST.1994.381604},
art_number={381604},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063212476&doi=10.1109%2fAUTEST.1994.381604&partnerID=40&md5=440b5dfdb787e27ffa3e0291f9d987c7},
affiliation={Support Systems Assoc.,Inc., United States},
abstract={To obtain a consensusmodel of a complex system utilizing a diverse p u p of people requires a standard automated modeling tool Without thb type of modeling tool, various models must be interpretedto be incorporated into a single model Mkinterpretationof the differentviews is inevitabk causingmod&?to be incorrect. Use of an automatedtool such ss IDEFO will eliminate integration problems because of its abilityto espmn the pointof the model in a standard graphic way. © IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{NoAuthor19941,
title={1st International Eurospace-Ada-Europe Symposium, 1994},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1994},
volume={887 LNCS},
pages={1-521},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027399309&partnerID=40&md5=fffd3f8d9e11e11231377083a0173463},
abstract={The proceedings contain 41 papers. The special focus in this conference is on Eurospace. The topics include: Adequacy of the new generation of multithreading operating systems to the ada tasking model; the AECSS fault tolerant distributed ada testbed and application; a front-end to HOOD; tool support for high integrity ada software; testing ada abstract data types using formal specifications; formal methods for a space software development environment; test methods and tools for SOHO mass memory unit software; integrating modular, object oriented programming, and application generator technologies in large real time and distributed developments; a new approach for HOOD/ada mapping; evolving an ada curriculum to 9x; recommendations and proposals for an ada strategy in the space software development environment; an apse integrating multiple compilers; development of a lightweight object-based software process model under pragmatic constraints; test philosophy and validation stategy of on-board real time software in envisat-1 satellite radar-altimeter; a knowledge-based system for diagnosis in veterinary medicine; event diagnosis and recovery in real-time on-board autonomous mission control; ada controls the European robotic arm; automatic generation of ada source code for the rafale mission computer; interfacing computer communications from ada in a diverse and evolving environment; cost-benefit analysis for software-reuse — a decision procedure; integrating ada and extra support in a doubly portable extended executive designed for hard real time systems; distribution of tasks within a centrally scheduled local area network and FAA certification of ada run-time systems.},
document_type={Conference Review},
source={Scopus},
}

@ARTICLE{Lovric1994309,
author={Lovric, T.},
title={Systematic and design diversity - Software techniques for hardware fault detection},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={1994},
volume={852 LNCS},
pages={309-326},
doi={10.1007/3-540-58426-9_138},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969392701&doi=10.1007%2f3-540-58426-9_138&partnerID=40&md5=c8a8291d9c8b8976485d84102e1140de},
affiliation={Universität Dortmund, Fachbereich Informatik, Dormund, 44221, Germany},
abstract={For the detection of hardware operational faults in most safe systems static redundancy is used. Thus, in the most simple case we have the well known Duplex System. If design fault detection is required, design diversity in the software has to be used, too. We suggest the combined utilization of so called systematic diversity and design diversity in a time-redundant system instead of the structural redundant Duplex System. For this purpose two diversly designed and systematically transformed variants of an application program are executed sequentially on the same processor. We call this new approach a Virtual Duplex System. In this paper we investigate the safety of a Virtual Duplex System. We propose the use of software diversity techniques (i.e. systematic diversity) to detect nearly all hardware faults in this system. Transient faults are effectively detected through the time redundancy and permanent faults by the new software diversity approach. In addition software design faults and even compiler-, library-, operating system- and underlying hardware design faults can be detected. The proposed software techniques are either new or never considered systematically for the detection of hardware faults in a general purpose system environment with design diversity. As an example the new systematic diversity technique ‘simple register permutation’ was applied on different application programs by means of a simple heuristic. The technique was evaluated experimentally by injecting permanent hardware faults with the fault injection tool ProFI and measuring the safety of Virtual Duplex Systems. The results are compared to systems that do not use special fault detection (Simplex Systems) and Virtual Duplex Systems that use pure design diversity. The experiments show that even by simple systematic diversity most permanent hardware faults are detected. © 1994, Springer Verlag. All rights reserved.},
author_keywords={Absolute test;  Design diversity;  Design faults;  Fail-safe;  Fault detection coverage;  Operational faults;  Relative test;  Self-checking;  Software implemented hardware-fault injection;  Systematic diversity;  Virtual duplex system},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Mitchell1993,
author={Mitchell, K.},
title={The cold performance of diesel engines},
journal={SAE Technical Papers},
year={1993},
doi={10.4271/932768},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072438568&doi=10.4271%2f932768&partnerID=40&md5=14008359d36bb3684d830129c0090817},
affiliation={Shell Canada Ltd., Canada},
abstract={This paper describes a test program where up to eighteen diesel fuels of varying qualities were tested for cold performance in sixteen commercial diesel engines. In this study, cold performance was defined as the time to start, intensity and time of white smoke emissions after the cold start and engine knock, if present, after the cold start. Initial tests were run at -20°C with starting aids (such as block heat and/or ether use) and at -5°C with no starting aids. Subsequent tests were only run under the latter conditions, as this was found to be more discriminating regarding fuel quality effects. The diesel engines were chosen to represent the diversity of engine design in North America, Europe and the Far East. Both Direct and In-Direct Injection engines were tested as were naturally aspirated and turbocharged engines. engine build dates varied from 1980 to 1989. This range covers most of the current diesel powered fleet in North America. The fuel matrix tested was sourced to cover the general range of commercial fuels in North America in terms of cetane number, volatility and viscosity. Ignition improver was included in a number of the fuels. Results of the test program showed that engine type and calibration was the major factor controlling cold performance. The most important fuel parameters influencing cold performance were, in order, cetane number and volatility. The use of an ignition improver to increase a fuel's cetane number, was found to be comparable to naturally derived cetane at the levels of cetane number boost examined. © Copyright 1993 Society of Automotive engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ström1993,
author={Ström, M. and Ström, G.},
title={A statistically designed study of atmospheric corrosion simulating automotive field conditions under laboratory conditions - Final volvo report on the AISI cosmetic corrosion set of materials},
journal={SAE Technical Papers},
year={1993},
doi={10.4271/932338},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072421885&doi=10.4271%2f932338&partnerID=40&md5=75d6fa973d9ea4f580aecf49fcdfd844},
affiliation={Volvo Car Corp.; AB Volvo},
abstract={An extensive atmospheric corrosion test program to simulate automotive field conditions has been successfully completed. This paper focuses on the corrosion results from the widely exposed AISI set of correlation panels for cosmetic corrosion. Eight factorially designed 12-week tests have been performed in the laboratory, using high performance test equipment, capable of simulating diverse outdoor conditions. The results have been compared with those of outdoor scab exposures and reference panels on vehicles, running in Canada and Sweden. The influences of six corrosive test variables on the response creep-back from scribe on the painted panels are demonstrated, based on a statistic evaluation of the test matrix. All higher settings of the introduced accelerating test variables have each resulted in a decrease in the test correlation with on-vehicle exposures. A high degree of correlation regarding on-vehicle exposure was reached when the test conditions were comparatively mild (acceleration factor < 10). However, to achieve a high degree of correlation to the Volvo outdoor scab test, the test conditions corresponded to the high settings of the variables. The results of the test program clearly suggest that the predictability of a test is generally improved when simulating outdoor conditions under mild acceleration combined with linear extrapolation instead of forcing the corrosion process in order to establish the postulated damage within a comparatively short time. From the results of the statistical design in the high performance equipment, simplified test methods, suitable as standards for the small scale laboratory, have been worked out. The results of the AISI set from two such tests are presented. One provides excellent correlation to on-vehicle behaviour. The scribe creep propagation for zinc and zinc-iron coated materials are convincingly shown to be inversely related to the zinc-coating thickness, both in the eight Volvo tests and in field conditions on vehicle. Hence, the paint undercutting is directly determined by the progress of the dissolution front of the zinc coating and the anodic consumption rate of zinc is constant. A plausible explanation for this effect is that the overall corrosion rate is limited by constant-rate oxygen reduction, either in the scribe (at limited undercutting) or by a stabilised cathodic zone tailing the anodic dissolution front (at extensive creep-back from scribe).},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mitchell199249,
author={Mitchell, M.J.},
title={The use of application model validation in testing a proposed standard},
journal={Proceedings of the ASME Design Engineering Technical Conference},
year={1992},
volume={1992-August},
pages={49-59},
doi={10.1115/EDM1992-0140},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103457800&doi=10.1115%2fEDM1992-0140&partnerID=40&md5=c626886170509bc7044c361100568f2a},
affiliation={National Institute of Standards and Technology, Gaithersburg, MD, United States},
abstract={The validation testing of data models is needed to ensure technical solutions provided by the integrated model will work in a practical sense. An Application Protocol (AP) within STEP is a specification of data sharing requirements for a particular application area. Application Protocols are designed to permit practical implementations of STEP. The model validation focuses on the principal mechanism for specifying the data sharing requirements, an application specific data model. The body of the paper describes the process by which an application model is validated. Application model development and validation are complex processes that rely extensively on human capabilities for analysis, judgement and synthesis of large amounts of diverse information. These activities require the support from automation to produce a technically complete AP. The model validation process and the STEP development methods place unique requirements on the software that will be needed to support the effective testing of STEP. The National PDES Testbed at the National Institute of Standards and Technology has undertaken a software project to support these activities. The current direction of this project has been formulated from our initial experiences in exercising this process and with software automation for the model validation testing. In addition, this paper introduces the potential contribution that application model validation and validation tools could make to the conformance testing of AP implementations. © 1992 American Society of Mechanical Engineers (ASME). All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Harrison1992,
author={Harrison, A.C. and Marlow, M.E. and Levi, L.D.},
title={Evaluation of environmentally acceptable cleaners as replacements for methyl ethyl ketone and 1,1,1 trichloroethane in solid rocket motor production and maintenance applications},
journal={AIAA/ASME/SAE/ASEE 28th Joint Propulsion Conference and Exhibit, 1992},
year={1992},
doi={10.2514/6.1992-3393},
art_number={AIAA 92-3393},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006008100&doi=10.2514%2f6.1992-3393&partnerID=40&md5=c36391ced0b382f59d2615afeac2f161},
affiliation={Aerojet Propulsion Division, Sacramento, CA, United States},
abstract={The use of 1,1,1 trichloroethane (TCA) will be restricted and ultimately prohibited under the U. S. Clean Air Act and the Montreal Protocol. Use of methyl ethyl ketone (MEK) is currently being restricted by various state air quality districts in California. Used widely in the production of composite, metallic, and polymeric components for solid rocket motors as cleaning solvents, TCA and MEK have long been accepted as diverse and effective solvents by many manufacturers. Therefore, evaluation of potential TCA and MEK replacements requires numerous application considerations, ranging from simple hardware and tooling cleanup to pre-bond cleaning preparation of critical bonds. This test program evaluates a wide variety of potential solvent replacements within this wide range of common applications. © 1992 by the American Institute of Aeronautics and Astronautics, Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Allred1992455,
author={Allred, L.G. and Kelly, G.E.},
title={A system for fault diagnosis in electronic circuits using thermal imaging},
journal={AUTOTESTCON (Proceedings)},
year={1992},
volume={1992-September},
pages={455-458},
doi={10.1109/AUTEST.1992.270076},
art_number={270076},
note={cited By 12},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-70449492275&doi=10.1109%2fAUTEST.1992.270076&partnerID=40&md5=bbb3744b5ce5fc2d2a037bd79f22a64b},
affiliation={Software Engineering Division (TIS), Hill AFB, UT  84056, United States},
abstract={In many instances, the electronic state of an electronic circuit card contains insufficient information for correct fault diagnosis. Infrared thermal images, captured during the initial warmup of an electronic circuit card, are employed to enhance the available information and to assist the technician in performing the fault diagnosis. This system has proven beneficial for radar and microwave technologies where probing changes the electronic response of the circuit and signal levels can be extremely small. The development of this system has required the integration of technologies from a diversity of fields including image capture, image compression, image enhancement, neural networks, genetic algorithms, thermodynamics, and automatic test equipment (ATE) software. These technologies and preliminary system performance are discussed. © 1992 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{English1991,
author={English, R.E.},
title={Evolving the SP-100 reactor in order to boost large payloads to geo and to low lunar orbit via nuclear-electric propulsion},
journal={AIAA/NASA/OAI Conference on Advanced SEI Technologies, 1991},
year={1991},
doi={10.2514/6.1991-3562},
art_number={A91-53712},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007044572&doi=10.2514%2f6.1991-3562&partnerID=40&md5=3493bae1f9c2c3f3fb50b067a3f2db5f},
affiliation={National Aeronautics and Space Administration, Lewis Research Center, Cleveland, OH  44135, United States},
abstract={As NASA contemplates sending personnel on long-term missions to the Moon and to Mars, we technologists and mission planners are responsible for delineating the costs and risks involved and for formulating program plans reducing these costs and risks. In our striving to reduce costs and risks, up a crucial aspect of those plans is program continuity, that is, the continuing application of a given technology over a long period so that experience will accumulate from extended testing here on Earth and from a diversity of applications in space. We need to form an integrated view of the missions SEI will carry out, near-term as well as far, and of the ways in which these missions can mutually support one another. Near-term programs should be so constituted as to provide for the long-term missions both the enabling technologies and the accumulation of experience they need. In achieving this, missions in Earth orbit should both evolve and demonstrate the technologies crucial to long-term missions on the lunar surface, and the program for the lunar laboratories should evolve and demonstrate the enabling technologies for exploration of the surface of Mars and for flights of human beings Mars and return. In the near term, the program for the Space Station should be directed and funded to develop and demonstrate the solar Brayton powerplant that will be most useful as the power generator for the SP-100 nuclear reactor. © 1991 by the American Institute of Aeronautics and Astronautics, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kimelman1991635,
author={Kimelman, D.N. and Ngo, T.A.},
title={RP3 program visualization environment},
journal={IBM Journal of Research and Development},
year={1991},
volume={35},
number={5-6},
pages={635-651},
doi={10.1147/rd.355.0635},
note={cited By 24},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0026217715&doi=10.1147%2frd.355.0635&partnerID=40&md5=68c04da6cd9c9bafc97716f1aaa5968f},
affiliation={Watson Research Cent, Yorktown Heights, NY, United States},
abstract={The performance promised for parallel systems often proves to be somewhat elusive. This paper discusses one important technique for improving the performance of parallel software: program visualization - helping programmers visualize the real behavior of an application or system by presenting its state and progress in a continuous graphic fashion. An environment for visualization of program execution is described. Within this visualization environment, programmers dynamically establish views of the behavior of a program in execution and watch for trends, anomalies, and correlations as information is displayed. By continually refining the view of the program and replaying the execution of the program, programmers can gain an understanding of program (mis)behavior. This is essential for the debugging, performance analysis, and tuning of parallel software. Design goals for the visualization environment include expandibility, portability, and the ability to accommodate diverse architectures, including highly parallel shared-memory systems and large-scale message-passing systems. Results from visualization of systems and applications running on the RP3, an experimental shared-memory multiprocessor, are presented in the form of color reproductions of typical, useful displays.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Dahll1990677,
author={Dahll, G. and Barnes, M. and Bishop, P.},
title={Software diversity: Way to enhance safety?},
journal={Information and Software Technology},
year={1990},
volume={32},
number={10},
pages={677-685},
doi={10.1016/0950-5849(90)90100-6},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-38249017454&doi=10.1016%2f0950-5849%2890%2990100-6&partnerID=40&md5=7aa5597a31d9ae4cb54f28d230ac5fc5},
affiliation={Institutt fur Energiteteknikk, OECD Halden Reactor Project, PO Box 173, N-1751 Halden, Norway; Safety and Reliability Directorate, United Kingdom; National Power Technology and Environment Centre, United Kingdom},
abstract={The topic of the paper is the use of diversely produced programs to enhance the safety of computer-based systems applied in safety-critical areas. The paper starts with a survey of scientific investigations on the impact of software redundancy made at various institutions around the world. Main emphasis will, however, be put on the PODS/STEM projects, which have been performed at the OECD Halden Project in cooperation with the Technical Research Center of Finland, the Safety and Reliability Directorate, AEA Technology, UK, and Central Electricity Research Laboratory (now National Power Technology and Environment Centre), UK. In these projects, three program versions were made independently by three different teams, all based on the same specification. The three programs were tested back-to-back with a large amount of test data. The experience and results from this process were carefully logged and used for further analysis. Various strategies for test data selection were compared, with respect to fault finding strategies, as well as to branch and statement coverages of the tested programs. The assumption of independence of failures in diversely produced programs was investigated. A particularly interesting effect, namely, failure masking due to program structure, was revealed. Static analysis techniques, software measures, and software reliability estimates were also studied. © 1990.},
author_keywords={back-to-back testing;  safety-critical systems;  software diversity;  software testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kelly1990153,
author={Kelly, J.P.J. and Murphy, S.C.},
title={Achieving Dependability Throughout the Development Process: A Distributed Software Experiment},
journal={IEEE Transactions on Software Engineering},
year={1990},
volume={16},
number={2},
pages={153-165},
doi={10.1109/32.44379},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025384344&doi=10.1109%2f32.44379&partnerID=40&md5=4e4fe0acb294c1036a1eb12cffe98505},
affiliation={Department of Electrical and Computer Engineering, University of California, Santa Barbara, CA 93106, United States; Department of Computer Science, Moorpark College, Moorpark, CA 93021, United States},
abstract={For more than ten years, design diversity experiments have been conducted to study fault-tolerant multiple-version software systems. Design diversity is the approach by which multiple versions of a software system are independently developed. Our current focus is on distributed software engineering techniques and methods for improving the specification and testing phases. With multiversion development, multiple implementations allow the use of an automated approac li to testing called Back-to-Back (BIB) Testing in which the outputs are compared to detect any discrepancies. This obviates the need to determine the correct response a priori, allowing automated execution of a large number of test cases. However, a specification defect may lead to similar errors in the multiple versions and the underlying fault may not be detected with a B/B testing approach. The use of diverse formal specifications is a proposed solution to this problem since defects in independently-written specifications are likely to be different. To examine these issues, an experiment was performed using the design diversity approach in the specification, design, implementation, and testing of distributed software. In the experiment, three diverse formal specifications were used to produce multiple independent implementations of a distributed communication protocol in Ada. Another important aspect of this study was the investigation of problems encountered in building complex concurrent processing systems in Ada. Many pitfalls were discovered in mapping the formal specifications into Ada implementations. In the experiment, the process of controlling human factors, collecting accurate and appropriate data, and drawing valid conclusions was a continuing challenge. © 1990 IEEE},
author_keywords={Back-to-back testing;  Comparison testing;  Design diversity;  Distributed software engineering;  Experimentation;  Fault-tolerant software;  Formal specifications;  Multiversion software;  N-version programming},
document_type={Article},
source={Scopus},
}

@ARTICLE{Clarke1989153,
author={Clarke, L.A. and Richardson, D.J. and Zeil, S.J.},
title={Team: A Support Environment for Testing, Evaluation, and Analysis},
journal={ACM SIGPLAN Notices},
year={1989},
volume={24},
number={2},
pages={153-162},
doi={10.1145/64140.65018},
note={cited By 9},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976851436&doi=10.1145%2f64140.65018&partnerID=40&md5=10908e44300e74a62fc21eba644db907},
affiliation={Univ. of Massachusetts, Amherst, United States; Univ. of California, Irvine, United States; Old Dominion Univ., Norfolk, VA, United States},
abstract={Current research indicates that software reliability needs to be achieved through the careful integration of a number of diverse testing and analysis techniques. To address this need, the TEAM environment has been designed to support the integration of and experimentation with an ever growing number of software testing and analysis tools. To achieve this flexibility, we exploit three design principles: component technology so that common underlying functionality is recognized; generic realizations so that these common functions can be instantiated as diversely as possible; and language independence so that tools can work on multiple languages, even allowing some tools to be applicable to different phases of the software lifecycle. The result is an environment that contains building blocks for easily constructing and experimenting with new testing and analysis techniques. Although the first prototype has just recently been implemented, we feel it demonstrates how modularity, genericity, and language independence further extensibility and integration. © 1989, ACM. All rights reserved.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Clarke1989153,
author={Clarke, L.A. and Zeil, S.J. and Richardson, D.J.},
title={TEAM: A support environment for testing, evaluation, and analysis},
journal={Proceedings of the 3rd ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments, SDE 1988},
year={1989},
pages={153-162},
doi={10.1145/64135.65018},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033857883&doi=10.1145%2f64135.65018&partnerID=40&md5=961e8745632d55ceda6f7ee1ede92370},
affiliation={Computer and Information Science, University of Massachusetts, Amherst, Amherst, MA  01003, United States; Information and Computer Science, University of California, Irvine, Irvine, CA  92717, United States; Computer Science, Old Dominion University, Norfolk, VA  23508, United States},
abstract={Current research indicates that software reliability needs to be achieved through the careful integration of a number of diverse testing and analysis techniques. To address this need, the TEAM environment has been designed to support the integration of and experimentation with an ever growing number of software testing and analysis tools. To achieve this flexibility, we exploit three design principles: component technology so that common underlying functionality is recognized; generic realizations so that these common functions can be instantiated as diversely as possible; and language independence so that tools can work on multiple languages, even allowing some tools to be applicable to different phases of the software lifecycle. The result is an environment that contains building blocks for easily constructing and experimenting with new testing and analysis techniques. Although the first prototype has just recently been implemented, we feel it demonstrates how modularity, genericity, and language independence further extensibility and integration. © 1988 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Barnes1987,
author={Barnes, M.},
title={STEM PROJECT (SOFTWARE TESTING AND EVALUATION OF METHODS).},
journal={Publ},
year={1987},
pages={v 2p},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023595663&partnerID=40&md5=5f433635eb25a96634857e53489546f9},
affiliation={UKAEA, UKAEA},
abstract={The software testing and evaluation of methods (STEM) Project has investigated methods of fault detection, and fault prediction, for software. These methods have been applied to three diverse implementations of a real-time program, which were developed in the project on diverse software (PODS) project. The results of the methods can be compared with the well-documented errors from the programs, in order to validate the methods.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Clark1986,
author={Clark, Steve},
title={SOFTWARE TOOLS FOR TESTING PERSONAL COMPUTER PRODUCTS.},
journal={Wescon Conference Record},
year={1986},
page_count={4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022984074&partnerID=40&md5=c39e184a77ba068389f71a2ad84a2a94},
affiliation={Apple Computer, Cupertino, CA, USA, Apple Computer, Cupertino, CA, USA},
abstract={This paper documents the important role that software testing tools play in the fast-paced, creative world of testing personal computer products. Testing tools amplify and preserve the special talents required of the tester, and they enable the tester to subject the product to the most comprehensive test at the end of its development, where it is most needed. Several kinds of tools for module and whole-system tests are discussed, especially automated scripting facilities. Some problems associated with the use of tools are posed, along with possible solutions. Strategies for the use of tools are presented.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Hinnrichs1986,
author={Hinnrichs, R. and Whitsett, J. and Phillips, R. and Allen, W. and Cecka, J.},
title={Electrical power system integration for the space station},
journal={AIAA Space Station in the 21st Century, 1986},
year={1986},
art_number={AIAA-86-2351},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006147390&partnerID=40&md5=1a0403d4f553bd89f88d6bb586952d9f},
affiliation={Rockwell International/Rocketdyne Division, Canoga Park, CA, United States},
abstract={The Space Station by virtue of its high electrical power requirements, multiple AC and DC power sources, and diverse loads located throughout a large space structure presents unique challenges to electrical power system design and integration. To satisfy these requirements, Rocketdyne has conducted a series of architecture and integration trade studies that have sized and configured the power sources and the Power Management and Distribution System. The most significant of the elements considered were: user accommodations, automation, power source selection and synchronization, use of other station resources, interface complexity, and power distribution control and protection. In addition, the software was sized and test cases were programmed in ADA. © American Institute of Aeronautics and Astronautics Inc, AIAA. All right reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Bishop1985153,
author={Bishop, P. and Esp, D. and Barnes, M. and Humphreys, P. and Dahll, G. and Lahti, J. and Yoshimura, S.},
title={PROJECT ON DIVERSE SOFTWARE - AN EXPERIMENT IN SOFTWARE RELIABILITY.},
year={1985},
pages={153-158},
doi={10.1016/s1474-6670(17)60099-5},
note={cited By 8},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022284135&doi=10.1016%2fs1474-6670%2817%2960099-5&partnerID=40&md5=3288e459044c6224112ec464abb2e095},
affiliation={Central Electricity Research Lab, Leatherhead, Engl, Central Electricity Research Lab, Leatherhead, Engl},
abstract={The authors discuss the Project on Diverse Software (PODS), an experiment which has attempted to quantify the impact of a number of commonly used software development techniques on software reliability. They describe the experimental design, organisation and documentation of the project and present an analysis of the results.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{HowleyJr.1984261,
author={Howley Jr., Paul P.},
title={ASSESSMENT OF SOFTWARE TESTING TECHNIQUES FOR MAINTENANCE.},
year={1984},
pages={261-266},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0021561493&partnerID=40&md5=1d3f270cdfa4bed4975b44a9080d7f2e},
affiliation={Boeing Co, Seattle, WA, USA, Boeing Co, Seattle, WA, USA},
abstract={The author defines a taxonomy of software testing tools and techniques, describes the generic categories, and provides an assessment of their applicability and feasibility in the software maintenance environment. The taxonomy provides a framework for coalescing a wide diversity of tools and techniques into a consistent, well-organized structure. The applicability and appropriateness of software testing techniques and tools for the maintainability environment is assessed. Recommendations are provided for adapting testing tools and techniques for two software maintenance environments: block update and continuous maintenance.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{BeckJr.1984,
author={Beck Jr., W.E. and Carlson, J.G. and Baas, J.P.},
title={Model test results of the split-fan cross-ducted propulsion system concept for medium speed V/STOL aircraft},
journal={SAE Technical Papers},
year={1984},
doi={10.4271/841495},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072359432&doi=10.4271%2f841495&partnerID=40&md5=7808c926c27af7a1495c909d41aa20d5},
affiliation={Lockheed-California Company, Burbank, CA, United States},
abstract={Propulsion systems for Navy V/STOL aircraft have unique design requirements from which has emerged a diversity of proposed configurations. During recent years the Lockheed-California Company has been involved in design studies of a split-fan, cross-ducted propulsion system concept that meets the V/STOL requirements with conventional turbofan engine configurations. This concept utilizes fixed engines/nacelles with thrust vectoring for lift, and transfer of engine fan air between variable area nacelle nozzles for aircraft control and trim. A simple, lightweight cross-duct coupling between engines also provides engine-out roll trim, and is adaptable to either a twin or four engine configuration. A joint Lockheed/Navy experimental test program was conducted, utilizing a 0.25 scale propulsion internal flow model, to establish the flow transfer performance and the capability to generate nozzle forces for aircraft thrust, lift, and control. A description of the model, test set-up, and preliminary performance results that demonstrate concept feasibility are presented herein. © 1984 Society of Automotive Engineers, Inc.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gill1980553,
author={Gill, Chris and Thompson, John A.},
title={UNIFIED TEST HARNESS SYSTEM FOR AVIONICS SOFTWARE DEVELOPMENT.},
journal={IEEE Proceedings of the National Aerospace and Electronics Conference},
year={1980},
pages={553-560},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019184145&partnerID=40&md5=6df38aa4e66c13dd42a054e581f13aac},
abstract={Automated software tools can greatly assist in the early, economical detection and analysis of software errors. These tools include static and dynamic HOL analyzers, symbolic debuggers, instruction level simulators, and automated testing systems. Boeing has conducted a requirements analysis for embedded computer software testing and has designed a software Test Harness system that unifies many of the features available through these diverse testing tools. This study outlines the need for a test system oriented to error detection at the HOL level and traces the design evolution of such a system. An overview of the final design is provided, including an outline of the command language used to specify inputs and actions, and examples of the various printed reports and traces that can be generated by the system.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mustaffi19781199,
author={Mustaffi, Z. and Amann, H.},
title={Ocean mining and projection of the marine environment in the Red Sea},
journal={Proceedings of the Annual Offshore Technology Conference},
year={1978},
volume={1978-May},
pages={1199-1206},
doi={10.4043/3188-ms},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059212152&doi=10.4043%2f3188-ms&partnerID=40&md5=e53d1769736fa1e15b5e1dc5d4d50279},
affiliation={Saudi Arabian Sudanese Red Sea Joint Commission, Saudi Arabia; Preussag AG, Germany},
abstract={By all standards the Red Sea constitutes aunique marine environment. A highly diversified, rich and varying fauna and flora at the reef covered coasts and delicately organized pelagic life and benthos in deep graben waters combine with the yet largely unknown hydrography and the particular geology of a nascent ocean into an ecological system of great importance to the scientific, community. This ecosystem is, at the same time, exposed to extreme natural influences: intensive sun irradiation. constant and hot winds, and subsequent evaporation with negligible inflow of terrestrial water and a reduced exchange of fresh ocean water over the southern sill at Bab el Mandab. Salinity and temperatures of the water are thus higher and oxygen and nutrient contents are lower than in other seas of the world. This results in carefully balanced metabolisms of the ecosystem. New technologies such as ocean mining of the Red Sea metalliferous muds must be concerned with the environment and its safeguarding. With this understanding the Saudi Sudanese Red Sea Joint Commission has entrusted Preussag in 1976 with the technical development of occurrences of ore bearing muds (Zn, Cu, Ag) in the deep sea graben. A comprehensive program is being carried out: monitoring the deep sea and coastal environment and designing for their protection, test production and controlled redeposition of tailings and sediments in graben areas as well as beneficiation and metallurgy of the complex marine ores. A description of the task and preliminary results of recent research and development work until early 1978 are given together with an outlook on forthcoming steps. © Copyright 1978, Offshore Technology Conference.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Witmer1977461,
author={Witmer, D.R. and Pearson, R.E.},
title={Acoustic telemetry - An underwater alternative},
journal={Proceedings of the Annual Offshore Technology Conference},
year={1977},
volume={1977-May},
pages={461-466},
doi={10.4043/2865-ms},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060290873&doi=10.4043%2f2865-ms&partnerID=40&md5=e4ef1aa752dfbfaa9d88c3674ae9896a},
affiliation={Honeywell Inc., United States},
abstract={This paper describes an experimental acoustic investigation program conducted during 1976 and sponsored by Gulf Research and Development Co. of Houston, Texas, to determine the most effective and reliable means of communicating at long range in shallow water. An example of the use of such a system would be remote control and monitoring of subsea wellheads. A key feature of this program was the flexibility of system parameter values that was built into the test hardware. This allowed, system design tradeoffs to be performed in real time. Additional benefits were the ability to determine the limiting effects of environmental characteristics on system performance. Results included the demonstration of a system based on frequency-shift keying with frequency diversity. This system was shown to be effective in operating under conditions of severe multipath fading and ray bending in depths of 200 to 400 feet at ranges of 2 to 3 nautical miles. © Copyright 1977, Offshore Technology Conference.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Smith1977266,
author={Smith, E.F.},
title={The site machine computer-aided instruction in architectural education},
journal={Proceedings - Design Automation Conference},
year={1977},
pages={266-274},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-0017417288&partnerID=40&md5=0b7e6ed4b995c773b959b5d90807219f},
affiliation={Graduate School of Architecture, University of Utah, Salt Lake City, UT  84112, United States},
abstract={THE SITE MACHINE is one of two prototype, graphics-based programs written to evaluate the potential of computer-aided instruction (CAI) programs in architectural education. Some CAI systems are now highly developed but almost no use has occurred in architectural education. The special characteristics and diverse nature of architectural education as well as the requirement for fast, highquality graphics, especially in the design areas, requires special applications and consideration. THE SITE MACHINE was a research attempt in open ended problem solving at the high end of a hierarchy of computer use in CAI in architecture. The major objectives of the test programs were: I. To determine the feasibility of implementing viable CAI programs using existing computing resources at reasonable development and user costs and to determine the useability of existing developed programs, techniques, capabilities and algorithms from CAI, computer graphics, mathematical and spatial modeling. 2. Identify problems of acceptance and use by present faculty members, compatability and ease of integration in the curriculum, acceptance, use and motivation of student users. 3. Develop test programs that will identify student differences in learning, recognize subject mastery and demonstrate that learning has occurred. 4. Test initial ideas, concepts, instructional models and provide guidance and direction for future research and development efforts. THE SITE MACHINE provides site descriptions and display and a problem solving environment for site analysis and design. Several aspects of the site including sun, wind, soil, trees, water, subsurface conditions, setbacks, easements and view may be considered in selecting the best location and orientation for a proposed building. Several analytical tools are provided to aid in the decision making process. The program was tested with students in an architectural design class, and with student and faculty volunteers from architecture, urban planning and landscape architecture. Analysis and design decisions reflecting methodologies and strategies were recorded and used to identify costs, program use problems, student differences and learning effects. The test programs demonstrated that it is possible to implement viable CAI programs with widely varying capability, in diverse areas of architecture, on available computing systems and with reasonable development and use costs. Level of learning, subject mastery, differences between students and between faculty and students were easily identified. The research indicated a high potential for success of expanded, rigorously structured and broadly based research in this area. © 1977 Institute of Electrical and Electronics Engineers Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Rice1976,
author={Rice, R.S. and Sacks, S.},
title={The effects of braking on the directional controllability of automobiles},
journal={SAE Technical Papers},
year={1976},
doi={10.4271/760345},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072431133&doi=10.4271%2f760345&partnerID=40&md5=2f1e65fb8e4b0f90db2e710a17a6bcd3},
affiliation={Calspan Corp.; Natl. Highway Traffic Safety Admin., U.S. Dept. of Transportation, United States},
abstract={This paper describes an approach to the development of a test procedure for measuring the combined steering and braking performance of automobiles with possible application to the formulation of a safety standard. The study was performed under Contract No. DOT-HS-4-00971 for the NHTSA. The influences and interactions of the many factors which must be considered in developing a meaningful and objective procedure are discussed. These include such diverse items as definition of test conditions and equipment requirements, identification of the significant operational variables (initial speed, permissible control functions, performance metrics, etc.), means for discriminating between vehicle and driver effects, and determination of acceptable performance levels. A full-scale test program, aimed at obtaining actual performance information on the effect of various factors, is described. Emphasis is placed on identifying operational situations having distinct for accident potential (i.e., conditions in which excessive brake pedal force might be applied as in a panic stop) and on the role of the driver in attempting to minimize loss-of-control possibilities at this condition. Test results showing the marked differences in directional behavior which can occur as functions of magnitude of applied brake pedal force (and, consequently, of wheel lock-up patterns) are presented in the form of graphs and tables. Comparisons between driver-controlled and machine-controlled operations have been made in order to devise a low-cost unambiguous procedure that properly reflects accident potential. Copyright © 1976 Society of Automotive Engineers, Inc. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Miles1975,
author={Miles, D.L. and Faix, L.J. and Lyon, H.H. and Niepoth, G.W.},
title={Catalytic emission control system field test program},
journal={SAE Technical Papers},
year={1975},
doi={10.4271/750179},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072355808&doi=10.4271%2f750179&partnerID=40&md5=cc60d617a850446d580ad9262b97ac03},
affiliation={Chevrolet Engineering, United States; Oldsmobile Division, United States; General Motors Corporation, United States},
abstract={A fleet of nearly 250 cars equipped with experimental catalytic converter systems were tested in taxi, police, state, and municipal fleets in various cities throughout the country. This provided a diversified range of customer service and altitude and climatic conditions. The objective was to evaluate the performance and durability in high mileage field service of experimental catalytic emission control systems. The fleet comprised groups of cars with hardware and calibration variations designed toward the 1975 Federal and California and more advanced emission requirements. The converter systems evaluated were primarily a 260 cubic inch underfloor converter and a 140 cubic inch manifold converter. Both bead and monolith substrate catalysts were examined. Test results showed that on the average the systems successfully controlled emissions to below the 1975 Federal and California requirements for greater than 50,000 miles. Engine misfire conditions did cause converter damage in some instances. Systems designed for the low emission requirements of.41/3.4/.40 grams/mile HC/CO/NOx exceeded those levels at relatively low mileage due to catalyst deterioration. Copyright © Society of Automotive Engineers, Inc. 1975 All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Reeds1972I779,
author={Reeds, C.B. and Seligman, P.R.},
title={Diverless subsea completion system for deep water oil wells},
journal={Proceedings of the Annual Offshore Technology Conference},
year={1972},
volume={1972-May},
pages={I779-I796},
doi={10.4043/1594-ms},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057570140&doi=10.4043%2f1594-ms&partnerID=40&md5=12c15c66c241975b9b3b344cc8e0c671},
affiliation={Standard Oil Co. of California, Western Operations, Inc., United States},
abstract={A prototype single well subsea completion system, compatible with established floating rig drilling, has been built and tested successfully. The equipment is suitable for water depths to 1500' and is designed to be installed and operated without the aid of divers or submersibles. Although initially planned for Santa Barbara Channel subsea environment, the system is appropriate for a majority of offshore areas worldwide. It is suitable for either flowing or artificial lift well conditions. Provision is made for well maintenance either through the flow line (TFL) or by vertical access through riser pipe. Descriptions of the wellhead equipment, tree, flow line connectors, control systems, and related running tools are included. TFL tools tested with the subsea system are described. Some of the tools are new development outgrowths of the test program. In late 1971, after more than two years of on-land testing and debugging, a nonproducing subsea test was conducted in 214 of water, 9 miles offshore from Ventura, California. Results of all tests are presented. © 1972 Proceedings of the Annual Offshore Technology Conference.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Bartholomew1961,
author={Bartholomew, E.},
title={New knock-testing methods needed to match engine and fuel progress},
journal={SAE Technical Papers},
year={1961},
doi={10.4271/610200},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072478940&doi=10.4271%2f610200&partnerID=40&md5=69d8fa853619b681b1eadb1a45222b83},
affiliation={Research and Development Department, Ethyl Corporation, Detroit, MI, United States},
abstract={Basic specifications of the Research method for evaluating the antiknock quality of motor gasolines were established more than thirty years ago, and those for the Motor method in 1932. The objective in the development of the Motor method was duplication of the behavior of gasolines that had been observed in automobiles during the 1932 Uniontown cooperative test program. Car engines of that era included such design features as short-length cylinder jackets, hot valves and manifold stoves, and mufflers with large flow resistance. These items were responsible for high metal temperature in the cylinders and much mixture dilution by exhaust gas. Similar effects on knocking were obtainable in test engines by extreme heating of the intake manifold or cylinder jacket. The former was adopted for the Motor method. The Research method was retained for evaluation of fuels under "mild" conditions which it was hoped might be achieved in future car engines. When the present laboratory test methods were adopted, the maximum-knock speed of automobiles operated on the usual commercial gasolines ordinarily was below 15 miles per hour (less than 750 engine rpm). Subsequent experience proved that high metal temperatures in combustion chambers of cars decreased engine durability and permissible compression ratio. Alteration of design which lowered the temperatures also markedly reduced severity toward sensitive fuels at low speeds and lessened the significance of Motor-method ratings. Then two trends of automobile design began which were destined to have additional important effects on the knocking of gasolines: 1.Commercialization of automatic transmissions having relatively high stall speeds; and 2.Drastic boost of engine power at high speeds through alteration of valve timing and opening up of fuel-induction systems. The modifications have boosted the median speed of maximum knock of U.S. automobile engines to about 1700 rpm. Refining methods now in general use but unknown in 1932 have greatly altered the chemical composition of gasolines. New antiknock compounds have come into use, whose volatility and combustion characteristics differ from those of tetraethyllead. All of these engine and refining advances have caused Research and Motor ratings to become less reliable indices of the antiknock quality of gasolines in automobiles. Heating of the intake manifold or cylinder jacket of test engines has proved to be an unsatisfactory substitute for higher speed in simulation of knocking conditions in currently produced automobile engines. Also, a large need exists for an instrument with response to knocking which is parallel to that of the human ear. Better knock-testing methods, it appears, can be developed around an engine which includes the CFR high-speed crankcase and a new cylinder. Modern refining processes and new antiknock compounds often produce patterns of distribution of antiknock quality over the distillation range of gasolines, which were not visualized 30 years ago. Imported foreign cars, as well as U.S. cars with manual transmissions and maximum-knock speeds near 1000 rpm, are highly sensitive to these altered fuel characteristics. A knock-testing method intended to reflect fuel behavior at low speed in cars of these types must provide for simulation of the pattern of fuel vaporization and distribution that occurs during acceleration. A wide variety of skills is represented by the scientists and engineers who, during the past three decades, have been responsible for the advances in automobile design, petroleum refining, instrumentation, and understanding of the combustion process. A major task ahead is organized application of these diverse talents to the development of new knock-testing methods. The technical and economic incentives are high. At the beginning of the sixties they provide a challenge equal to that which, three decades ago, motivated the pioneers in the creation of the Research and Motor methods.},
document_type={Conference Paper},
source={Scopus},
}


@ARTICLE{Guo2023,
author={Guo, J. and Zhao, B. and Liu, H. and Leng, D. and An, Y. and Shu, G.},
title={DeepDual-SD: Deep Dual Attribute-Aware Embedding for Binary Code Similarity Detection},
journal={International Journal of Computational Intelligence Systems},
year={2023},
volume={16},
number={1},
doi={10.1007/s44196-023-00206-9},
art_number={35},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150928845&doi=10.1007%2fs44196-023-00206-9&partnerID=40&md5=807b01e25871f42da62b0a17b1642757},
abstract={Binary code similarity detection (BCSD) is a task of detecting similarity of binary functions which are not available to the corresponding source code. It has been widely utilized to facilitate various kinds of crucial security analysis in software engineering. Because of the complexity of the program compilation process, identifying binary code similarity presents tough challenges. The most sensible binary similarity detector relies on a robust vector representation of binary code. However, few BCSD approaches are suitable to form vector representations for analyzing similarities between binaries, which may not only diverge in semantics but also in structures. And the existing solutions which only depend on hands-on feature engineering to form feature vectors, fail to take into consideration the relationships between instructions. To resolve these problems, we propose a novel and unified approach called DeepDual-SD that aims to combine the dual attributes (semantic and structural attribute). More specifically, DeepDual-SD consists of two branches, in which one text-based feature representation is driven by semantic attribute learning to exploit instruction semantics, another graph-based feature representation for structural attribute learning to investigate structural differences. Meanwhile deep embedding (DE) technology is utilized to map this information into low-dimensional vector representation. In addition, to get together the dual attributes, a fusion mechanism based on gate architecture is designed for learning to pay proper attention between the two attribute-aware embeddings. Experimental verifications are conducted on Openssl and Debian datasets for several tasks, including cross-compiler, cross-architecture and cross-version scenarios. The results demonstrate that our method outperforms the state-of-the-art BCSD methods in different scenarios in terms of detection accuracy. © 2023, The Author(s).},
author_keywords={Code similarity detection;  Deep embedding;  Fusion mechanism;  Semantic attribute learning;  Structural attribute learning},
document_type={Article},
source={Scopus},
}

@ARTICLE{DeAngelis2023,
author={De Angelis, E. and De Angelis, G. and Pellegrini, A. and Proietti, M.},
title={What makes test programs similar in microservices applications?},
journal={Journal of Systems and Software},
year={2023},
volume={201},
doi={10.1016/j.jss.2023.111674},
art_number={111674},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150786348&doi=10.1016%2fj.jss.2023.111674&partnerID=40&md5=041c6b2b99d4f2214eba8579d486044f},
abstract={The emergence of microservices architecture calls for novel methodologies and technological frameworks that support the design, development, and maintenance of applications structured according to this new architectural style. In this paper, we consider the issue of designing suitable strategies for the governance of testing activities within the microservices paradigm. We focus on the problem of discovering implicit relations between test programs that help to avoid re-running all the available test suites each time one of its constituents evolves. We propose a dynamic analysis technique and its supporting framework that collects information about the invocations of local and remote APIs. Information on test program execution is obtained in two ways: instrumenting the test program code or running a symbolic execution engine. The extracted information is processed by a rule-based automated reasoning engine, which infers implicit similarities among test programs. We show that our analysis technique can be used to support the reduction of test suites, and therefore has good application potential in the context of regression test optimisation. The proposed approach has been validated against two real-world microservices applications. © 2023 Elsevier Inc.},
author_keywords={Automated reasoning;  Microservices architecture;  Program instrumentation;  Software testing;  Symbolic execution;  Test program similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang2023,
author={Jiang, Y. and Su, X. and Treude, C. and Shang, C. and Wang, T.},
title={Does Deep Learning improve the performance of duplicate bug report detection? An empirical study},
journal={Journal of Systems and Software},
year={2023},
volume={198},
doi={10.1016/j.jss.2023.111607},
art_number={111607},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146288613&doi=10.1016%2fj.jss.2023.111607&partnerID=40&md5=620cb8740e982a7088cba95112a2fd6f},
abstract={Do Deep Learning (DL) techniques actually help to improve the performance of duplicate bug report detection? Prior studies suggest that they do, if the duplicate bug report detection task is treated as a binary classification problem. However, in realistic scenarios, the task is often viewed as a ranking problem, which predicts potential duplicate bug reports by ranking based on similarities with existing historical bug reports. There is little empirical evidence to support that DL can be effectively applied to detect duplicate bug reports in the ranking scenario. Therefore, in this paper, we investigate whether well-known DL-based methods outperform classic information retrieval (IR) based methods on the duplicate bug report detection task. In addition, we argue that both IR- and DL-based methods suffer from incompletely evaluating the similarity between bug reports, resulting in the loss of important information. To address this problem, we propose a new method that combines IR and DL techniques to compute textual similarity more comprehensively. Our experimental results show that the DL-based method itself does not yield high performance compared to IR-based methods. However, our proposed combined method improves on the MAP metric of classic IR-based methods by a median of 7.09%–11.34% and a maximum of 17.228%–28.97%. © 2023 Elsevier Inc.},
author_keywords={Deep learning;  Duplicate bug report detection;  Information retrieval;  Realistic evaluation;  Similarity measure},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ghorui20232403,
author={Ghorui, N. and Mondal, S.P. and Chatterjee, B. and Ghosh, A. and Pal, A. and De, D. and Giri, B.C.},
title={Selection of cloud service providers using MCDM methodology under intuitionistic fuzzy uncertainty},
journal={Soft Computing},
year={2023},
volume={27},
number={5},
pages={2403-2423},
doi={10.1007/s00500-022-07772-8},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146391943&doi=10.1007%2fs00500-022-07772-8&partnerID=40&md5=fc01532a457cf9a90102c920e3b050fa},
abstract={Cloud computing concept has taken prodigious growth over the last decade. With the vast options of Cloud Service Providers available nowadays and a variety of services and facilities to choose from, it is of paramount necessity to opt for the best cloud service provider based on multiple criteria and requirements ascertained by any organization or an individual. This study selects the cloud service provider based on various conflicting criteria. In this paper, pentagonal intuitionistic fuzzy number (PIFN) with MCDM tool analytic hierarchy process (AHP) and Technique for Order Preference by Similarity to Ideal Solution (TOPSIS) methods have been used to rank the Cloud Service Providers (CSPs). Firstly, the criteria PIFN weights are calculated using comparison matrices with the help of decision-makers (DMs), and then, FTOPSIS is done to obtain the final ranking. Sensitivity and comparative analyses have been conducted to see the changes in ranking obtained. These analyses help analyze the most sensitive criteria and thus help the researchers mark and evaluate for future scope and further research. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={AHP-TOPSIS method;  Cloud Service Providers selection;  Intuitionistic fuzzy number;  Multi-criterion decision-making method},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2023,
author={Wang, W. and Wu, S. and Li, Z. and Zhao, R.},
title={Parallel evolutionary test case generation for web applications},
journal={Information and Software Technology},
year={2023},
volume={155},
doi={10.1016/j.infsof.2022.107113},
art_number={107113},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145548932&doi=10.1016%2fj.infsof.2022.107113&partnerID=40&md5=1bc98ef9b754aa548ecdc3f6f63e55ae},
abstract={Context: Web applications follow a client–server schema, so it is more appropriate for evolutionary test case generation considering both client and server. However, test cases from the client-side are composed of event sequences, which are quite time-consuming when executed due to the interaction with the browser. Furthermore, premature convergence is a problem for evolutionary algorithms because of the decline of population diversity. These problems restrict the applicability of evolutionary algorithms in test case generation for web applications. Objective: Parallelization has been proven helpful in optimizing test case generation. So, to improve the efficiency and effectiveness of test generation for web applications, this paper proposes a parallel evolutionary test case generation approach where test cases are generated from the client-side behavior model to cover the sensitive paths of server-side code by using a parallel genetic algorithm based on the island model. Method: A parallel execution strategy is presented to drive multi-individuals to execute on multi-browsers simultaneously to shorten the execution time of populations during evolution. And an island model with a corresponding migration mechanism and subpopulation evolution strategy is well-designed to increase population diversity during evolution. Meanwhile, the server-side code triggered by parallel individuals is identified to guide the evolution process. Results: Experiments are conducted on six widely-used web applications, and the results show that compared with the sequential evolutionary test case generation, our approach decreases the iterations and evolution time required by 33.43% and 63.10% on average, respectively. The efficiency of test generation has been greatly enhanced. Conclusion: This paper provides a parallel evolutionary test case generation for web applications, where the parallel execution strategy is presented to shorten the execution time of populations during evolution, increasing test generation efficiency. Moreover, the island model with a migration mechanism is introduced to increase population diversity during evolution, improving the test generation effectiveness. © 2022 Elsevier B.V.},
author_keywords={Evolutionary algorithms;  Parallelization;  Test case generation;  Web applications},
document_type={Article},
source={Scopus},
}

@ARTICLE{Viggiato20231027,
author={Viggiato, M. and Paas, D. and Buzon, C. and Bezemer, C.-P.},
title={Identifying Similar Test Cases That Are Specified in Natural Language},
journal={IEEE Transactions on Software Engineering},
year={2023},
volume={49},
number={3},
pages={1027-1043},
doi={10.1109/TSE.2022.3170272},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129676358&doi=10.1109%2fTSE.2022.3170272&partnerID=40&md5=3bd38c856cf1f1f8b68b01b1475f48da},
abstract={Software testing is still a manual process in many industries, despite the recent improvements in automated testing techniques. As a result, test cases (which consist of one or more test steps that need to be executed manually by the tester) are often specified in natural language by different employees and many redundant test cases might exist in the test suite. This increases the (already high) cost of test execution. Manually identifying similar test cases is a time-consuming and error-prone task. Therefore, in this paper, we propose an unsupervised approach to identify similar test cases. Our approach uses a combination of text embedding, text similarity and clustering techniques to identify similar test cases. We evaluate five different text embedding techniques, two text similarity metrics, and two clustering techniques to cluster similar test steps and three techniques to identify similar test cases from the test step clusters. Through an evaluation in an industrial setting, we showed that our approach achieves a high performance to cluster test steps (an F-score of 87.39%) and identify similar test cases (an F-score of 86.13%). Furthermore, a validation with developers indicates several different practical usages of our approach (such as identifying redundant test cases), which help to reduce the testing manual effort and time. © 2022 IEEE.},
author_keywords={clustering;  Software testing;  test case similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Abbas202323,
author={Abbas, M. and Ferrari, A. and Shatnawi, A. and Enoiu, E. and Saadatmand, M. and Sundmark, D.},
title={On the relationship between similar requirements and similar software: A case study in the railway domain},
journal={Requirements Engineering},
year={2023},
volume={28},
number={1},
pages={23-47},
doi={10.1007/s00766-021-00370-4},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123067513&doi=10.1007%2fs00766-021-00370-4&partnerID=40&md5=4fe7276932619df532f4ebab93cd82f2},
abstract={Recommender systems for requirements are typically built on the assumption that similar requirements can be used as proxies to retrieve similar software. When a stakeholder proposes a new requirement, natural language processing (NLP)-based similarity metrics can be exploited to retrieve existing requirements, and in turn, identify previously developed code. Several NLP approaches for similarity computation between requirements are available. However, there is little empirical evidence on their effectiveness for code retrieval. This study compares different NLP approaches, from lexical ones to semantic, deep-learning techniques, and correlates the similarity among requirements with the similarity of their associated software. The evaluation is conducted on real-world requirements from two industrial projects from a railway company. Specifically, the most similar pairs of requirements across two industrial projects are automatically identified using six language models. Then, the trace links between requirements and software are used to identify the software pairs associated with each requirements pair. The software similarity between pairs is then automatically computed with JPLag. Finally, the correlation between requirements similarity and software similarity is evaluated to see which language model shows the highest correlation and is thus more appropriate for code retrieval. In addition, we perform a focus group with members of the company to collect qualitative data. Results show a moderately positive correlation between requirements similarity and software similarity, with the pre-trained deep learning-based BERT language model with preprocessing outperforming the other models. Practitioners confirm that requirements similarity is generally regarded as a proxy for software similarity. However, they also highlight that additional aspect comes into play when deciding software reuse, e.g., domain/project knowledge, information coming from test cases, and trace links. Our work is among the first ones to explore the relationship between requirements and software similarity from a quantitative and qualitative standpoint. This can be useful not only in recommender systems but also in other requirements engineering tasks in which similarity computation is relevant, such as tracing and change impact analysis. © 2022, The Author(s).},
author_keywords={Correlation;  Language models;  Perception of similarity;  Requirements similarity;  Software similarity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Pathak2023,
author={Pathak, K. and Patel, P. and Kamani, M. and Tiwari, S.},
title={Inclusivity Checker: A Testing Tool to Detect Inclusivity Bugs in Websites},
journal={ACM International Conference Proceeding Series},
year={2023},
doi={10.1145/3578527.3578547},
art_number={23},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149149685&doi=10.1145%2f3578527.3578547&partnerID=40&md5=6835683f18d656b92eafbff4fe0e2e08},
abstract={At the current speed of technological advancement, particularly with the pandemic driving the shift from physical to virtual, everyone must have access to the digital world. Guidelines for accessible technology exist to assist software engineers in creating websites accessible to disabled populations but are extensive and, therefore, difficult to follow completely. Additionally, no standard guidelines are available to address a wider range of human diversity beyond disabilities. In this paper, we present a browser extension tool, Inclusivity Checker, that can assist in building more inclusive websites by performing tests on checkpoints categorized into various user groups based on disability type and diversity parameters. These checkpoints are based on existing guidelines and present developers with errors, warnings, and tips. To gauge the efficiency of the developed tool, we evaluated the results manually. Furthermore, we collected the developers' feedback on the tool to test its usability. © 2023 ACM.},
author_keywords={Accessibility evaluation;  Human-Computer Interaction;  Inclusive Design;  Software Engineering},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Cheng2023,
author={Cheng, J. and Zhao, J. and Xu, W. and Zhang, T. and Xue, F. and Liu, S.},
title={Semantic Similarity-Based Mobile Application Isomorphic Graphical User Interface Identification},
journal={Mathematics},
year={2023},
volume={11},
number={3},
doi={10.3390/math11030527},
art_number={527},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147800950&doi=10.3390%2fmath11030527&partnerID=40&md5=5a9393c0051ebc535df1f0a021a2f28c},
abstract={Applying robots to mobile application testing is an emerging approach to automated black-box testing. The key to supporting automated robot testing is the efficient modeling of GUI elements. Since the application under testing often contains a large number of similar GUIs, the GUI model obtained often contains many redundant nodes. This causes the state space explosion of GUI models which has a serious effect on the efficiency of GUI testing. Hence, how to accurately identify isomorphic GUIs and construct quasi-concise GUI models are key challenges faced today. We thus propose a semantic similarity-based approach to identifying isomorphic GUIs for mobile applications. Using this approach, the information of GUI elements is first identified by deep learning network models, then, the GUI structure model feature vector and the semantic model feature vector are extracted and finally merged to generate a GUI embedding vector with semantic information. Finally, the isomorphic GUIs are identified by cosine similarity. Then, three experiments are conducted to verify the generalizability and effectiveness of the method. The experiments demonstrate that the proposed method can accurately identify isomorphic GUIs and shows high compatibility in terms of cross-platform and cross-device applications. © 2023 by the authors.},
author_keywords={isomorphic GUI;  mobile application testing;  semantic similarity},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Liu2023530,
author={Liu, J. and Lin, J. and Ruffy, F. and Tan, C. and Li, J. and Panda, A. and Zhang, L.},
title={NNSmith: Generating Diverse and Valid Test Cases for Deep Learning Compilers},
journal={International Conference on Architectural Support for Programming Languages and Operating Systems - ASPLOS},
year={2023},
volume={2},
pages={530-543},
doi={10.1145/3575693.3575707},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147732300&doi=10.1145%2f3575693.3575707&partnerID=40&md5=15d884799ec113800d0770bc2fe3e0ad},
abstract={Deep-learning (DL) compilers such as TVM and TensorRT are increasingly being used to optimize deep neural network (DNN) models to meet performance, resource utilization and other requirements. Bugs in these compilers can result in models whose semantics differ from the original ones, producing incorrect results that corrupt the correctness of downstream applications. However, finding bugs in these compilers is challenging due to their complexity. In this work, we propose a new fuzz testing approach for finding bugs in deep-learning compilers. Our core approach consists of (i) generating diverse yet valid DNN test models that can exercise a large part of the compiler's transformation logic using light-weight operator specifications; (ii) performing gradient-based search to find model inputs that avoid any floating-point exceptional values during model execution, reducing the chance of missed bugs or false alarms; and (iii) using differential testing to identify bugs. We implemented this approach in NNSmith which has found 72 new bugs for TVM, TensorRT, ONNXRuntime, and PyTorch to date. Of these 58 have been confirmed and 51 have been fixed by their respective project maintainers. © 2023 ACM.},
author_keywords={Compiler Testing;  Deep Learning Compilers;  Fuzzing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Jiang2023,
author={Jiang, Z. and Li, H. and Wang, R.},
title={Efficient generation of valid test inputs for deep neural networks via gradient search},
journal={Journal of Software: Evolution and Process},
year={2023},
doi={10.1002/smr.2550},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151400790&doi=10.1002%2fsmr.2550&partnerID=40&md5=49a821377571eae42b7a0db347ef2d5a},
abstract={The safety and robustness of deep neural networks (DNNs) are currently of great concern. Adequate testing is commonly an effective technique to ensure the software's trustworthiness. However, existing DNN testing methods generate many invalid test inputs, which inevitably brings increased computational overhead and reduces the efficiency of DNN testing. In this paper, we focus on testing task-specific DNN and investigating diverse, valid and natural test input generation based on data augmentation techniques. Specifically, we propose AugTest, a DNN testing method based on stochastic optimization with momentum, searching for optimal compositions of data augmentation parameters to efficiently generate diverse and valid test inputs. Experimental results show that our proposed method can effectively explore the data manifold space and find valid test inputs with high diversity and naturalness. Compared with the best-performing baseline, AugTest can generate more test inputs with more average diversity and less average time. Furthermore, the generated test inputs have competitive generalizability to DNNs with different structures. The test error rates exceed 70% when testing other DNN models performing similar tasks using the test inputs generated by AugTest. This implies that our method can produce more valid and generalized data to unveil DNNs' errors. © 2023 John Wiley & Sons Ltd.},
author_keywords={data augmentation;  diversity;  DNN testing;  gradient ascent;  valid test input},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tan2023,
author={Tan, S.H. and Li, Z. and Yan, L.},
title={CrossFix: Resolution of GitHub issues via similar bugs recommendation},
journal={Journal of Software: Evolution and Process},
year={2023},
doi={10.1002/smr.2554},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150981184&doi=10.1002%2fsmr.2554&partnerID=40&md5=93a81343821239283c4eb3ddbfc00795},
abstract={With the increasing popularity of Open-Source Software (OSS), the number of GitHub issues reported daily in these OSS projects has been growing rapidly. To resolve these issues, developers need to spend time and effort in debugging and fixing these issues. Meanwhile, a recent approach shows that similar bugs exist across different projects, and one could use the GitHub issues from a different project for finding new bugs for a related project. To locate similar bugs for our approach, we first conduct a study of similar bugs in GitHub. Our study redefines similar bugs as bugs that share the (1) same libraries, (2) same functionalities, (3) same reproduction steps, (4) same configurations, (5) same outcomes, or (6) same errors. Moreover, our study revealed the usefulness of similar bugs in helping developers to find more contexts about the bug and fixing. Based on our study, we design CrossFix, a tool that automatically suggests relevant GitHub issues based on an open GitHub issue. The suggested GitHub issues may contain solutions written in natural language or pull requests that help developers in resolving the given issue. Our evaluation on 249 open issues from Java and Android projects shows that CrossFix could suggest similar bugs to help developers in debugging and fixing. © 2023 John Wiley & Sons Ltd.},
author_keywords={debugging and program repair;  mining software repository;  open-source software},
document_type={Article},
source={Scopus},
}

@ARTICLE{Farmahinifarahani202312:1,
author={Farmahinifarahani, F. and Lopes, C.V.},
title={Similarity Detection for Neural Functions},
journal={Art, Science, and Engineering of Programming},
year={2023},
volume={7},
number={3},
pages={12:1-12:30},
doi={10.22152/PROGRAMMING-JOURNAL.ORG/2023/7/12},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150353448&doi=10.22152%2fPROGRAMMING-JOURNAL.ORG%2f2023%2f7%2f12&partnerID=40&md5=ed4462ca223343667c30b1528568a1b2},
abstract={Similarity, or clone, detection has important applications in copyright violation, software theft, code search, and the detection of malicious components. There is now a good number of open source and proprietary clone detectors for programs written in traditional programming languages. However, the increasing adoption of deep learning models in software poses a challenge to these tools: these models implement functions that are inscrutable black boxes. As more software includes these DNN functions, new techniques are needed in order to assess the similarity between deep learning components of software. Previous work has unveiled techniques for comparing the representations learned at various layers of deep neural network models by feeding canonical inputs to the models. Our goal is to be able to compare DNN functions when canonical inputs are not available – because they may not be in many application scenarios. The challenge, then, is to generate appropriate inputs and to identify a metric that, for those inputs, is capable of representing the degree of functional similarity between two comparable DNN functions. Our approach uses random input with values between −1 and 1, in a shape that is compatible with what the DNN models expect. We then compare the outputs by performing correlation analysis. Our study shows how it is possible to perform similarity analysis even in the absence of meaningful canonical inputs. The response to random inputs of two comparable DNN functions exposes those functions’ simi-larity, or lack thereof. Of all the metrics tried, we find that Spearman’s rank correlation coefficient is the most powerful and versatile, although in special cases other methods and metrics are more expressive. We present a systematic empirical study comparing the effectiveness of several similarity metrics using a dataset of 56, 355 classifiers collected from GitHub. This is accompanied by a sensitivity analysis that reveals how certain models’ training related properties affect the effectiveness of the similarity metrics. To the best of our knowledge, this is the first work that shows how similarity of DNN functions can be detected by using random inputs. Our study of correlation metrics, and the identification of Spearman correlation coefficient as the most powerful among them for this purpose, establishes a complete and practical method for DNN clone detection that can be used in the design of new tools. It may also serve as inspiration for other program analysis tasks whose approaches break in the presence of DNN components. © Farima Farmahinifarahani and Cristina V. Lopes This work is licensed under a “CC BY 4.0” license.},
author_keywords={CCA;  neural networks;  similarity detection;  Spearman},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Nguyen202397,
author={Nguyen, H.L. and Grunske, L.},
title={BeDivFuzz: Integrating Behavioral Diversity into Generator-based Fuzzing - Summary},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2023},
volume={P-332},
pages={97-98},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150073122&partnerID=40&md5=5982d767b346d94d05933b3fb2c22a40},
abstract={This paper summarizes our work”BeDivFuzz: Integrating Behavioral Diversity into Generator-based Fuzzing” [NG22], presented at the 44th International Conference on Software Engineering (ICSE 2022). © 2023 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={behavioral diversity;  random testing;  Structure-aware fuzzing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Vogel2023121,
author={Vogel, T. and Tran, C. and Grunske, L.},
title={A comprehensive empirical evaluation of generating test suites for mobile applications with diversity - Summary},
journal={Lecture Notes in Informatics (LNI), Proceedings - Series of the Gesellschaft fur Informatik (GI)},
year={2023},
volume={P-332},
pages={121-122},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150056452&partnerID=40&md5=a7a1dc9bc42f7b2249eb1586fe6419c9},
abstract={In this extended abstract, we summarize our work on analyzing the fitness landscape of the search-based app testing problem and building on that, improving and evaluating a specific solution for this problem. This work has been published under the title of “A comprehensive empirical evaluation of generating test suites for mobile applications with diversity” in the journal Information and Software Technology (IST) in 2021 [VTG21]. © 2023 Gesellschaft fur Informatik (GI). All rights reserved.},
author_keywords={Fitness landscape analysis;  Mobile apps;  Search-based testing},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ren2023770,
author={Ren, T. and Williams, R. and Ganguly, S. and De Carli, L. and Lu, L.},
title={Breaking Embedded Software Homogeneity with Protocol Mutations},
journal={Lecture Notes of the Institute for Computer Sciences, Social-Informatics and Telecommunications Engineering, LNICST},
year={2023},
volume={462 LNICST},
pages={770-790},
doi={10.1007/978-3-031-25538-0_40},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148039648&doi=10.1007%2f978-3-031-25538-0_40&partnerID=40&md5=b1366c7c2bb2f8e41fe551544241aa4d},
abstract={Network-connected embedded devices suffer from easy-to-exploit security issues. Due to code and platform reuse the same vulnerability oftentimes ends up affecting a large installed base. These circumstances enable destructive types of attacks, like ones in which compromised devices disrupt the power grid. We tackle an enabling factors of these attacks: software homogeneity. We propose techniques to inject syntax mutations in application-level network protocols used in the embedded/IoT space. Our approach makes it easy to diversify a protocol into syntactically different dialects, at the granularity of individual deployments. This form of moving-target defense disrupts batch compromise of devices, preventing reusable network exploits. Our approach identifies candidate program data structures and functions via a set of heuristics, mutate them via static transformations, and selects correctness-preserving mutations using dynamic testing. Evaluation on 4 popular protocols shows that we mitigate known exploitable vulnerabilities, while introducing no bugs. © 2023, ICST Institute for Computer Sciences, Social Informatics and Telecommunications Engineering.},
author_keywords={MTD;  Protocol mutations;  Software diversity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Hoffmann2023211,
author={Hoffmann, M. and Mendez, D. and Fagerholm, F. and Luckhardt, A.},
title={The Human Side of Software Engineering Teams: An Investigation of Contemporary Challenges},
journal={IEEE Transactions on Software Engineering},
year={2023},
volume={49},
number={1},
pages={211-225},
doi={10.1109/TSE.2022.3148539},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124763511&doi=10.1109%2fTSE.2022.3148539&partnerID=40&md5=32ebd8e9ac6f8e09c5eea4f3f839a667},
abstract={Context: There have been numerous recent calls for research on the human side of software engineering and its impact on various factors such as productivity, developer happiness and project success. An analysis of which challenges in software engineering teams are most frequent is still missing. As teams are more international, it is more frequent that their members have different human values as well as different communication habits. Additionally, virtual team setups (working geographically separated, remote communication using digital tools and frequently changing team members) are increasingly prevalent. Objective: We aim to provide a starting point for a theory about contemporary human challenges in teams and their causes in software engineering. To do so, we look to establish a reusable set of challenges and start out by investigating the effect of team virtualization. Virtual teams often use digital communication and consist of members with different nationalities that may have more divergent human values due to cultural differences compared to single nationality teams. Method: We designed a survey instrument and asked respondents to assess the frequency and criticality of a set of challenges, separated in context 'within teams' as well as 'between teams and clients', compiled from previous empirical work, blog posts, and pilot survey feedback. For the team challenges, we asked if mitigation measures were already in place to tackle the challenge. Respondents were also asked to provide information about their team setup. The survey included the Personal Value Questionnaire to measure Schwartz human values. Finally, respondents were asked if there were additional challenges at their workplace. The survey was first piloted and then distributed to professionals working in software engineering teams via social networking sites and personal business networks. Result: In this article, we report on the results obtained from 192 respondents. We present a set of challenges that takes the survey feedback into account and introduce two categories of challenges; 'interpersonal' and 'intrapersonal'. We found no evidence for links between human values and challenges. We found some significant links between the number of distinct nationalities in a team and certain challenges, with less frequent and critical challenges occurring if 2-3 different nationalities were present compared to a team having members of just one nationality or more than three. A higher degree of virtualization seems to increase the frequency of some human challenges, which warrants further research about how to improve working processes when teams work from remote or in a distributed fashion. Conclusion: We present a set of human challenges in software engineering that can be used for further research on causes and mitigation measures, which serves as our starting point for a theory about causes of contemporary human challenges in software engineering teams. We report on evidence that a higher degree of virtualization of teams leads to an increase of certain challenges. This warrants further research to gather more evidence and test countermeasures, such as whether the employment of virtual reality software incorporating facial expressions and movements can help establish a less detached way of communication. © 2022 IEEE.},
author_keywords={diversity;  human challenges;  human values;  Software engineering;  survey research;  virtual teams},
document_type={Article},
source={Scopus},
}

@ARTICLE{Wang2023226,
author={Wang, H. and Ma, P. and Yuan, Y. and Liu, Z. and Wang, S. and Tang, Q. and Nie, S. and Wu, S.},
title={Enhancing DNN-Based Binary Code Function Search with Low-Cost Equivalence Checking},
journal={IEEE Transactions on Software Engineering},
year={2023},
volume={49},
number={1},
pages={226-250},
doi={10.1109/TSE.2022.3149240},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124742592&doi=10.1109%2fTSE.2022.3149240&partnerID=40&md5=04efb7ba39cf35bc35b1decaa0f07252},
abstract={Binary code function search has been used as the core basis of various security and software engineering applications, including malware clustering, code clone detection, and vulnerability audits. Recognizing logically similar assembly functions, however, remains a challenge. Most binary code search tools rely on program structure-level information, such as control flow and data flow graphs, that is extracted using program analysis techniques or deep neural networks (DNNs). However, DNN-based techniques capture lexical-, control structure-, or data flow-level information of binary code for representation learning, which is often too coarse-grained and does not accurately denote program functionality. Additionally, it may exhibit low robustness to a variety of challenging settings, such as compiler optimizations and obfuscations. This paper proposes a general solution for enhancing the top-$k$k ranked candidates in DNN-based binary code function search. The key idea is to design a low-cost and comprehensive equivalence check that quickly exposes functionality deviations between the target function and its top-$k$k matched functions. Functions that fail this equivalence check can be shaved from the top-$k$k list, and functions that pass the check can be revisited to move ahead on the top-$k$k ranked candidates, in a deliberate way. We design a practical and efficient equivalence check, named BinUSE, using under-constrained symbolic execution (USE). USE, a variant of symbolic execution, improves scalability by initiating symbolic execution directly from function entry points and relaxing constraints on function parameters. It eliminates the overhead incurred by path explosion and costly constraints. BinUSE is specifically designed to deliver an assembly function-level equivalence check, enhancing DNN-based binary code search by reducing its false alarms with low cost. Our evaluation shows that BinUSE can enable a general and effective enhancement of four state-of-the-art DNN-based binary code search tools when confronted with challenges posed by different compilers, optimizations, obfuscations, and architectures. © 2022 IEEE.},
author_keywords={deep learning;  Reverse engineering;  software similarity;  symbolic execution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cao2022738,
author={Cao, H. and He, Z. and Meng, Y. and Chu, Y.},
title={Automatic Repair of Java Programs Weighted Fusion Similarity via Genetic Programming},
journal={Information Technology and Control},
year={2022},
volume={51},
number={4},
pages={738-756},
doi={10.5755/j01.itc.51.4.30515},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143790501&doi=10.5755%2fj01.itc.51.4.30515&partnerID=40&md5=b4726f8bb465e3fd5c24c88cdbd60c44},
abstract={Recently, automated program repair techniques have been proven to be useful in the process of software devel-opment. However, how to reduce the large search space and the random of ingredient selection is still a chal-lenging problem. In this paper, we propose a repair approach for buggy program based on weighted fusion similarity and genetic programming. Firstly, the list of modification points is generated by selecting modification points from the suspicious statements. Secondly, the buggy repair ingredient is selected according to the value of the weighted fusion similarity, and the repair ingredient is applied to the corresponding modification points according to the selected operator. Finally, we use the test case execution information to prioritize the test cas-es to improve individual verification efficiency. We have implemented our approach as a tool called WSGRe-pair. We evaluate WSGRepair in Defects4J and compare with other program repair techniques. Experimental results show that our approach improve the success rate of buggy program repair by 28.6%, 64%, 29%, 64% and 112% compared with the GenProg, CapGen, SimFix, jKali and jMutRepair. © 2022, Kauno Technologijos Universitetas. All rights reserved.},
author_keywords={automated program repair;  code similarity;  genetic programming;  test case prioritization},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ahn2022361,
author={Ahn, S. and Ahn, S. and Koo, H. and Paek, Y.},
title={Practical Binary Code Similarity Detection with BERT-based Transferable Similarity Learning},
journal={ACM International Conference Proceeding Series},
year={2022},
pages={361-374},
doi={10.1145/3564625.3567975},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144069279&doi=10.1145%2f3564625.3567975&partnerID=40&md5=a6bc7f1f339ead536b850c0f1512d107},
abstract={Binary code similarity detection (BCSD) serves as a basis for a wide spectrum of applications, including software plagiarism, malware classification, and known vulnerability discovery. However, the inference of contextual meanings of a binary is challenging due to the absence of semantic information available in source codes. Recent advances leverage the benefits of a deep learning architecture into a better understanding of underlying code semantics and the advantages of the Siamese architecture into better BCSD. In this paper, we propose BinShot, a BERT-based similarity learning architecture that is highly transferable for effective BCSD. We tackle the problem of detecting code similarity with one-shot learning (a special case of few-shot learning). To this end, we adopt a weighted distance vector with a binary cross entropy as a loss function on top of BERT. With the prototype of BinShot, our experimental results demonstrate the effectiveness, transferability, and practicality of BinShot, which is robust to detecting the similarity of previously unseen functions. We show that BinShot outperforms the previous state-of-the-art approaches for BCSD. © 2022 ACM.},
author_keywords={Binary Analysis;  Deep Neural Network;  Similarity Detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Zhang2022,
author={Zhang, W. and Xu, Z. and Xiao, Y. and Xue, Y.},
title={Unleashing the power of pseudo-code for binary code similarity analysis},
journal={Cybersecurity},
year={2022},
volume={5},
number={1},
doi={10.1186/s42400-022-00121-0},
art_number={23},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142935849&doi=10.1186%2fs42400-022-00121-0&partnerID=40&md5=e7d62d73688c7368f4eb9ce453fce325},
abstract={Code similarity analysis has become more popular due to its significant applicantions, including vulnerability detection, malware detection, and patch analysis. Since the source code of the software is difficult to obtain under most circumstances, binary-level code similarity analysis (BCSA) has been paid much attention to. In recent years, many BCSA studies incorporating AI techniques focus on deriving semantic information from binary functions with code representations such as assembly code, intermediate representations, and control flow graphs to measure the similarity. However, due to the impacts of different compilers, architectures, and obfuscations, binaries compiled from the same source code may vary considerably, which becomes the major obstacle for these works to obtain robust features. In this paper, we propose a solution, named UPPC (Unleashing the Power of Pseudo-code), which leverages the pseudo-code of binary function as input, to address the binary code similarity analysis challenge, since pseudo-code has higher abstraction and is platform-independent compared to binary instructions. UPPC selectively inlines the functions to capture the full function semantics across different compiler optimization levels and uses a deep pyramidal convolutional neural network to obtain the semantic embedding of the function. We evaluated UPPC on a data set containing vulnerabilities and a data set including different architectures (X86, ARM), different optimization options (O0-O3), different compilers (GCC, Clang), and four obfuscation strategies. The experimental results show that the accuracy of UPPC in function search is 33.2% higher than that of existing methods. © 2022, The Author(s).},
author_keywords={Binary code similarity;  Machine learning;  Pseudo-code;  Software security},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jalal20226461,
author={Jalal, A.A. and Jasim, A.A. and Mahawish, A.A.},
title={A web content mining application for detecting relevant pages using Jaccard similarity},
journal={International Journal of Electrical and Computer Engineering},
year={2022},
volume={12},
number={6},
pages={6461-6471},
doi={10.11591/ijece.v12i6.pp6461-6471},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139044936&doi=10.11591%2fijece.v12i6.pp6461-6471&partnerID=40&md5=0d3ee598a58205607dd7f780056a8e7d},
abstract={The tremendous growth in the availability of enormous text data from a variety of sources creates a slew of concerns and obstacles to discovering meaningful information. This advancement of technology in the digital realm has resulted in the dispersion of texts over millions of web sites. Unstructured texts are densely packed with textual information. The discovery of valuable and intriguing relationships in unstructured texts demands more computer processing. So, text mining has developed into an attractive area of study for obtaining organized and useful data. One of the purposes of this research is to discuss text pre-processing of automobile marketing domains in order to create a structured database. Regular expressions were used to extract data from unstructured vehicle advertisements, resulting in a well-organized database. We manually develop unique rule-based ways of extracting structured data from unstructured web pages. As a result of the information retrieved from these advertisements, a systematic search for certain noteworthy qualities is performed. There are numerous approaches for query recommendation, and it is vital to understand which one should be employed. Additionally, this research attempts to determine the optimal value similarity for query suggestions based on user-supplied parameters by comparing MySQL pattern matching and Jaccard similarity. © 2022 Institute of Advanced Engineering and Science. All rights reserved.},
author_keywords={Data mining;  Jaccard similarity;  Query suggestions;  Text mining;  Web content mining},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gouveia2022,
author={Gouveia, I.P. and Völp, M. and Esteves-Verissimo, P.},
title={Behind the last line of defense: Surviving SoC faults and intrusions},
journal={Computers and Security},
year={2022},
volume={123},
doi={10.1016/j.cose.2022.102920},
art_number={102920},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138450964&doi=10.1016%2fj.cose.2022.102920&partnerID=40&md5=33c0fa2002239e635e58c2a1be3166bd},
abstract={Today, leveraging the enormous modular power, diversity and flexibility of manycore systems-on-a-chip (SoCs) requires careful orchestration of complex and heterogeneous resources, a task left to low-level software, e.g., hypervisors. In current architectures, this software forms a single point of failure and worthwhile target for attacks: once compromised, adversaries can gain access to all information and full control over the platform and the environment it controls. This article proposes Midir, an enhanced manycore architecture, effecting a paradigm shift from SoCs to distributed SoCs. Midir changes the way platform resources are controlled, by retrofitting tile-based fault containment through well known mechanisms, while securing low-overhead quorum-based consensus on all critical operations, in particular privilege management and, thus, management of containment domains. Allowing versatile redundancy management, Midir promotes resilience for all software levels, including at low level. We explain this architecture, its associated algorithms and hardware mechanisms and show, for the example of a Byzantine fault tolerant microhypervisor, that it outperforms the highly efficient MinBFT by one order of magnitude. © 2022},
author_keywords={Fault and intrusion tolerance;  Hypervisor;  MPSoCs;  Processor architecture;  Reliability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yan2022,
author={Yan, D. and Liu, K. and Niu, Y. and Li, L. and Liu, Z. and Liu, Z. and Klein, J. and Bissyandé, T.F.},
title={CREX: Predicting patch correctness in automated repair of C programs through transfer learning of execution semantics},
journal={Information and Software Technology},
year={2022},
volume={152},
doi={10.1016/j.infsof.2022.107043},
art_number={107043},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136634004&doi=10.1016%2fj.infsof.2022.107043&partnerID=40&md5=7dcc642bc3e1f8c5e2cc0fdeffd174dd},
abstract={A significant body of automated program repair literature relies on test suites to assess the validity of generated patches. Because such oracles are weak, state-of-the-art repair tools can validate some patches that overfit the test cases but are actually incorrect. This situation has become a prime concern in APR, hindering its adoption by the industry. This work investigates execution semantic features based on micro-traces, a form of under-constrained dynamic traces. We build on transfer learning to explore function code representations that are amenable to semantic similarity computation and can therefore be leveraged for classifying patch correctness. Our CREX prototype implementation is based on the TREX framework. Experimental results on patches generated by the CoCoNut APR tool on CodeFlaws programs indicate that our approach can yield high accuracy in predicting patch correctness. The learned embeddings were proven to capture semantic similarities between functions, which was instrumental in training a classifier that identifies patch correctness by learning to discriminate between correctly patched code and incorrectly patched code based on their semantic similarity with the buggy function. © 2022 Elsevier B.V.},
author_keywords={Patch correctness;  Program repair;  Semantic feature;  Transfer learning},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhang2022,
author={Zhang, M. and Yang, L. and Hu, H. and Liu, T. and Wang, J.},
title={Efficient index-free SimRank similarity search in large graphs by discounting path lengths},
journal={Expert Systems with Applications},
year={2022},
volume={206},
doi={10.1016/j.eswa.2022.117746},
art_number={117746},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132771122&doi=10.1016%2fj.eswa.2022.117746&partnerID=40&md5=539dbc618594f02639dde347a60e063b},
abstract={Link-based similarity search aims to find similar nodes for a given query node in a graph, which arises in numerous applications, including web spam detection, social network analysis and web search. Among existing methods, SimRank is a well-known similarity model, which provides an effective and trustful function for similarity search. A large amount of techniques on SimRank similarity search are devoted recently, which compute the similarity scores by traversing the paths between query and candidate nodes. However, the number of paths increases exponentially as path length increases, which makes the computation expensive and cannot support fast similarity search over large graphs. In this paper, we propose an efficient index-free SimRank similarity search approach, namely DisSim, which reduces the computational cost by discounting path length. We observe that SimRank could rapidly converge at a stable state and the results change little after a few of iterations. Based on the fast convergence, the similarity between nodes is defined as the SimRank score at the second iteration. For the computation of DisSim, we divide the similarity into one-step and two-step first-meeting probabilities. The one-step first-meeting probabilities are computed by path traverses from query to candidate nodes, which reduces computational cost by skipping unnecessary nodes. And the two-step first-meeting probabilities are computed by integrating the repeated parts of the paths. For further speeding up query processing, we develop a pruning algorithm, which prunes unpromising path traverses by setting a threshold, and the accuracy loss under threshold is given through mathematical analysis. Extensive experiments on real graphs demonstrate the performance of DisSim through comparing with the state-of-the-art algorithms. © 2022 Elsevier Ltd},
author_keywords={Graph;  Index-free algorithm;  Similarity search;  SimRank},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Smytzek20221701,
author={Smytzek, M. and Zeller, A.},
title={SFLKit: a workbench for statistical fault localization},
journal={ESEC/FSE 2022 - Proceedings of the 30th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
year={2022},
pages={1701-1705},
doi={10.1145/3540250.3558915},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143077506&doi=10.1145%2f3540250.3558915&partnerID=40&md5=34b0ec88bf48ca2cb5b93df69e118a6f},
abstract={Statistical fault localization aims at detecting execution features that correlate with failures, such as whether individual lines are part of the execution. We introduce SFLKit, an out-of-the-box workbench for statistical fault localization. The framework provides straightforward access to the fundamental concepts of statistical fault localization. It supports five predicate types, four coverage-inspired spectra, like lines, and 44 similarity coefficients, e.g., TARANTULA or OCHIAI, for statistical program analysis. SFLKit separates the execution of tests from the analysis of the results and is therefore independent of the used testing framework. It leverages program instrumentation to enable the logging of events and derives the predicates and spectra from these logs. This instrumentation allows for introducing multiple programming languages and the extension of new concepts in statistical fault localization. Currently, SFLKit supports the instrumentation of Python programs. It is highly configurable, requiring only the logging of the required events. © 2022 ACM.},
author_keywords={similarity coefficient;  spectrum-based fault localization;  statistical debugging;  statistical fault localization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Xiang2022,
author={Xiang, Y. and Yang, X. and Huang, H. and Huang, Z. and Li, M.},
title={Sampling configurations from software product lines via probability-aware diversification and SAT solving},
journal={Automated Software Engineering},
year={2022},
volume={29},
number={2},
doi={10.1007/s10515-022-00348-8},
art_number={54},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137542951&doi=10.1007%2fs10515-022-00348-8&partnerID=40&md5=0867d5470645308aa9a7f3c73fe72aca},
abstract={Sampling a small, valid and representative set of configurations from software product lines (SPLs) is important, yet challenging due to a huge number of possible configurations to be explored. Recently, the sampling strategy based on satisfiability (SAT) solving has enjoyed great popularity due to its high efficiency and good scalability. However, this sampling offers no guarantees on diversity, especially in terms of the number of selected features, an important property to characterize a configuration. In this paper, we propose a probability-aware diversification (PaD) strategy to cooperate with SAT solving in generating diverse configurations, with the effect that valid configurations are efficiently generated by SAT solving while also maintaining diversity brought by PaD. Experimental results on 51 public SPLs show that, when working cooperatively with PaD, the performance (regarding diversity) of off-the-shelf SAT solvers has substantial improvements, with large effect sizes observed on more than 71% of all the cases. Furthermore, we propose a general search-based framework where PaD and evolutionary algorithms can work together, and instantiate this framework in the context of search-based diverse sampling and search-based multi-objective SPL configuration (where there is a practical need of generating diverse configurations). It is demonstrated by the experimental results that PaD also brings abundant performance gains to these search-based approaches. Finally, we apply PaD to a practical problem, i.e., machine learning based performance predictions of SPLs, and show that using PaD tends to improve the accuracy of performance prediction models. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Diverse sampling;  Probability-aware diversification;  SAT solving;  Software product lines},
document_type={Article},
source={Scopus},
}

@ARTICLE{Kessel2022,
author={Kessel, M. and Atkinson, C.},
title={Diversity-driven unit test generation},
journal={Journal of Systems and Software},
year={2022},
volume={193},
doi={10.1016/j.jss.2022.111442},
art_number={111442},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136455761&doi=10.1016%2fj.jss.2022.111442&partnerID=40&md5=1d131432993ca103be1c1ee53d002a03},
abstract={The goal of automated unit test generation tools is to create a set of test cases for the software under test that achieve the highest possible coverage for the selected test quality criteria. The most effective approaches for achieving this goal at the present time use meta-heuristic optimization algorithms to search for new test cases using fitness functions defined on existing sets of test cases and the system under test. Regardless of how their search algorithms are controlled, however, all existing approaches focus on the analysis of exactly one implementation, the software under test, to drive their search processes, which is a limitation on the information they have available. In this paper we investigate whether the practical effectiveness of white box unit test generation tools can be increased by giving them access to multiple, diverse implementations of the functionality under test harvested from widely available Open Source software repositories. After presenting a basic implementation of such an approach, DivGen (Diversity-driven Generation), on top of the leading test generation tool for Java (EvoSuite), we assess the performance of DivGen compared to EvoSuite when applied in its traditional, mono-implementation oriented mode (MonoGen). The results show that while DivGen outperforms MonoGen in 33% of the sampled classes for mutation coverage (+16% higher on average), MonoGen outperforms DivGen in 12.4% of the classes for branch coverage (+10% higher average). © 2022 Elsevier Inc.},
author_keywords={Automation;  Behavior;  Diversity;  Evaluation;  Experiment;  Test amplification;  Test generation;  Test quality},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mahdieh2022,
author={Mahdieh, M. and Mirian-Hosseinabadi, S.-H. and Mahdieh, M.},
title={Test case prioritization using test case diversification and fault-proneness estimations},
journal={Automated Software Engineering},
year={2022},
volume={29},
number={2},
doi={10.1007/s10515-022-00344-y},
art_number={50},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135276036&doi=10.1007%2fs10515-022-00344-y&partnerID=40&md5=db4d6cd9a6d19ed9273882a54c64830b},
abstract={Regression testing activities greatly reduce the risk of faulty software release. However, the size of the test suites grows throughout the development process, resulting in time-consuming execution of the test suite and delayed feedback to the software development team. This has urged the need for approaches such as test case prioritization (TCP) and test-suite reduction to reach better results in case of limited resources. In this regard, proposing approaches that use auxiliary sources of data such as bug history can be interesting. We aim to propose an approach for TCP that takes into account test case coverage data, bug history, and test case diversification. To evaluate this approach we study its performance on real-world open-source projects. The bug history is used to estimate the fault-proneness of source code areas. The diversification of test cases is preserved by incorporating fault-proneness on a clustering-based approach scheme. The proposed methods are evaluated on datasets collected from the development history of five real-world projects including 357 versions in total. The experiments show that the proposed methods are superior to coverage-based TCP methods. The proposed approach shows that improvement of coverage-based and fault-proneness-based methods is possible by using a combination of diversification and fault-proneness incorporation. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={Bug History;  Defect prediction;  Regression testing;  Test case diversification;  Test case prioritization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tang20224411,
author={Tang, Y. and Jiang, H. and Zhou, Z. and Li, X. and Ren, Z. and Kong, W.},
title={Detecting Compiler Warning Defects Via Diversity-Guided Program Mutation},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={11},
pages={4411-4432},
doi={10.1109/TSE.2021.3119186},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117258535&doi=10.1109%2fTSE.2021.3119186&partnerID=40&md5=4025186df049466da0524fb0f2c5901a},
abstract={Compiler diagnostic warnings help developers identify potential programming mistakes during program compilation. However, these warnings could be erroneous due to the defects of compiler warning diagnostics. Although the existing technique (i.e., Epiphron) can automatically generate test programs for compiler warning defect detection, the effectiveness of Epiphron on defect-finding is still limited, due to the limitation for generating warning-sensitive test program structures. Therefore, in this paper, we propose a DIversity-guided PROgram Mutation approach, called DIPROM, to construct diverse warning-sensitive programs for effective compiler warning defect detection. Given a seed test program, DIPROM first removes its dead code to reduce false positive warning defects. Then, the abstract syntax tree (AST) of the test program is constructed; DIPROM iteratively mutates the structures of the AST to generate warning-sensitive program variants. To effectively construct diverse warning-sensitive structures, DIPROM applies a novel diversity-guided strategy to generate program variants in each iteration. With the generated program variants, differential testing is conducted to detect warning defects in different compilers. In the experiments, we evaluate DIPROM with two popular C compilers (i.e., GCC and Clang). Experimental results show that DIPROM significantly outperforms three state-of-the-art approaches (i.e., HiCOND, Epiphron, and Hermes) by up to 18.93%∼76.74% in terms of the bug-finding capability on average. Meanwhile, DIPROM is efficient, which spends less time on finding the same average number of warning defects. We at last applied DIPROM to the latest development versions of GCC and Clang. After two months' running, we reported 8 new warning defects; 5 of them have been confirmed/fixed by developers. © 1976-2012 IEEE.},
author_keywords={Compiler testing;  differential testing;  program mutation;  test program generation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yu2022,
author={Yu, Y. and Huang, Z. and Shen, G. and Li, W. and Shao, Y.},
title={ASTENS-BWA: Searching partial syntactic similar regions between source code fragments via AST-based encoded sequence alignment},
journal={Science of Computer Programming},
year={2022},
volume={222},
doi={10.1016/j.scico.2022.102839},
art_number={102839},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133383897&doi=10.1016%2fj.scico.2022.102839&partnerID=40&md5=45565acadbdae7dc26dfe5448ad9e53f},
abstract={Code search is a common activity in software development, and code-to-code search can benefit in a wide range of use-case scenarios. Code-to-code search uses a code fragment as the query for searching similar code fragments from large corpora. The results of a search can be applied to some software engineering tasks, such as search-based code recommendation, data-driven program repairing, and software plagiarism detection. To be put into daily use, the code-to-code search needs to find similar code fragments accurately and efficiently in a large dataset. Some search engines can locate exactly similar code, but are not able to search syntactical clones. Therefore, we propose ASTENS-BWA, a novel approach for searching syntactic similar code regions between code fragments via a tree-based sequence alignment. Source code has been transformed into a tree-based sequence that contains the structure information, and a sequence alignment algorithm has been applied to find similar regions. We evaluate ASTENS-BWA on three different tasks, the results demonstrate that our approach can find syntactical similar regions for programming code and retrieve similar code fragments fast and with high accuracy. As a code clone detection tool, ASTENS-BWA can report clone pairs in a high recall, but it needs manually check to reduce the false alarms. ASTENS-BWA is scalable and can report cloned code fragments in seconds for a code corpus of million lines of code. © 2022 Elsevier B.V.},
author_keywords={Abstract syntax tree;  Code clone search;  Code representation model;  Sequence alignment algorithm;  Syntactical similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Zhao2022,
author={Zhao, C. and Mu, Y. and Chen, X. and Zhao, J. and Ju, X. and Wang, G.},
title={Can test input selection methods for deep neural network guarantee test diversity? A large-scale empirical study},
journal={Information and Software Technology},
year={2022},
volume={150},
doi={10.1016/j.infsof.2022.106982},
art_number={106982},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132739978&doi=10.1016%2fj.infsof.2022.106982&partnerID=40&md5=fdb7583bb37e2b151e6dc57157c859f0},
abstract={Context: Recently, various methods on test input selection for deep neural network (TIS-DNN) have been proposed. These methods can effectively reduce the labeling cost by selecting a subset from the original test inputs, which can still accurately estimate the performance (such as accuracy) of the target DNN models. Objective: Previous studies on TIS-DNN mainly focused on the performance on all the classes. However, the selected subset may miss the coverage of some classes or decrease the performance on some classes, which will reduce the test diversity of the original test inputs. Methods: Therefore, we conducted a large-scale empirical study to investigate whether previous TIS-DNN methods can guarantee test diversity in the subset. In our study, we selected five state-of-the-art TIS-DNN methods: SRS, CSS, CES, DeepReduce and PACE. Then we selected 18 pairs of DNN models and the corresponding test inputs from seven popular DNN datasets. Results: Our experimental results can be summarized as follows. (1) Previous TIS-DNN methods can guarantee the performance on all the classes. However, these methods have a negative impact on the test diversity and the performance on each class is not satisfactory. (2) Reducing the performance estimation error on each class can help reduce the estimation error on the test adequacy of the original inputs based on DNN-based coverage criteria (especially for the criterion NC and the criterion TKNC). (3) There still exists great room for performance improvement (i.e., 7.637% improvement on all the classes and 12.833% improvement on each class) after comparing the TIS-DNN method PACE with approximately optimal solutions. Conclusion: The above experimental findings implicate there is still a long way for the TIS-DNN issue to go. Given this, we present observations about the road ahead for this issue. © 2022 Elsevier B.V.},
author_keywords={Deep neural network testing;  Empirical study;  Test diversity;  Test input selection},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mehrotra20223771,
author={Mehrotra, N. and Agarwal, N. and Gupta, P. and Anand, S. and Lo, D. and Purandare, R.},
title={Modeling Functional Similarity in Source Code With Graph-Based Siamese Networks},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={10},
pages={3771-3789},
doi={10.1109/TSE.2021.3105556},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113236624&doi=10.1109%2fTSE.2021.3105556&partnerID=40&md5=23c76e79cd41b745e2448d8240e34e32},
abstract={Code clones are duplicate code fragments that share (nearly) similar syntax or semantics. Code clone detection plays an important role in software maintenance, code refactoring, and reuse. A substantial amount of research has been conducted in the past to detect clones. A majority of these approaches use lexical and syntactic information to detect clones. However, only a few of them target semantic clones. Recently, motivated by the success of deep learning models in other fields, including natural language processing and computer vision, researchers have attempted to adopt deep learning techniques to detect code clones. These approaches use lexical information (tokens) and(or) syntactic structures like abstract syntax trees (ASTs) to detect code clones. However, they do not make sufficient use of the available structural and semantic information, hence limiting their capabilities. This paper addresses the problem of semantic code clone detection using program dependency graphs and geometric neural networks, leveraging the structured syntactic and semantic information. We have developed a prototype tool Holmes, based on our novel approach and empirically evaluated it on popular code clone benchmarks. Our results show that Holmes performs considerably better than the other state-of-the-art tool, TBCCD. We also assessed Holmes on unseen projects and performed cross dataset experiments to evaluate the generalizability of Holmes. Our results affirm that Holmes outperforms TBCCD since most of the pairs that Holmes detected were either undetected or suboptimally reported by TBCCD. © 1976-2012 IEEE.},
author_keywords={graph-based neural networks;  program dependency graphs;  Program representation learning;  semantic code clones;  siamese neural networks},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ghanbari2022,
author={Ghanbari, A. and Marcus, A.},
title={Shibboleth: Hybrid Patch Correctness Assessment in Automated Program Repair},
journal={ACM International Conference Proceeding Series},
year={2022},
doi={10.1145/3551349.3559519},
art_number={166},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146968803&doi=10.1145%2f3551349.3559519&partnerID=40&md5=d83486ce360bf95630e7d6cedcd6ee6e},
abstract={Test-based generate-and-validate automated program repair (APR) systems generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, a task that tends to be time-consuming, thereby diminishing the role of APR in reducing debugging costs. We present the design and implementation of a novel tool, named Shibboleth, for automatic assessment of the patches generated by test-based generate-and-validate APR systems. Shibboleth leverages lightweight static and dynamic heuristics from both test and production code to rank and classify the patches. Shibboleth is based on the idea that the buggy program is almost correct and the bugs are small mistakes that require small changes to fix and specifically the fix does not remove the code implementing correct functionality of the program. Thus, the tool measures the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage) to separate the patches that result in similar programs and that do not remove desired program elements. We have evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art raking and classification techniques. Specifically, in our ranking data set, in 66% of the cases, Shibboleth ranks the correct patch in top-1 or top-2 positions and, in our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively, in classification mode. A demo video of the tool is available at https://bit.ly/3NvYJN8. © 2022 ACM.},
author_keywords={Automated Program Repair;  Branch Coverage;  Patch Correctness Assessment;  Similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Priamo2022,
author={Priamo, G. and D'Elia, D.C. and Querzoni, L.},
title={Principled Composition of Function Variants for Dynamic Software Diversity and Program Protection},
journal={ACM International Conference Proceeding Series},
year={2022},
doi={10.1145/3551349.3559553},
art_number={183},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146952001&doi=10.1145%2f3551349.3559553&partnerID=40&md5=71534e655800e825cbd79570cc66edc5},
abstract={Artificial diversification of a software program can be a versatile tool in a wide range of software engineering and security scenarios. For example, randomizing implementation aspects can increase the costs for attackers as it prevents them from benefiting of precise knowledge of their target. A promising angle for diversification can be having two runs of a program on the same input yield inherently diverse instruction traces. Inspired by on-stack replacement designs for managed runtimes, in this paper we study how to transform a C program to realize continuous transfers of control and program state among function variants as they run. We discuss the technical challenges toward such goal and propose effective compiler techniques for it that enable the re-use of existing techniques for static diversification with no modifications. We implement our approach in LLVM and evaluate it on both synthetic and real-world subjects. © 2022 ACM.},
author_keywords={hardening;  obfuscation;  on-stack replacement.;  Software diversity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xie2022,
author={Xie, X. and Yin, P. and Chen, S.},
title={Boosting the Revealing of Detected Violations in Deep Learning Testing: A Diversity-Guided Method},
journal={ACM International Conference Proceeding Series},
year={2022},
doi={10.1145/3551349.3556919},
art_number={17},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146951528&doi=10.1145%2f3551349.3556919&partnerID=40&md5=8633d112f342d055a22cc652a5878255},
abstract={Due to the ability to bypass the oracle problem, Metamorphic Testing (MT) has been a popular technique to test deep learning (DL) software. However, no work has taken notice of the prioritization for Metamorphic test case Pairs (MPs), which is quite essential and beneficial to the effectiveness of MT in DL testing. When the fault-sensitive MPs apt to trigger violations and expose defects are not prioritized, the revealing of some detected violations can be greatly delayed or even missed to conceal critical defects. In this paper, we propose the first method to prioritize the MPs for DL software, so as to boost the revealing of detected violations in DL testing. Specifically, we devise a new type of metric to measure the execution diversity of DL software on MPs based on the distribution discrepancy of the neuron outputs. The fault-sensitive MPs are next prioritized based on the devised diversity metric. Comprehensive evaluation results show that the proposed prioritization method and diversity metric can effectively prioritize the fault-sensitive MPs, boost the revealing of detected violations, and even facilitate the selection and design of the effective Metamorphic Relations for the image classification DL software. © 2022 ACM.},
author_keywords={deep learning testing;  diversity;  metamorphic testing;  test case selection and prioritization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Fontaine2022,
author={Fontaine, M.C. and Nikolaidis, S.},
title={Evaluating Human-Robot Interaction Algorithms in Shared Autonomy via Quality Diversity Scenario Generation},
journal={ACM Transactions on Human-Robot Interaction},
year={2022},
volume={11},
number={3},
doi={10.1145/3476412},
art_number={25},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130800914&doi=10.1145%2f3476412&partnerID=40&md5=268b4c781705c12f21042929bc1aac72},
abstract={The growth of scale and complexity of interactions between humans and robots highlights the need for new computational methods to automatically evaluate novel algorithms and applications. Exploring diverse scenarios of humans and robots interacting in simulation can improve understanding of the robotic system and avoid potentially costly failures in real-world settings. We formulate this problem as a quality diversity (QD) problem, of which the goal is to discover diverse failure scenarios by simultaneously exploring both environments and human actions. We focus on the shared autonomy domain, in which the robot attempts to infer the goal of a human operator, and adopt the QD algorithms CMA-ME and MAP-Elites to generate scenarios for two published algorithms in this domain: shared autonomy via hindsight optimization and linear policy blending. Some of the generated scenarios confirm previous theoretical findings, while others are surprising and bring about a new understanding of state-of-the-art implementations. Our experiments show that the QD algorithms CMA-ME and MAP-Elites outperform Monte-Carlo simulation and optimization-based methods in effectively searching the scenario space, highlighting their promise for automatic evaluation of algorithms in human-robot interaction. © 2022 Copyright held by the owner/author(s).},
author_keywords={automatic scenario generation;  human-robot interaction;  Quality diversity optimization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jang2022,
author={Jang, W.S. and Kim, R.Y.C.},
title={Automatic Cause–Effect Graph Tool with Informal Korean Requirement Specifications},
journal={Applied Sciences (Switzerland)},
year={2022},
volume={12},
number={18},
doi={10.3390/app12189310},
art_number={9310},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138635152&doi=10.3390%2fapp12189310&partnerID=40&md5=b4d26c4d4f5161eb483909e770b883be},
abstract={In requirement engineering, it is a very important issue to generate test cases with natural language automatically. However, no test case tools deal with informal Korean requirement specifications. In the Korean military software system and airspace industrial area, it is strongly suggested to automatically make just 30% of all possible test cases with requirements. Unlike the previous approaches, we adapted Gary E. Mogyorodi’s cause-effect graphing approach and the model-driven architecture (MDA) approach for automatic test case generation with natural language. In order to generate test cases with informal Korean requirement specifications, we propose an automatic cause–effect tool as an intermediate model for (1) simplifying complicated requirements; (2) modeling the C3Tree (that is, condition and result); (3) identifying incomplete requirements; (4) constructing causes, effects, and relationships; and (5) integrating with two units (that is, similar causes or effects) to remove redundant requirements. We evaluated the accuracy of two generated cause–effect graphs in two ways. With our approach, we can also remove requirement redundancy. © 2022 by the authors.},
author_keywords={automatic test case generation;  C3Tree model;  cause–effect graph;  Korean natural language analysis;  model-driven architecture (MDA);  requirement redundancy;  requirement similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ullah20225768,
author={Ullah, F. and Naeem, M.R. and Naeem, H. and Cheng, X. and Alazab, M.},
title={CroLSSim: Cross-language software similarity detector using hybrid approach of LSA-based AST-MDrep features and CNN-LSTM model},
journal={International Journal of Intelligent Systems},
year={2022},
volume={37},
number={9},
pages={5768-5795},
doi={10.1002/int.22813},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122681204&doi=10.1002%2fint.22813&partnerID=40&md5=719c48bb2b302c8e38efd2ab53f1fa76},
abstract={Software similarity in different programming codes is a rapidly evolving field because of its numerous applications in software development, software cloning, software plagiarism, and software forensics. Currently, software researchers and developers search cross-language open-source repositories for similar applications for a variety of reasons, such as reusing programming code, analyzing different implementations, and looking for a better application. However, it is a challenging task because each programming language has a unique syntax and semantic structure. In this paper, a novel tool called Cross-Language Software Similarity (CroLSSim) is designed to detect similar software applications written in different programming codes. First, the Abstract Syntax Tree (AST) features are collected from different programming codes. These are high-quality features that can show the abstract view of each program. Then, Methods Description (MDrep) in combination with AST is used to examine the relationship among different method calls. Second, the Term Frequency Inverse Document Frequency approach is used to retrieve the local and global weights from AST-MDrep features. Third, the Latent Semantic Analysis-based features extraction and selection method is proposed to extract the semantic anchors in reduced dimensional space. Fourth, the Convolution Neural Network (CNN)-based features extraction method is proposed to mine the deep features. Finally, a hybrid deep learning model of CNN-Long-Short-Term Memory is designed to detect semantically similar software applications from these latent variables. The data set contains approximately 9.5K Java, 8.8K C#, and 7.4K C++ software applications obtained from GitHub. The proposed approach outperforms as compared with the state-of-the-art methods. © 2022 Wiley Periodicals LLC.},
author_keywords={abstract syntax tree;  data mining;  deep learning;  latent semantic analysis;  software similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Menendez20223540,
author={Menendez, H.D. and Clark, D.},
title={Hashing Fuzzing: Introducing Input Diversity to Improve Crash Detection},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={9},
pages={3540-3553},
doi={10.1109/TSE.2021.3100858},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112596038&doi=10.1109%2fTSE.2021.3100858&partnerID=40&md5=3a926444fb86e3266f72f1f365d2cbe2},
abstract={The utility of a test set of program inputs is strongly influenced by its diversity and its size. Syntax coverage has become a standard proxy for diversity. Although more sophisticated measures exist, such as proximity of a sample to a uniform distribution, methods to use them tend to be type dependent. We use r-wise hash functions to create a novel, semantics preserving, testability transformation for C programs that we call HashFuzz. Use of HashFuzz improves the diversity of test sets produced by instrumentation-based fuzzers. We evaluate the effect of the HashFuzz transformation on eight programs from the Google Fuzzer Test Suite using four state-of-the-art fuzzers that have been widely used in previous research. We demonstrate pronounced improvements in the performance of the test sets for the transformed programs across all the fuzzers that we used. These include strong improvements in diversity in every case, maintenance or small improvement in branch coverage-up to 4.8 perent improvement in the best case, and significant improvement in unique crash detection numbers-between 28 to 97 perent increases compared to test sets for untransformed programs. © 1976-2012 IEEE.},
author_keywords={fuzz testing;  HashFuzz;  System testing;  universal hashing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tian2022,
author={Tian, H. and Li, Y. and Pian, W. and Kaboré, A.K. and Liu, K. and Habib, A. and Klein, J. and Bissyandé, T.F.},
title={Predicting Patch Correctness Based on the Similarity of Failing Test Cases},
journal={ACM Transactions on Software Engineering and Methodology},
year={2022},
volume={31},
number={4},
doi={10.1145/3511096},
art_number={77},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133510958&doi=10.1145%2f3511096&partnerID=40&md5=894387b672907a202713207bd5dcef12},
abstract={How do we know a generated patch is correct? This is a key challenging question that automated program repair (APR) systems struggle to address given the incompleteness of available test suites. Our intuition is that we can triage correct patches by checking whether each generated patch implements code changes (i.e., behavior) that are relevant to the bug it addresses. Such a bug is commonly specified by a failing test case. Towards predicting patch correctness in APR, we propose a novel yet simple hypothesis on how the link between the patch behavior and failing test specifications can be drawn: similar failing test cases should require similar patches. We then propose BATS, an unsupervised learning-based approach to predict patch correctness by checking patch Behavior Against failing Test Specification. BATS exploits deep representation learning models for code and patches: For a given failing test case, the yielded embedding is used to compute similarity metrics in the search for historical similar test cases to identify the associated applied patches, which are then used as a proxy for assessing the correctness of the APR-generated patches. Experimentally, we first validate our hypothesis by assessing whether ground-truth developer patches cluster together in the same way that their associated failing test cases are clustered. Then, after collecting a large dataset of 1,278 plausible patches (written by developers or generated by 32 APR tools), we use BATS to predict correct patches: BATS achieves AUC between 0.557 to 0.718 and recall between 0.562 and 0.854 in identifying correct patches. Our approach outperforms state-of-the-art techniques for identifying correct patches without the need for large labeled patch datasets-as is the case with machine learning-based approaches. While BATS is constrained by the availability of similar test cases, we show that it can still be complementary to existing approaches: When combined with a recent approach that relies on supervised learning, BATS improves the overall recall in detecting correct patches. We finally show that BATS is complementary to the state-of-the-art PATCH-SIM dynamic approach for identifying correct patches generated by APR tools. © 2022 Copyright held by the owner/author(s).},
author_keywords={patch correctness;  patch semantics;  Program repair;  test behavior},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yan20221179,
author={Yan, Y. and Jiang, S. and Wang, R. and Zhang, C. and Wang, C. and Zhang, S. and Wen, M.},
title={A Fault Localization Approach Based on BiRNN and Multi-Dimensional Features},
journal={International Journal of Software Engineering and Knowledge Engineering},
year={2022},
volume={32},
number={8},
pages={1179-1201},
doi={10.1142/S0218194022500425},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136476341&doi=10.1142%2fS0218194022500425&partnerID=40&md5=ca801c957708f564085964d5f1e44a21},
abstract={Software fault localization is notoriously tedious and time-consuming. Developed rapidly, machine learning techniques have been adopted for fault localization by researchers. Most existing approaches use the test coverage information as feature input to the learning model, ignoring the limited ability of the single-dimensional features. The e®ectiveness of fault localization is not greatly improved. To overcome the limitation, we propose a fault localization approach based on Bidirectional Recurrent Neural Networks (BiRNNs) and multi-dimensional features. Our approach collects suspiciousness-based, text similarity-based and fault-proneness-based features from the traditional fault localization areas and software metrics. To evaluate our approach, the experiments have been studied on the real-fault benchmark Defects4J and seeded fault program NanoXML. The experimental results show that our approach e®ectively improves fault localization accuracy. #c World Scienti¯c Publishing Company.},
author_keywords={multi-dimensional features;  recurrent neural networks;  Software fault localization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Leveau2022,
author={Leveau, J. and Blanc, X. and Réveillère, L. and Falleri, J.-R. and Rouvoy, R.},
title={Fostering the diversity of exploratory testing in web applications},
journal={Software Testing Verification and Reliability},
year={2022},
volume={32},
number={5},
doi={10.1002/stvr.1827},
art_number={e1827},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132180593&doi=10.1002%2fstvr.1827&partnerID=40&md5=12daad61eefab4c00f542c6e9e3f381b},
abstract={Exploratory testing (ET) is a software testing approach that complements automated testing by leveraging business expertise. It has gained momentum over the last decades as it appeals testers to exploit their business knowledge to stress the system under test (SUT). Exploratory tests, unlike automated tests, are defined and executed on-the-fly by testers. However, testers who perform exploratory tests may be biased by their experience and, incidentally, miss anomalies or unusual interactions proposed by the SUT. This is even more complex in the context of web applications, which typically expose a huge number of interaction paths to their users. As testers of these applications cannot remember all the sequences of interactions they performed, they may fail to deeply explore the application scope. This article, therefore, introduces a new approach to assist testers in widely exploring any web application. In particular, our approach monitors the online interactions performed by the testers to suggest in real-time the probabilities of performing next interactions. Looking at these probabilities, we claim that the testers who favour interactions that have a low probability (because they were rarely performed), will increase the diversity of their explorations. Our approach defines a prediction model, based on (Formula presented.) -grams, that encodes the history of past interactions and that supports the estimation of the probabilities. Integrated within a web browser extension, it automatically and transparently injects feedback within the application itself. We conduct a controlled experiment and a qualitative study to assess our approach. Results show that it prevents testers to be trapped in already tested loops, and succeeds to assist them in performing deeper explorations of the SUT. © 2022 The Authors. Software Testing, Verification & Reliability published by John Wiley & Sons Ltd.},
author_keywords={exploratory test;  n-gram;  software testing;  web applications},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ma20227837,
author={Ma, X. and Yin, J. and Zhu, A. and Li, X. and Yu, Y. and Wang, L. and Qi, Y. and Zhu, Z.},
title={Enhanced Multifactorial Evolutionary Algorithm With Meme Helper-Tasks},
journal={IEEE Transactions on Cybernetics},
year={2022},
volume={52},
number={8},
pages={7837-7851},
doi={10.1109/TCYB.2021.3050516},
note={cited By 10},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100854244&doi=10.1109%2fTCYB.2021.3050516&partnerID=40&md5=46f18b17c53902f798719d20393106d9},
abstract={Evolutionary multitasking (EMT) is an emerging research direction in the field of evolutionary computation. EMT solves multiple optimization tasks simultaneously using evolutionary algorithms with the aim to improve the solution for each task via intertask knowledge transfer. The effectiveness of intertask knowledge transfer is the key to the success of EMT. The multifactorial evolutionary algorithm (MFEA) represents one of the most widely used implementation paradigms of EMT. However, it tends to suffer from noneffective or even negative knowledge transfer. To address this issue and improve the performance of MFEA, we incorporate a prior-knowledge-based multiobjectivization via decomposition (MVD) into MFEA to construct strongly related meme helper-tasks. In the proposed method, MVD creates a related multiobjective optimization problem for each component task based on the corresponding problem structure or decision variable grouping to enhance positive intertask knowledge transfer. MVD can reduce the number of local optima and increase population diversity. Comparative experiments on the widely used test problems demonstrate that the constructed meme helper-tasks can utilize the prior knowledge of the target problems to improve the performance of MFEA. © 2013 IEEE.},
author_keywords={Evolutionary multitasking (EMT);  helper-task;  knowledge transfer;  multifactorial evolutionary algorithm (MFEA);  multiobjectivization;  multiobjectivization via decomposition (MVD)},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Ghanbari2022654,
author={Ghanbari, A. and Marcus, A.},
title={Patch correctness assessment in automated program repair based on the impact of patches on production and test code},
journal={ISSTA 2022 - Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2022},
pages={654-665},
doi={10.1145/3533767.3534368},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136827281&doi=10.1145%2f3533767.3534368&partnerID=40&md5=794136fa784a1a7567520c381f1b0ee8},
abstract={Test-based generate-and-validate automated program repair (APR) systems often generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, so previous research proposed various techniques for automatic correctness assessment of APR-generated patches. Among them, dynamic patch correctness assessment techniques rely on the assumption that, when running the originally passing test cases, the correct patches will not alter the program behavior in a significant way, e.g., removing the code implementing correct functionality of the program. In this paper, we propose and evaluate a novel technique, named Shibboleth, for automatic correctness assessment of the patches generated by test-based generate-and-validate APR systems. Unlike existing works, the impact of the patches is captured along three complementary facets, allowing more effective patch correctness assessment. Specifically, we measure the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage of passing tests) to separate the patches that result in similar programs and that do not delete desired program elements. Shibboleth assesses the correctness of patches via both ranking and classification. We evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art ranking and classification techniques. Specifically, in our ranking data set, in 43% (66%) of the cases, Shibboleth ranks the correct patch in top-1 (top-2) positions, and in classification mode applied on our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively. © 2022 ACM.},
author_keywords={Automated Program Repair;  Branch Coverage;  Patch Correctness Assessment;  Similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang202264,
author={Zhang, X. and Gong, Y. and Liang, B. and Huang, J. and You, W. and Shi, W. and Zhang, J.},
title={Hunting bugs with accelerated optimal graph vertex matching},
journal={ISSTA 2022 - Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2022},
pages={64-76},
doi={10.1145/3533767.3534393},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136800726&doi=10.1145%2f3533767.3534393&partnerID=40&md5=455108699f1e0b3f4426ef99a9995f5d},
abstract={Various techniques based on code similarity measurement have been proposed to detect bugs. Essentially, the code fragment can be regarded as a kind of graph. Performing code graph similarity comparison to identify the potential bugs is a natural choice. However, the logic of a bug often involves only a few statements in the code fragment, while others are bug-irrelevant. They can be considered as a kind of noise, and can heavily interfere with the code similarity measurement. In theory, performing optimal vertex matching can address the problem well, but the task is NP-complete and cannot be applied to a large-scale code base. In this paper, we propose a two-phase strategy to accelerate code graph vertex matching for detecting bugs. In the first phase, a vertex matching embedding model is trained and used to rapidly filter a limited number of candidate code graphs from the target code base, which are likely to have a high vertex matching degree with the seed, i.e., the known buggy code. As a result, the number of code graphs needed to be further analyzed is dramatically reduced. In the second phase, a high-order similarity embedding model based on graph convolutional neural network is built to efficiently get the approximately optimal vertex matching between the seed and candidates. On this basis, the code graph similarity is calculated to identify the potential buggy code. The proposed method is applied to five open source projects. In total, 31 unknown bugs were successfully detected and confirmed by developers. Comparative experiments demonstrate that our method can effectively mitigate the noise problem, and the detection efficiency can be improved dozens of times with the two-phase strategy. © 2022 ACM.},
author_keywords={bug detection;  code similarity;  graph convolutional neural network;  optimal vertex matching},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Wang20221,
author={Wang, H. and Qu, W. and Katz, G. and Zhu, W. and Gao, Z. and Qiu, H. and Zhuge, J. and Zhang, C.},
title={JTrans: Jump-aware transformer for binary code similarity detection},
journal={ISSTA 2022 - Proceedings of the 31st ACM SIGSOFT International Symposium on Software Testing and Analysis},
year={2022},
pages={1-13},
doi={10.1145/3533767.3534367},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136784760&doi=10.1145%2f3533767.3534367&partnerID=40&md5=13c17b931c9213a93b4e5e2363b0698d},
abstract={Binary code similarity detection (BCSD) has important applications in various fields such as vulnerabilities detection, software component analysis, and reverse engineering. Recent studies have shown that deep neural networks (DNNs) can comprehend instructions or control-flow graphs (CFG) of binary code and support BCSD. In this study, we propose a novel Transformer-based approach, namely jTrans, to learn representations of binary code. It is the first solution that embeds control flow information of binary code into Transformer-based language models, by using a novel jump-aware representation of the analyzed binaries and a newly-designed pre-training task. Additionally, we release to the community a newly-created large dataset of binaries, BinaryCorp, which is the most diverse to date. Evaluation results show that jTrans outperforms state-of-the-art (SOTA) approaches on this more challenging dataset by 30.5% (i.e., from 32.0% to 62.5%). In a real-world task of known vulnerability searching, jTrans achieves a recall that is 2X higher than existing SOTA baselines. © 2022 Owner/Author.},
author_keywords={Binary Analysis;  Datasets;  Neural Networks;  Similarity Detection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gupt2022719,
author={Gupt, K.K. and Kshirsagar, M. and Rosenbauer, L. and Sullivan, J.P. and Dias, D.M. and Ryan, C.},
title={PreDive: Preserving Diversity in Test Cases for Evolving Digital Circuits using Grammatical Evolution},
journal={GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference},
year={2022},
pages={719-722},
doi={10.1145/3520304.3529006},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136325867&doi=10.1145%2f3520304.3529006&partnerID=40&md5=ac1e4ec6c51f41551a805e4dd2d66bb5},
abstract={The ever-present challenge in the domain of digital devices is how to test their behavior efficiently. We tackle the issue in two ways. We switch to an automated circuit design using Grammatical Evolution (GE). Additionally, we provide two diversity-based methodologies to improve testing efficiency. The first approach extracts a minimal number of test cases from subsets formed through clustering. Moreover, the way we perform clustering can easily be used for other domains as it is problem-agnostic. The other uses complete test set and introduces a novel fitness function hitPlex that incorporates a test case diversity measure to speed up the evolutionary process. Experimental and statistical evaluations on six benchmark circuits establish that the automatically selected test cases result in good coverage and enable the system to evolve a highly accurate digital circuit. Evolutionary runs using hitPlex indicate promising improvements, with up to 16% improvement in convergence speed and up to 30% in success rate for complex circuits when compared to the system without the diversity extension. © 2022 Owner/Author.},
author_keywords={black-box testing;  digital circuits design;  diversity;  fitness function;  grammatical evolution;  test case selection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Langdon2022574,
author={Langdon, W.B.},
title={Failed disruption propagation in integer genetic programming},
journal={GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference},
year={2022},
pages={574-577},
doi={10.1145/3520304.3528878},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136325722&doi=10.1145%2f3520304.3528878&partnerID=40&md5=2c25334c4c0f5eded62447eaaed983ca},
abstract={We inject a random value into the evaluation of highly evolved deep integer GP trees 9 743 720 times and find 99.7% of test outputs are unchanged. Suggesting crossover and mutation's impact are dissipated and seldom propagate outside the program. Indeed only errors near the root node have impact and disruption falls exponentially with depth at between e-depth/3 and e-depth/5 for recursive Fibonacci GP trees, allowing five to seven levels of nesting between the runtime perturbation and an optimal test oracle for it to detect most errors. Information theory explains this locally flat fitness landscape is due to FDP. Overflow is not important and instead, integer GP, like deep symbolic regression floating point GP and software in general, is not fragile, is robust, is not chaotic and suffers little from Lorenz' butterfly. © 2022 Owner/Author.},
author_keywords={correctness attraction;  diversity;  entropy;  evolvability;  genetic programming;  information funnels;  information loss;  introns;  mutational robustness;  neutral networks;  optimal test oracle placement;  SBSE;  software robustness;  software testing;  theory of bloat},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Allard202258,
author={Allard, M. and Smith, S.C. and Chatzilygeroudis, K. and Cully, A.},
title={Hierarchical quality-diversity for online damage recovery},
journal={GECCO 2022 - Proceedings of the 2022 Genetic and Evolutionary Computation Conference},
year={2022},
pages={58-67},
doi={10.1145/3512290.3528751},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135214793&doi=10.1145%2f3512290.3528751&partnerID=40&md5=75763e6baa3b0129bd5d7070c01ca45a},
abstract={Adaptation capabilities, like damage recovery, are crucial for the deployment of robots in complex environments. Several works have demonstrated that using repertoires of pre-trained skills can enable robots to adapt to unforeseen mechanical damages in a few minutes. These adaptation capabilities are directly linked to the behavioural diversity in the repertoire. The more alternatives the robot has to execute a skill, the better are the chances that it can adapt to a new situation. However, solving complex tasks, like maze navigation, usually requires multiple different skills. Finding a large behavioural diversity for these multiple skills often leads to an intractable exponential growth of the number of required solutions. In this paper, we introduce the Hierarchical Trial and Error algorithm, which uses a hierarchical behavioural repertoire to learn diverse skills and leverages them to make the robot more adaptive to different situations. We show that the hierarchical decomposition of skills enables the robot to learn more complex behaviours while keeping the learning of the repertoire tractable. The experiments with a hexapod robot show that our method solves maze navigation tasks with 20% less actions in the most challenging scenarios than the best baseline while having 57% less complete failures. © 2022 ACM.},
author_keywords={Hierarchical Learning;  Quality-Diversity;  Robotics},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Manchala2022,
author={Manchala, P. and Bisi, M.},
title={Diversity based imbalance learning approach for software fault prediction using machine learning models[Formula presented]},
journal={Applied Soft Computing},
year={2022},
volume={124},
doi={10.1016/j.asoc.2022.109069},
art_number={109069},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131760288&doi=10.1016%2fj.asoc.2022.109069&partnerID=40&md5=86882bba030f253790fea512ad32e906},
abstract={The Software fault prediction (SFP) target is to distinguish between faulty and non-faulty modules. The prediction model's performance is vulnerable to the class imbalance issue in SFP. The existing oversampling approaches generate relatively identical synthetic data, which results in over-generalization and less diverse data. Moreover, many undesirable noisy modules are introduced while generating synthetic data. In this study, we propose the Weighted Average Centroid based Imbalance Learning Approach (WACIL), an effective synthetic over-sampling technique to mitigate the imbalance issue. The WACIL first finds borderline instances, then generates pseudo-data of them through a weighted average centroid concept and filters out inappropriate noise data through a filtration process. We conducted experiments on 24 PROMISE and NASA projects and compared them with some of the existing sampling approaches using K-Nearest Neighbors (KNN), Logistic Regression (LR), Naive Bayes (NB), Support Vector Machine (SVM), Decision Tree (DT) and Deep Neural Network (DNN) as classification models. WACIL achieves superior results in terms of Fall Out Rate (FOR), F-measure and Area Under Curve (AUC) and obtains comparable results in terms of Recall and G-mean compared to the competitive approaches. The statistical analysis indicates that WACIL's ability to outperform the other over-sampling techniques is significant under the statistical Wilcoxon signed rank test and matched pairs rank biserial correlation coefficient effect size. Hence, WACIL is advisable as a competent choice to deal with the imbalance issue in SFP. © 2022 Elsevier B.V.},
author_keywords={Deep Neural Network;  Imbalance learning;  Machine Learning Model;  Oversampling;  Software fault prediction},
document_type={Article},
source={Scopus},
}

@ARTICLE{Liang20224457,
author={Liang, Z. and Liang, W. and Wang, Z. and Ma, X. and Liu, L. and Zhu, Z.},
title={Multiobjective Evolutionary Multitasking With Two-Stage Adaptive Knowledge Transfer Based on Population Distribution},
journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
year={2022},
volume={52},
number={7},
pages={4457-4469},
doi={10.1109/TSMC.2021.3096220},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112603314&doi=10.1109%2fTSMC.2021.3096220&partnerID=40&md5=6a2f971eadcd28b636c44d6682c19267},
abstract={Multitasking optimization can achieve better performance than traditional single-tasking optimization by leveraging knowledge transfer between tasks. However, the current multitasking optimization algorithms suffer from some deficiencies. Particularly, on high similar problems, the existing algorithms might fail to take full advantage of knowledge transfer to accelerate the convergence of the search, or easily get trapped in the local optima. Whereas, on low similar problems, they tend to suffer from negative transfer, resulting in performance degradation. To solve these issues, this article proposes an evolutionary multitasking optimization algorithm for multiobjective/many-objective optimization with two-stage adaptive knowledge transfer based on population distribution. The resultant algorithm named EMT-PD can improve the convergence performance of the target optimization tasks based on the knowledge extracted from the probability model that reflects the search trend of the whole population. At the first stage of knowledge transfer, an adaptive weight is used to adjust the search step size of each individual, which can reduce the impact of negative transfer. At the second stage of knowledge transfer, the search range of each individual is further adjusted dynamically, which can improve the population diversity and be beneficial for jumping out of the local optima. Experimental results on multitasking multiobjective optimization test suites show that EMT-PD is superior to other state-of-the-art evolutionary multitasking/single-tasking algorithms. To further investigate the effectiveness of EMT-PD on many-objective optimization problems, a multitasking many-objective optimization test suite is also designed in this article. The experimental results on the new test suite also demonstrate the competitiveness of EMT-PD. © 2013 IEEE.},
author_keywords={Evolutionary multitasking (EMT);  knowledge transfer;  many-objective optimization;  multiobjective optimization;  population distribution},
document_type={Article},
source={Scopus},
}

@ARTICLE{Sondhi20222262,
author={Sondhi, D. and Jobanputra, M. and Rani, D. and Purandare, S. and Sharma, S. and Purandare, R.},
title={Mining Similar Methods for Test Adaptation},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={7},
pages={2262-2276},
doi={10.1109/TSE.2021.3057163},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100850825&doi=10.1109%2fTSE.2021.3057163&partnerID=40&md5=1bb3d340cfeb0360af59cb8b2db9cb57},
abstract={Developers may choose to implement a library despite the existence of similar libraries, considering factors such as computational performance, language or platform dependency, accuracy, convenience, and completeness of an API. As a result, GitHub hosts several library projects that have overlaps in their functionalities. These overlaps have been of interest to developers from the perspective of code reuse or the preference of one implementation over the other. Through an empirical study, we explore the extent and nature of existence of these similarities in the library functions. We have further studied whether the similarity of functions across different libraries and their associated test suites can be leveraged to reveal defects in one another. We see scope for effectively using the mining of test suites from the perspective of revealing defects in a program or its documentation. Another noteworthy observation made in the study is that similar functions may exist across libraries implemented in the same language as well as in different languages. Identifying the challenges that lie in building a testing tool, we automate the entire process in Metallicus, a test mining and recommendation tool. Metallicus returns a test suite for the given input of a query function and a template for its test suite. On a dataset of query functions taken from libraries implemented in Java or Python, Metallicus revealed 46 defects. © 1976-2012 IEEE.},
author_keywords={Function similarity;  Mining;  Software testing;  Test suites},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xiang20222317,
author={Xiang, Y. and Huang, H. and Li, M. and Li, S. and Yang, X.},
title={Looking for Novelty in Search-Based Software Product Line Testing},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={7},
pages={2317-2338},
doi={10.1109/TSE.2021.3057853},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100850109&doi=10.1109%2fTSE.2021.3057853&partnerID=40&md5=5f225d6d5b5f4f2fa4b1a0f1d5d89780},
abstract={Testing software product lines (SPLs) is difficult due to a huge number of possible products to be tested. Recently, there has been a growing interest in similarity-based testing of SPLs, where similarity is used as a surrogate metric for the t-wise coverage. In this context, one of the primary goals is to sample, by optimizing similarity metrics using search-based algorithms, a small subset of test cases (i.e., products) as dissimilar as possible, thus potentially making more t-wise combinations covered. Prior work has shown, by means of empirical studies, the great potential of current similarity-based testing approaches. However, the rationale of this testing technique deserves a more rigorous exploration. To this end, we perform correlation analyses to investigate how similarity metrics are correlated with the t-wise coverage. We find that similarity metrics generally have significantly positive correlations with the t-wise coverage. This well explains why similarity-based testing works, as the improvement on similarity metrics will potentially increase the t-wise coverage. Moreover, we explore, for the first time, the use of the novelty search (NS) algorithm for similarity-based SPL testing. The algorithm rewards 'novel' individuals, i.e., those being different from individuals discovered previously, and this well matches the goal of similarity-based SPL testing. We find that the novelty score used in NS has (much) stronger positive correlations with the t-wise coverage than previous approaches relying on a genetic algorithm (GA) with a similarity-based fitness function. Experimental results on 31 software product lines validate the superiority of NS over GA, as well as other state-of-the-art approaches, concerning both t-wise coverage and fault detection capacity. Finally, we investigate whether it is useful to combine two satisfiability solvers when generating new individuals in NS, and how the performance of NS is affected by its key parameters. In summary, looking for novelty provides a promising way of sampling diverse test cases for SPLs. © 1976-2012 IEEE.},
author_keywords={Correlation analysis;  Novelty search;  Product sampling;  Similarity-based testing;  Software product line testing;  T-wise coverage},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alsewari20223361,
author={Alsewari, A.R.A. and Poston, R. and Zamli, K.Z. and Balfaqih, M. and Aloufi, K.S.},
title={Combinatorial test list generation based on Harmony Search Algorithm},
journal={Journal of Ambient Intelligence and Humanized Computing},
year={2022},
volume={13},
number={7},
pages={3361-3377},
doi={10.1007/s12652-020-01696-7},
note={cited By 7},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078134496&doi=10.1007%2fs12652-020-01696-7&partnerID=40&md5=ecf209756a26304a933384dcab0b4c13},
abstract={Combinatorial test case generation faces a problem on how to reduce the test cases by uncover the unnecessary test cases. So, there is a need for expert applications or strategies that generate the most optimum test cases keeping in mind the most important combinations. Complementing existing work on combinatorial test case generation strategies, also known as t-way testing strategies (i.e., where t represents interaction degree), this paper presents the design and the implementation of a new combinatorial test list generation strategy based on Harmony Search (HS) algorithm, called General T-way Harmony Search-based Strategy (GTHS). HS has been chosen to be the main engine of test generation because it could balance between intensification and diversification. Benchmarking experimental results show that GTHS produces competitive results as compared to other existing well-known optimization-based strategies and provides the support for high combination degrees (i.e., t ≤ 12). © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Combinatorial optimization problems;  Computational ontelligence;  Intelligent test list generators;  Optimization algorithms},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Huang202265,
author={Huang, C. and Zhou, H. and Ye, C. and Li, B.},
title={Code Clone Detection based on Event Embedding and Event Dependency},
journal={ACM International Conference Proceeding Series},
year={2022},
pages={65-74},
doi={10.1145/3545258.3545277},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139547004&doi=10.1145%2f3545258.3545277&partnerID=40&md5=b0e89d8686aa4fea3274035e8433e71a},
abstract={The code clone detection method based on semantic similarity has important value in software engineering tasks (e.g., software evolution, software reuse). Traditional code clone detection technologies pay more attention to the similarity of code at the syntax level, and less attention to the semantic similarity of the code. As a result, candidate codes similar in semantics are ignored. To address this issue, we propose a code clone detection method based on semantic similarity. By treating code as a series of interdependent events that occur continuously, we design a model namely EDAM to encode code semantic information based on event embedding and event dependency. The EDAM model uses the event embedding method to model the execution characteristics of program statements and the data dependence information between all statements. In this way, we can embed the program semantic information into a vector and use the vector to detect codes similar in semantics. Experimental results show that the performance of our EDAM model is superior to state-of-the-art open source models for code clone detection. © 2022 Association for Computing Machinery.},
author_keywords={code clone detection;  event dependency;  event embedding},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Batot20221159,
author={Batot, E.R. and Sahraoui, H.},
title={Promoting social diversity for the automated learning of complex MDE artifacts},
journal={Software and Systems Modeling},
year={2022},
volume={21},
number={3},
pages={1159-1178},
doi={10.1007/s10270-021-00969-9},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123063597&doi=10.1007%2fs10270-021-00969-9&partnerID=40&md5=a3784ebf2e5594084e7842117fd85d49},
abstract={Software modeling activities typically involve a tedious and time-consuming effort by specially trained personnel. This lack of automation hampers the adoption of model-driven engineering (MDE). Nevertheless, in the recent years, much research work has been dedicated to learn executable MDE artifacts instead of writing them manually. In this context, mono- and multi-objective genetic programming (GP) has proven being an efficient and reliable method to derive automation knowledge by using, as training data, a set of examples representing the expected behavior of an artifact. Generally, conformance to the training example set is the main objective to lead the learning process. Yet, single fitness peak, or local optima deadlock, a common challenge in GP, hinders the application of GP to MDE. In this paper, we propose a strategy to promote populations’ social diversity during the GP learning process. We evaluate our approach with an empirical study featuring the case of learning well-formedness rules in MDE with a multi-objective genetic programming algorithm. Our evaluation shows that integration of social diversity leads to more efficient search, faster convergence, and more generalizable results. Moreover, when the social diversity is used as crowding distance, this convergence is uniform through a hundred of runs despite the probabilistic nature of GP. It also shows that genotypic diversity strategies cannot achieve comparable results. © 2022, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.},
author_keywords={Genetic programming;  Model-driven engineering;  Social diversity},
document_type={Article},
source={Scopus},
}

@ARTICLE{López2022967,
author={López, J.A.H. and Cánovas Izquierdo, J.L. and Cuadrado, J.S.},
title={ModelSet: a dataset for machine learning in model-driven engineering},
journal={Software and Systems Modeling},
year={2022},
volume={21},
number={3},
pages={967-986},
doi={10.1007/s10270-021-00929-3},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117576971&doi=10.1007%2fs10270-021-00929-3&partnerID=40&md5=4dfe6f4506c21606c462b383c383b789},
abstract={The application of machine learning (ML) algorithms to address problems related to model-driven engineering (MDE) is currently hindered by the lack of curated datasets of software models. There are several reasons for this, including the lack of large collections of good quality models, the difficulty to label models due to the required domain expertise, and the relative immaturity of the application of ML to MDE. In this work, we present ModelSet, a labelled dataset of software models intended to enable the application of ML to address software modelling problems. To create it we have devised a method designed to facilitate the exploration and labelling of model datasets by interactively grouping similar models using off-the-shelf technologies like a search engine. We have built an Eclipse plug-in to support the labelling process, which we have used to label 5,466 Ecore meta-models and 5,120 UML models with its category as the main label plus additional secondary labels of interest. We have evaluated the ability of our labelling method to create meaningful groups of models in order to speed up the process, improving the effectiveness of classical clustering methods. We showcase the usefulness of the dataset by applying it in a real scenario: enhancing the MAR search engine. We use ModelSet to train models able to infer useful metadata to navigate search results. The dataset and the tooling are available at https://figshare.com/s/5a6c02fa8ed20782935c and a live version at http://modelset.github.io. © 2021, The Author(s).},
author_keywords={Dataset;  Machine learning;  Model-driven engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Gomez-Perez2022,
author={Gomez-Perez, S.L. and Zhang, Y. and Byrne, C. and Wakefield, C. and Geesey, T. and Sclamberg, J. and Peterson, S.},
title={Concordance of Computed Tomography Regional Body Composition Analysis Using a Fully Automated Open-Source Neural Network versus a Reference Semi-Automated Program with Manual Correction},
journal={Sensors},
year={2022},
volume={22},
number={9},
doi={10.3390/s22093357},
art_number={3357},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128767753&doi=10.3390%2fs22093357&partnerID=40&md5=70a2237ac8d7ef278b3cacde5cb41e95},
abstract={Quick, efficient, fully automated open-source programs to segment muscle and adipose tissues from computed tomography (CT) images would be a great contribution to body composition research. This study examined the concordance of cross-sectional areas (CSA) and densities for muscle, visceral adipose tissue (VAT), subcutaneous adipose tissue (SAT), and intramuscular adipose tissue (IMAT) from CT images at the third lumbar (L3) between an automated neural network (test method) and a semi-automatic human-based program (reference method). Concordance was further evaluated by disease status, sex, race/ethnicity, BMI categories. Agreement statistics applied included Lin’s Concordance (CCC), Spearman correlation coefficient (SCC), Sorensen dice-similarity coefficient (DSC), and Bland–Altman plots with limits of agreement (LOA) within 1.96 standard deviation. A total of 420 images from a diverse cohort of patients (60.35 ± 10.92 years; body mass index (BMI) of 28.77 ± 7.04 kg/m2; 55% female; 53% Black) were included in this study. About 30% of patients were healthy (i.e., received a CT scan for acute illness or pre-surgical donor work-up), while another 30% had a diagnosis of colorectal cancer. The CCC, SCC, and DSC estimates for muscle, VAT, SAT were all greater than 0.80 (&gt;0.80 indicates good performance). Agreement analysis by diagnosis showed good performance for the test method except for critical illness (DSC 0.65–0.87). Bland–Altman plots revealed narrow LOA suggestive of good agreement despite minimal proportional bias around the zero-bias line for muscle, SAT, and IMAT CSA. The test method shows good performance and almost perfect concordance for L3 muscle, VAT, SAT, and IMAT per DSC estimates, and Bland–Altman plots even after stratification by sex, race/ethnicity, and BMI categories. Care must be taken to assess the density of the CT images from critically ill patients before applying the automated neural network (test method). © 2022 by the authors. Licensee MDPI, Basel, Switzerland.},
author_keywords={adipose tissue;  agreement;  artificial intelligence;  automated segmentation;  body composition;  computed tomography;  muscle;  validation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Alrabaee2022,
author={Alrabaee, S.},
title={A stratified approach to function fingerprinting in program binaries using diverse features},
journal={Expert Systems with Applications},
year={2022},
volume={193},
doi={10.1016/j.eswa.2021.116384},
art_number={116384},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123267415&doi=10.1016%2fj.eswa.2021.116384&partnerID=40&md5=3944fd549cd836981cc41a591ef4436b},
abstract={Fingerprinting individual functions in binary code is useful in many security applications ranging from digital forensic analysis of malware corpora to the detection of critical security vulnerabilities. However, existing approaches for fingerprinting functions are typically not resilient to code transformation methods or the use of different compilers. Moreover, another common weakness with these approaches is that when they report a similarity, they do not provide reverse engineers with any insight into the underlying evidence. In order to bridge this gap, our paper presents PLUMERIA, an obfuscation-resilient and scalable approach based on a stratified architecture comprised of three layers. The first layer retrieves as many candidates as possible by capturing statistical characteristics, function behavior, and function neighborhood relationships. The second layer then trains a linear conditional random field to learn the correlations between the features of the function and its semantics. This layer is designed to reduce the number of false positives. Finally, the third layer is designed to provide insights into the underlying evidence by collecting the side effects exhibited from the candidates selected by the previous layer. Our study evaluates PLUMERIA in the context of several scenarios: fingerprinting functions in obfuscated/de-obfuscated binaries; fingerprinting functions across different compilers; fingerprinting various vulnerabilities across compilers and versions; and fingerprinting standard library functions. We then benchmark PLUMERIA on real-world projects and malware binaries, comparing it with existing state-of-the-art solutions. Our results show that PLUMERIA outperforms existing solutions, with an average precision of over 89% © 2021 Elsevier Ltd},
author_keywords={Binary code;  Machine learning;  Reverse engineering},
document_type={Article},
source={Scopus},
}

@ARTICLE{Cao2022,
author={Cao, J. and Li, M. and Li, Y. and Wen, M. and Cheung, S.-C. and Chen, H.},
title={SemMT: A Semantic-Based Testing Approach for Machine Translation Systems},
journal={ACM Transactions on Software Engineering and Methodology},
year={2022},
volume={31},
number={2},
doi={10.1145/3490488},
art_number={34e},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130742894&doi=10.1145%2f3490488&partnerID=40&md5=dd76ec4fa1b6c0784b251c19b2bc71e4},
abstract={Machine translation has wide applications in daily life. In mission-critical applications such as translating official documents, incorrect translation can have unpleasant or sometimes catastrophic consequences. This motivates recent research on the testing methodologies for machine translation systems. Existing methodologies mostly rely on metamorphic relations designed at the textual level (e.g., Levenshtein distance) or syntactic level (e.g., distance between grammar structures) to determine the correctness of translation results. However, these metamorphic relations do not consider whether the original and the translated sentences have the same meaning (i.e., semantic similarity). To address this problem, in this article we propose SemMT, an automatic testing approach for machine translation systems based on semantic similarity checking. SemMT applies round-trip translation and measures the semantic similarity between the original and the translated sentences. Our insight is that the semantics concerning logical relations and quantifiers in sentences can be captured by regular expressions (or deterministic finite automata) where efficient semantic equivalence/similarity checking algorithms can be applied. Leveraging the insight, we propose three semantic similarity metrics and implement them in SemMT. We compared SemMT with related state-of-the-art testing techniques, demonstrating the effectiveness of mistranslation detection. The experiment results show that SemMT outperforms existing metrics, achieving an increase of 34.2% and 15.4% on accuracy and F-score, respectively. We also study the possibility of further enhancing the performance by combining various metrics. Finally, we discuss a solution to locate the suspicious trip in round-trip translation, which provides hints for bug diagnosis. © 2022 Association for Computing Machinery.},
author_keywords={Machine translation;  metamorphic testing;  semantic equivalent;  semantic similarity;  testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rajpathak2022,
author={Rajpathak, D. and Peranandam, P.M. and Ramesh, S.},
title={Automatic development of requirement linking matrix based on semantic similarity for robust software development},
journal={Journal of Systems and Software},
year={2022},
volume={186},
doi={10.1016/j.jss.2021.111211},
art_number={111211},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122634295&doi=10.1016%2fj.jss.2021.111211&partnerID=40&md5=9f727a56152276ec54686db48522f4d4},
abstract={With growing complexity of modern software, it is important that the relevant textual requirements are correctly linked into a ‘requirement liking matrix’ during early system development stages. The resulting requirement linking matrix highlights direct and indirect interactions between different requirements, thus facilitating improved design, development, and testing of complex software systems, e.g. automotive software, electrical/electronic architectures. The sheer volume of textual requirements collected in real-life coupled with data noises makes the task of automatic requirement linking a non-trivial exercise. In this paper, we propose a novel semantic similarity model for automatically linking different requirements to organize them into a requirement linking matrix. The model computes the similarity in terms of term-to-term, tuple-to-tuple, and text-to-text scores. The scores are ranked to determine whether the links are having “High”, “Low”, or “No” relationship with each other in a requirement linking matrix. The model is deployed as a prototype tool and its performance is validated by using the real-life data. We also compare our approach with the alternative approaches proposed in literature. The system achieved the average F1 score of 0.93 in correctly linking the heterogeneous requirements. © 2022 Elsevier Inc.},
author_keywords={Automotive;  Decision support;  Requirement engineering;  Requirement linking;  Semantic similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Melegati2022,
author={Melegati, J. and Guerra, E. and Wang, X.},
title={HyMap: Eliciting hypotheses in early-stage software startups using cognitive mapping},
journal={Information and Software Technology},
year={2022},
volume={144},
doi={10.1016/j.infsof.2021.106807},
art_number={106807},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122334888&doi=10.1016%2fj.infsof.2021.106807&partnerID=40&md5=8fa059ad29b5218310fe83a7745a74fa},
abstract={Context: Software startups develop innovative, software-intensive products. Given the uncertainty associated with such an innovative context, experimentation, an approach based on validating assumptions about the software product through data obtained from diverse techniques, like A/B tests or interviews, is valuable for these companies. Relying on data rather than opinions reduces the chance of developing unnecessary products or features, improving the likelihood of success, especially in early development stages, when implementing unnecessary features represents a higher risk for companies’ survival. Nevertheless, researchers have argued that the lack of clearly defined practices led to limited adoption of experimentation. Since the first step of the approach is to define hypotheses, testable statements about the software product features, based on which software development teams will create experiments, eliciting hypotheses is a natural first step to develop practices. Objective: We aim to develop a systematic technique for identifying hypotheses in early-stage software startups to support experimentation in these companies and, consequently, improve their software products. Methods: We followed a Design Science approach consisting of an artifact construction process, divided in three phases, and an evaluation within three startups. Results: We developed the HyMap, a hypotheses elicitation technique based on cognitive mapping. It consists of a process conducted by a facilitator using pre-defined questions, supported by a visual language to depict a cognitive map representing the founder's understanding of the product. Our evaluation showed that founders perceived the artifacts as clear, easy to use, and useful leading to hypotheses and facilitating their idea's visualization. Conclusion: From a theoretical perspective, our study provides a better understanding of the guidance founders use to develop their startups and, from a practical point of view, a technique to identify hypotheses in early-stage software startups. © 2022 Elsevier B.V.},
author_keywords={Experimentation;  Hypotheses elicitation;  Hypotheses engineering;  Software startups},
document_type={Article},
source={Scopus},
}

@ARTICLE{Jiang20221249,
author={Jiang, L. and Liu, H. and Jiang, H. and Zhang, L. and Mei, H.},
title={Heuristic and Neural Network Based Prediction of Project-Specific API Member Access},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={4},
pages={1249-1267},
doi={10.1109/TSE.2020.3017794},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099412855&doi=10.1109%2fTSE.2020.3017794&partnerID=40&md5=1991f947bb2ac7fcad0c4bdf77b9fb37},
abstract={Code completion is to predict the rest of a statement a developer is typing. Although advanced code completion approaches have greatly improved the accuracy of code completion in modern IDEs, it remains challenging to predict project-specific API method invocations or field accesses because little knowledge about such elements could be learned in advance. To this end, in this paper we propose an accurate approach called HeeNAMA to suggesting the next project-specific API member access. HeeNAMA focuses on a specific but common case of code completion: suggesting the following member access whenever a project-specific API instance is followed by a dot on the right hand side of an assignment. By focusing on such a specific case, HeeNAMA can take full advantages of the context of the code completion, including the type of the left hand side expression of the assignment, the identifier on the left hand side, the type of the base instance, and similar assignments typed in before. All such information together enables highly accurate code completion. Given an incomplete assignment, HeeNAMA generates the initial candidate set according to the type of the base instance, and excludes those candidates that are not type compatible with the left hand side of the assignment. If the enclosing project contains assignments highly similar to the incomplete assignment, it makes suggestions based on such assignments. Otherwise, it selects the one from the initial candidate set that has the greatest lexical similarity with the left hand side of the assignment. Finally, it employs a neural network to filter out risky predictions, which guarantees high precision. Evaluation results on open-source applications suggest that compared to the state-of-the-art approaches and the state-of-the-practice tools HeeNAMA improves precision and recall by 70.68 and 25.23 percent, relatively. © 1976-2012 IEEE.},
author_keywords={Code completion;  deep learning;  heuristic;  LSTM;  non-API},
document_type={Article},
source={Scopus},
}

@ARTICLE{Tsantalis2022930,
author={Tsantalis, N. and Ketkar, A. and Dig, D.},
title={RefactoringMiner 2.0},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={3},
pages={930-950},
doi={10.1109/TSE.2020.3007722},
note={cited By 25},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127582313&doi=10.1109%2fTSE.2020.3007722&partnerID=40&md5=6c1d2b43f85bf226ddb7d2c2da6cce3c},
abstract={Refactoring detection is crucial for a variety of applications and tasks: (i) empirical studies about code evolution, (ii) tools for library API migration, (iii) code reviews and change comprehension. However, recent research has questioned the accuracy of the state-of-the-art refactoring mining tools, which poses threats to the reliability of the detected refactorings. Moreover, the majority of refactoring mining tools depend on code similarity thresholds. Finding universal threshold values that can work well for all projects, regardless of their architectural style, application domain, and development practices is extremely challenging. Therefore, in a previous work [N. Tsantalis, M. Mansouri, L. M. Eshkevari, D. Mazinanian, and D. Dig, Accurate and efficient refactoring detection in commit history, in 40th International Conference on Software Engineering, 2018, pp. 483-494], we introduced the first refactoring mining tool that does not require any code similarity thresholds to operate. In this work, we extend our tool to support low-level refactorings that take place within the body of methods. To evaluate our tool, we created one of the most accurate, complete, and representative refactoring oracles to date, including 7,226 true instances for 40 different refactoring types detected by one (minimum) up to six (maximum) different tools, and validated by one up to four refactoring experts. Our evaluation showed that our approach achieves the highest average precision (99.6 percent) and recall (94 percent) among all competitive tools, and on median is 2.6 times faster than the second faster competitive tool. © 1976-2012 IEEE.},
author_keywords={commit;  execution time;  git;  precision;  recall;  Refactoring mining;  refactoring oracle},
document_type={Article},
source={Scopus},
}

@ARTICLE{Clarisó2022,
author={Clarisó, R. and Cabot, J.},
title={User-driven diverse scenario exploration in model finders},
journal={Science of Computer Programming},
year={2022},
volume={215},
doi={10.1016/j.scico.2021.102745},
art_number={102745},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120949152&doi=10.1016%2fj.scico.2021.102745&partnerID=40&md5=a4061b1c0d48bda6860b7b390c135553},
abstract={Model finders can build instances of declarative specifications that satisfy a set of correctness constraints. Some model finders ensure some degree of diversity among the instances they compute. Nevertheless, each model finder uses its own definition of diversity, that may or may not match designer intent. In this paper, we propose a procedure that enables designers to capture the desired notion of diversity they are looking for. Using a simple domain-specific language, they can specify what elements in the specification are relevant when comparing the differences between two instances. This information can then be used to make any model finder diversity-aware while using it as a black box. As a proof of concept, this approach has been implemented on top of the Alloy Analyzer. © 2021 The Author(s)},
author_keywords={Clustering;  Diversity;  Model-driven engineering;  Testing;  Verification and validation},
document_type={Article},
source={Scopus},
}

@ARTICLE{Langdon202271,
author={Langdon, W.B.},
title={Genetic programming convergence},
journal={Genetic Programming and Evolvable Machines},
year={2022},
volume={23},
number={1},
pages={71-104},
doi={10.1007/s10710-021-09405-9},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113860512&doi=10.1007%2fs10710-021-09405-9&partnerID=40&md5=d0154b07ce5219d1f1180795f416a646},
abstract={We study both genotypic and phenotypic convergence in GP floating point continuous domain symbolic regression over thousands of generations. Subtree fitness variation across the population is measured and shown in many cases to fall. In an expanding region about the root node, both genetic opcodes and function evaluation values are identical or nearly identical. Bottom up (leaf to root) analysis shows both syntactic and semantic (including entropy) similarity expand from the outermost node. Despite large regions of zero variation, fitness continues to evolve and near zero crossover disruption suggests improved GP systems within existing memory use. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
author_keywords={AVX vector instructions;  Bottom up incremental evaluation;  Diversity;  Evolutionary computation;  PIE, propagation, infection, and execution;  SIMD parallel processing;  Stochastic search},
document_type={Article},
source={Scopus},
}

@ARTICLE{Reinhartz-Berger2022773,
author={Reinhartz-Berger, I. and Zamansky, A.},
title={Reuse of Similarly Behaving Software Through Polymorphism-Inspired Variability Mechanisms},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={3},
pages={773-785},
doi={10.1109/TSE.2020.3001512},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086717356&doi=10.1109%2fTSE.2020.3001512&partnerID=40&md5=9fea33047982550fd79748d1c43e297d},
abstract={In many cases, software artifacts share similarity across projects and development teams. However, often this similarity is only partially reflected on the level of design and implementation, and therefore the possibilities for its detection are limited in current variability analysis, clone detection, and application search approaches. In this paper, we propose a method for identification and comparison of similarly behaving software. The method, supported by a prototype tool, analyzes the behavioral similarity of object-oriented code artifacts based on shallow (behavior interface) and deep (behavior transformation) descriptions of the exhibited operations. It further recommends on suitable mechanisms inspired by the notion of polymorphism in order to guide and support current and future reuse. The approach was evaluated on two data-sets, obtained following two different scenarios: clone-and-own and independent development by different teams. © 1976-2012 IEEE.},
author_keywords={code clones;  polymorphism;  reuse;  software product line engineering;  Variability},
document_type={Article},
source={Scopus},
}

@ARTICLE{Rafi2022,
author={Rafi, S. and Akbar, M.A. and Yu, W. and Alsanad, A. and Gumaei, A. and Sarwar, M.U.},
title={Exploration of DevOps testing process capabilities: An ISM and fuzzy TOPSIS analysis},
journal={Applied Soft Computing},
year={2022},
volume={116},
doi={10.1016/j.asoc.2021.108377},
art_number={108377},
note={cited By 11},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123267672&doi=10.1016%2fj.asoc.2021.108377&partnerID=40&md5=5ac377307969dca74a0819a9a9666f74},
abstract={DevOps is an emerging paradigm that refer to a collaborative culture of development and operation teams aiming to develop the high quality software product. Software organizations are adopting DevOps culture for software development and easy maintenance instead of using traditional SDLC mechanism. To enter the production stage, in DevOps process, the software product have to pass through quality gates were the software are tested during development phase to meet the established targeted criteria. This indicates that the mechanism of testing in DevOps process is not straightforward, and to establish strong DevOps testing platform there is a need to explore more automated testing practices. Thus, using multivocal literature review approach, we have selected 39 studies and identify the 20 testing capabilities. Finally, the interpretive structure modeling (ISM) and fuzzy technique for order preference by similarity to ideal solution (fuzzy TOPSIS) were applied. The results shows that (C2, CCi=0.808; C6, CCi=0.720; and C3, CCi=0.705) are top ranked testing capabilities. Using analysis results, we develop a holistic structure of testing capabilities to show their inter-relationship with each other and their priorities to select the best testing capabilities for DevOps process. © 2021 Elsevier B.V.},
author_keywords={DevOps;  Fuzzy TOPSIS;  ISM;  MICMAC;  Testing capabilities},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mehlawat2022,
author={Mehlawat, M.K. and Mahajan, D.},
title={A fuzzy approach for evaluation and selection of performance testing tools for modular software development},
journal={International Journal of Reliability, Quality and Safety Engineering},
year={2022},
volume={29},
number={1},
doi={10.1142/S021853932150039X},
art_number={2150039},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116739716&doi=10.1142%2fS021853932150039X&partnerID=40&md5=a400fddb60e5bdfa06caf8350582d642},
abstract={Performance of a software is an important feature to determine the quality of the software developed. Performance testing of modular software is a time consuming and costly task. Several performance testing tools (PTTs) are available in the market which help software developers to test their software performance. In this paper, we propose an integrated multiobjective optimization model for evaluation and selection of best-fit PTT for modular software system. The total performance tool cost is minimized and the fitness evaluation score of the PTTs is maximized. The fitness evaluation of PTT is done based on various attributes by making use of the Technique for Order Preference by Similarity to Ideal Solution (TOPSIS). The model allows the software developers to select the number of PTTs as per their requirement. The individual performance of the modules is considered based on some performance properties. The reusability constraints are considered, as a PTT can be used in the same module to test different properties and/or it can be used in different modules to test same or different performance properties. A real-world case study from the domain of enterprise resource planning (ERP) is used to show the working of the suggested optimization model. © 2022 World Scientific Publishing Company.},
author_keywords={fuzzy optimization;  modular software development;  Multiple criteria optimization;  performance testing tools selection;  software performance;  TOPSIS},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Gu20221004,
author={Gu, S. and Liu, J. and Hui, Z. and Liu, W. and Chen, Z.},
title={MetaA: Multi-Dimensional Evaluation of Testing Ability via Adversarial Examples in Deep Learning},
journal={IEEE International Conference on Software Quality, Reliability and Security, QRS},
year={2022},
volume={2022-December},
pages={1004-1013},
doi={10.1109/QRS57517.2022.00104},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151469987&doi=10.1109%2fQRS57517.2022.00104&partnerID=40&md5=d306d431542e53c1a0df4e98274a999e},
abstract={Deep learning (DL) has shown superior performance in many areas, making the quality assurance of DL-based software particularly important. Adversarial examples are generated by deliberately adding subtle perturbations in input samples and can easily attack less reliable DL models. Most existing works only utilize a single metric to evaluate the generated adversarial examples, such as attacking success rate or structure similarity measure. The problem is that they cannot avoid extreme testing situations and provide multifaceted evaluation results.This paper presents MetaA, a multi-dimensional evaluation framework for testing ability of adversarial examples in deep learning. Evaluating the testing ability represents measuring the testing performance to make improvements. Specifically, MetaA performs comprehensive validation on generating adversarial examples from two horizontal and five vertical dimensions. We design MetaA according to the definition of the adversarial examples and the issue mentioned in [1] that how to enrich the evaluation dimension rather than merely quantifying the improvement of DL and software.We conduct several analyses and comparative experiments vertically and horizontally to evaluate the reliability and effectiveness of MetaA. The experimental results show that MetaA can avoid speculation and reach agreement among different indicators when they reflect inconsistencies. The detailed and comprehensive analysis of evaluation results can further guide the optimization of adversarial examples and the quality assurance of DL-based software. © 2022 IEEE.},
author_keywords={adversarial samples;  data augmentation;  deep learning testing;  educational exploration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Zhang2022537,
author={Zhang, Y. and Wang, Y. and Liu, Y. and Pang, Z. and Fang, B.},
title={PDG2Vec: Identify the Binary Function Similarity with Program Dependence Graph},
journal={IEEE International Conference on Software Quality, Reliability and Security, QRS},
year={2022},
volume={2022-December},
pages={537-548},
doi={10.1109/QRS57517.2022.00061},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151439345&doi=10.1109%2fQRS57517.2022.00061&partnerID=40&md5=0cf27f9235642f97ddcd85a43810858b},
abstract={Binary code similarity identification is an important technique applied to many security applications (e.g., plagiarism detection, bug search). The primary challenge of this research topic is how to extract sufficient information from the binary code for similarity comparison. Although numerous approaches have been proposed to address the challenge, most of them leverage features determined by human experience or extracted using machine learning methods and ignore some critical technique semantic information. Additionally, they assess their approach exclusively in laboratory environments and lack real-world datasets. Both problems lead to the limited effectiveness of these methods in real application scenarios (e.g., vulnerable function search).In this paper, we propose a novel approach PDG2Vec, which extracts the data dependence graph and control dependence graph (i.e., program dependence graph (PDG)) as the features of functions and uses them for identifying function similarity. Meanwhile, we design several strategies to optimize the PDG's construction and use them in similarity comparison to balance time-consuming and accuracy. We implement the prototype of PDG2Vec, which can perform binary code similarity comparison across architectures of x86, x86-64, MIPS32, ARM32, and ARM64. We evaluate PDG2Vec with two datasets. The experimental results show that PDG2Vec is resilient to cross-architecture and extracts more precise semantics than other approaches. Moreover, PDG2Vec outperforms the state-of-the-art tools in the vulnerable function search scenario and has excellent performance. © 2022 IEEE.},
author_keywords={Binary function similarity;  binary lifting;  semantic extraction;  static analysis;  vulnerable function search},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Doroshchenko20221,
author={Doroshchenko, I.A. and Znamenskaya, I.A. and Sysoev, N.N. and Lutsky, A.E.},
title={High-speed Flow Structures Detection and Tracking in Multiple Shadow Images with Matching to CFD using Convolutional Neural Networks},
journal={Scientific Visualization},
year={2022},
volume={14},
number={4},
pages={1-11},
doi={10.26583/sv.14.4.01},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148286588&doi=10.26583%2fsv.14.4.01&partnerID=40&md5=5f895ec19cd04296af317dbffa70e3eb},
abstract={Shadowgraph imaging has been widely used to study flow fields in experimental fluid dy-namics. Nowadays high-speed cameras allow to obtain millions of frames per second. Thus, it is not possible to analyze and process such large data sets manually and automatic image pro-cessing software is required. In the present study a software for automatic flow structures de-tection and tracking was developed based on the convolutional neural network (the network architecture is based on the YOLOv2 algorithm). Auto ML techniques were used to automati-cally tune model and hyperparameters and speed-up model development and training pro-cess. The neural network was trained to detect shock waves, thermal plumes, and solid parti-cles in the flow with high precision. We successfully tested out software on high-speed shad-owgraph recordings of gas flow in shock tube with shock wave Mach number M = 2-4.5. Also, we performed CFD to simulate the same flow. In recent decades, the amount of data in nu-merical simulations has grown significantly due to the growth in performance of computers. Thus, machine learning is also required to process large arrays of CFD results. We developed another ML tool for experimental and simulated by CFD shadowgraph images matching. Our algorithm is based on the VGG16 deep neural network for feature vector extraction and k-nearest neighbors algorithm for finding the most similar images based on the cosine similari-ty. We successfully applied our algorithm to automatically find the corresponding experi-mental shadowgraph image for each CFD image of the flow in shock tube with a rectangular obstacle in the flow channel. © 2022 National Research Nuclear University. All rights reserved.},
author_keywords={Auto ML;  CFD;  convolutional neural network;  cosine similarity;  High-speed shadowgraphy;  k-nearest neighbors;  object detection;  shock wave;  VGG16;  YOLOv2},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Mosin202276,
author={Mosin, V. and Staron, M. and Durisic, D. and Neto, F.G.D.O. and Pandey, S.K. and Koppisetty, A.C.},
title={Comparing Input Prioritization Techniques for Testing Deep Learning Algorithms},
journal={Proceedings - 48th Euromicro Conference on Software Engineering and Advanced Applications, SEAA 2022},
year={2022},
pages={76-83},
doi={10.1109/SEAA56994.2022.00020},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147715090&doi=10.1109%2fSEAA56994.2022.00020&partnerID=40&md5=de3987f41056d79a5f2ad0d0ecbe5fe6},
abstract={Deep learning (DL) systems are becoming an essential part of software systems, so it is necessary to test them thoroughly. This is a challenging task since the test sets can grow over time as the new data is being acquired, and it becomes time-consuming. Input prioritization is necessary to reduce the testing time since prioritized test inputs are more likely to reveal the erroneous behavior of a DL system earlier during test execution. Input prioritization approaches have been rudimentary analyzed against each other, this study compares different input prioritization techniques regarding their effectiveness and efficiency. This work considers surprise adequacy, autoencoder-based, and similarity-based input prioritization approaches in the example of testing a DL image classification algorithms applied on MNIST, Fashion-MNIST, CIFAR-10, and STL-10 datasets. To measure effectiveness and efficiency, we use a modified APFD (Average Percentage of Fault Detected), and set up & execution time, respectively. We observe that the surprise adequacy is the most effective (0.785 to 0.914 APFD). The autoencoder-based and similarity-based techniques are less effective, with the performance from 0.532 to 0.744 APFD and 0.579 to 0.709 APFD, respectively. In contrast, the similarity-based and surprise adequacy-based approaches are the most and least efficient, respectively. The findings in this work demonstrate the trade-off between the considered input prioritization techniques to understanding their practical applicability for testing DL algorithms. © 2022 IEEE.},
author_keywords={deep learning;  diversity-based testing;  test prioritization},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Shi2022306,
author={Shi, Y. and Shi, J.},
title={An adaptive disturbance multi-objective evolutionary algorithm based on decomposition},
journal={International Journal of Modelling, Identification and Control},
year={2022},
volume={41},
number={4},
pages={306-315},
doi={10.1504/ijmic.2022.128314},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147541026&doi=10.1504%2fijmic.2022.128314&partnerID=40&md5=994184a018d9c0ddb92f7068059b7d8a},
abstract={In solving multi-objective optimisation problems, the uniformly distributed weight vector of decomposition based multi-objective evolutionary algorithm (MOEA/D) is not completely suitable for the non-uniformly distributed Pareto front (PF). In order to solve the situation above, this paper proposes an adaptive disturbance multi-objective evolutionary algorithm based on decomposition (AD-MOEA/D), which introduces the disturbance individuals and disturbance weight vectors during the evolution. The disturbance individuals maintain the population diversity and improve convergence accuracy. The disturbance weight vectors assist the weight vectors to adjust adaptively and improve the distribution of PF. Besides, both disturbance individuals and disturbance weight vectors are produced according to the actual evolution, which will not participate in evolution when it is not necessary. The experimental results on multi-objective test functions show that the PF optimised by AD-MOEA/D has better convergence and distribution. Copyright © 2022 Inderscience Enterprises Ltd.},
author_keywords={decomposition;  disturbance individuals;  disturbance weight vectors;  multi-objective evolutionary algorithm},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Parthasarathy2022239,
author={Parthasarathy, A. and Krishnamachari, B.},
title={Partitioning and Placement of Deep Neural Networks on Distributed Edge Devices to Maximize Inference Throughput},
journal={2022 32nd International Telecommunication Networks and Applications Conference, ITNAC 2022},
year={2022},
pages={239-246},
doi={10.1109/ITNAC55475.2022.9998427},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146701197&doi=10.1109%2fITNAC55475.2022.9998427&partnerID=40&md5=8a43b7fc86fbe10389abb3e08d32a339},
abstract={Edge inference has become more widespread, as its diverse applications range from retail to wearable technology. Clusters of networked resource-constrained edge devices are becoming common, yet no system exists to split a DNN across these clusters while maximizing the inference throughput of the system. We present an algorithm which partitions DNNs and distributes them across a set of edge devices with the goal of minimizing the bottleneck latency and therefore maximizing inference throughput. The system scales well to systems of different node memory capacities and numbers of nodes. We find that we can reduce the bottleneck latency by 10× over a random algorithm and 35% over a greedy joint partitioning-placement algorithm. Furthermore we find empirically that for the set of representative models we tested, the algorithm produces results within 9.2% of the optimal bottleneck latency. © 2022 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mukhesh2022504,
author={Mukhesh, R.S.V. and Soni, A.},
title={Identifying similar sentences when processed by Apache Spark},
journal={Proceedings of 4th International Conference on Cybernetics, Cognition and Machine Learning Applications, ICCCMLA 2022},
year={2022},
pages={504-510},
doi={10.1109/ICCCMLA56841.2022.9989023},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146322552&doi=10.1109%2fICCCMLA56841.2022.9989023&partnerID=40&md5=38e3308fa94dcae4f0fb135569c7cde3},
abstract={Machine learning, simply put, is the practice of using computer programs to learn from data without hard-coded instructions. One use for this type of AI computation is 'similarity matching' - identifying how two texts might be similar. Advanced natural language processing technologies can now be used on a large scale to identify similar sentences when processed by Apache Spark and other open-source frameworks on top of distributed file systems such as HDFS, Hadoop Distributed File System (HDFS). Basic sentence similarity matching techniques are more appropriate for large scale tasks where limited resources are expected but overtime they would provide more accuracy. This paper will be focused on how to setup the training of specific question pairs on Apache Spark, which can allow algorithms to learn patterns in close matrices. The authors apply clustering algorithms, Apache Spark, and machine learning algorithms in this research to develop a model that evaluates whether two questions are the same in a dataset. This method yielded a 70% accuracy using relatively simple methods. © 2022 IEEE.},
author_keywords={Apache Spark;  clustering algorithms;  insert;  machine learning;  question similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shimari2022564,
author={Shimari, K. and Tanaka, M. and Ishio, T. and Matsushita, M. and Inoue, K. and Takanezawa, S.},
title={Selecting Test Cases based on Similarity of Runtime Information: A Case Study of an Industrial Simulator},
journal={Proceedings - 2022 IEEE International Conference on Software Maintenance and Evolution, ICSME 2022},
year={2022},
pages={564-567},
doi={10.1109/ICSME55016.2022.00077},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146249000&doi=10.1109%2fICSME55016.2022.00077&partnerID=40&md5=f744b6145353f71b669aa445d1933c59},
abstract={Regression testing is required to check the changes in behavior whenever developers make any changes to a software system. The cost of regression testing is a major problem because developers have to frequently update dependent components to minimize security risks and potential bugs. In this paper, we report a current practice in a company that maintains an industrial simulator as a critical component of their business. The simulator automatically records all the users' requests and the simulation results in storage. The feature provides a huge number of test cases for regression testing to developers; however, their time budget for testing is limited (i.e., at most one night). Hence, the developers need to select a small number of test cases to confirm both the simulation result and execution performance are unaffected by an update of a dependent component. In other words, the test cases should achieve high coverage while keeping diversity of execution time. To solve the problem, we have developed a clustering-based method to select test cases, using the similarity of execution traces produced by them. The developers have used the method for a half year; they recognize that the method is better than the previous rule-based method used in the company. © 2022 IEEE.},
author_keywords={Clustering;  Dynamic Analysis;  Software De-pendency;  Test Selection},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{DeAlmeidaNeto2022,
author={De Almeida Neto, A. and Pereira, R.L. and De Oliveira, R.C.L.},
title={Application of a Genetic Algorithm with Social Interaction to Search Based Testing},
journal={Proceedings - 2022 IEEE Latin American Conference on Computational Intelligence, LA-CCI 2022},
year={2022},
doi={10.1109/LA-CCI54402.2022.9981733},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146231318&doi=10.1109%2fLA-CCI54402.2022.9981733&partnerID=40&md5=68e1c260c9a741b746b780fa06ede5fc},
abstract={Genetic Algorithms are widely used in the Search Based Testing field [1] and in this work, a variation of the Genetic Algorithm with Social Interaction [2] is implemented and applied to the problem of generating software test data. This algorithm is shown to be robust and comparable to the state-of-the-art, having a greater population diversity while converging faster to the global optimal solution. © 2022 IEEE.},
author_keywords={game theory;  genetic algorithm;  search based testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Rabbani202216,
author={Rabbani, S.M. and Ahmad Gulzar, N. and Arshad, S. and Abid, S. and Shamail, S.},
title={A Comparative Analysis of Clone Detection Techniques on SemanticCloneBench},
journal={Proceedings - 2022 IEEE 16th International Workshop on Software Clones, IWSC 2022},
year={2022},
pages={16-22},
doi={10.1109/IWSC55060.2022.00011},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145781006&doi=10.1109%2fIWSC55060.2022.00011&partnerID=40&md5=ad6d98cab8caed0fb8bcba0a84433c4f},
abstract={Semantic code clone detection involves the detection of functionally similar code fragments which may otherwise be lexically, syntactically, or structurally dissimilar. The detection of semantic code clones has important applications in aspect mining and product line analysis. The accurate detection of semantic code clones is a challenging task and various techniques have been proposed. However, the evaluation of these techniques is performed using various datasets and we do not have a clear picture of the performance of these techniques relative to each other. Recently, SemanticCloneBench has been introduced as a benchmark for semantic clones. Now, we can use the SemanticCloneBench to effectively evaluate and compare the performance of semantic code clone detection techniques. In this paper, we compare the semantic code clone detection performance of three different code clone detection techniques namely FACER-CD, CodeBERT and NIL for Java code clones using SemanticCloneBench. FACER-CD performs API usage similarity-based clustering to detect clones, while CodeBERT is a deep-learning based approach which uses a pre-trained programming language model, and NIL is a token-based large-gapped code clones detector. FACER-CD, NIL, and CodeBERT show a recall of 64.3%, 12.7%, and 83.2% respectively on SemanticCloneBench. Using all three techniques together on the SemanticCloneBench dataset gives us an overall recall of 95.5% which is currently the best performance achieved on SemanticCloneBench. © 2022 IEEE.},
author_keywords={CodeBERT;  Deep Learning;  Large-Variance Clones;  Semantic Clone Detection;  Semantic Similarity;  Semantic-CloneBench},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Pizzolotto2022124491,
author={Pizzolotto, D. and Inoue, K.},
title={BinCC: Scalable Function Similarity Detection in Multiple Cross-Architectural Binaries},
journal={IEEE Access},
year={2022},
volume={10},
pages={124491-124506},
doi={10.1109/ACCESS.2022.3225100},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144093515&doi=10.1109%2fACCESS.2022.3225100&partnerID=40&md5=8ae32a214898625e92d94c6752ecb549},
abstract={With the undeniable increase in popularity of open source software, also the availability and reuse of source code have increased. While the detection of code clones helps tracking reuse and evolution while dealing with source code, little prior work exists that can be used in binary code. This is complicated by the increased difficulty posed by the compilation transformations. In this paper, we present a CFG refinement useful to find function-level clones in a fast and scalable way by comparing the high-level structure of multiple disassembled binaries altogether. We are capable of determining if functions belonging to other programs have been copied or reused, even when the processor architecture is different. Specifically, our algorithm consists in the extraction of the various functions flows and the reconstruction of a higher level structure, leveraging architectural differences and allowing efficient comparison in linear time with structural hashing. We implemented our idea in a tool called BinCC, and analyzed 24 million functions spanning different architectures and optimization levels. Results show that our approach can achieve precision between 91% and 99% within the same architecture and 75% in detecting clones among different architectures, and can also detect the presence of specific library functions inside an executable. Our approach can reach comparable precision of current state-of-the-art learning approaches while being three order of magnitude faster. © 2013 IEEE.},
author_keywords={Code clones;  compilers;  reverse engineering;  static code analysis},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Yunardi202289,
author={Yunardi, D.H. and Saputra, K. and Gunawan, B.},
title={Design and Development of Object Detection System in Augmented Reality Based Indoor Navigation Application (Case Study: Faculty of Mathematics and Sciences Building, Syiah Kuala University)},
journal={Proceedings of the International Conference on Electrical Engineering and Informatics},
year={2022},
volume={2022-September},
pages={89-94},
doi={10.1109/ICELTICs56128.2022.9932093},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142058093&doi=10.1109%2fICELTICs56128.2022.9932093&partnerID=40&md5=090c04ba3a65b16d0a6587e1439e31d8},
abstract={The Faculty of Mathematics and Natural Sciences building in Syiah Kuala University is a three-story building which consists of many rooms. The number of rooms causes the process of finding a room will take a lot of time. Currently, the informatics team in Syiah Kuala University is developing an application which implements Indoor Positioning System. One of the features is an indoor navigation feature based on Augmented Reality (AR). However, the route from one point to another is not always straightforward. Therefore, this research will design and develop an application which aims to detect obstacles present in the route from starting point to ending point. Obstacles that can be detected in this application are 3D objects, classified as human, bag, chair, potted plant and table or desk. In the application, TensorFlow Lite is used to detect obstacles in the form of 3D objects, where the distance is calculated using a triangle similarity method. Once an object is identified, the application will provide sound/voice notification, vibration as well as augmented reality image on the screen to inform the user about the obstacle. This is done by having ARCore Software Development Kit. To evaluate the system, tests were carried out which included walking speed limit testing, and usability testing. In the walking speed limit test, the maximum speed when using the application is in the range of 1.27 m/s to 1.35 m/s. Finally, the results of usability testing using Post-Study System Usability Questionnaire (PSSUQ) also show that the application can run well and can be accepted by users with a usability test score of 5.87. © 2022 IEEE.},
author_keywords={ARCore;  Augmented Reality;  Obstacle;  Post-Study System Usability Questionnaire;  TensorFlow Lite;  Triangle Similarity;  Usability},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yin2022661,
author={Yin, Y. and Han, R. and Song, Y. and Sun, S.},
title={Design and implementation of intelligent marking system based on deep learning algorithm},
journal={2022 IEEE International Conference on Advances in Electrical Engineering and Computer Applications, AEECA 2022},
year={2022},
pages={661-666},
doi={10.1109/AEECA55500.2022.9919090},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141437562&doi=10.1109%2fAEECA55500.2022.9919090&partnerID=40&md5=56f58279e76baf7d1eb851650ad0e9fc},
abstract={In the practical application of intelligent marking system, the feedback time is too long. An intelligent marking system based on deep learning algorithm is designed. Hardware part: through THE GPIO pin address expansion access space, the circuit and SDRAM device DQM connection; Software: obtain the text similarity of the test paper, summarize the equivalent semantics of the program, locate the position of the target answer sheet from the target image, optimize the matching mode of the answer sheet template with deep learning algorithm, and set the software correction function of the intelligent marking system. Test results: The average feedback time of the intelligent marking system in this paper is 130.043ms, 155.423ms and 154.058ms, respectively, compared with the other two intelligent marking systems, indicating that the feedback time of the intelligent marking system in this paper is lower after the combination of deep learning algorithm. © 2022 IEEE.},
author_keywords={data information;  deep learning;  intelligent marking;  marking machine;  marking method;  technology education},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shi2022153,
author={Shi, Y.-Q. and Huang, S. and Wan, J.-Y.},
title={A Reuse-oriented Clustering Method for Test Cases},
journal={Proceedings - 2022 9th International Conference on Dependable Systems and Their Applications, DSA 2022},
year={2022},
pages={153-162},
doi={10.1109/DSA56465.2022.00028},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141391543&doi=10.1109%2fDSA56465.2022.00028&partnerID=40&md5=2a56fddf83915ef3662f2257f0091ce9},
abstract={Different types of data are generated in each stage of software testing. There are a lot of test case data in the historical test asset library, including some case data with high similarity. Clustering test cases can effectively reduce the resource consumption and time cost of reusing test cases and improve the efficiency of test case recommendation. This paper proposes a reuse-oriented clustering method for test cases. Firstly, test case data of historical test projects of command-and-control system are collected, test case corpus is constructed, and word segmentation experiments are carried out using this corpus. Experimental tools are determined by comparing the experimental effects of current mainstream natural language word segmentation tools with self-defining dictionaries. Then, the keywords of test cases are extracted by keyword extraction algorithm, and the test case package is obtained by Spectral Clustering algorithm based on the test case similarity matrix and keyword similarity matrix. Finally, the validity of the proposed method is verified by experimental comparison on the constructed test case corpus. © 2022 IEEE.},
author_keywords={clustering method for test cases;  reuse test cases;  test asset library},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20222929,
author={Chen, Y. and Alhanahnah, M. and Sabelfeld, A. and Chatterjee, R. and Fernandes, E.},
title={Practical Data Access Minimization in Trigger-Action Platforms},
journal={Proceedings of the 31st USENIX Security Symposium, Security 2022},
year={2022},
pages={2929-2945},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140970131&partnerID=40&md5=02e5f1bbd5f13c8c99a9cde691918154},
abstract={Trigger-Action Platforms (TAPs) connect disparate online services and enable users to create automation rules in diverse domains such as smart homes and business productivity. Unfortunately, the current design of TAPs is flawed from a privacy perspective, allowing unfettered access to sensitive user data. We point out that it suffers from two types of overprivilege: (1) attribute-level, where it has access to more data attributes than it needs for running user-created rules; and (2) token-level, where it has access to more APIs than it needs. To mitigate overprivilege and subsequent privacy concerns we design and implement minTAP, a practical approach to data access minimization in TAPs. Our key insight is that the semantics of a user-created automation rule implicitly specifies the minimal amount of data it needs. This allows minTAP to leverage language-based data minimization to apply the principle of least-privilege by releasing only the necessary attributes of user data to TAPs and fending off unrelated API access. Using real user-created rules on the popular IFTTT TAP, we demonstrate that minTAP sanitizes a median of 4 sensitive data attributes per rule, with modest performance overhead and without modifying IFTTT. © USENIX Security Symposium, Security 2022.All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gao2022,
author={Gao, T. and Jia, X. and Jiang, R. and He, Y. and Yang, M.},
title={SaaS Service Combinatorial Trustworthiness Measurement Method Based on Markov Theory and Cosine Similarity},
journal={Security and Communication Networks},
year={2022},
volume={2022},
doi={10.1155/2022/7080367},
art_number={7080367},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138921635&doi=10.1155%2f2022%2f7080367&partnerID=40&md5=848831d6ec0f2fbb5bccbee728d6288c},
abstract={With the rapid progress of information technology, cloud computing and cloud services are widely accepted and applied to all aspects of social life. In the cloud computing environment, SaaS (Software-as-a-Service) services have become the main form of software services. For SaaS services, evolutionary and iterative development methods have become the main methods of software system construction. For systems with high trustworthiness, the independent trustworthiness of each SaaS service has a great impact on the overall status. However, SaaS services with high independent trustworthiness do not always build highly trusted software systems. The combinatorial trustworthiness between SaaS services is as important as the independent trustworthiness of each SaaS service. This paper takes combinatorial trustworthiness between SaaS services as the research object. Combinatorial trustworthiness measurement method based on Markov and cosine similarity theory is proposed. The feasibility and effectiveness of the proposed method are verified through simulation experiments. Applicable scenarios, advantages, and disadvantages of the proposed method are shown through the comparison of different measurement methods. The proposed method provides theoretical and technical support for users to select SaaS services suitable for their application scenarios, build cloud service systems, and monitor the operation status of cloud service systems. © 2022 Tilei Gao et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Huang202264,
author={Huang, S. and Yang, Z. and Zheng, C. and Wang, Y. and Du, J. and Ding, Y. and Wan, J.},
title={Intellectual Property Right Confirmation System Oriented to Crowdsourced Testing Services},
journal={Proceedings - 2022 International Conference on Blockchain Technology and Information Security, ICBCTIS 2022},
year={2022},
pages={64-68},
doi={10.1109/ICBCTIS55569.2022.00026},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137333002&doi=10.1109%2fICBCTIS55569.2022.00026&partnerID=40&md5=cf8f5a3fbdfd0caec7bfd889a13f407e},
abstract={In the process of crowdsourced testing service, the intellectual property of crowdsourced testing has been faced with problems such as code plagiarism, difficulties in confirming rights and unreliability of data. Blockchain is a decentralized, tamper-proof distributed ledger, which can help solve current problems. This paper proposes an intellectual property right confirmation system oriented to crowdsourced testing services, combined with blockchain, IPFS (Interplanetary file system), digital signature, code similarity detection to realize the confirmation of crowdsourced testing intellectual property. The performance test shows that the system can meet the requirements of normal crowdsourcing business as well as high concurrency situations. © 2022 IEEE.},
author_keywords={Crowdsourced Testing;  Intellectual Property;  Right Confirmation},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Ahmad20223,
author={Ahmad, H. and Cashin, P. and Forrest, S. and Weimer, W.},
title={Digging into Semantics: Where Do Search-Based Software Repair Methods Search?},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
year={2022},
volume={13399 LNCS},
pages={3-18},
doi={10.1007/978-3-031-14721-0_1},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137265997&doi=10.1007%2f978-3-031-14721-0_1&partnerID=40&md5=5978635e814aedc97aa75177a135e3a9},
abstract={Search-based methods are a popular approach for automatically repairing software bugs, a field known as automated program repair (APR). There is increasing interest in empirical evaluation and comparison of different APR methods, typically measured as the rate of successful repairs on benchmark sets of buggy programs. Such evaluations, however, fail to explain why some approaches succeed and others fail. Because these methods typically use syntactic representations, i.e., source code, we know little about how the different methods explore their semantic spaces, which is relevant for assessing repair quality and understanding search dynamics. We propose an automated method based on program semantics, which provides quantitative and qualitative information about different APR search-based techniques. Our approach requires no manual annotation and produces both mathematical and human-understandable insights. In an empirical evaluation of 4 APR tools and 34 defects, we investigate the relationship between search-space exploration, semantic diversity and repair success, examining both the overall picture and how the tools’ search unfolds. Our results suggest that population diversity alone is not sufficient for finding repairs, and that searching in the right place is more important than searching broadly, highlighting future directions for the research community. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
author_keywords={Patch diversity;  Program repair;  Semantic search spaces},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jiang2022537,
author={Jiang, B. and Li, Z. and Huang, Y. and Zhang, Z. and Chan, W.K.},
title={WasmFuzzer: A Fuzzer for WebAssembly Virtual Machines},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2022},
pages={537-542},
doi={10.18293/SEKE2022-165},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137166999&doi=10.18293%2fSEKE2022-165&partnerID=40&md5=326b50e1f883f2f809c9ea095ede3485},
abstract={WebAssembly is a fast, safe, and portable low-level language suitable for diverse application scenarios. And The WebAssembly virtual machines are widely used by Web browsers or Blockchain platforms as execution engine. When there is a bug in the implementation of the Wasm virtual machine, the execution of WebAssembly may lead to errors or vulnerability in the application. Due to the grammar checks by WASM VMs, fuzzing at the binary level is ineffective to expose the bugs because most inputs cannot reach the deep logic within the WASM VM. In this work, we propose WasmFuzzer, a bytecode level fuzzing tool for WASM VMs. WasmFuzzer proposes to generate initial seeds for Fuzzing at the Wasm bytecode level and it also designs a systematic set of mutation operators for Wasm bytecode. Furthermore, WasmFuzzer proposes an adaptive mutation strategy to search for the best mutation operators for different fuzzing targets. Our evaluation on 3 real-life Wasm VMs shows that WasmFuzzer can significantly outperform AFL in terms of both code coverage and unique crash. © 2022 Knowledge Systems Institute Graduate School. All rights reserved.},
author_keywords={fuzzing;  Virtual Machine;  WebAssembly},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ullah2022190,
author={Ullah, M.R. and Chowdhury, N.S. and Tawsif, F.M.},
title={Impact of Combining Syntactic and Semantic Similarities on Patch Prioritization while using the Insertion Mutation Operators},
journal={Proceedings of the International Conference on Software Engineering and Knowledge Engineering, SEKE},
year={2022},
pages={190-195},
doi={10.18293/SEKE2022-047},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137163477&doi=10.18293%2fSEKE2022-047&partnerID=40&md5=8974f880b6e62cd9809b61a5795914f7},
abstract={Patch prioritization ranks candidate patches based on their likelihood of being correct. The fixing ingredients that are more likely to be the fix for a bug, share a high contextual similarity. A recent study shows that combining both syntactic and semantic similarity for capturing the contextual similarity, can do better in prioritizing patches. In this study, we evaluate the impact of combining the syntactic and semantic features on patch prioritization using the Insertion mutation operators. This study inspects the result of different combinations of syntactic and semantic features on patch prioritization. As a pilot study, the approach uses genealogical similarity to measure the semantic similarity and normalized longest common subsequence, normalized edit distance, cosine similarity, and Jaccard similarity index to capture the syntactic similarity. It also considers Anti-Pattern to filter out the incorrect plausible patches. The combination of both syntactic and semantic similarity can reduce the search space to a great extent. Also, the approach generates fixes for the bugs before the incorrect plausible one. We evaluate the techniques on the IntroClassJava benchmark using Insertion mutation operators and successfully generate fixes for 6 bugs before the incorrect plausible one. So, considering the previous study, the approach of combining syntactic and semantic similarity can able to solve a total number of 25 bugs from the benchmark, and to the best of our knowledge, it is the highest number of bug solved than any other approach. The correctness of the generated fixes are further checked using the publicly available results of CapGen and thus for the generated fixes, the approach achieves a precision of 100%. © 2022 Knowledge Systems Institute Graduate School. All rights reserved.},
author_keywords={automated program repair;  contextual similarity;  mutation operation;  patch prioritization;  semantic similarity;  syntactic similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Tebes2022,
author={Tebes, G. and Lew, P. and Olsina, L.},
title={Syntactic and Semantic Similarities and Discrepancies between Terms of Glossaries for Software Testing},
journal={CIbSE 2022 - XXV Ibero-American Conference on Software Engineering},
year={2022},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137050289&partnerID=40&md5=56f15511da35d37ac2fa92d1aac24673},
abstract={For software testing, this work performs a comparison and analysis of syntactic and semantic similarities and discrepancies between 3 glossaries. To conduct the study, 8 terminological categories were conceived, which were used to categorize each glossary term, considering the intended semantics. Also, to count the occurrence frequency of a term in the glossaries, a tool was built that also takes into account the matching of synonyms. Then, the analysis of similarities and discrepancies, as well as absent terms for a subset of them, is performed using metrics and expert interpretations. This study identifies several disagreements in standard terminologies that should merit further attention and efforts to promote harmonization amongst the authors/ publishers of these glossaries with the overarching end goal of assisting their readers in learning and understanding the domain of software testing. © 2022 CIbSE 2022 - XXV Ibero-American Conference on Software Engineering. All rights reserved.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Jha202288,
author={Jha, S. and Cui, S. and Tsai, T. and Hari, S.K.S. and Sullivan, M.B. and Kalbarczyk, Z.T. and Keckler, S.W. and Iyer, R.K.},
title={Exploiting Temporal Data Diversity for Detecting Safety-critical Faults in AV Compute Systems},
journal={Proceedings - 52nd Annual IEEE/IFIP International Conference on Dependable Systems and Networks, DSN 2022},
year={2022},
pages={88-100},
doi={10.1109/DSN53405.2022.00021},
note={cited By 3},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136327862&doi=10.1109%2fDSN53405.2022.00021&partnerID=40&md5=378844a25c108069eea05c9cf8de37e6},
abstract={Silent data corruption caused by random hardware faults in autonomous vehicle (AV) computational elements is a significant threat to vehicle safety. Previous research has explored design diversity, data diversity, and duplication techniques to detect such faults in other safety-critical domains. However, these are challenging to use for AVs in practice due to significant resource overhead and design complexity. We propose, DiverseAV, a low-cost data-diversity-based redundancy technique for detecting safety-critical random hardware faults in computational elements. DiverseAV introduces data-diversity between the redundant agents by exploiting the temporal semantic consistency available in the AV sensor data. DiverseAV is a black-box technique that offers a plug-and-play solution as it requires no knowledge of the internals of the AI agent responsible for executing driving decisions, requiring little to no modification to the agent itself for achieving high coverage of transient and permanent hardware faults. It is commercially viable because it avoids software modifications to agents that are costly in terms of development and testing time. Specifically, DiverseAV distributes the sensor data between the two software agents in a round-robin manner. As a result, the sensor data for two consecutive time steps are semantically similar in terms of their worldview but significantly different at the bit level, thus ensuring the state and data diversity between the two agents necessary for detecting faults. We demonstrate DiverseAV using an open-source self-driving AI agent which is controlling a car in an open-source world simulator. © 2022 IEEE.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Merlo2022983,
author={Merlo, E. and Margier, M. and Jourdan, G.-V. and Onut, I.-V.},
title={Phishing Kits Source Code Similarity Distribution: A Case Study},
journal={Proceedings - 2022 IEEE International Conference on Software Analysis, Evolution and Reengineering, SANER 2022},
year={2022},
pages={983-994},
doi={10.1109/SANER53432.2022.00116},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135791796&doi=10.1109%2fSANER53432.2022.00116&partnerID=40&md5=78424caf1426395f467e333cef1375bc},
abstract={Attackers ('phishers') typically deploy source code in some host website to impersonate a brand or in general a situation in which a user is expected to provide some personal information of interest to phishers (e.g. credentials, credit card number). Phishing kits are ready-to-deploy sets of files that can be simply copied on a web server and used almost as they are. In this paper, we consider the static similarity analysis of the source code of 20871 phishing kits totalling over 182 million lines of PHP, Javascript and HTML code, that have been collected during phishing attacks and recovered by forensics teams. Reported experimental results show that as much as 90% of the analyzed kits share 90% or more of their source code with at least another kit. Differences are small, less than about 1000 programming words - identifiers, constants, strings and so on - in 40% of cases. A plausible lineage of phishing kits is presented by connecting together kits with the highest similarity. Obtained results show a very different reconstructed lineage for phishing kits when compared to a publicly available application such as Wordpress. Observed kits similarity distribution is consistent with the assumed hypothesis that kit propagation is often based on identical or near-identical copies at low cost changes. The proposed approach may help classifying new incoming phishing kits as 'near-copy' or 'intellectual leaps' from known and already encountered kits. This could facilitate the identification and classification of new kits as derived from older known kits. © 2022 IEEE.},
author_keywords={n/a},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Ghanbari202216,
author={Ghanbari, A.},
title={Revisiting Object Similarity-based Patch Ranking in Automated Program Repair: An Extensive Study},
journal={Proceedings - International Workshop on Automated Program Repair, APR 2022},
year={2022},
pages={16-23},
doi={10.1145/3524459.3527354},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135194780&doi=10.1145%2f3524459.3527354&partnerID=40&md5=6e83f641209a07ae02dce342654c13f7},
abstract={Test-based generate-and-validate automated program repair (APR) systems often generate plausible patches that pass the test suite without fixing the bug. So far, several approaches for automatic assessment of the APR-generated patches are proposed. Among them, dynamic patch correctness assessment relies on comparing run-time information obtained from the program before and after patching. Object similarity-based dynamic patch ranking approaches, specifically, capture system state snapshots after the impact point of patches and express behavior differences in term of object graphs similarities. Dynamic approaches rely on the assumption that, when running the originally passing test cases, the correct patches will not alter the program behavior in a significant way, but such patches will significantly change program behavior for the failing test cases. This paper presents the results of an extensive empirical study on two object similarity-based approaches, i.e., ObjSim and CIP, to rank 1,290 APR-generated patches, used in previous APR research. We found that although ObjSim outperforms CIP, in terms of the number of patches ranked in top-1 position, it still does not offer an improvement over random baseline ranking, representing the setting with no automatic patch correctness assessment in place. This observation warrants further research on the validity of the assumptions underlying these two techniques and the techniques based on similar assumptions. © 2022 ACM.},
author_keywords={Automated Program Repair;  Object Similarity;  Patch Ranking},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Biswas2022,
author={Biswas, J. and Yau, D.K.Y. and Zihao, L. and Ming, Y. and Lunn, K.I. and Nan, T.K. and Zhimin, Z. and Chua, J. and Ann, T.W. and Kean, H.Y.},
title={Reliability assessment of patched SCADA EMS/DMS servers through similarity matching},
journal={2022 IEEE Power and Energy Society Innovative Smart Grid Technologies Conference, ISGT 2022},
year={2022},
doi={10.1109/ISGT50606.2022.9817486},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134894611&doi=10.1109%2fISGT50606.2022.9817486&partnerID=40&md5=b20a758ab68a645a8d93a3a8a067070a},
abstract={From a national cybersecurity standpoint, patching of software and operating system vulnerabilities is a critical need. However, since patching may render a system unreliable, utility companies in the energy sector are reluctant to carry out patching on their substation SCADA control servers. In this paper, we present a similarity matching algorithm for assessing the reliability of software after patch application. Our methodology makes use of a backup control center in order to test the patched software. By running the test on input from field remote terminal units streamed from the same input source as that presented to the main control system, our algorithm is able to examine the degree of similarity of logged outputs, and highlight possible departures from expected behaviour. A metric called the fidelity score for the degree of similarity is presented along with initial results from a laboratory testbed. Initial results show that when patches are acceptable the algorithm consistently produces fidelity scores above 95%. © 2022 IEEE.},
author_keywords={EMS/DMS;  SCADA;  server patching;  syslog},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Yanis2022328,
author={Yanis, R.Z.I. and Priyadi, Y. and Puspitasari, S.Y.},
title={Measurement of Similarity between Use Case Description and Sequence Diagram in Software Requirement Specification using Text Analysis for Dtrain Application},
journal={Proceedings - 2022 2nd International Conference on Electronic and Electrical Engineering and Intelligent System, ICE3IS 2022},
year={2022},
pages={328-333},
doi={10.1109/ICE3IS56585.2022.10010259},
note={cited By 5},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134879493&doi=10.1109%2fICE3IS56585.2022.10010259&partnerID=40&md5=c121d88aa37814d1e2032f6342f788b0},
abstract={Dtrain application is built by implementing software documents, one of which is the Software Requirement Specification (SRS). In compiling software documents, there is a mismatch between one document element with other elements. Therefore, this study will measure the similarity between the Use Case Description (UCD) on the SRS case study Dtrain with Sequence Diagrams (SD) to determine how similar or compatible the two SRS elements are. This study aims to measure the similarity of UML artifacts between UCD and SD contained in a software development document. The suitability of the two artifacts can be done by analyzing the text in the description (UCD) and object (SD) sections. Based on the results and discussion, this study resulted in the following. First, a study that finds level of similarity between UCD and the SD by comparing the step results performed an extraction on the UCD and the extraction of messages and actors on the SD. Second, there are five use cases in the Dtrain application, so the UCD and SD are each 5. Third, the highest similarity value results are found in SD 1 and SD 5, with a similarity value of 0.712. Fourth, the results of the validation value using algorithm code to get a kappa score of 0.331. Fifth, the results of the validation obtained from results of the expert questionnaire got a value of 0.8905. © 2022 IEEE.},
author_keywords={Similarity;  Software Requirement Specification;  Text Analysis;  Use Case;  Validity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Chen2022,
author={Chen, J. and Zhang, C. and Cai, S. and Zhang, Z. and Liu, L. and Huang, L.},
title={Malware recognition approach based on self-similarity and an improved clustering algorithm},
journal={IET Software},
year={2022},
doi={10.1049/sfw2.12067},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133702321&doi=10.1049%2fsfw2.12067&partnerID=40&md5=765787a42a7bb82e806cec6133c40e3d},
abstract={The recognition of malware in network traffic is an important research problem. However, existing solutions addressing this problem rely heavily on the source code and misrecognise vulnerabilities (i.e. incur a high false positive rate (FPR)) in some cases. In this paper, we initially use the K-means clustering algorithm to extract malware patterns under user to root attacks in network traffic. Since the traditional K-means algorithm needs to determine the number of clusters in advance and it is easily affected by the initial cluster centres, we propose an improved K-means clustering algorithm (NIKClustering algorithm) for cluster analysis. Furthermore, we propose the use of self-similarity and our improved clustering algorithm to recognise buffer overflow vulnerabilities for malware in network traffic. This motivates us to design and implement a recognition approach for buffer overflow vulnerabilities based on self-similarity and our improved clustering algorithm, called Reliable Self-Similarity with Improved K-means Clustering (RSS-IKClustering). Extensive experiments conducted on two different datasets demonstrate that the RSS-IKClustering can achieve much fewer false positives than other notable approaches while increasing accuracy. We further apply our RSS-IKClustering approach on a public dataset (Center for Applied Internet Data Analysis), which also exhibited a high accuracy and low FPR of 96% and 1.5%, respectively. © 2022 The Authors. IET Software published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Guizani202290,
author={Guizani, M. and Steinmacher, I. and Emard, J. and Fallatah, A. and Burnett, M. and Sarma, A.},
title={How to Debug Inclusivity Bugs? A Debugging Process with Information Architecture},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
pages={90-101},
doi={10.1109/ICSE-SEIS55304.2022.9794009},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133672763&doi=10.1109%2fICSE-SEIS55304.2022.9794009&partnerID=40&md5=b52a4450614d85de67078140c4836525},
abstract={Although some previous research has found ways to find inclusivity bugs (biases in software that introduce inequities), little attention has been paid to how to go about fixing such bugs. Without a process to move from finding to fixing, acting upon such findings is an ad-hoc activity, at the mercy of the skills of each individual developer. To address this gap, we created Why/Where/Fix, a systematic inclusivity debugging process whose inclusivity fault localization harnesses Information Architecture(IA)-the way user-facing information is organized, structured and labeled. We then conducted a multi-stage qualitative empirical evaluation of the effectiveness of Why/Where/Fix, using an Open Source Software (OSS) project's infrastructure as our setting. In our study, the OSS project team used the Why/Where/Fix process to find inclusivity bugs, localize the IA faults behind them, and then fix the IA to remove the inclusivity bugs they had found. Our results showed that using Why/Where/Fix reduced the number of inclusivity bugs that OSS newcomer participants experienced by 90%. Diverse teams have been shown to be more productive as well as more innovative. One form of diversity, cognitive diversity - differences in cognitive styles - helps generate diversity of thoughts. However, cognitive diversity is often not supported in software tools. This means that these tools are not inclusive of individuals with different cognitive styles (e.g., those who like to learn through process vs. those who learn by tinkering), which burdens these individuals with a cognitive 'tax' each time they use the tool. In this work, we present an approach that enables software developers to: (1) evaluate their tools, especially those that are information-heavy, to find 'inclusivity bugs'- cases where diverse cognitive styles are unsupported, (2) find where in the tool these bugs lurk, and (3) fix these bugs. Our evaluation in an open source project shows that by following this approach developers were able to reduce inclusivity bugs in their projects by 90%. © 2022 IEEE.},
author_keywords={Diversity;  Inclusivity Bugs;  Information Architecture;  Open Source},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shetty2022549,
author={Shetty, M. and Bansal, C. and Nath, S. and Bowles, S. and Wang, H. and Arman, O. and Ahari, S.},
title={DeepAnalyze: Learning to Localize Crashes at Scale},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={549-560},
doi={10.1145/3510003.3512759},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133548485&doi=10.1145%2f3510003.3512759&partnerID=40&md5=3dc73991d65ea6968026387344763845},
abstract={Crash localization, an important step in debugging crashes, is challenging when dealing with an extremely large number of diverse applications and platforms and underlying root causes. Large-scale error reporting systems, e.g., Windows Error Reporting (WER), commonly rely on manually developed rules and heuristics to localize blamed frames causing the crashes. As new applications and features are routinely introduced and existing applications are run under new environments, developing new rules and maintaining existing ones become extremely challenging. We propose a data-driven solution to address the problem. We start with the first large-scale empirical study of 362K crashes and their blamed methods reported to WER by tens of thousands of applications running in the field. The analysis provides valuable insights on where and how the crashes happen and what methods to blame for the crashes. These insights enable us to develop Deep-Analyze, a novel multi-task sequence labeling approach for identifying blamed frames in stack traces. We evaluate our model with over a million real-world crashes from four popular Microsoft applications and show that DeepAnalyze, trained with crashes from one set of applications, not only accurately localizes crashes of the same applications, but also bootstrap crash localization for other applications with zero to very little additional training data. © 2022 ACM.},
author_keywords={Crash Localization;  Machine Learning;  Software Engineering},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Nguyen2022249,
author={Nguyen, H.L. and Grunske, L.},
title={BEDIVFUZZ: Integrating Behavioral Diversity into Generator-based Fuzzing},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={249-261},
doi={10.1145/3510003.3510182},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133527634&doi=10.1145%2f3510003.3510182&partnerID=40&md5=4933207e0d1a0100ca6408c64cef9aaa},
abstract={A popular metric to evaluate the performance of fuzzers is branch coverage. However, we argue that focusing solely on covering many different branches (i.e., the richness) is not sufficient since the majority of the covered branches may have been exercised only once, which does not inspire a high confidence in the reliability of the covered code. Instead, the distribution of the executed branches (i.e., the evenness) should also be considered. That is, behavioral diversity is only given if the generated inputs not only trigger many different branches, but also trigger them evenly often with diverse inputs. We introduce BEDIVFUZZ, a feedback-driven fuzzing technique for generator-based fuzzers. BEDIVFUZZ distinguishes between structure-preserving and structure-changing mutations in the space of syntactically valid inputs, and biases its mutation strategy towards validity and behavioral diversity based on the received program feedback. We have evaluated BEDIVFUZZ on Ant, Maven, Rhino, Closure, Nashorn, and Tomcat. The results show that BE-DIVFUZZ achieves better behavioral diversity than the state of the art, measured by established biodiversity metrics, namely the Hill numbers, from the field of ecology. © 2022 ACM.},
author_keywords={behavioral diversity;  random testing;  Structure-aware fuzzing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Mir20222241,
author={Mir, A.M. and Latoskinas, E. and Proksch, S. and Gousios, G.},
title={Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={2241-2252},
doi={10.1145/3510003.3510124},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133516935&doi=10.1145%2f3510003.3510124&partnerID=40&md5=c17e481860811dd1463100215ec2e459},
abstract={Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 %, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python. © 2022 ACM.},
author_keywords={Machine Learning;  Mean Reciprocal Rank;  Python;  Similarity Learning;  Type Inference},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chen20222327,
author={Chen, Q. and Lacomis, J. and Schwartz, E.J. and Neubig, G. and Vasilescu, B. and Goues, C.L.},
title={VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={2327-2339},
doi={10.1145/3510003.3510162},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133510554&doi=10.1145%2f3510003.3510162&partnerID=40&md5=7c99120c351fab201fee2622fc93db5b},
abstract={Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture 'relatedness' (whether two variables are linked at all), rather than 'similarity' (whether they actually have the same meaning). We propose Varclr, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from GitHub edits. We show that Varclr enables the effective application of sophisticated, general-purpose language models like BERT, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. Varclr produces models that significantly outperform the state-of-the-art on IDBENCH, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names. © 2022 ACM.},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Xiang20221945,
author={Xiang, Y. and Huang, H. and Zhou, Y. and Li, S. and Luo, C. and Lin, Q. and Li, M. and Yang, X.},
title={Search-based Diverse Sampling from Real-world Software Product Lines},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={1945-1957},
doi={10.1145/3510003.3510053},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133500127&doi=10.1145%2f3510003.3510053&partnerID=40&md5=d0630ad369f46aeb152fe75d7c0e85eb},
abstract={Real-world software product lines (SPLs) often encompass enormous valid configurations that are impossible to enumerate. To understand properties of the space formed by all valid configurations, a feasible way is to select a small and valid sample set. Even though a number of sampling strategies have been proposed, they either fail to produce diverse samples with respect to the number of selected features (an important property to characterize behaviors of configurations), or achieve diverse sampling but with limited scalability (the handleable configuration space size is limited to 1013). To resolve this dilemma, we propose a scalable diverse sampling strategy, which uses a distance metric in combination with the novelty search algorithm to produce diverse samples in an incremental way. The distance metric is carefully designed to measure similarities between configurations, and further diversity of a sample set. The novelty search incrementally improves diversity of samples through the search for novel configurations. We evaluate our sampling algorithm on 39 real-world SPLs. It is able to generate the required number of samples for all the SPLs, including those which cannot be counted by sharpSAT, a state-of-the-art model counting solver. Moreover, it performs better than or at least competitively to state-of-the-art samplers regarding diversity of the sample set. Experimental results suggest that only the proposed sampler (among all the tested ones) achieves scalable diverse sampling. © 2022 ACM.},
author_keywords={distance metric;  diverse sampling;  novelty search;  Software product lines},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Greca2022115,
author={Greca, R. and Miranda, B. and Gligoric, M. and Bertolino, A.},
title={Comparing and Combining File-based Selection and Similarity-based Prioritization towards Regression Test Orchestration},
journal={Proceedings - 3rd ACM/IEEE International Conference on Automation of Software Test, AST 2022},
year={2022},
pages={115-125},
doi={10.1145/3524481.3527223},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133444657&doi=10.1145%2f3524481.3527223&partnerID=40&md5=2b8f64dd6816efbc2240ed2ea270c137},
abstract={Test case selection (TCS) and test case prioritization (TCP) techniques can reduce time to detect the first test failure. Although these techniques have been extensively studied in combination and isolation, they have not been compared one against the other. In this paper, we perform an empirical study directly comparing TCS and TCP approaches, represented by the tools Ekstazi and FAST, respectively. Furthermore, we develop the first combination, named Fastazi, of file-based TCS and similarity-based TCP and evaluate its benefit and cost against each individual technique. We performed our experiments using 12 Java-based open-source projects. Our results show that, in the median case, the combined approach detects the first failure nearly two times faster than either Ekstazi alone (with random test ordering) or FAST alone (without TCS). Statistical analysis shows that the effectiveness of Fastazi is higher than that of Ekstazi, which in turn is higher than that of FAST. On the other hand, FAST adds the least overhead to testing time, while the difference between the additional time needed by Ekstazi and Fastazi is negligible. Fastazi can also improve failure detection in scenarios where the time available for testing is restricted. CCS CONCEPTS • Software and its engineering →Software testing and debugging. © 2022 ACM.},
author_keywords={Fastazi;  regression testing;  test case prioritization;  test case selection;  test orchestration},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Shimmi202265,
author={Shimmi, S. and Rahimi, M.},
title={Leveraging Code-Test Co-evolution Patterns for Automated Test Case Recommendation},
journal={Proceedings - 3rd ACM/IEEE International Conference on Automation of Software Test, AST 2022},
year={2022},
pages={65-76},
doi={10.1145/3524481.3527222},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133418413&doi=10.1145%2f3524481.3527222&partnerID=40&md5=2a0218db502f6a44dabd3bd1d57852bd},
abstract={Context: Prior research revealed that code components with similar structures tend to require structurally similar test cases and they often co-evolve over time. Objective: Leveraging this pattern, we implemented a prototype tool, Test Suite Evolver (TSE), to support the generation of test cases for structurally similar methods. Our prototype tool is applicable to both, incomplete test suites with the purpose of test case augmentation, as well as evolving test suites when a newer version of a software becomes available. Method: Making use of the aforementioned code-test co-evolution pattern, TSE leverages source code structural similarity and a set of existing test cases to synthesise and recommend new unit tests. For this, TSE initially identifies groups of structurally similar methods in the code, determines changes at the token-level, and integrates the retrieved information to synthesise updated test cases. Results: Our evaluations indicated that 38% of the source code of five large open-source applications consist of methods which are structurally similar to each other and therefore for which, the co-evolution pattern holds. Applying TSE to these methods, TSE generated 72% of the minimum required test cases with only 11% token-based edit distance between the generated test cases and those written by developers. To evaluate the test cases' validity, we used code2vec, a neural network to measure the structural and semantic similarity between the generated and developed test cases. The results indicated that TSE successfully generated test cases with a similarity measure of more than 80% in more than 95.6% of test cases and similarity measure of 100% for 88.35% of the test cases. Conclusion: The evaluations revealed that TSE is able to generate the majority of the necessary test cases with an insignificant amount of modifications required from the developers. ACM Reference Format: Samiha Shimmi, Mona Rahimi. 2022. Leveraging Code-Test Co-evolution Patterns for Automated Test Case Recommendation. In IEEE/ACM 3rd International Conference on Automation of Software Test (AST '22), May 17-18, 2022, Pittsburgh, PA, USA. ACM, Pittsburgh, PA, USA, 12 pages. https://doi.org/10.1145/3524481.3527222 © 2022 ACM.},
author_keywords={test suite maintenance, test suite evolution, automatic test case generation, patterns of co evolution},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guarnieri2022179,
author={Guarnieri, G.F. and Pizzoleto, A.V. and Ferrari, F.C.},
title={An Automated Framework for Cost Reduction of Mutation Testing Based on Program Similarity},
journal={Proceedings - 2022 IEEE 14th International Conference on Software Testing, Verification and Validation Workshops, ICSTW 2022},
year={2022},
pages={179-188},
doi={10.1109/ICSTW55395.2022.00041},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133250831&doi=10.1109%2fICSTW55395.2022.00041&partnerID=40&md5=6332e8442e656a098a2c79ac07a386cd},
abstract={This paper presents an implementation and assessment of a framework named SiMut. The framework was introduced in a previous paper with the objective of helping reducing the cost for testing a program based on groups of similar programs previously tested with mutation. The implementation presented in the paper handles Java programs and includes a set of variants that relate to three types of program abstraction (original source code, processed source code, and internal complexity metrics), three similarity calculation strategies (clustering, information diversity, and plagiarism), and one mutation cost reduction approach (inspired by the One-Op mutation technique). Our evaluation encompasses 20 variant combinations, also referred to as SiMut configurations, and 35 small Java programs. A cross-comparison involving the formed clusters and a comparison with randomly formed clusters points to configurations that tend to reach high effectiveness in foreseeing the best mutation operators for untested programs. © 2022 IEEE.},
author_keywords={cost reduction;  mutation testing;  program similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Romdhana202224,
author={Romdhana, A. and Ceccato, M. and Merlo, A. and Tonella, P.},
title={IFRIT: Focused Testing through Deep Reinforcement Learning},
journal={Proceedings - 2022 IEEE 15th International Conference on Software Testing, Verification and Validation, ICST 2022},
year={2022},
pages={24-34},
doi={10.1109/ICST53961.2022.00013},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133237386&doi=10.1109%2fICST53961.2022.00013&partnerID=40&md5=11d1f3aa88bede5a373863e465e7b9f0},
abstract={Software is constantly changing as developers add new features or make changes. This directly impacts the effectiveness of the test suite associated with that software, especially when the new modifications are in an area where no test case exists. This article addresses the issue of developing a high-quality test suite to repeatedly cover a given point in a program, with the ultimate goal of exposing faults affecting the given program point. Our approach, IFRIT, uses Deep Reinforcement Learning to generate diverse inputs while keeping a high level of reachability of the desired program point. IFRIT achieves better results than state-of-the-art and baseline tools, improving reachability, diversity and fault detection. © 2022 IEEE.},
author_keywords={focused testing;  reinforcement learning;  software testing},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Guo2022366,
author={Guo, Y. and Li, P. and Luo, Y. and Wang, X. and Wang, Z.},
title={Exploring GNN Based Program Embedding Technologies for Binary Related Tasks},
journal={IEEE International Conference on Program Comprehension},
year={2022},
volume={2022-March},
pages={366-377},
doi={10.1145/3524610.3527900},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133161669&doi=10.1145%2f3524610.3527900&partnerID=40&md5=7849e312ebe5217cf738f249435b5fea},
abstract={With the rapid growth of program scale, program analysis, mainte-nance and optimization become increasingly diverse and complex. Applying learning-assisted methodologies onto program analysis has attracted ever-increasing attention. However, a large number of program factors including syntax structures, semantics, running platforms and compilation configurations block the effective re-alization of these methods. To overcome these obstacles, existing works prefer to be on a basis of source code or abstract syntax tree, but unfortunately are sub-optimal for binary-oriented analysis tasks closely related to the compilation process. To this end, we propose a new program analysis approach that aims at solving program-level and procedure-level tasks with one model, by taking advantage of the great power of graph neural networks from the level of binary code. By fusing the semantics of control flow graphs, data flow graphs and call graphs into one model, and embedding instructions and values simultaneously, our method can effectively work around emerging compilation-related problems. By testing the proposed method on two tasks, binary similarity detection and dead store prediction, the results show that our method is able to achieve as high accuracy as 83.25%, and 82.77%. © 2022 ACM.},
author_keywords={binary similarity de-tection;  dead store detection;  graph neural network;  program embedding},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{An202265,
author={An, G. and Yoon, J. and Sohn, J. and Hong, J. and Hwang, D. and Yoo, S.},
title={Automatically Identifying Shared Root Causes of Test Breakages in SAP HANA},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
pages={65-74},
doi={10.1109/ICSE-SEIP55303.2022.9793878},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132836115&doi=10.1109%2fICSE-SEIP55303.2022.9793878&partnerID=40&md5=60cc32dd9ba7dcabec512f7ede3c757a},
abstract={Continuous Integration (CI) of a largescale software system such as SAP HANA can produce a non-trivial number of test breakages. Each breakage that newly occurs from daily runs needs to be manually inspected, triaged, and eventually assigned to developers for debugging. However, not all new breakages are unique, as some test breakages would share the same root cause; in addition, human errors can produce duplicate bug tickets for the same root cause. An automated identification of breakages with shared root causes will be able to significantly reduce the cost of the (typically manual) post-breakage steps. This paper investigates multiple similarity functions between test breakages to assist and automate the identification of test breakages that are caused by the same root cause. We consider multiple information sources, such as static (i.e., the code itself), historical (i.e., whether the test results have changed in a similar way in the past), as well as dynamic (i.e., whether the coverage of test cases are similar to each other), for the purpose of such automation. We evaluate a total of 27 individual similarity functions, using realworld CI data of SAP HANA from a six-month period. Further, using these individual similarity functions as in-put features, we construct a classification model that can predict whether two test breakages share the same root cause or not. When trained using ground truth labels extracted from the issue tracker of SAP HANA, our model achieves an F1 score of 0.743 when evaluated using a set of unseen test breakages collected over three months. Our results show that a classification model based on test similarity functions can successfully support the bug triage stage of a CI pipeline. © 2022 IEEE.},
author_keywords={Continuous Integration;  Root Cause Analysis;  Test Similarity},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Dutta2022165,
author={Dutta, S. and Garbervetsky, D. and Lahiri, S.K. and Schäfer, M.},
title={InspectJS: Leveraging Code Similarity and User-Feedback for Effective Taint Specification Inference for JavaScript},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
pages={165-174},
doi={10.1109/ICSE-SEIP55303.2022.9794015},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132822895&doi=10.1109%2fICSE-SEIP55303.2022.9794015&partnerID=40&md5=7660ae929fdacc8b41092018ae86a1d2},
abstract={Static analysis has established itself as a weapon of choice for detecting security vulnerabilities. Taint analysis in particular is a very general and powerful technique, where security policies are expressed in terms of forbidden flows, either from untrusted input sources to sensitive sinks (in integrity policies) or from sensitive sources to untrusted sinks (in confidentiality policies). The appeal of this approach is that the taint-tracking mechanism has to be implemented only once, and can then be parameterized with different taint specifications (that is, sets of sources and sinks, as well as any sanitizers that render otherwise problematic flows innocuous) to detect many different kinds of vulnerabilities. But while techniques for implementing scalable inter-procedural static taint tracking are fairly well established, crafting taint specifications is still more of an art than a science, and in practice tends to involve a lot of manual effort. Past work has focussed on automated techniques for inferring taint specifications for libraries either from their implementation or from the way they tend to be used in client code. Among the latter, machine learning-based approaches have shown great promise. In this work we present our experience combining an existing machine-learning approach to mining sink specifications for JavaScript libraries with manual taint modelling in the context of GitHub's CodeQL analysis framework. We show that the machine-learning component can successfully infer many new taint sinks that either are not part of the manual modelling or are not detected due to analysis incompleteness. Moreover, we present techniques for organizing sink predictions using automated ranking and code-similarity metrics that allow an analysis engineer to efficiently sift through large numbers of predictions to identify true positives. © 2022 IEEE.},
author_keywords={JavaScript;  Machine Learning;  Taint Analysis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Fan2022871,
author={Fan, M. and Wei, W. and Jin, W. and Yang, Z. and Liu, T.},
title={Explanation-Guided Fairness Testing through Genetic Algorithm},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={871-882},
doi={10.1145/3510003.3510137},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131370347&doi=10.1145%2f3510003.3510137&partnerID=40&md5=fb72d0b07c4e81770a3251cce2741ca6},
abstract={The fairness characteristic is a critical attribute of trusted AI systems. A plethora of research has proposed diverse methods for individual fairness testing. However, they are suffering from three major limitations, i.e., low efficiency, low effectiveness, and model-specificity. This work proposes ExpGA, an explanation-guided fairness testing approach through a genetic algorithm (GA). ExpGA employs the explanation results generated by interpretable methods to collect high-quality initial seeds, which are prone to derive discriminatory samples by slightly modifying feature values. ExpGA then adopts GA to search discriminatory sample candidates by optimizing a fitness value. Benefiting from this combination of explanation results and GA, ExpGA is both efficient and effective to detect discriminatory individuals. Moreover, ExpGA only requires prediction probabilities of the tested model, resulting in a better generalization capability to various models. Experiments on multiple real-world benchmarks, including tabular and text datasets, show that ExpGA presents higher efficiency and effectiveness than four state-of-the-art approaches. © 2022 ACM.},
author_keywords={Explanation result;  fairness testing;  genetic algorithm},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Chapagain2022,
author={Chapagain, J. and Tamang, L. and Banjade, R. and Oli, P. and Rus, V.},
title={Automated Assessment of Student Self-explanation During Source Code Comprehension},
journal={Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS},
year={2022},
volume={35},
page_count={6},
doi={10.32473/flairs.v35i.130540},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131127515&doi=10.32473%2fflairs.v35i.130540&partnerID=40&md5=fbaaccc202c32f0c90a536bbe9e16311},
abstract={This paper presents a novel method to automatically assess self-explanations generated by students during code comprehension activities. The self-explanations are produced in the context of an online learning environment that asks students to freely explain Java code examples line-by-line. We explored a number of models consisting of textual features in conjunction with machine learning algorithms such as Support Vector Regression (SVR), Decision Trees (DT), and Random Forests (RF). Support Vector Regression (SVR) performed best having a correlation score with human judgments of 0.7088. The best model used a combination of features such as semantic measures obtained using a Sentence BERT pre-trained model and from previously developed semantic algorithms used in a state-of-the-art intelligent tutoring system. © 2021 by the authors. All rights reserved.},
author_keywords={self-explanation;  semantic similarity;  source code comprehension},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Balos202216,
author={Balos, C.J. and Luszczek, P. and Osborn, S. and Willenbring, J. and Yang, U.M.},
title={Challenges of and Opportunities for a Large Diverse Software Team},
journal={Computing in Science and Engineering},
year={2022},
volume={24},
number={3},
pages={16-24},
doi={10.1109/MCSE.2022.3172873},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130834682&doi=10.1109%2fMCSE.2022.3172873&partnerID=40&md5=7019c82d03dd469ece69ad968b1055a2},
abstract={A large software team consisting of members with different expertise, skillsets, personalities, ethnicities, and involving collaboration on a large and complex software product presents many technical and cultural challenges, but also provides unique opportunities. In this article, we discuss the essential issues we faced when successfully transforming a collection of various independently developed software libraries into one large integrated product: the eXtreme-scale scientific Software Development Kit (xSDK). We argue it is just as important to pay attention to cultural challenges, such as establishment of reliable communication channels that considers, among others, differences in personalities and backgrounds as well as overcoming geographical separation and time-zone distribution when collaborating, as technical challenges. Finally, we discuss opportunities stemming from participating in a large diverse software team, such as increased internal expertise, variety of skillsets, broadened connections to external experts, and access to a larger pool of ideas or solutions. © 1999-2011 IEEE.},
document_type={Article},
source={Scopus},
}

@ARTICLE{Khan2022,
author={Khan, H.U. and Ali, F. and Alshehri, Y. and Nazir, S.},
title={Towards Enhancing the Capability of IoT Applications by Utilizing Cloud Computing Concept},
journal={Wireless Communications and Mobile Computing},
year={2022},
volume={2022},
doi={10.1155/2022/2335313},
art_number={2335313},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130524598&doi=10.1155%2f2022%2f2335313&partnerID=40&md5=c9346b95f6c1df1ffbbb84807aac09fa},
abstract={The emergence of smart and innovative applications in diverse domains has inspired our lives by presenting many state-of-The art applications ranging from offline to smart online systems, smart communication system to tracking systems, and many others. The availability of smart internet enabled systems has made the world as a global village where people can collaborate, communicate, and share information in secure and timely manner. Innovation in information technology focuses on investigating characteristics that make it easier for the people to accept and distribute innovative IT-based processes or products. To provide elastic services and resource the Internet service provider developed cloud computing to support maximal number of users. Cloud computing is a subscription paradigm in which users do not buy various resources permanently, but they purchase it with block chain-driven payment schemes (credit cards). A flexible, on-demand, and dynamically scalable computer infrastructure is offered by cloud providers to its clients on charging some amount of subscription. This research article provides an introduction of cloud computing and the integration of IoT concept, its impacts on crowd and organizations, provision of various services, and analyzing and selecting the appropriate features using probability distribution function for enhancing cloud-based IoT capabilities. In ambiguous and complex situations, decision makers use quantitative techniques combined with traditional approaches to select the appropriate one among a group of features. Probability distribution function is used to evaluate the appropriate features that will enhance the capabilities of cloud-based IoT application. © 2022 Habib Ullah Khan et al.},
document_type={Article},
source={Scopus},
}

@CONFERENCE{First2022749,
author={First, E. and Brun, Y.},
title={Diversity-Driven Automated Formal Verification},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
volume={2022-May},
pages={749-761},
doi={10.1145/3510003.3510138},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129509824&doi=10.1145%2f3510003.3510138&partnerID=40&md5=62e873c4bf04d0f36177d20e4fefdf57},
abstract={Formally verified correctness is one of the most desirable properties of software systems. But despite great progress made via interactive theorem provers, such as Coq, writing proof scripts for verification remains one of the most effort-intensive (and often prohibitively difficult) software development activities. Recent work has created tools that automatically synthesize proofs or proof scripts. For example, CoqHammer can prove 26.6% of theorems completely automatically by reasoning using precomputed facts, while TacTok and ASTactic, which use machine learning to model proof scripts and then perform biased search through the proof-script space, can prove 12.9% and 12.3% of the theorems, respectively. Further, these three tools are highly complementary; together, they can prove 30.4% of the theorems fully automatically. Our key insight is that control over the learning process can produce a diverse set of models, and that, due to the unique nature of proof synthesis (the existence of the theorem prover, an oracle that infallibly judges a proof's correctness), this diversity can significantly improve these tools' proving power. Accordingly, we develop Diva, which uses a diverse set of models with TacTok's and ASTactic's search mech-anism to prove 21.7% of the theorems. That is, Diva proves 68% more theorems than TacTok and 77% more than ASTactic. Complementary to CoqHammer, Diva proves 781 theorems (27% added value) that CoqHammer does not, and 364 theorems no existing tool has proved automatically. Together with CoqHammer, Diva proves 33.8% of the theorems, the largest fraction to date. We explore nine dimensions for learning diverse models, and identify which dimensions lead to the most useful diversity. Further, we develop an optimization to speed up Diva's execution by 40×. Our study introduces a completely new idea for using diversity in machine learning to improve the power of state-of-the-art proof-script synthesis techniques, and empirically demonstrates that the improvement is significant on a dataset of 68K theorems from 122 open-source software projects. © 2022 ACM.},
author_keywords={Automated formal verification;  Coq;  interactive proof assistants;  language models;  proof synthesis},
document_type={Conference Paper},
source={Scopus},
}

@CONFERENCE{Gröpler2022,
author={Gröpler, R. and Kutty, L. and Sudhi, V. and Smalley, D.},
title={Automated Requirement Formalization Using Product Design Specifications},
journal={CEUR Workshop Proceedings},
year={2022},
volume={3122},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128715774&partnerID=40&md5=6e87b52c6b3aaa7186831eb9b0d81717},
abstract={Assuring the quality of complex and highly configurable software systems is a demanding and time-consuming process. Especially for safety-critical systems, extensive testing based on requirements is necessary. Methods for model-based test automation in agile software development offer the possibility to overcome these difficulties. However, it is still a major effort to create formal models from functional requirements in natural language on a large scale. In this paper, we present and evaluate automated support for the requirements formalization process to reduce cost and effort. We present a new approach based on Natural Language Processing (NLP) and textual similarity using requirements and product design specifications to generate human- and machine-readable models. The method is evaluated on an industrial use case from the railway domain. The recommended requirement models for the considered propulsion system show an average accuracy of more than 90% and an exact match of the entire models of about 55%. These results show that our approach can support the requirements formalization process, which can be further used for test case generation and execution, as well as for requirements and design verification. © 2022 Copyright for this paper by its authors},
author_keywords={natural language processing;  Requirements engineering;  requirements modeling;  textual similarity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Gupt202238694,
author={Gupt, K.K. and Youssef, A. and Murphy, A. and Raja, M.A. and Ryan, C.},
title={GELAB - The Cutting Edge of Grammatical Evolution},
journal={IEEE Access},
year={2022},
volume={10},
pages={38694-38708},
doi={10.1109/ACCESS.2022.3166115},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128281046&doi=10.1109%2fACCESS.2022.3166115&partnerID=40&md5=db687e581de103111998e5af089ed65d},
abstract={The advent of cloud-based super-computing platforms has given rise to a Data Science (DS) boom. Many types of technological problems that were once considered prohibitively expensive to tackle are now candidates for exploration. Machine Learning (ML) tools that were valued only in academic environments are quickly being embraced by industrial giants and tiny startups alike. Coupled with modern-day computing power, ML tools can be looked at as hammers that can deal with even the most stubborn nails. ML tools have become so ubiquitous that the current industrial expectation is that they should not only deliver accurate and intelligent solutions but also do so rapidly. In order to keep pace with these requirements, a new enterprise, referred to as MLOps has blossomed in recent years. MLOps combines the process of ML and DS with an agile software engineering technique to develop and deliver solutions in a fast and iterative way. One of the key challenges to this is that ML and DS tools should be efficient and have better usability characteristics than were traditionally offered. In this paper, we present a novel software for Grammatical Evolution (GE) that meets both of these expectations. Our tool, GELAB, is a toolbox for GE in Matlab which has numerous features that distinguish it from existing contemporary GE software. Firstly, it is user-friendly and its development was aimed for use by non-specialists. Secondly, it is capable of hybrid optimization, in which standard numerical optimization techniques can be added to GE. We have shown experimentally that when hybridized with meta-heuristics GELAB has an overall better performance as compared with standard GE. © 2013 IEEE.},
author_keywords={diversity;  Grammatical evolution;  hybrid optimization},
document_type={Article},
source={Scopus},
}

@ARTICLE{Ali202233909,
author={Ali, I. and Ahmedy, I. and Gani, A. and Munir, M.U. and Anisi, M.H.},
title={Data Collection in Studies on Internet of Things (IoT), Wireless Sensor Networks (WSNs), and Sensor Cloud (SC): Similarities and Differences},
journal={IEEE Access},
year={2022},
volume={10},
pages={33909-33931},
doi={10.1109/ACCESS.2022.3161929},
note={cited By 6},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128240617&doi=10.1109%2fACCESS.2022.3161929&partnerID=40&md5=eb52a2df06398901a2ce8dbf5f6122b2},
abstract={Data collection is an essential part of sensor devices, particularly in such technologies like Internet of Things (IoT), wireless sensor networks (WSN), and sensor cloud (SC). In recent years, various literature had been published in these research areas to propose different models, architectures, and contributions in the domains. Due to the importance of efficient data collection regarding reducing energy consumption, latency, network lifetime, and general cost, a momentous literature volume has been published to facilitate data collection. Hence, review studies have been conducted on data collection in these domains in isolation. However, a lack of comprehensive review collectively identifies and analyzes the differences and similarities among the data collection proposals in IoT, WSN, and SC. The main objective of this research is to conduct a comprehensive survey to explore the current state, use cases, contributions, performance measures, evaluation measures, and architecture in the IoT, WSN, and SC research domains. The findings indicate that studies on data collection in IoT, WSN, and SC are relatively consistent with stable output in the last five years. Nine novel contributions are found with models, algorithms, and frameworks being the most utilized by the selected studies. In conclusion, key research challenges and future research directions have been identified and discussed. © 2013 IEEE.},
author_keywords={algorithms;  architecture;  data collection in Internet of Things (IoT);  Data collection in sensor cloud (SC);  data collection in wireless sensor networks (WSNs);  frameworks;  models;  sensor cloud;  similarity and difference},
document_type={Article},
source={Scopus},
}

@CONFERENCE{Groce2022185,
author={Groce, A. and Jain, K. and Van Tonder, R. and Kalburgi, G.T. and Goues, C.L.},
title={Looking for Lacunae in Bitcoin Core's Fuzzing Efforts},
journal={Proceedings - International Conference on Software Engineering},
year={2022},
pages={185-186},
doi={10.1109/ICSE-SEIP55303.2022.9794086},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126873753&doi=10.1109%2fICSE-SEIP55303.2022.9794086&partnerID=40&md5=5cc70b2afadcbd964267aabd3313bd66},
abstract={Bitcoin is one of the most prominent distributed software systems in the world. This paper describes an effort to investigate and enhance the effectiveness of the Bitcoin Core fuzzing effort. The effort initially began as a query about how to escape saturation in the fuzzing effort, but developed into a more general exploration. This paper summarizes the outcomes of a two-week focused effort. While the effort found no smoking guns indicating major test/fuzz weaknesses, it produced a large number of additional fuzz corpus entries, increased the set of fuzzers used for Bitcoin Core, and ran mutation analysis of Bitcoin Core fuzz targets, with a comparison to Bitcoin functional tests and other cryptocurrencies' tests. Our conclusion is that for high quality fuzzing efforts, improvements to the oracle may be the best way to get more out of fuzzing. © 2022 IEEE.},
author_keywords={fuzzing;  mutation analysis;  oracle strength;  saturation;  test diversity},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Kapur2022132,
author={Kapur, R. and Rao, P.U. and Dewam, A. and Sodhi, B.},
title={XtraLibD: Detecting Irrelevant Third-Party Libraries in Java and Python Applications},
journal={Communications in Computer and Information Science},
year={2022},
volume={1556 CCIS},
pages={132-155},
doi={10.1007/978-3-030-96648-5_7},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125279768&doi=10.1007%2f978-3-030-96648-5_7&partnerID=40&md5=9af7721b6b441959fcda5195d8f65773},
abstract={Software development comprises the use of multiple Third-Party Libraries (TPLs). However, the irrelevant libraries present in software application’s distributable often lead to excessive consumption of resources such as CPU cycles, memory, and modile-devices’ battery usage. Therefore, the identification and removal of unused TPLs present in an application are desirable. We present a rapid, storage-efficient, obfuscation-resilient method to detect the irrelevant-TPLs in Java and Python applications. Our approach’s novel aspects are i) Computing a vector representation of a.class file using a model that we call Lib2Vec. The Lib2Vec model is trained using the Paragraph Vector Algorithm. ii) Before using it for training the Lib2Vec models, a.class file is converted to a normalized form via semantics-preserving transformations. iii) A eXtra Library Detector (XtraLibD) developed and tested with 27 different language-specific Lib2Vec models. These models were trained using different parameters and &gt;30,000.class and &gt;478,000.py files taken from &gt;100 different Java libraries and 43,711 Python available at MavenCentral.com and Pypi.com, respectively. XtraLibD achieves an accuracy of 99.48% with an F1 score of 0.968 and outperforms the existing tools, viz., LibScout, LiteRadar, and LibD with an accuracy improvement of 74.5%, 30.33%, and 14.1%, respectively. Compared with LibD, XtraLibD achieves a response time improvement of 61.37% and a storage reduction of 87.93% (99.85% over JIngredient). Our program artifacts are available at https://www.doi.org/10.5281/zenodo.5179747. © 2022, Springer Nature Switzerland AG.},
author_keywords={Code similarity;  Obfuscation;  Paragraph vectors;  Software bloat;  Third-party library detection},
document_type={Conference Paper},
source={Scopus},
}

@ARTICLE{Menendez2022295,
author={Menendez, H.D. and Boreale, M. and Gorla, D. and Clark, D.},
title={Output Sampling for Output Diversity in Automatic Unit Test Generation},
journal={IEEE Transactions on Software Engineering},
year={2022},
volume={48},
number={1},
pages={295-308},
doi={10.1109/TSE.2020.2987377},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123183897&doi=10.1109%2fTSE.2020.2987377&partnerID=40&md5=09e017de477f100b23dcb41ab5e5ca5f},
abstract={Diverse test sets are able to expose bugs that test sets generated with structural coverage techniques cannot discover. Input-diverse test set generators have been shown to be effective for this, but also have limitations: e.g., they need to be complemented with semantic information derived from the Software Under Test. We demonstrate how to drive the test set generation process with semantic information in the form of output diversity. We present the first totally automatic output sampling for output diversity unit test set generation tool, called OutGen. OutGen transforms a program into an SMT formula in bit-vector arithmetic. It then applies universal hashing in order to generate an output-based diverse set of inputs. The result offers significant diversity improvements when measured as a high output uniqueness count. It achieves this by ensuring that the test set's output probability distribution is uniform, i.e., highly diverse. The use of output sampling, as opposed to any of input sampling, CBMC, CAVM, behaviour diversity or random testing improves mutation score and bug detection by up to 4150 and 963 percent respectively on programs drawn from three different corpora: the R-project, SIR and CodeFlaws. OutGen test sets achieve an average mutation score of up to 92 percent, and 70 percent of the test sets detect the defect. Moreover, OutGen is the only automatic unit test generation tool that is able to detect bugs on the real number C functions from the R-project. © 1976-2012 IEEE.},
author_keywords={OutGen;  output diversity;  output sampling;  SMT solver;  Unit testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Yang202292692,
author={Yang, Y. and Chen, X.},
title={Crowdsourced Test Report Prioritization Based on Text Classification},
journal={IEEE Access},
year={2022},
volume={10},
pages={92692-92705},
doi={10.1109/ACCESS.2021.3128726},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122580162&doi=10.1109%2fACCESS.2021.3128726&partnerID=40&md5=064a5f86b3aa2093c6a57bedbcd02cf4},
abstract={In crowdsourced testing, crowd workers from different places help developers conduct testing and submit test reports for the observed abnormal behaviors. Developers manually inspect each test report and make an initial decision for the potential bug. However, due to the poor quality, test reports are handled extremely slowly. Meanwhile, due to the limitation of resources, some test reports are not handled at all. Therefore, some researchers attempt to resolve the problem of test report prioritization and have proposed many methods. However, these methods do not consider the impact of duplicate test reports. In this paper, we focus on the problem of test report prioritization and present a new method named DivClass by combining a diversity strategy and a classification strategy. First, we leverage Natural Language Processing (NLP) techniques to preprocess crowdsourced test reports. Then, we build a similarity matrix by introducing an asymmetric similarity computation strategy. Finally, we combine the diversity strategy and the classification strategy to determine the inspection order of test reports. To validate the effectiveness of DivClass, experiments are conducted on five crowdsourced test report datasets. Experimental results show that DivClass achieves 0.8887 in terms of APFD (Average Percentage of Fault Detected) and improves the state-of-the-art technique DivRisk by 14.12% on average. The asymmetric similarity computation strategy can improve DivClass by 4.82% in terms of APFD on average. In addition, empirical results show that DivClass can greatly reduce the number of inspected test reports. © 2013 IEEE.},
author_keywords={asymmetric similarity;  Crowdsourced testing;  diversity strategy;  test report prioritization;  text classification},
document_type={Article},
source={Scopus},
}

@ARTICLE{Xu2022167,
author={Xu, J. and Yuan, Q.},
title={LibRoad: Rapid, Online, and Accurate Detection of TPLs on Android},
journal={IEEE Transactions on Mobile Computing},
year={2022},
volume={21},
number={1},
pages={167-180},
doi={10.1109/TMC.2020.3003336},
note={cited By 4},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121028276&doi=10.1109%2fTMC.2020.3003336&partnerID=40&md5=5bcb7a6345f6ced3d02a7840e3f1d20a},
abstract={Third-party library (TPL) detection plays a very important role in Android malware analysis. The focus of recent works has been shifted to the signature-based approach. However, previous methods have several limitations such as high time complexity and low precision, especially with the presence of similar TPLs and various versions of a TPL. To solve these issues, we propose a rapid, online, and accurate TPL detection approach, named LibRoad, which also follows the line of the signature-based research. To reduce the time cost, our approach integrates an application preprocessing component and a pairwise package matching component. The former divides an application into primary modules and non-primary modules to enable us to focus on analyzing packages in non-primary modules that are the most possibly imported from a TPL. The latter adopts a combination of the package name based matching policy for non-obfuscated packages and the signature-based matching policy for obfuscated packages, where the package name based matching policy has a lower time complexity than the signature based one. Further, to improve performance, our approach integrates a perfectly matched package and TPL determination component, which adopts the package filter mechanism, online TPL detection, and local TPL discovery to identify TPLs with low false positive and false negative. We conduct several groups of experiments on real-world applications and two ground truth bases. Experimental results show that compared to state-of-the-art approaches, LibRoad can achieve a high recall of 99.86 percent and a low false positive rate of 11.48 percent without the loss of efficiency. © 2002-2012 IEEE.},
author_keywords={library detection;  obfuscation;  similarity;  Third-party libraries},
document_type={Article},
source={Scopus},
}

@ARTICLE{Mao2022,
author={Mao, Q. and Wang, W. and You, F. and Zhao, R. and Li, Z.},
title={User behavior pattern mining and reuse across similar Android apps},
journal={Journal of Systems and Software},
year={2022},
volume={183},
doi={10.1016/j.jss.2021.111085},
art_number={111085},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115971196&doi=10.1016%2fj.jss.2021.111085&partnerID=40&md5=f20a19d11eef9d90ad1485f95607691d},
abstract={Nowadays, Android apps have penetrated all aspects of our lives. Despite their popularity, understanding their behaviors is still a challenging task. Considering that many Android apps are in the same category and share similar workflows, in this paper, we propose a user behavior pattern mining and reuse approach across similar Android apps, thereby reducing the cost of understanding new apps. Particularly, for a specific new app, to figure out its typical behaviors, the behavior patterns that refer to the frequently-occurring workflows can be obtained from another similar app and transferred to this app. Moreover, to reuse the behavior patterns on this app, a semantic-based event fuzzy matching strategy and continuous workflow generation strategy are raised to generate workflows for this app. To evaluate our approach's effectiveness and rationality, we conduct a series of experiments on 25 Android apps in five categories. Furthermore, the experimental results show that 88.3% of behavior patterns can be completely reused on similar apps, and the generated workflows cover 89.1% of the top 20% of important states. © 2021 Elsevier Inc.},
author_keywords={Android apps;  Behavior pattern reuse;  GUI model;  Semantic-based event fuzzy matching},
document_type={Article},
source={Scopus},
}

@ARTICLE{Irshad2022,
author={Irshad, M. and Börstler, J. and Petersen, K.},
title={Supporting refactoring of BDD specifications—An empirical study},
journal={Information and Software Technology},
year={2022},
volume={141},
doi={10.1016/j.infsof.2021.106717},
art_number={106717},
note={cited By 1},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113689195&doi=10.1016%2fj.infsof.2021.106717&partnerID=40&md5=239cf67a7d78813f33693a1702143169},
abstract={Context: Behavior-driven development (BDD) is a variant of test-driven development where specifications are described in a structured domain-specific natural language. Although refactoring is a crucial activity of BDD, little research is available on the topic. Objective: To support practitioners in refactoring BDD specifications by (1) proposing semi-automated approaches to identify refactoring candidates; (2) defining refactoring techniques for BDD specifications; and (3) evaluating the proposed identification approaches in an industry context. Method: Using Action Research, we have developed an approach for identifying refactoring candidates in BDD specifications based on two measures of similarity and applied the approach in two projects of a large software organization. The accuracy of the measures for identifying refactoring candidates was then evaluated against an approach based on machine learning and a manual approach based on practitioner perception. Results: We proposed two measures of similarity to support the identification of refactoring candidates in a BDD specification base; (1) normalized compression similarity (NCS) and (2) similarity ratio (SR). A semi-automated approach based on NCS and SR was developed and applied to two industrial cases to identify refactoring candidates. Our results show that our approach can identify candidates for refactoring 6o times faster than a manual approach. Our results furthermore showed that our measures accurately identified refactoring candidates compared with a manual identification by software practitioners and outperformed an ML-based text classification approach. We also described four types of refactoring techniques applicable to BDD specifications; merging candidates, restructuring candidates, deleting duplicates, and renaming specification titles. Conclusion: Our results show that NCS and SR can help practitioners in accurately identifying BDD specifications that are suitable candidates for refactoring, which also decreases the time for identifying refactoring candidates. © 2021 The Authors},
author_keywords={BDD;  Behavior-driven development;  Normalized Compression Distance (NCD);  Normalized Compression Similarity (NCS);  Refactoring;  Reuse;  Similarity ratio (SR);  Specifications;  Testing},
document_type={Article},
source={Scopus},
}

@ARTICLE{Nyamawe2022770,
author={Nyamawe, A.S. and Bakhti, K. and Sandiwarno, S.},
title={Identifying rename refactoring opportunities based on feature requests},
journal={International Journal of Computers and Applications},
year={2022},
volume={44},
number={8},
pages={770-778},
doi={10.1080/1206212X.2021.1922151},
note={cited By 0},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105404345&doi=10.1080%2f1206212X.2021.1922151&partnerID=40&md5=7b629f666bcdff82177ed3b77b070a46},
abstract={Refactoring is a widespread practice of improving the quality of software systems by applying changes on their internal structures without affecting their observable behaviors. Rename is one of the most recurring and widely used refactoring operation. A rename refactoring is often required when a software entity was poorly named in the beginning or its semantics have changed and therefore should be renamed to reflect its new semantics. However, identifying renaming opportunities is often challenging as it involves several aspects including source code semantics, natural language understanding and developer's experience. To this end, we propose a new approach to identify rename refactoring opportunities by leveraging feature requests. The rationale is that, when implementing a feature request there are chances that the semantics of software entities could significantly change to fulfill the requested feature. Consequently, their names should be modified as well to portray their latest semantics. The approach employs textual similarity to assess the similarity between a feature request description and identifiers. The approach has been validated on the dataset of 15 open source Java applications by comparing the recommended renaming opportunities against those recovered from the refactoring history of the involved subject applications. The evaluation results suggest that, the proposed approach can identify renaming opportunities on up to (Formula presented.) precision and (Formula presented.) recall. © 2021 Informa UK Limited, trading as Taylor & Francis Group.},
author_keywords={Feature requests;  refactoring opportunity;  rename refactoring;  software refactoring;  text similarity},
document_type={Article},
source={Scopus},
}

@ARTICLE{Massarelli20222259,
author={Massarelli, L. and Di Luna, G.A. and Petroni, F. and Querzoni, L. and Baldoni, R.},
title={Function Representations for Binary Similarity},
journal={IEEE Transactions on Dependable and Secure Computing},
year={2022},
volume={19},
number={4},
pages={2259-2273},
doi={10.1109/TDSC.2021.3051852},
note={cited By 2},
url={https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099729625&doi=10.1109%2fTDSC.2021.3051852&partnerID=40&md5=79fa278794bd152a342dad00cab8e2e1},
abstract={The binary similarity problem consists in determining if two functions are similar considering only their compiled form. Advanced techniques for binary similarity recently gained momentum as they can be applied in several fields, such as copyright disputes, malware analysis, vulnerability detection, etc. In this article we describe SAFE, a novel architecture for function representation based on a self-attentive neural network. SAFE works directly on disassembled binary functions, does not require manual feature extraction, is computationally more efficient than existing solutions, and is more general as it works on stripped binaries and on multiple architectures. Results from our experimental evaluation show how SAFE provides a performance improvement with respect to previous solutions. Furthermore, we show how SAFE can be used in widely different use cases, thus providing a general solution for several application scenarios. © 2004-2012 IEEE.},
author_keywords={Binary analysis;  binary similarity;  deep learning;  malware},
document_type={Article},
source={Scopus},
}
