@article{10.1145/1050849.1050862,
author = {Gill, Nasib S.},
title = {Factors Affecting Effective Software Quality Management Revisited},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1050862},
doi = {10.1145/1050849.1050862},
abstract = {Developing a good software system is a very complex task. In order to produce a good software product, several measures for software quality attributes need to be taken into account. System complexity measurement plays a vital role in controlling and managing software quality because it generally affects the software quality attributes like software reliability, software testability and software maintainability. Thus, software quality assurance (SQA) needs to be addressed keeping in view the new strategies, tool, methodologies and techniques applicable to software development life cycle.This paper is primarily aimed at revisiting and examining peculiar aspects of software development process that affect software quality management process. These aspects of software development process include software reliability measurement, ISO approach applicable to software quality and some aspects related to software testing improvement. Software testing and evaluation methods/tools/techniques do not guarantee effective testing and ensure high software quality. The way to improve the effectiveness of testing is to improve the attitude of software developers towards testing.In this paper, all these factors affecting software quality management have been discussed as well as all the possible improvements have been suggested. The results of this paper may be quite helpful to the researchers in quantifying the specific measuring tools for these software quality attributes.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–4},
numpages = {4},
keywords = {software quality management, software reliability, software quality assurance, software quality, ISO}
}

@article{10.1145/596992.597001,
author = {Jeon, Taewoong and Seung, Hyon Woo and Lee, Sungyoung},
title = {Embedding Built-in Tests in Hot Spots of an Object-Oriented Framework},
year = {2002},
issue_date = {August 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/596992.597001},
doi = {10.1145/596992.597001},
abstract = {This paper describes a scheme of encapsulating test support code as built-in test (BIT) components and embedding them into the hot spots of an object-oriented framework so that defects caused by the modification and extension of the framework can be detected effectively and efficiently through testing. The test components embedded into a framework in this way increase the testability of the framework by making it easy to control and observe the process of framework testing. The proposed technique is illustrated using the facilities of C++. Our testing scheme, however, is equally applicable to other object-oriented languages. Using our scheme, test components can be designed and embedded into the hot spots of a framework without incurring changes or intervention to the framework code, and also can be attached and detached dynamically to/from the framework as needed at run-time.},
journal = {SIGPLAN Not.},
month = {aug},
pages = {25–34},
numpages = {10},
keywords = {object-oriented framework, testability, built-in test (BIT), hook classes}
}

@inproceedings{10.1145/3195538.3195541,
author = {Adlemo, Anders and Tan, He and Tarasov, Vladimir},
title = {Test Case Quality as Perceived in Sweden},
year = {2018},
isbn = {9781450357494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195538.3195541},
doi = {10.1145/3195538.3195541},
abstract = {In order to reach an acceptable level of confidence in the quality of a software product, testing of the software is paramount. To obtain "good" quality software it is essential to rely on "good" test cases. To define the criteria for what make up for a "good" test case is not a trivial task. Over the past 15 years, a short list of publications have presented criteria for "good" test cases but without ranking them based on their importance. This paper presents a non-exhaustive and non-authoritative tentative list of 15 criteria and a ranking of their relative importance. A number of the criteria come from previous publications but also from discussions with our industrial partners. The ranking is based on results collected via a questionnaire that was sent out to a limited number of randomly chosen respondents in the Swedish software industry. This means that the results are more indicative than conclusive.},
booktitle = {Proceedings of the 5th International Workshop on Requirements Engineering and Testing},
pages = {9–12},
numpages = {4},
keywords = {test-driven development, quality criteria, requirement-driven development, good software test cases},
location = {Gothenburg, Sweden},
series = {RET '18}
}

@inproceedings{10.1145/3427228.3427266,
author = {G\"{u}ler, Emre and G\"{o}rz, Philipp and Geretto, Elia and Jemmett, Andrea and \"{O}sterlund, Sebastian and Bos, Herbert and Giuffrida, Cristiano and Holz, Thorsten},
title = {Cupid : Automatic Fuzzer Selection for Collaborative Fuzzing},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427266},
doi = {10.1145/3427228.3427266},
abstract = {Combining the strengths of individual fuzzing methods is an appealing idea to find software faults more efficiently, especially when the computing budget is limited. In prior work, EnFuzz introduced the idea of ensemble fuzzing and devised three heuristics to classify properties of fuzzers in terms of diversity. Based on these heuristics, the authors manually picked a combination of different fuzzers that collaborate. In this paper, we generalize this idea by collecting and applying empirical data from single, isolated fuzzer runs to automatically identify a set of fuzzers that complement each other when executed collaboratively. To this end, we present Cupid, a collaborative fuzzing framework allowing automated, data-driven selection of multiple complementary fuzzers for parallelized and distributed fuzzing. We evaluate the automatically selected target-independent combination of fuzzers by Cupid on Google’s fuzzer-test-suite, a collection of real-world binaries, as well as on the synthetic Lava-M dataset. We find that Cupid outperforms two expert-guided, target-specific and hand-picked combinations on Google’s fuzzer-test-suite in terms of branch coverage, and improves bug finding on Lava-M by 10%. Most importantly, we improve the latency for obtaining 95% and 99% of the coverage by 90% and 64%, respectively. Furthermore, Cupid reduces the amount of CPU hours needed to find a high-performing combination of fuzzers by multiple orders of magnitude compared to an exhaustive evaluation.},
booktitle = {Annual Computer Security Applications Conference},
pages = {360–372},
numpages = {13},
keywords = {collaborative fuzzing, ensemble fuzzing, automated bug finding, fuzzing, parallel fuzzing},
location = {Austin, USA},
series = {ACSAC '20}
}

@article{10.1145/3422590,
author = {Xu, Harry},
title = {Technical Perspective: BLeak: Semantics-Aware Leak Detection in the Web},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/3422590},
doi = {10.1145/3422590},
journal = {Commun. ACM},
month = {oct},
pages = {145},
numpages = {1}
}

@article{10.1145/3512345,
author = {Zhu, Xiaogang and Wen, Sheng and Camtepe, Seyit and Xiang, Yang},
title = {Fuzzing: A Survey for Roadmap},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3512345},
doi = {10.1145/3512345},
abstract = {Fuzz testing (fuzzing) has witnessed its prosperity in detecting security flaws recently. It generates a large number of test cases and monitors the executions for defects. Fuzzing has detected thousands of bugs and vulnerabilities in various applications. Although effective, there lacks systematic analysis of gaps faced by fuzzing. As a technique of defect detection, fuzzing is required to narrow down the gaps between the entire input space and the defect space. Without limitation on the generated inputs, the input space is infinite. However, defects are sparse in an application, which indicates that the defect space is much smaller than the entire input space. Besides, because fuzzing generates numerous test cases to repeatedly examine targets, it requires fuzzing to perform in an automatic manner. Due to the complexity of applications and defects, it is challenging to automatize the execution of diverse applications. In this paper, we systematically review and analyze the gaps as well as their solutions, considering both breadth and depth. This survey can be a roadmap for both beginners and advanced developers to better understand fuzzing.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {jan},
keywords = {Automation, Input Space, Fuzz Testing, Fuzzing Theory, Security}
}

@article{10.1145/3360600,
author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Simon, Laurent and Vijayakumar, Hayawardh},
title = {FuzzFactory: Domain-Specific Fuzzing with Waypoints},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360600},
doi = {10.1145/3360600},
abstract = {Coverage-guided fuzz testing has gained prominence as a highly effective method of finding security vulnerabilities such as buffer overflows in programs that parse binary data. Recently, researchers have introduced various specializations to the coverage-guided fuzzing algorithm for different domain-specific testing goals, such as finding performance bottlenecks, generating valid inputs, handling magic-byte comparisons, etc. Each such solution can require non-trivial implementation effort and produces a distinct variant of a fuzzing tool. We observe that many of these domain-specific solutions follow a common solution pattern.  In this paper, we present FuzzFactory, a framework for developing domain-specific fuzzing applications without requiring changes to mutation and search heuristics. FuzzFactory allows users to specify the collection of dynamic domain-specific feedback during test execution, as well as how such feedback should be aggregated. FuzzFactory uses this information to selectively save intermediate inputs, called waypoints, to augment coverage-guided fuzzing. Such waypoints always make progress towards domain-specific multi-dimensional objectives. We instantiate six domain-specific fuzzing applications using FuzzFactory: three re-implementations of prior work and three novel solutions, and evaluate their effectiveness on benchmarks from Google's fuzzer test suite. We also show how multiple domains can be composed to perform better than the sum of their parts. For example, we combine domain-specific feedback about strict equality comparisons and dynamic memory allocations, to enable the automatic generation of LZ4 bombs and PNG bombs.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {174},
numpages = {29},
keywords = {waypoints, fuzz testing, frameworks, domain-specific fuzzing}
}

@inbook{10.1109/ICSE-Companion52605.2021.00049,
author = {Sun, Jingling},
title = {SetDroid: Detecting User-Configurable Setting Issues of Android Apps via Metamorphic Fuzzing},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00049},
abstract = {Android, the most popular mobile system, offers a number of app-independent, user-configurable settings (e.g., network, location and permission) for controlling the devices and the apps. However, apps may fail to properly adapt their behaviors when these settings are changed, and thus frustrate users. We name such issues as setting issues, which reside in the apps and are induced by the changes of settings. According to our investigation, the majority of setting issues are non-crash (logic) bugs, which however cannot be detected by existing automated app testing techniques due to the lack of test oracles. To this end, we designed and introduced, setting-wise metamorphic fuzzing, the first automated testing technique to overcome the oracle problem in detecting setting issues. Our key insight is that, in most cases, the app behaviors should keep consistent if a given setting is changed and later properly restored. We realized this technique as an automated GUI testing tool, SetDroid, and applied it on 26 popular, open-source Android apps. SetDroid successfully found 32 unique, previously-unknown setting issues in these apps. So far, 25 have been confirmed and 17 were already fixed. We further applied SetDroid on 4 commercial apps with billions of monthly active users and successfully detected 15 previously unknown setting issues, all of which have been confirmed and under fixing. The majority of all these bugs (37 out of 47) are noncrash bugs, which cannot be detected by prior testing techniques.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {108–110},
numpages = {3}
}

@inbook{10.1145/3375627.3375807,
author = {Matthews, Jeanna Neefe and Northup, Graham and Grasso, Isabella and Lorenz, Stephen and Babaeianjelodar, Marzieh and Bashaw, Hunter and Mondal, Sumona and Matthews, Abigail and Njie, Mariama and Goldthwaite, Jessica},
title = {When Trusted Black Boxes Don't Agree: Incentivizing Iterative Improvement and Accountability in Critical Software Systems},
year = {2020},
isbn = {9781450371100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3375627.3375807},
abstract = {Software increasingly plays a key role in regulated areas like housing, hiring, and credit, as well as major public functions such as criminal justice and elections. It is easy for there to be unintended defects with a large impact on the lives of individuals and society as a whole. Preventing, finding, and fixing software defects is a key focus of both industrial software development efforts as well as academic research in software engineering. In this paper, we discuss flaws in the larger socio-technical decision-making processes in which critical black-box software systems are developed, deployed, and trusted. We use criminal justice software, specifically probabilistic genotyping (PG) software, as a concrete example. We describe how PG software systems, designed to do the same job, produce different results. We highlight the under-appreciated impact of changes in key parameters and the disparate impact that one such parameter can have on different racial/ethnic groups. We propose concrete changes to the socio-technical decision-making processes surrounding the use of PG software that could be used to incentivize iterative improvements in the accuracy, fairness, reliability, and accountability of these systems.},
booktitle = {Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages = {102–108},
numpages = {7}
}

@inproceedings{10.1145/2950290.2983959,
author = {Zhong, Hua and Zhang, Lingming and Khurshid, Sarfraz},
title = {Combinatorial Generation of Structurally Complex Test Inputs for Commercial Software Applications},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983959},
doi = {10.1145/2950290.2983959},
abstract = { Despite recent progress in automated test generation research, significant challenges remain for applying these techniques on large-scale software systems. These systems under test often require structurally complex test inputs within a large input domain. It is challenging to automatically generate a reasonable number of tests that are both legal and behaviorally-diverse to exercise these systems. Constraint-based test generation is an effective approach for generating structurally complex inputs for systematic testing. While this approach can typically generate large numbers of tests, it has limited scalability – tests generated are usually only up to a small bound on input size. Combinatorial test generation, e.g., pair-wise testing, is a more scalable approach but is challenging to apply on commercial software systems that require complex input structures that cannot be formed by using arbitrary combinations. This paper introduces comKorat, which unifies constraint-based generation of structurally complex tests with combinatorial testing. Specifically, comKorat integrates Korat and ACTS test generators to generate test suites for large scale software systems with structurally complex test inputs. We have successfully applied comKorat on four software applications developed at eBay and Yahoo!. The experimental results show that comKorat outperforms existing solutions in execution time and test coverage. Furthermore, comKorat found a total of 59 previously unknown bugs in the four applications. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {981–986},
numpages = {6},
keywords = {Korat, ACTS, Constraint-based test generation, Combinatorial test generation},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/2786805.2807556,
author = {White, Thomas},
title = {Increasing the Efficiency of Search-Based Unit Test Generation Using Parameter Control},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2807556},
doi = {10.1145/2786805.2807556},
abstract = { Automatically generating test suites with high coverage is of great importance to software engineers, but this process is hindered by the vast amount of parameters the tools use to generate tests. Developers usually lack knowledge about the workings of the tools that generate test suites to set the parameters to optimal values, and the optimal values usually change during runtime. Parameter Control automatically adapts parameters during test generation, and has shown to help solve this problem in other areas. To investigate any improvements parameter control could have in search-based generation of test suites, we adapted multiple methods of controlling mutation and crossover rate in EvoSuite, a tool that automatically generates unit test suites. Upon evaluation, clear benefits to controlling parameters were found, but surprisingly, controlling some parameters can sometimes be more harmful to the search than beneficial through increased computation costs. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {1042–1044},
numpages = {3},
keywords = {search based software engineering, Parameter control, genetic algorithm, testing, test suite generation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/1414004.1414011,
author = {Engstr\"{o}m, Emelie and Skoglund, Mats and Runeson, Per},
title = {Empirical Evaluations of Regression Test Selection Techniques: A Systematic Review},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414011},
doi = {10.1145/1414004.1414011},
abstract = {Regression testing is the verification that previously functioning software remains after a change. In this paper we report on a systematic review of empirical evaluations of regression test selection techniques, published in major software engineering journals and conferences. Out of 2,923 papers analyzed in this systematic review, we identified 28 papers reporting on empirical comparative evaluations of regression test selection techniques. They report on 38 unique studies (23 experiments and 15 case studies), and in total 32 different techniques for regression test selection are evaluated. Our study concludes that no clear picture of the evaluated techniques can be provided based on existing empirical evidence, except for a small group of related techniques. Instead, we identified a need for more and better empirical studies were concepts are evaluated rather than small variations. It is also necessary to carefully consider the context in which studies are undertaken.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {22–31},
numpages = {10},
keywords = {test selection, regression testing, systematic review},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@article{10.1145/3533818,
author = {Birchler, Christian and Khatiri, Sajad and Derakhshanfar, Pouria and Panichella, Sebastiano and Panichella, Annibale},
title = {Single and Multi-Objective Test Cases Prioritization for Self-Driving Cars in Virtual Environments},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3533818},
doi = {10.1145/3533818},
abstract = {However, these tests are very expensive and are too many to be run frequently within limited time constraints. In this paper, we investigate test case prioritization techniques to increase the ability to detect SDC regression faults with virtual tests earlier. Our empirical study conducted in the SDC domain shows that },
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
keywords = {Test Case Prioritization, Software Simulation, Autonomous Systems}
}

@inproceedings{10.1145/3489048.3522655,
author = {Xiao, Dongwei and Liu, Zhibo and Yuan, Yuanyuan and Pang, Qi and Wang, Shuai},
title = {Metamorphic Testing of Deep Learning Compilers},
year = {2022},
isbn = {9781450391412},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489048.3522655},
doi = {10.1145/3489048.3522655},
abstract = {The prosperous trend of deploying deep neural network (DNN) models to diverse hardware platforms has boosted the development of deep learning (DL) compilers. DL compilers take high-level DNN model specifications as input and generate optimized DNN executables for diverse hardware architectures like CPUs, GPUs, and hardware accelerators. We introduce MT-DLComp, a metamorphic testing framework specifically designed for DL compilers to uncover erroneous compilations. Our approach leverages deliberately-designed metamorphic relations (MRs) to launch semantics-preserving mutations toward DNN models to generate their variants. This way, DL compilers can be automatically tested for compilation correctness by comparing the execution outputs of the compiled DNN models and their variants without manual intervention. We detected over 435 inputs that can result in erroneous compilations in four popular DL compilers, all of which are industry-strength products maintained by Amazon, Facebook, Microsoft, and Google. We uncovered four bugs in these compilers by debugging them using the error-triggering inputs.},
booktitle = {Abstract Proceedings of the 2022 ACM SIGMETRICS/IFIP PERFORMANCE Joint International Conference on Measurement and Modeling of Computer Systems},
pages = {65–66},
numpages = {2},
keywords = {deep learning, metamorphic testing},
location = {Mumbai, India},
series = {SIGMETRICS/PERFORMANCE '22}
}

@inbook{10.1145/3461002.3473073,
author = {Pett, Tobias and Krieter, Sebastian and Th\"{u}m, Thomas and Lochau, Malte and Schaefer, Ina},
title = {AutoSMP: An Evaluation Platform for Sampling Algorithms},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473073},
abstract = {Testing configurable systems is a challenging task due to the combinatorial explosion problem. Sampling is a promising approach to reduce the testing effort for product-based systems by finding a small but still representative subset (i.e., a sample) of all configurations for testing. The quality of a generated sample wrt. evaluation criteria such as run time of sample generation, feature coverage, sample size, and sampling stability depends on the subject systems and the sampling algorithm. Choosing the right sampling algorithm for practical applications is challenging because each sampling algorithm fulfills the evaluation criteria to a different degree. Researchers keep developing new sampling algorithms with improved performance or unique properties to satisfy application-specific requirements. Comparing sampling algorithms is therefore a necessary task for researchers. However, this task needs a lot of effort because of missing accessibility of existing algorithm implementations and benchmarks. Our platform AutoSMP eases practitioners and researchers lifes by automatically executing sampling algorithms on predefined benchmarks and evaluating the sampling results wrt. specific user requirements. In this paper, we introduce the open-source application of AutoSMP and a set of predefined benchmarks as well as a set of T-wise sampling algorithms as examples.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {41–44},
numpages = {4}
}

@inproceedings{10.1145/1291535.1291536,
author = {Kicillof, Nicolas and Grieskamp, Wolfgang and Tillmann, Nikolai and Braberman, Victor},
title = {Achieving Both Model and Code Coverage with Automated Gray-Box Testing},
year = {2007},
isbn = {9781595938503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1291535.1291536},
doi = {10.1145/1291535.1291536},
abstract = {We have devised a novel technique to automatically generate test cases for a software system, combining black-box model-based testing with white-box parameterized unit testing. The former provides general guidance for the structure of the tests in the form of test sequences, as well as the oracle to check for conformance of an application under test with respect to a behavioral model. The latter finds a set of concrete parameter values that maximize code coverage using symbolic analysis. By applying these techniques together, we can produce test definitions (expressed as code to be run in a test management framework) that exercise all selected paths in the model, while also covering code branches specific to the implementation. These results cannot be obtained from any of the individual approaches alone, as the model cannot predict what values are significant to a particular implementation, while parameterized unit testing requires manually written test sequences and correctness validations. We provide tool support, integrated into our model-based testing tool.},
booktitle = {Proceedings of the 3rd International Workshop on Advances in Model-Based Testing},
pages = {1–11},
numpages = {11},
keywords = {parameterized unit testing, model-based testing, concolic execution, symbolic execution, test-case generation},
location = {London, United Kingdom},
series = {A-MOST '07}
}

@inproceedings{10.1109/ASE.2015.14,
author = {Hong, Shin and Lee, Byeongcheol and Kwak, Taehoon and Jeon, Yiru and Ko, Bongsuk and Kim, Yunho and Kim, Moonzoo},
title = {Mutation-Based Fault Localization for Real-World Multilingual Programs},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.14},
doi = {10.1109/ASE.2015.14},
abstract = {Programmers maintain and evolve their software in a variety of programming languages to take advantage of various control/data abstractions and legacy libraries. The programming language ecosystem has diversified over the last few decades, and non-trivial programs are likely to be written in more than a single language. Unfortunately, language interfaces such as Java Native Interface and Python/C are difficult to use correctly and the scope of fault localization goes beyond language boundaries, which makes debugging multilingual bugs challenging. To overcome the aforementioned limitations, we propose a mutation-based fault localization technique for real-world multilingual programs. To improve the accuracy of locating multilingual bugs, we have developed and applied new mutation operators as well as conventional mutation operators. The results of the empirical evaluation for six non-trivial real-world multilingual bugs are promising in that the proposed technique identifies the buggy statements as the most suspicious statements for all six bugs.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {464–475},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3134600.3134607,
author = {Jiang, Jianyu and Zhao, Shixiong and Alsayed, Danish and Wang, Yuexuan and Cui, Heming and Liang, Feng and Gu, Zhaoquan},
title = {Kakute: A Precise, Unified Information Flow Analysis System for Big-Data Security},
year = {2017},
isbn = {9781450353458},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3134600.3134607},
doi = {10.1145/3134600.3134607},
abstract = {Big-data frameworks (e.g., Spark) enable computations on tremendous data records generated by third parties, causing various security and reliability problems such as information leakage and programming bugs. Existing systems for big-data security (e.g., Titian) track data transformations in a record level, so they are imprecise and too coarse-grained for these problems. For instance, when we ran Titian to drill down input records that produced a buggy output record, Titian reported 3 to 9 orders of magnitude more input records than the actual ones. Information Flow Tracking (IFT) is a conventional approach for precise information control. However, extant IFT systems are neither efficient nor complete for big-data frameworks, because theses frameworks are data-intensive, and data flowing across hosts is often ignored by IFT.This paper presents Kakute, the first precise, fine-grained information flow analysis system for big-data. Our insight on making IFT efficient is that most fields in a data record often have the same IFT tags, and we present two new efficient techniques called Reference Propagation and Tag Sharing. In addition, we design an efficient, complete cross-host information flow propagation approach. Evaluation on seven diverse big-data programs (e.g., WordCount) shows that Kakute had merely 32.3% overhead on average even when fine-grained information control was enabled. Compared with Titian, Kakute precisely drilled down the actual bug inducing input records, a huge reduction of 3 to 9 orders of magnitude. Kakute's performance overhead is comparable with Titian. Furthermore, Kakute effectively detected 13 real-world security and reliability bugs in 4 diverse problems, including information leakage, data provenance, programming and performance bugs. Kakute's source code and results are available on https://github.com/hku-systems/kakute.},
booktitle = {Proceedings of the 33rd Annual Computer Security Applications Conference},
pages = {79–90},
numpages = {12},
keywords = {Information Flow Tracking, Data-intensive Scalable Computing System, Big-data},
location = {Orlando, FL, USA},
series = {ACSAC 2017}
}

@inproceedings{10.1109/ICSE.2017.68,
author = {Rojas, Jos\'{e} Miguel and White, Thomas D. and Clegg, Benjamin S. and Fraser, Gordon},
title = {Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.68},
doi = {10.1109/ICSE.2017.68},
abstract = {Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program; automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {677–688},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@article{10.1145/3360614,
author = {Song, Dowon and Lee, Myungho and Oh, Hakjoo},
title = {Automatic and Scalable Detection of Logical Errors in Functional Programming Assignments},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360614},
doi = {10.1145/3360614},
abstract = {We present a new technique for automatically detecting logical errors in functional programming assignments. Compared to syntax or type errors, detecting logical errors remains largely a manual process that requires hand-made test cases. However, designing proper test cases is nontrivial and involves a lot of human effort. Furthermore, manual test cases are unlikely to catch diverse errors because instructors cannot predict all corner cases of diverse student submissions. We aim to reduce this burden by automatically generating test cases for functional programs. Given a reference program and a student's submission, our technique generates a counter-example that captures the semantic difference of the two programs without any manual effort. The key novelty behind our approach is the counter-example generation algorithm that combines enumerative search and symbolic verification techniques in a synergistic way. The experimental results show that our technique is able to detect 88 more errors not found by mature test cases that have been improved over the past few years, and performs better than the existing property-based testing techniques. We also demonstrate the usefulness of our technique in the context of automated program repair, where it effectively helps to eliminate test-suite-overfitted patches.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {188},
numpages = {30},
keywords = {Symbolic Execution, Automated Test Case Generation, Program Synthesis}
}

