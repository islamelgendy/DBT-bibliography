@inproceedings{10.1145/3319008.3319715,
author = {Betrand, Maryjane},
title = {Fault Aware Software Engineering (FASE): Reducing Code Faults by Controlling Developer Characteristics},
year = {2019},
isbn = {9781450371452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3319008.3319715},
doi = {10.1145/3319008.3319715},
abstract = {Human factors play a significant role in software development. When the human element is not considered in software development, it may lead to faulty software which may cause delays in software delivery. Faults in software are mostly because of human errors, which consequently results in software failure. Therefore, the ability to model developer-fault relationship will subsequently lead to the production of a proactive adaptive recommendation system that will profile developers' characteristics against fault characteristic. Subsequently, reducing the number of faults developers seed into code. The overall aim of this research is to reduce the number of faults developers introduce into code by modelling a developer-fault relationship which will be implemented into a profoundly new adaptive IDE plug-in. The objectives of this study are: to establish the personalities of developers that influence the introduction of fault in code by the developer; to determine the characteristics of faults that are influenced by the human personality; to identify the relationship between the fault characteristics and human characteristics; and model the relationship between the fault characteristics and human personality. This study provides conceptual knowledge, reports findings, and presents both real and hypothesised challenges facing software engineering community. The study will evaluate the personality of software developers using the IPIP-NEO five-factor-personality test and the SZZ algorithm to evaluate faults found in code from Open source repositories. The result of this study will be used for the implementation of the model in a proof-of-concept IDE plug-in that adapts to developers, helping them avoid introducing faults they are individually prone.},
booktitle = {Proceedings of the Evaluation and Assessment on Software Engineering},
pages = {375–378},
numpages = {4},
keywords = {faults, personality, human factors, recommender system},
location = {Copenhagen, Denmark},
series = {EASE '19}
}

@inproceedings{10.1109/ICSE.2017.68,
author = {Rojas, Jos\'{e} Miguel and White, Thomas D. and Clegg, Benjamin S. and Fraser, Gordon},
title = {Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.68},
doi = {10.1109/ICSE.2017.68},
abstract = {Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program; automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {677–688},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/1388969.1388979,
author = {Ribeiro, Jos\'{e} Carlos Bregieiro},
title = {Search-Based Test Case Generation for Object-Oriented Java Software Using Strongly-Typed Genetic Programming},
year = {2008},
isbn = {9781605581316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1388969.1388979},
doi = {10.1145/1388969.1388979},
abstract = {In evolutionary testing, meta-heuristic search techniques are used to generate high-quality test data. The focus of our on-going work is on employing evolutionary algorithms for the structural unit-testing of object-oriented Java programs.Test cases are evolved using the Strongly-Typed Genetic Programming technique. Test data quality evaluation includes instrumenting the test object, executing it with the generated test cases, and tracing the structures traversed in order to derive coverage metrics. The strategy for efficiently guiding the search process towards achieving full structural coverage involves favouring test cases that exercise problematic structures and control-flow paths. Static analysis and instrumentation is performed solely with basis on the information extracted from the test objects' Java Bytecode.Relevant contributions include the introduction of novel methodologies for automation, search guidance and input domain reduction, and the presentation of the eCrash automated test case generation tool.},
booktitle = {Proceedings of the 10th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {1819–1822},
numpages = {4},
keywords = {search-based test case generation, strongly-typed genetic programming, evolutionary testing, object-orientation},
location = {Atlanta, GA, USA},
series = {GECCO '08}
}

@article{10.1145/3310013.3310019,
author = {Turlea, Ana},
title = {Model-in-the-Loop Testing for Cyber Physical Systems},
year = {2022},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3310013.3310019},
doi = {10.1145/3310013.3310019},
abstract = {Context: Nowadays, there is of high interest to use automated testing, not only because it optimizes the manual testing by reducing the needed time and cost, but also because manual testing is more likely to produce errors. Increasing the safety of software controlled complex systems, that use many distributed electronic controlled units, requires extensive testing. In model based testing, the test specification is derived from the system requirements and a model that describes selected functional and nonfunctional aspects of the system under test.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {37–41},
numpages = {5},
keywords = {search based testing, genetic algorithms, cruise control, test case generation, model based testing, cyberphysical systems, model-in-the-loop, electronic controlled unit}
}

@inbook{10.1145/3368089.3409760,
author = {Lou, Yiling and Chen, Zhenpeng and Cao, Yanbin and Hao, Dan and Zhang, Lu},
title = {Understanding Build Issue Resolution in Practice: Symptoms and Fix Patterns},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409760},
abstract = {Build systems are essential for modern software maintenance and development, while build failures occur frequently across software systems, inducing non-negligible costs in development activities. Build failure resolution is a challenging problem and multiple studies have demonstrated that developers spend non-trivial time in resolving encountered build failures; to relieve manual efforts, automated resolution techniques are emerging recently, which are promising but still limitedly effective. Understanding how build failures are resolved in practice can provide guidelines for both developers and researchers on build issue resolution. Therefore, this work presents a comprehensive study of fix patterns in practical build failures. Specifically, we study 1,080 build issues of three popular build systems Maven, Ant, and Gradle from Stack Overflow, construct a fine-granularity taxonomy of 50 categories regarding to the failure symptoms, and summarize the fix patterns for different failure types. Our key findings reveal that build issues stretch over a wide spectrum of symptoms; 67.96% of the build issues are fixed by modifying the build script code related to plugins and dependencies; and there are 20 symptom categories, more than half of whose build issues can be fixed by specific patterns. Furthermore, we also address the challenges in applying non-intuitive or simplistic fix patterns for developers.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {617–628},
numpages = {12}
}

@article{10.1145/3510418,
author = {Ahmed, Umair Z. and Fan, Zhiyu and Yi, Jooyong and Al-Bataineh, Omar I. and Roychoudhury, Abhik},
title = {Verifix: Verified Repair of Programming&nbsp;Assignments},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3510418},
doi = {10.1145/3510418},
abstract = {Automated feedback generation for introductory programming assignments is useful for programming education. Most works try to generate feedback to correct a student program by comparing its behavior with an instructor’s reference program on selected tests. In this work, our aim is to generate verifiably correct program repairs as student feedback. A student-submitted program is aligned and composed with a reference solution in terms of control flow, and the variables of the two programs are automatically aligned via predicates describing the relationship between the variables. When verification attempt for the obtained aligned program fails, we turn a verification problem into a MaxSMT problem whose solution leads to a minimal repair. We have conducted experiments on student assignments curated from a widely deployed intelligent tutoring system. Our results show that generating verified repair without sacrificing the overall repair rate is possible. In fact, our implementation, Verifix, is shown to outperform Clara, a state-of-the-art tool, in terms of repair rate. This shows the promise of using verified repair to generate high confidence feedback in programming pedagogy settings.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan}
}

@inproceedings{10.1145/2970276.2970294,
author = {Cheng, Lin and Chang, Jialiang and Yang, Zijiang and Wang, Chao},
title = {GUICat: GUI Testing as a Service},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970294},
doi = {10.1145/2970276.2970294},
abstract = { GUIs are event-driven applications where the flow of the program is determined by user actions such as mouse clicks and key presses. GUI testing is a challenging task not only because of the combinatorial explosion in the number of event sequences, but also because of the difficulty to cover the large number of data values. We propose GUICat, the first cloud based GUI testing framework that simultaneously generates event sequences and data values. It is a white-box GUI testing tool that augments traditional sequence generation techniques with concolic execution. We also propose a cloudbased parallel algorithm for mitigating both event sequence explosion and data value explosion, by distributing the concolic execution tasks over public clouds such as Amazon EC2. We have evaluated the tool on standard GUI testing benchmarks and showed that GUICat significantly outperforms state-of-the-art GUI testing tools. The video demo URL is https://youtu.be/rfnnQOmZqj4. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {858–863},
numpages = {6},
keywords = {Test generation, Symbolic execution, GUI testing, Cloud},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@article{10.1145/201055.201058,
author = {Howden, W. E. and Huang, Yudong},
title = {Software Trustability Analysis},
year = {1995},
issue_date = {Jan. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/201055.201058},
doi = {10.1145/201055.201058},
abstract = {A measure of software dependability called trustability is described. A program p has trustability T if we are at least T confident that p is free of faults. Trustability measurement depends on detectability. The detectability of a method is the probability that it will detect faults, when there are faults present. Detectability research can be used to characterize conditions under which one testing and analysis method is more effective than another. Several detectability results that were only previously described informally, and illustrated by example, are proved. Several new detectability results are also proved. The trustability model characterizes the kind of information that is needed to justify a given level of trustability. When the required information is available, the  trustability approach can be used to determine strategies in which methods are combined for maximum effectiveness. It can be used to determine the minimum amount of resources needed to guarantee a required degree of trustability, and the maximum trustability that is achievable with a given amount of resources. Theorems proving several optimization results are given. Applications of the trustability model are discussed. Methods for the derivation of detectability factors, the relationship between trustability and operational reliability, and the relationship between the software development process and trustability are described.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
pages = {36–64},
numpages = {29},
keywords = {dependability, detectability, trustability, analysis, testability, failure density, statistical, testing}
}

@inproceedings{10.1145/1390817.1390819,
author = {Ayewah, Nathaniel and Pugh, William},
title = {A Report on a Survey and Study of Static Analysis Users},
year = {2008},
isbn = {9781605580517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390817.1390819},
doi = {10.1145/1390817.1390819},
abstract = {As static analysis tools mature and attract more users, vendors and researchers have an increased interest in understanding how users interact with them, and how they impact the software development process. The FindBugs project has conducted a number of studies including online surveys, interviews and a preliminary controlled user study to better understand the practices, experiences and needs of its users. Through these studies we have learned that many users are interested in even low priority warnings, and some organizations are building custom solutions to more seamlessly and automatically integrate FindBugs into their software processes. We've also observed that developers can make decisions about the accuracy and severity of warnings fairly quickly and independent reviewers will generally reach the same conclusions about warnings.},
booktitle = {Proceedings of the 2008 Workshop on Defects in Large Software Systems},
pages = {1–5},
numpages = {5},
keywords = {bugs, software quality, software defects, false positives, static analysis, Java, bug patterns, FindBugs},
location = {Seattle, Washington},
series = {DEFECTS '08}
}

@inproceedings{10.1145/3324884.3416584,
author = {Wang, Shuai and Su, Zhendong},
title = {Metamorphic Object Insertion for Testing Object Detection Systems},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416584},
doi = {10.1145/3324884.3416584},
abstract = {Recent advances in deep neural networks (DNNs) have led to object detectors (ODs) that can rapidly process pictures or videos, and recognize the objects that they contain. Despite the promising progress by industrial manufacturers such as Amazon and Google in commercializing deep learning-based ODs as a standard computer vision service, ODs --- similar to traditional software --- may still produce incorrect results. These errors, in turn, can lead to severe negative outcomes for the users. For instance, an autonomous driving system that fails to detect pedestrians can cause accidents or even fatalities. However, despite their importance, principled, systematic methods for testing ODs do not yet exist.To fill this critical gap, we introduce the design and realization of MetaOD, a metamorphic testing system specifically designed for ODs to effectively uncover erroneous detection results. To this end, we (1) synthesize natural-looking images by inserting extra object instances into background images, and (2) design metamorphic conditions asserting the equivalence of OD results between the original and synthetic images after excluding the prediction results on the inserted objects. MetaOD is designed as a streamlined workflow that performs object extraction, selection, and insertion. We develop a set of practical techniques to realize an effective workflow, and generate diverse, natural-looking images for testing. Evaluated on four commercial OD services and four pretrained models provided by the TensorFlow API, MetaOD found tens of thousands of detection failures. To further demonstrate the practical usage of MetaOD, we use the synthetic images that cause erroneous detection results to retrain the model. Our results show that the model performance is significantly increased, from an mAP score of 9.3 to an mAP score of 10.5.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1053–1065},
numpages = {13},
keywords = {object detection, computer vision, testing, deep neural networks},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/1985793.1986036,
author = {Zhang, Sai},
title = {Palus: A Hybrid Automated Test Generation Tool for Java},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986036},
doi = {10.1145/1985793.1986036},
abstract = {In object-oriented programs, a unit test often consists of a sequence of method calls that create and mutate objects. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible.This paper proposes a combined static and dynamic test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests.Our Palus tool implements this approach. We compared it with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on six popular open-source Java programs. Tests generated by Palus achieved 35% higher structural coverage on average. Palus is also internally used in Google, and has found 22 new bugs in four well-tested products.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1182–1184},
numpages = {3},
keywords = {static and dynamic analyses, automated test generation},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/3464298.3493400,
author = {Tak, Byungchul and Han, Wook-Shin},
title = {Lognroll: Discovering Accurate Log Templates by Iterative Filtering},
year = {2021},
isbn = {9781450385343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464298.3493400},
doi = {10.1145/3464298.3493400},
abstract = {Modern IT systems rely heavily on log analytics for critical operational tasks. Since the volume of logs produced from numerous distributed components is overwhelming, it requires us to employ automated processing. The first step of automated log processing is to convert streams of log lines into the sequence of log format IDs, called log templates. A log template serves as a base string with unfilled parts from which logs are generated during runtime by substitution of contextual information. The problem of log template discovery from the volume of collected logs poses a great challenge due to the semi-structured nature of the logs and the computational overheads. Our investigation reveals that existing techniques show various limitations. We approach the log template discovery problem as search-based learning by applying the ILP (Inductive Logic Programming) framework. The algorithm core consists of narrowing down the logs into smaller sets by analyzing value compositions on selected log column positions. Our evaluation shows that it produces accurate log templates from diverse application logs with small computational costs compared to existing methods. With the quality metric we defined, we obtained about 21%-51% improvements of log template quality.},
booktitle = {Proceedings of the 22nd International Middleware Conference},
pages = {273–285},
numpages = {13},
keywords = {log analysis, log template, sequential covering},
location = {Qu\'{e}bec city, Canada},
series = {Middleware '21}
}

@inproceedings{10.5555/564092.564110,
author = {Liu, Yan and Gorton, Ian and Liu, Anna and Jiang, Ning and Chen, Shiping},
title = {Designing a Test Suite for Empirically-Based Middleware Performance Prediction},
year = {2002},
isbn = {0909925887},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {One of the major problems in building large-scale enterprise systems is anticipating the performance of the eventual solution before it has been built. This problem is especially germane to modern Internet-based e-business applications, where failure to provide high performance and scalability can lead to application and business failure. The fundamental software engineering problem is compounded by many factors, including application diversity, architectural trade-offs and options, COTS component integration requirements, and differences in performance of various software and hardware infrastructures. In the ForeSight project, a practical solution to this problem, based on empirical testing is being investigated. The approach constructs useful models that act as predictors of the performance and the effects of architectural trade-offs for component-based systems such as CORBA, COM+ and J2EE. This paper focuses on describing the issues involved in designing and executing a test suite that is efficient to characterize the behavior and performance profile of a J2EE application server product. The aims of the test suite are described, along with its design and some illustrative empirical results to show it's effectiveness..},
booktitle = {Proceedings of the Fortieth International Conference on Tools Pacific: Objects for Internet, Mobile and Embedded Applications},
pages = {123–130},
numpages = {8},
keywords = {performance modelling, prototype, empirical results, COTS, middleware, component-based system},
location = {Sydney, Australia},
series = {CRPIT '02}
}

@inproceedings{10.1145/2554850.2554978,
author = {Papadakis, Mike and Le Traon, Yves},
title = {Effective Fault Localization via Mutation Analysis: A Selective Mutation Approach},
year = {2014},
isbn = {9781450324694},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2554850.2554978},
doi = {10.1145/2554850.2554978},
abstract = {When programs fail, developers face the problem of identifying the code fragments responsible for this failure. To this end, fault localization techniques try to identify suspicious program places (program statements) by observing the spectrum of the failing and passing test executions. These statements are then pointed out to assist the debugging activity. This paper considers mutation-based fault localization and suggests the use of a sufficient mutant set to locate effectively the faulty statements. Experimentation reveals that mutation-based fault localization is significantly more effective than current state-of-the-art fault localization techniques. Additionally, the results show that the proposed approach is capable of reducing the overheads of mutation analysis. In particular the number of mutants to be considered is reduced to 20% with only a limited loss on the method's effectiveness.},
booktitle = {Proceedings of the 29th Annual ACM Symposium on Applied Computing},
pages = {1293–1300},
numpages = {8},
keywords = {program debugging, mutation analysis, fault localization},
location = {Gyeongju, Republic of Korea},
series = {SAC '14}
}

@inproceedings{10.1145/1287624.1287730,
author = {Hartman, Alan and Katara, Mika and Paradkar, Amit},
title = {Domain Specific Approaches to Software Test Automation},
year = {2007},
isbn = {9781595938114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287624.1287730},
doi = {10.1145/1287624.1287730},
abstract = {In this paper, we describe the contributions to the workshop on domain specific approaches to software test automation. The workshop consists of six accepted papers and a keynote speech. The papers tackle a wide range of topics related to the testing of applications within specific domains including event driven software, synchronous safety critical software, web applications, wireless sensor networks, and mobile phone applications. Each of the papers describes the influence of the domain on the testing process.},
booktitle = {Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {621–622},
numpages = {2},
keywords = {test prioritization, constraint logic programming, domain specific test automation, safety critical software, event driven systems, feature testing, wireless sensor software, exploratory testing, model based testing, data exchangeability, multi-threaded debugging},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE '07}
}

@inproceedings{10.1145/1295014.1295062,
author = {Hartman, Alan and Katara, Mika and Paradkar, Amit},
title = {Domain Specific Approaches to Software Test Automation},
year = {2007},
isbn = {9781595938121},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1295014.1295062},
doi = {10.1145/1295014.1295062},
abstract = {In this paper, we describe the contributions to the workshop on domain specific approaches to software test automation. The workshop consists of six accepted papers and a keynote speech. The papers tackle a wide range of topics related to the testing of applications within specific domains including event driven software, synchronous safety critical software, web applications, wireless sensor networks, and mobile phone applications. Each of the papers describes the influence of the domain on the testing process.},
booktitle = {The 6th Joint Meeting on European Software Engineering Conference and the ACM SIGSOFT Symposium on the Foundations of Software Engineering: Companion Papers},
pages = {621–622},
numpages = {2},
keywords = {feature testing, multi-threaded debugging, safety critical software, model based testing, domain specific test automation, test prioritization, event driven systems, wireless sensor software, exploratory testing, constraint logic programming, data exchangeability},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE companion '07}
}

@inproceedings{10.1145/2786805.2804430,
author = {Janjua, Muhammad Umar},
title = {OnSpot System: Test Impact Visibility during Code Edits in Real Software},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2804430},
doi = {10.1145/2786805.2804430},
abstract = { For maintaining the quality of software updates to complex software products (e.g. Windows 7 OS), an extensive, broad level regression testing is conducted whenever releasing new code fixes or updates. Despite the huge cost and investment in the test infrastructure to execute these massive tests, the developer of the code fix has to wait for the regression test failures to be reported after checkin. These regression tests typically run way later from the code editing stage and consequently the developer has no test impact visibility while introducing the code changes at compile time or before checkin. We argue that it is valuable and practically feasible to tailor the entire development/testing process to provide valuable and actionable test feedback at the development/compilation stage as well. With this goal, this paper explores a system model that provides a near real-time test feedback based on regression tests while the code change is under development or as soon as it becomes compilable. OnSpot system dynamically overlays the results of tests on relevant source code lines in the development environment; thereby highlighting test failures akin to syntax failures enabling quicker correction and re-run at compile time rather than late when the damage is already done. We evaluate OnSpot system with the security fixes in Windows 7 while considering various factors like test feedback time, coverage ratio. We found out that on average nearly 40% of the automated Windows 7 regression test collateral could run under 30 seconds providing same level of coverage; thereby making OnSpot approach practically feasible and manageable during compile time },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {994–997},
numpages = {4},
keywords = {software, testing, real product testing, code writing, analysis, early regression, development},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.5555/1788446.1788479,
author = {Christiansen, Jan and Fischer, Sebastian},
title = {EasyCheck: Test Data for Free},
year = {2008},
isbn = {3540789685},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {We present a lightweight, automated tool for specification-based testing of declarative programs written in the functional logic programming language Curry and emphasize the usefulness of logic features in its implementation and use. Free variables, nondeterminism and encapsulated search turn out to be elegant and powerful means to express test-data generation.},
booktitle = {Proceedings of the 9th International Conference on Functional and Logic Programming},
pages = {322–336},
numpages = {15},
keywords = {encapsulated search, nondeterminism, testing, curry},
location = {Ise, Japan},
series = {FLOPS'08}
}

@inproceedings{10.1145/2905055.2905283,
author = {Johri, Prashant and Nasar, Md. and Das, Sanjoy and Kumar, Mithun},
title = {Open Source Software Reliability Growth Models for Distributed Environment Based on Component-Specific Testing-Efforts},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905283},
doi = {10.1145/2905055.2905283},
abstract = {Because of availability, redistributable, affordability, modifiability, of source code, free and no restriction in choice, open source is a favorite platform for lot of software industries and peoples, who consider using the power of extremely reliable and superior quality software. Numeouus SRGMs have been proposed to estimate the reliability of the software of OSSs; however, no one has proven to perform very well considering diverse project characteristics. In the models for OSSs, the error deletion experience for the reused and the newly developed components based on component-specific testing-effort is demonstrated. It is considered that there are several different types of faults for newly developed component and single type of faults for reused components for obtaining the unambiguous expressions for the mean number of individual types of errors. For OSSs system components testing-efforts have to be modeled separately for each and every component in the system. The total effort of the system is then calculated from the summation of component-specific testing-effort functions. We have employed MATLAB as implementation framework for performing all the estimations. Our approach partitions the testing effort with growth curves of varying nature among different components of the same OSS. To validate our analytical results, numerical illustrations have also been provided.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {75},
numpages = {9},
keywords = {Software Reliability Growth Models (SRGMs), Testing-effort function (TEF), Non Homogeneous Poisson Process (NHPP), Open Source Software (OSS), Distributed development environment (DDE)},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1109/WI-IAT.2014.62,
author = {Freitas, Artur and Vieira, Renata},
title = {An Ontology for Guiding Performance Testing},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.62},
doi = {10.1109/WI-IAT.2014.62},
abstract = {Software test is a technique to obtain information about software systems quality. Performance test is a type of software test that aims at evaluating software performance at a given load scenario, but it requires specialized knowledge about tools, activities and metrics of the domain. Since ontology is a promising knowledge representation technique, this paper presents a literature review to identify trends and compare researches of ontologies in the fields of software testing and software performance. Also, to investigate this issue from a practical perspective, it was developed an ontology for representing the core knowledge of performance testing. This paper presents the ontology and compare it with related ones. Then, semantic technologies are explored to demonstrate the practical feasibility of developing ontology-based applications for assisting testers with performance test planning and management.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
pages = {400–407},
numpages = {8},
series = {WI-IAT '14}
}

