@inbook{10.1109/ICSE-SEIP52600.2021.00023,
author = {Ahlgren, John and Berezin, Maria Eugenia and Bojarczuk, Kinga and Dulskyte, Elena and Dvortsova, Inna and George, Johann and Gucevska, Natalija and Harman, Mark and Lomeli, Maria and Meijer, Erik and Sapora, Silvia and Spahr-Summers, Justin},
title = {Testing Web Enabled Simulation at Scale Using Metamorphic Testing},
year = {2021},
isbn = {9780738146690},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP52600.2021.00023},
abstract = {We report on Facebook's deployment of MIA (Metamorphic Interaction Automaton). MIA is used to test Facebook's Web Enabled Simulation, built on a web infrastructure of hundreds of millions of lines of code. MIA tackles the twin problems of test flakiness and the unknowable oracle problem. It uses metamorphic testing to automate continuous integration and regression test execution. MIA also plays the role of a test bot, automatically commenting on all relevant changes submitted for code review. It currently uses a suite of over 40 metamorphic test cases. Even at this extreme scale, a non-trivial metamorphic test suite subset yields outcomes within 20 minutes (sufficient for continuous integration and review processes). Furthermore, our offline mode simulation reduces test flakiness from approximately 50% (of all online tests) to 0% (offline). Metamorphic testing has been widely-studied for 22 years. This paper is the first reported deployment into an industrial continuous integration system.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Software Engineering in Practice},
pages = {140–149},
numpages = {10}
}

@inproceedings{10.1145/1479992.1480017,
author = {Brown, J. R. and Hoffman, R. H.},
title = {Evaluating the Effectiveness of Software Verification: Pratical Experience with an Automated Tool},
year = {1972},
isbn = {9781450379120},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1479992.1480017},
doi = {10.1145/1479992.1480017},
abstract = {From the point of view of the user, a reliable computer program is one which performs satisfactorily according to the computer program's specifications. The ability to determine if a computer program does indeed satisfy its specifications is most often based upon accumulated experience in using the software. This is due in part to general agreement that the quality of computer software increases as the software is extensively used and failures are discovered and corrected. In keeping with this philosphy, increasing emphasis has been placed on exhaustive testing of computer programs as the principal means of assuring sufficient quality.},
booktitle = {Proceedings of the December 5-7, 1972, Fall Joint Computer Conference, Part I},
pages = {181–190},
numpages = {10},
location = {Anaheim, California},
series = {AFIPS '72 (Fall, part I)}
}

@article{10.1145/3527317,
author = {Liu, Jiawei and Wei, Yuxiang and Yang, Sen and Deng, Yinlin and Zhang, Lingming},
title = {Coverage-Guided Tensor Compiler Fuzzing with Joint IR-Pass Mutation},
year = {2022},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {OOPSLA1},
url = {https://doi.org/10.1145/3527317},
doi = {10.1145/3527317},
abstract = {In the past decade, Deep Learning (DL) systems have been widely deployed in various application domains to facilitate our daily life, e.g., natural language processing, healthcare, activity recognition, and autonomous driving. Meanwhile, it is extremely challenging to ensure the correctness of DL systems (e.g., due to their intrinsic nondeterminism), and bugs in DL systems can cause serious consequences and may even threaten human lives. In the literature, researchers have explored various techniques to test, analyze, and verify DL models, since their quality directly affects the corresponding system behaviors. Recently, researchers have also proposed novel techniques for testing the underlying operator-level DL libraries (such as TensorFlow and PyTorch), which provide general binary implementations for each high-level DL operator and are the foundation for running DL models on different hardware platforms. However, there is still limited work targeting the reliability of the emerging tensor compilers (also known as DL compilers), which aim to automatically compile high-level tensor computation graphs directly into high-performance binaries for better efficiency, portability, and scalability than traditional operator-level libraries. Therefore, in this paper, we target the important problem of tensor compiler testing, and have proposed Tzer, a practical fuzzing technique for the widely used TVM tensor compiler. Tzer focuses on mutating the low-level Intermediate Representation (IR) for TVM due to the limited mutation space for the high-level IR. More specifically, Tzer leverages both general-purpose and tensor-compiler-specific mutators guided by coverage feedback for diverse and evolutionary IR mutation; furthermore, since tensor compilers provide various passes (i.e., transformations) for IR optimization, Tzer also performs pass mutation in tandem with IR mutation for more effective fuzzing. Our experimental results show that Tzer substantially outperforms existing fuzzing techniques on tensor compiler testing, with 75% higher coverage and 50% more valuable tests than the 2nd-best technique. Also, different components of Tzer have been validated via ablation study. To date, Tzer has detected 49 previously unknown bugs for TVM, with 37 bugs confirmed and 25 bugs fixed (PR merged).},
journal = {Proc. ACM Program. Lang.},
month = {apr},
articleno = {73},
numpages = {26},
keywords = {Machine Learning Systems, Compiler Testing, Fuzzing}
}

@inproceedings{10.1145/1083246.1083256,
author = {Sant, Jessica and Souter, Amie and Greenwald, Lloyd},
title = {An Exploration of Statistical Models for Automated Test Case Generation},
year = {2005},
isbn = {1595931260},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083246.1083256},
doi = {10.1145/1083246.1083256},
abstract = {In this paper, we develop methods that use logged user data to build models of a web application. Logged user data captures dynamic behavior of an application that can be useful for addressing the challenging problems of testing web applications. Our approach automatically builds statistical models of user sessions and automatically derives test cases from these models. We provide several alternative modeling approaches based on statistical machine learning methods. We investigate the effectiveness of the test suites generated from our methods by performing a preliminary study that evaluates the generated test cases. The results of this study demonstrate that our techniques are able to generate test cases that achieve high coverage and accurately model user behavior. This study provides insights into improving our methods and motivates a larger study with a more diverse set of applications and testing metrics.},
booktitle = {Proceedings of the Third International Workshop on Dynamic Analysis},
pages = {1–7},
numpages = {7},
location = {St. Louis, Missouri},
series = {WODA '05}
}

@article{10.1145/1082983.1083256,
author = {Sant, Jessica and Souter, Amie and Greenwald, Lloyd},
title = {An Exploration of Statistical Models for Automated Test Case Generation},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1083256},
doi = {10.1145/1082983.1083256},
abstract = {In this paper, we develop methods that use logged user data to build models of a web application. Logged user data captures dynamic behavior of an application that can be useful for addressing the challenging problems of testing web applications. Our approach automatically builds statistical models of user sessions and automatically derives test cases from these models. We provide several alternative modeling approaches based on statistical machine learning methods. We investigate the effectiveness of the test suites generated from our methods by performing a preliminary study that evaluates the generated test cases. The results of this study demonstrate that our techniques are able to generate test cases that achieve high coverage and accurately model user behavior. This study provides insights into improving our methods and motivates a larger study with a more diverse set of applications and testing metrics.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–7},
numpages = {7}
}

@inproceedings{10.1145/1868044.1868045,
author = {Walkinshaw, Neil and Bogdanov, Kirill and Damas, Christophe and Lambeau, Bernard and Dupont, Pierre},
title = {A Framework for the Competitive Evaluation of Model Inference Techniques},
year = {2010},
isbn = {9781450301473},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868044.1868045},
doi = {10.1145/1868044.1868045},
abstract = {This paper describes the STAMINA competition1, which is designed to drive the evaluation and improvement of software model-inference approaches. To this end, the target models have certain characteristics that tend to appear in software-models; they have large alphabets, and states are not evenly connected by transitions (as has been the case in previous similar competitions). The paper describes the set-up of the competition that extends previous similar competitions in the field of regular grammar inference. However, this competition focusses on target models that are characteristic of software systems, and features a suitably adapted protocol for the generation of training and testing samples. Besides providing details of the competition itself, it also discusses how outcomes from the competition will be used to gain broader insights into the relative accuracy and efficiency of competing techniques.},
booktitle = {Proceedings of the First International Workshop on Model Inference In Testing},
pages = {1–9},
numpages = {9},
location = {Trento, Italy},
series = {MIIT '10}
}

@inproceedings{10.1145/503720.503748,
author = {Wood, Thomas and Miller, Keith and Noonan, Robert E.},
title = {Local Exhaustive Testing: A Software Reliability Tool},
year = {1992},
isbn = {0897915062},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/503720.503748},
doi = {10.1145/503720.503748},
abstract = {We introduce local exhaustive testing as a simple strategy for creating test cases that uncover faults (a deficiency in the code that is responsible for incorrect behavior) with a higher probability than tests chosen randomly. To use local exhaustive testing, we identify certain inputs points as "critical," and then test all inputs close to that point. We expect that this strategy will be particularly effective in applications that include an emphasis on geometric or other regular organization. We demonstrate the effectiveness of local exhaustive testing on a collection of programs that are all implementations of a single specification, the proportional navigation problem.},
booktitle = {Proceedings of the 30th Annual Southeast Regional Conference},
pages = {77–84},
numpages = {8},
location = {Raleigh, North Carolina},
series = {ACM-SE 30}
}

@inproceedings{10.1145/1390817.1390823,
author = {Ramler, Rudolf},
title = {The Impact of Product Development on the Lifecycle of Defects},
year = {2008},
isbn = {9781605580517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390817.1390823},
doi = {10.1145/1390817.1390823},
abstract = {This paper investigates the defects of a large embedded software development project over a period of about two years. It describes how software development and product development are organized in parallel branches. By mapping the defects reported on product development branches to the releases on the main line of software development, the paper shows the impact of the product development strategy on the defect lifecycle in software development.},
booktitle = {Proceedings of the 2008 Workshop on Defects in Large Software Systems},
pages = {21–25},
numpages = {5},
keywords = {product development, defect lifecycle},
location = {Seattle, Washington},
series = {DEFECTS '08}
}

@inbook{10.1109/ICSE-SEET52601.2021.00011,
author = {Chong, Chun Yong and Thongtanunam, Patanamon and Tantithamthavorn, Chakkrit},
title = {Assessing the Students' Understanding and Their Mistakes in Code Review Checklists: An Experience Report of 1,791 Code Review Checklist Questions from 394 Students},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00011},
abstract = {Code review is a widely-used practice in software development companies to identify defects. Hence, code review has been included in many software engineering curricula at universities worldwide. However, teaching code review is still a challenging task because the code review effectiveness depends on the code reading and analytical skills of a reviewer. While several studies have investigated the code reading techniques that students should use to find defects during code review, little has focused on a learning activity that involves analytical skills. Indeed, developing a code review checklist should stimulate students to develop their analytical skills to anticipate potential issues (i.e., software defects). Yet, it is unclear whether students can anticipate potential issues given their limited experience in software development (programming, testing, etc.). We perform a qualitative analysis to investigate whether students are capable of creating code review checklists, and if the checklists can be used to guide reviewers to find defects. In addition, we identify common mistakes that students make when developing a code review checklist. Our results show that while there are some misconceptions among students about the purpose of code review, students are able to anticipate potential defects and create a relatively good code review checklist. Hence, our results lead us to conclude that developing a code review checklist can be a part of the learning activities for code review in order to scaffold students' skills.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {20–29},
numpages = {10}
}

@inproceedings{10.1145/1464291.1464295,
author = {Evans, Thomas G. and Darley, D. Lucille},
title = {On-Line Debugging Techniques: A Survey},
year = {1966},
isbn = {9781450378932},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1464291.1464295},
doi = {10.1145/1464291.1464295},
abstract = {One consequence of recent interest in the development of large-scale time-sharing systems to provide on-line computer access to a large number of users has been the widespread realization that the usefulness of such a system is critically dependent on the quality of the software provided to facilitate the interaction between user and machine. In particular, one area of critical importance for effective utilization of such a system is that of facilities for program debugging. In view of the important role they play, surprisingly little attention has been paid to the development of facilities to aid in the process of on-line program debugging. Furthermore, much of the work in this field has been described only in unpublished reports or passed on through the oral tradition, rather than in the published literature. The purpose of this paper is to survey the existing work in this area and discuss some possible extensions to it, with the dual goal of acquainting a wider public with currently-existing techniques and of stimulating further developments.},
booktitle = {Proceedings of the November 7-10, 1966, Fall Joint Computer Conference},
pages = {37–50},
numpages = {14},
location = {San Francisco, California},
series = {AFIPS '66 (Fall)}
}

@article{10.1145/3433928,
author = {Vandehei, Bailey and Costa, Daniel Alencar Da and Falessi, Davide},
title = {Leveraging the Defects Life Cycle to Label Affected Versions and Defective Classes},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3433928},
doi = {10.1145/3433928},
abstract = {Two recent studies explicitly recommend labeling defective classes in releases using the affected versions (AV) available in issue trackers (e.g., Jira). This practice is coined as the realistic approach. However, no study has investigated whether it is feasible to rely on AVs. For example, how available and consistent is the AV information on existing issue trackers? Additionally, no study has attempted to retrieve AVs when they are unavailable. The aim of our study is threefold: (1) to measure the proportion of defects for which the realistic method is usable, (2) to propose a method for retrieving the AVs of a defect, thus making the realistic approach usable when AVs are unavailable, (3) to compare the accuracy of the proposed method versus three SZZ implementations. The assumption of our proposed method is that defects have a stable life cycle in terms of the proportion of the number of versions affected by the defects before discovering and fixing these defects. Results related to 212 open-source projects from the Apache ecosystem, featuring a total of about 125,000 defects, reveal that the realistic method cannot be used in the majority (51%) of defects. Therefore, it is important to develop automated methods to retrieve AVs. Results related to 76 open-source projects from the Apache ecosystem, featuring a total of about 6,250,000 classes, affected by 60,000 defects, and spread over 4,000 versions and 760,000 commits, reveal that the proportion of the number of versions between defect discovery and fix is pretty stable (standard deviation &lt;2)—across the defects of the same project. Moreover, the proposed method resulted significantly more accurate than all three SZZ implementations in (i) retrieving AVs, (ii) labeling classes as defective, and (iii) in developing defects repositories to perform feature selection. Thus, when the realistic method is unusable, the proposed method is a valid automated alternative to SZZ for retrieving the origin of a defect. Finally, given the low accuracy of SZZ, researchers should consider re-executing the studies that have used SZZ as an oracle and, in general, should prefer selecting projects with a high proportion of available and consistent AVs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {24},
numpages = {35},
keywords = {defect origin, Affected version, developing defects repository, SZZ}
}

@inproceedings{10.1145/322807.322838,
author = {Knight, John C.},
title = {On the Assessment of Ada Performance},
year = {1990},
isbn = {089791354X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/322807.322838},
doi = {10.1145/322807.322838},
booktitle = {Proceedings of the Working Group on Ada Performance Issues 1990},
pages = {1–6},
numpages = {6},
location = {Baltimore, Maryland, USA}
}

@article{10.1145/322837.322838,
author = {Knight, John C.},
title = {On the Assessment of Ada Performance},
year = {1990},
issue_date = {Winter 1990},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {X},
number = {3},
issn = {1094-3641},
url = {https://doi.org/10.1145/322837.322838},
doi = {10.1145/322837.322838},
journal = {Ada Lett.},
month = {jan},
pages = {1–6},
numpages = {6}
}

@inproceedings{10.1145/2591062.2591186,
author = {St\r{a}hl, Daniel and Bosch, Jan},
title = {Automated Software Integration Flows in Industry: A Multiple-Case Study},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591186},
doi = {10.1145/2591062.2591186},
abstract = { There is a steadily increasing interest in the agile practice of continuous integration. Consequently, there is great diversity in how it is interpreted and implemented, and a need to study, document and analyze how automated software integration flows are implemented in the industry today. In this paper we study five separate cases, using a descriptive model developed to address the variation points in continuous integration practice discovered in literature. Each case is discussed and evaluated individually, whereupon six guidelines for the design and implementation of automated software integration are presented. Furthermore, the descriptive model used to document the cases is evaluated and evolved. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {54–63},
numpages = {10},
keywords = {Agile software development, automation, methodologies, continuous integration, software integration},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@article{10.1145/3527851,
author = {Li, Xueliang and Chen, Junyang and Liu, Yepang and Wu, Kaishun and Gallagher, John P.},
title = {Combatting Energy Issues for Mobile Applications},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3527851},
doi = {10.1145/3527851},
abstract = {Energy efficiency is an important criterion to judge the quality of mobile apps, but one third of our arbitrarily sampled apps suffer from energy issues that can quickly drain battery power. To understand these issues, we conduct an empirical study on 36 well-maintained apps such as Chrome and Firefox, whose issue tracking systems are publicly accessible. Our study involves issue causes, manifestation, fixing efforts, detection techniques, reasons of no-fixes and debugging techniques. Inspired by the empirical study, we propose a novel testing framework for detecting energy issues in real-world mobile apps. Our framework examines apps with well-designed input sequences and runtime context. We develop leading edge technologies, e.g. pre-designing input sequences with potential energy overuse and tuning tests on-the-fly, to achieve high efficacy in detecting energy issues. A large-scale evaluation shows that 90.4% of the detected issues in our experiments were previously unknown to developers. On average, these issues can double the energy consumption of the test cases where the issues were detected. And our test achieves a low number of false positives. Finally, we show how our test reports can help developers fix the issues.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
keywords = {Android, Energy Bugs, Energy Issues, Mobile Applications}
}

@inproceedings{10.1145/120807.120825,
author = {Marick, Brian},
title = {The Weak Mutation Hypothesis},
year = {1991},
isbn = {089791449X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/120807.120825},
doi = {10.1145/120807.120825},
booktitle = {Proceedings of the Symposium on Testing, Analysis, and Verification},
pages = {190–199},
numpages = {10},
location = {Victoria, British Columbia, Canada},
series = {TAV4}
}

@article{10.1109/TNET.2019.2927110,
author = {Zhou, Yu and Bi, Jun and Zhang, Cheng and Liu, Bingyang and Li, Zhaogeng and Wang, Yangyang and Yu, Mingli},
title = {P4DB: On-the-Fly Debugging for Programmable Data Planes},
year = {2019},
issue_date = {August 2019},
publisher = {IEEE Press},
volume = {27},
number = {4},
issn = {1063-6692},
url = {https://doi.org/10.1109/TNET.2019.2927110},
doi = {10.1109/TNET.2019.2927110},
abstract = {While extending network programmability to a more considerable extent, P4 raises the difficulty of detecting and locating bugs, e.g., P4 program bugs and missed table rules, in runtime. These runtime bugs, without prompt disposal, can ruin the functionality and performance of networks. Unfortunately, the absence of efficient debugging tools makes runtime bug troubleshooting intricate for operators. This paper is devoted to on-the-fly debugging of runtime bugs for programmable data planes. We propose P4DB, a general debugging platform that empowers operators to debug P4 programs in three levels of visibility with rich primitives. By P4DB, operators can use the watch primitive to quickly narrow the debugging scope from the network level or the device level to the table level, then use the break and next primitives to decompose match-action tables and finely locate bugs. We implement a prototype of P4DB and evaluate the prototype on two widely-used P4 targets. On the software target, P4DB merely introduces a small throughput penalty 1.3% to 13.8% and a little delay increase 0.6% to 11.9%. Notably, P4DB almost introduces no performance overhead on Tofino, the hardware P4 target.},
journal = {IEEE/ACM Trans. Netw.},
month = {aug},
pages = {1714–1727},
numpages = {14}
}

@article{10.1145/227607.227611,
author = {Bergadano, Francesco and Gunetti, Daniele},
title = {Testing by Means of Inductive Program Learning},
year = {1996},
issue_date = {April 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/227607.227611},
doi = {10.1145/227607.227611},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
pages = {119–145},
numpages = {27},
keywords = {program induction by examples}
}

@inproceedings{10.1145/3236024.3236077,
author = {Chen, Dongjie and Jiang, Yanyan and Xu, Chang and Ma, Xiaoxing and Lu, Jian},
title = {Testing Multithreaded Programs via Thread Speed Control},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236077},
doi = {10.1145/3236024.3236077},
abstract = {A multithreaded program's interleaving space is discrete and astronomically large, making effectively sampling thread schedules for manifesting concurrency bugs a challenging task. Observing that concurrency bugs can be manifested by adjusting thread relative speeds, this paper presents the new concept of speed space in which each vector denotes a family of thread schedules. A multithreaded program's speed space is approximately continuous, easy-to-sample, and preserves certain categories of concurrency bugs. We discuss the design, implementation, and evaluation of our speed-controlled scheduler for exploring adversarial/abnormal schedules. The experimental results confirm that our technique is effective in sampling diverse schedules. Our implementation also found previously unknown concurrency bugs in real-world multithreaded programs.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {15–25},
numpages = {11},
keywords = {testing, Concurrency, race detection, interleaving space sampling},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3377816.3381734,
author = {Byun, Taejoon and Rayadurgam, Sanjai},
title = {Manifold for Machine Learning Assurance},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381734},
doi = {10.1145/3377816.3381734},
abstract = {The increasing use of machine-learning (ML) enabled systems in critical tasks fuels the quest for novel verification and validation techniques yet grounded in accepted system assurance principles. In traditional system development, model-based techniques have been widely adopted, where the central premise is that abstract models of the required system provide a sound basis for judging its implementation. We posit an analogous approach for ML systems using an ML technique that extracts from the high-dimensional training data implicitly describing the required system, a low-dimensional underlying structure---a manifold. It is then harnessed for a range of quality assurance tasks such as test adequacy measurement, test input generation, and runtime monitoring of the target ML system. The approach is built on variational autoencoder, an unsupervised method for learning a pair of mutually near-inverse functions between a given high-dimensional dataset and a low-dimensional representation. Preliminary experiments establish that the proposed manifold-based approach, for test adequacy drives diversity in test data, for test generation yields fault-revealing yet realistic test cases, and for run-time monitoring provides an independent means to assess trustability of the target system's output.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {97–100},
numpages = {4},
keywords = {machine learning testing, variational autoencoder, neural networks},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.1007/11557432_32,
author = {Vieira, Marlon},
title = {Invited Presentation II: Experiences in Applying Model Based System Testing Generation},
year = {2005},
isbn = {3540290109},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11557432_32},
doi = {10.1007/11557432_32},
abstract = {The goal of this presentation is to illustrate the benefits of using an automated, model-based approach for improving system test design and generation. Our approach, TDE/UML, automatically generates system tests from behavioral models of an application using the Unified Modeling Language (UML.). TDE/UML builds on and combines existing techniques for data coverage and graph coverage. We focus here on the results of applying TDE/UML in diverse Siemens projects: its cost benefits and its fault detection capabilities.},
booktitle = {Proceedings of the 8th International Conference on Model Driven Engineering Languages and Systems},
pages = {430},
numpages = {1},
location = {Montego Bay, Jamaica},
series = {MoDELS'05}
}

@article{10.1145/2697399,
author = {Binder, Robert V. and Legeard, Bruno and Kramer, Anne},
title = {Model-Based Testing: Where Does It Stand?},
year = {2015},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2697399},
doi = {10.1145/2697399},
abstract = {MBT has positive effects on efficiency and effectiveness, even if it only partially fulfills high expectations.},
journal = {Commun. ACM},
month = {jan},
pages = {52–56},
numpages = {5}
}

