@inproceedings{10.1145/2491411.2491439,
author = {Wu, Jingyue and Hu, Gang and Tang, Yang and Yang, Junfeng},
title = {Effective Dynamic Detection of Alias Analysis Errors},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491439},
doi = {10.1145/2491411.2491439},
abstract = { Alias analysis is perhaps one of the most crucial and widely used analyses, and has attracted tremendous research efforts over the years. Yet, advanced alias analyses are extremely difficult to get right, and the bugs in these analyses are one key reason that they have not been adopted to production compilers. This paper presents NeonGoby, a system for effectively detecting errors in alias analysis implementations, improving their correctness and hopefully widening their adoption. NeonGoby detects the worst type of bugs where the alias analysis claims that two pointers never alias, but they actually alias at runtime. NeonGoby works by dynamically observing pointer addresses during the execution of a test program and then checking these addresses against an alias analysis for errors. It is explicitly designed to (1) be agnostic to the alias analysis it checks for maximum applicability and ease of use and (2) detect alias analysis errors that manifest on real-world programs and workloads. It emits no false positives as long as test programs do not have undefined behavior per ANSI C specification or call external functions that interfere with our detection algorithm. It reduces performance overhead using a practical selection of techniques. Evaluation on three popular alias analyses and real-world programs Apache and MySQL shows that NeonGoby effectively finds 29 alias analysis bugs with zero false positives and reasonable overhead; the most serious four bugs have been patched by the developers. To enable alias analysis builders to start using NeonGoby today, we have released it open-source at https://github.com/columbia/neongoby, along with our error detection results and proposed patches. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {279–289},
numpages = {11},
keywords = {Error Detection, Dynamic Analysis, Alias Analysis},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/1882362.1882382,
author = {Dwyer, Matthew B. and Elbaum, Sebastian},
title = {Unifying Verification and Validation Techniques: Relating Behavior and Properties through Partial Evidence},
year = {2010},
isbn = {9781450304276},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882362.1882382},
doi = {10.1145/1882362.1882382},
abstract = {The past decade has produced a range of techniques for assessing the correctness of software systems. These techniques, such as various forms of static analysis, automated verification, and test generation, are capable of producing a variety of forms of evidence showing that the software behavior meets its specified properties. We contend that, as currently formulated, existing techniques fail to externalize all of the useful pieces of evidence that they compute which limits the opportunities to obtain a comprehensive and accurate assessment of property-behavior conformance. Explicitly accounting for the ways that V&amp;V techniques produce partial evidence offers the potential to look beyond the boundaries of individual analysis, verification, and testing techniques to consider the larger question of how the techniques fit together to provide an explicit body of evidence about software system quality.},
booktitle = {Proceedings of the FSE/SDP Workshop on Future of Software Engineering Research},
pages = {93–98},
numpages = {6},
keywords = {evidence combination, correctness argument},
location = {Santa Fe, New Mexico, USA},
series = {FoSER '10}
}

@inproceedings{10.1145/1006147.1006187,
author = {Cargill, Thomas A.},
title = {The Blit Debugger: Preliminary Draft},
year = {1983},
isbn = {0897911113},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1006147.1006187},
doi = {10.1145/1006147.1006187},
abstract = {This paper describes the evolution to date of a flexible debugger for C programs on the Blit, a multi-processing bitmap terminal. The debugger is of interest for the following reasons:-- it is assisted by the terminal software's elegant separation of the debugger process from its subject process.-- it resides autonomously in the terminal and is bound dynamically to arbitrary subject processes.-- it executes asynchronously with its subject.-- its implementation is distributed as a small process in the terminal and a large process in the host timesharing system.-- its user interface uses graphics and a mouse.An opinion about the most fruitful direction for further application of graphics is offered.},
booktitle = {Proceedings of the Symposium on High-Level Debugging},
pages = {190–200},
numpages = {11},
location = {Pacific Grove, California},
series = {SIGSOFT '83}
}

@article{10.1145/1006140.1006187,
author = {Cargill, Thomas A.},
title = {The Blit Debugger: Preliminary Draft},
year = {1983},
issue_date = {August 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {8},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1006140.1006187},
doi = {10.1145/1006140.1006187},
abstract = {This paper describes the evolution to date of a flexible debugger for C programs on the Blit, a multi-processing bitmap terminal. The debugger is of interest for the following reasons:-- it is assisted by the terminal software's elegant separation of the debugger process from its subject process.-- it resides autonomously in the terminal and is bound dynamically to arbitrary subject processes.-- it executes asynchronously with its subject.-- its implementation is distributed as a small process in the terminal and a large process in the host timesharing system.-- its user interface uses graphics and a mouse.An opinion about the most fruitful direction for further application of graphics is offered.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {190–200},
numpages = {11}
}

@article{10.1145/1006142.1006187,
author = {Cargill, Thomas A.},
title = {The Blit Debugger: Preliminary Draft},
year = {1983},
issue_date = {August 1983},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {18},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/1006142.1006187},
doi = {10.1145/1006142.1006187},
abstract = {This paper describes the evolution to date of a flexible debugger for C programs on the Blit, a multi-processing bitmap terminal. The debugger is of interest for the following reasons:-- it is assisted by the terminal software's elegant separation of the debugger process from its subject process.-- it resides autonomously in the terminal and is bound dynamically to arbitrary subject processes.-- it executes asynchronously with its subject.-- its implementation is distributed as a small process in the terminal and a large process in the host timesharing system.-- its user interface uses graphics and a mouse.An opinion about the most fruitful direction for further application of graphics is offered.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {190–200},
numpages = {11}
}

@inproceedings{10.1145/1985394.1985397,
author = {Sengupta, Bikram and Roychoudhury, Abhik},
title = {Engineering Multi-Tenant Software-as-a-Service Systems},
year = {2011},
isbn = {9781450305914},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985394.1985397},
doi = {10.1145/1985394.1985397},
abstract = {Increasingly, Software-as-a-Service (SaaS) is becoming a dominant mechanism for the consumption of software by end users. From a vendor's perspective, the benefits of SaaS arise from leveraging economies of scale, by serving a large number of customers ("tenants") through a shared instance of a centrally hosted software service. Consequently, a SaaS provider would, in general, try to drive commonality amongst the requirements of different tenants, and at best, offer a fixed set of customization options. However, many tenants would also come with custom requirements, which may be a pre-requisite for them to adopt the SaaS system. These requirements should then be addressed by evolving the SaaS system in a controlled manner, while still supporting the needs of existing tenants. This need to balance tenant variability and commonality, and to optimize on development and testing effort, can make the evolution of multitenant SaaS systems an interesting engineering challenge; this has strong economic undertones as well, given the "pay-per-use" subscription model of SaaS, and the cost of incremental development and maintenance to cater to new tenant needs. In this paper, we outline a set of research issues in the design, testing and maintenance of multi-tenant SaaS systems, and highlight some of the interesting optimization questions that arise in the process. Presenting specific technical solutions is beyond the scope of this paper - instead, our goal is to help shape a research agenda for multi-tenant SaaS that can provide stimulus for further investigation into this area by the software and service engineering research community.},
booktitle = {Proceedings of the 3rd International Workshop on Principles of Engineering Service-Oriented Systems},
pages = {15–21},
numpages = {7},
keywords = {software-as-a-service, testing, cloud computing, multi-tenancy, refinement, semantics},
location = {Waikiki, Honolulu, HI, USA},
series = {PESOS '11}
}

@inproceedings{10.1145/1363686.1363696,
author = {Kl\"{u}gl, Franziska},
title = {A Validation Methodology for Agent-Based Simulations},
year = {2008},
isbn = {9781595937537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1363686.1363696},
doi = {10.1145/1363686.1363696},
abstract = {Validity forms the basic prerequisite for every simulation model, therefore also for reasonable usage of the agent-based simulation paradigm. However, models based on the multi-agent system metaphor tend to need some particular approaches. In this paper, I propose a process for validating agent-based simulation models that combines face validation, sensitivity analysis, calibration and statistical validation.},
booktitle = {Proceedings of the 2008 ACM Symposium on Applied Computing},
pages = {39–43},
numpages = {5},
location = {Fortaleza, Ceara, Brazil},
series = {SAC '08}
}

@inproceedings{10.1145/2000417.2000419,
author = {Tang, Lingjia and Mars, Jason and Soffa, Mary Lou},
title = {Contentiousness vs. Sensitivity: Improving Contention Aware Runtime Systems on Multicore Architectures},
year = {2011},
isbn = {9781450307086},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000417.2000419},
doi = {10.1145/2000417.2000419},
abstract = {Runtime systems to mitigate memory resource contention problems on multicore processors have recently attracted much research attention. One critical component of these runtimes is the indicators to rank and classify applications based on their contention characteristics. However, although there has been significant research effort, application contention characteristics remain not well understood and indicators have not been thoroughly evaluated.In this paper we performed a thorough study of applications' contention characteristics to develop better indicators to improve contention-aware runtime systems. The contention characteristics are composed of an application's contentiousness, and its sensitivity to contention. We show that contentiousness and sensitivity are not strongly correlated, and contrary to prior work, a single indicator is not adequate to predict both. Also, while prior work argues that last level cache miss rate is one of the best indicators to predict an application's contention characteristics, we show that depending on the workloads, it can often be misleading. We then present prediction models that consider contention in various memory resources. Our regression analysis establishes an accurate model to predict application contentiousness. The analysis also demonstrates that performance counters alone may not be sufficient to accurately predict application sensitivity to contention. Our evaluation using SPEC CPU2006 benchmarks shows that when predicting an application's contentiousness, the linear correlation coefficient R2 of our predictor and the real measured contentiousness is 0.834, as opposed to 0.224 when using last level cache miss rate.},
booktitle = {Proceedings of the 1st International Workshop on Adaptive Self-Tuning Computing Systems for the Exaflop Era},
pages = {12–21},
numpages = {10},
keywords = {contention aware runtimes, memory subsystems, scheduling, contentiousness vs sensitivity, multicore processors},
location = {San Jose, California, USA},
series = {EXADAPT '11}
}

@inproceedings{10.1145/774833.774851,
author = {Ruthruff, J. and Creswick, E. and Burnett, M. and Cook, C. and Prabhakararao, S. and Fisher, M. and Main, M.},
title = {End-User Software Visualizations for Fault Localization},
year = {2003},
isbn = {1581136420},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/774833.774851},
doi = {10.1145/774833.774851},
abstract = {End-user programming has become the most common form of programming today. However, despite this growth, there has been little investigation into bringing the benefits of software visualization to end-user programmers. Evidence from the spreadsheet paradigm, probably the most widely used end-user environment, reveals that end users' programs often contain faults. We would like to integrate software visualization into these end-user environments to help end users deal with the reliability issues in their programs. Towards this end, we have devised several fault localization visualization techniques for spreadsheets. This paper describes these techniques and reports the results of a formative study---using tests created by end users---to investigate how these fault localization techniques compare. Our results reveal some strengths and weaknesses of each technique, and provide insights into the cost-effectiveness of each technique for the interactive world of end-user spreadsheet development.},
booktitle = {Proceedings of the 2003 ACM Symposium on Software Visualization},
pages = {123–132},
numpages = {10},
keywords = {end-user programming, end-user software visualization, end-user software engineering, fault localization, spreadsheets},
location = {San Diego, California},
series = {SoftVis '03}
}

@article{10.1145/1598732.1598736,
author = {Kumar, Avadhesh and Grover, P. S. and Kumar, Rajesh},
title = {A Quantitative Evaluation of Aspect-Oriented Software Quality Model (AOSQUAMO)},
year = {2009},
issue_date = {September 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1598732.1598736},
doi = {10.1145/1598732.1598736},
abstract = {Aspect-Oriented (AO) technology is a new paradigm and emerging field of software development. Aspect-Oriented Programming (AOP) cleanly encapsulates crosscutting concerns which cannot be encapsulated or modularized by traditional programming approaches like Module-Oriented (MO) and Object-Oriented (OO). In order to evaluate quality of software systems, researchers and practitioners have proposed their software quality characteristics and models. As AO is a new abstraction, there is no dedicated software quality model, which can describe and include new features of AO technology. In this paper, a new Aspect-Oriented Software Quality Model (AOSQUAMO) has been proposed. Analytic Hierarchy Process (AHP) is used to evaluate quality of AO software systems as a single parameter. This proposed quality model further may be used to compare AO software systems which will help consumer to choose better quality software.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {oct},
pages = {1–9},
numpages = {9},
keywords = {aspect-oriented technology, software quality, analytic hierarchy process}
}

@inproceedings{10.1145/3196398.3196418,
author = {Pascarella, Luca and Palomba, Fabio and Di Penta, Massimiliano and Bacchelli, Alberto},
title = {How is Video Game Development Different from Software Development in Open Source?},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196418},
doi = {10.1145/3196398.3196418},
abstract = {Recent research has provided evidence that, in the industrial context, developing video games diverges from developing software systems in other domains, such as office suites and system utilities.In this paper, we consider video game development in the open source system (OSS) context. Specifically, we investigate how developers contribute to video games vs. non-games by working on different kinds of artifacts, how they handle malfunctions, and how they perceive the development process of their projects. To this purpose, we conducted a mixed, qualitative and quantitative study on a broad suite of 60 OSS projects. Our results confirm the existence of significant differences between game and non-game development, in terms of how project resources are organized and in the diversity of developers' specializations. Moreover, game developers responding to our survey perceive more difficulties than other developers when reusing code as well as performing automated testing, and they lack a clear overview of their system's requirements.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {392–402},
numpages = {11},
keywords = {video games, empirical studies, mining software repositories},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/3503222.3507729,
author = {Oleksenko, Oleksii and Fetzer, Christof and K\"{o}pf, Boris and Silberstein, Mark},
title = {Revizor: Testing Black-Box CPUs against Speculation Contracts},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507729},
doi = {10.1145/3503222.3507729},
abstract = {Speculative vulnerabilities such as Spectre and Meltdown expose speculative execution state that can be exploited to leak information across security domains via side-channels. Such vulnerabilities often stay undetected for a long time as we lack the tools for systematic testing of CPUs to find them.  In this paper, we propose an approach to automatically detect microarchitectural information leakage in commercial black-box CPUs. We build on speculation contracts, which we employ to specify the permitted side effects of program execution on the CPU's microarchitectural state. We propose a Model-based Relational Testing (MRT) technique to empirically assess the CPU compliance with these specifications.  We implement MRT in a testing framework called Revizor, and showcase its effectiveness on real Intel x86 CPUs. Revizor automatically detects violations of a rich set of contracts, or indicates their absence. A highlight of our findings is that Revizor managed to automatically surface Spectre, MDS, and LVI, as well as several previously unknown variants.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {226–239},
numpages = {14},
keywords = {spectre, microarchitecture, mds, testing, contracts, information flow},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{10.1145/2677832.2677833,
author = {Wang, Qianxiang and Li, Xuan},
title = {Bug Localization via Searching Crowd-Contributed Code},
year = {2014},
isbn = {9781450333030},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2677832.2677833},
doi = {10.1145/2677832.2677833},
abstract = { Bug localization, i.e., locating bugs in code snippets, is a frequent task in software development. Although static bug-finding tools are available to reduce manual effort in bug localization, these tools typically detect bugs with known project-independent bug patterns. However, many bugs in real-world code snippets are project-specific. To address this issue, in this paper, we propose a novel approach for LOcating Bugs By Searching the most similar sample snippet (LOBBYS). LOBBYS detects bugs with the help of crowd-contributed correct code, which implement the function that buggy code is expected to implement. Given a buggy code snippet, LOBBYS takes two steps to locate the bug: (1) normalize the bug-gy snippet, and then search for the most similar sample snippet from the code base; (2) align the buggy code and sample code snip-pets, find the difference between the two code snippets, and generate a bug report based on the difference. To evaluate LOBBYS, we build one algorithm-oriented code base and select some buggy snippets from two real-world systems. The result shows that LOBBYS can effectively locate bugs for buggy snippets with high precision. Under the similarity of 50%, 70% and 90%, LOBBYS achieves bug-localization precision as 67%, 83%, and 92%. },
booktitle = {Proceedings of the 6th Asia-Pacific Symposium on Internetware on Internetware},
pages = {1–10},
numpages = {10},
keywords = {Bug Localization, Semantically Equivalent Alignment, Deep Normalization},
location = {Hong Kong, China},
series = {INTERNETWARE 2014}
}

@article{10.1145/3412376,
author = {Shariffdeen, Ridwan Salihin and Tan, Shin Hwei and Gao, Mingyuan and Roychoudhury, Abhik},
title = {Automated Patch Transplantation},
year = {2021},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3412376},
doi = {10.1145/3412376},
abstract = {Automated program repair is an emerging area that attempts to patch software errors and vulnerabilities. In this article, we formulate and study a problem related to automated repair, namely automated patch transplantation. A patch for an error in a donor program is automatically adapted and inserted into a “similar” target program. We observe that despite standard procedures for vulnerability disclosures and publishing of patches, many un-patched occurrences remain in the wild. One of the main reasons is the fact that various implementations of the same functionality may exist and, hence, published patches need to be modified and adapted. In this article, we therefore propose and implement a workflow for transplanting patches. Our approach centers on identifying patch insertion points, as well as namespaces translation across programs via symbolic execution. Experimental results to eliminate five classes of errors highlight our ability to fix recurring vulnerabilities across various programs through transplantation. We report that in 20 of 24 fixing tasks involving eight application subjects mostly involving file processing programs, we successfully transplanted the patch and validated the transplantation through differential testing. Since the publication of patches make an un-patched implementation more vulnerable, our proposed techniques should serve a long-standing need in practice.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {6},
numpages = {36},
keywords = {dynamic program analysis, Program repair, patch transplantation, code transplantation}
}

@inbook{10.1145/3173162.3177161,
author = {Liu, Haopeng and Wang, Xu and Li, Guangpu and Lu, Shan and Ye, Feng and Tian, Chen},
title = {FCatch: Automatically Detecting Time-of-Fault Bugs in Cloud Systems},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3177161},
abstract = {It is crucial for distributed systems to achieve high availability. Unfortunately, this is challenging given the common component failures (i.e., faults). Developers often cannot anticipate all the timing conditions and system states under which a fault might occur, and introduce time-of-fault (TOF) bugs that only manifest when a node crashes or a message drops at a special moment. Although challenging, detecting TOF bugs is fundamental to developing highly available distributed systems. Unlike previous work that relies on fault injection to expose TOF bugs, this paper carefully models TOF bugs as a new type of concurrency bugs, and develops FCatch to automatically predict TOF bugs by observing correct execution. Evaluation on representative cloud systems shows that FCatch is effective, accurately finding severe TOF bugs.},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {419–431},
numpages = {13}
}

@article{10.1145/3296957.3177161,
author = {Liu, Haopeng and Wang, Xu and Li, Guangpu and Lu, Shan and Ye, Feng and Tian, Chen},
title = {FCatch: Automatically Detecting Time-of-Fault Bugs in Cloud Systems},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3177161},
doi = {10.1145/3296957.3177161},
abstract = {It is crucial for distributed systems to achieve high availability. Unfortunately, this is challenging given the common component failures (i.e., faults). Developers often cannot anticipate all the timing conditions and system states under which a fault might occur, and introduce time-of-fault (TOF) bugs that only manifest when a node crashes or a message drops at a special moment. Although challenging, detecting TOF bugs is fundamental to developing highly available distributed systems. Unlike previous work that relies on fault injection to expose TOF bugs, this paper carefully models TOF bugs as a new type of concurrency bugs, and develops FCatch to automatically predict TOF bugs by observing correct execution. Evaluation on representative cloud systems shows that FCatch is effective, accurately finding severe TOF bugs.},
journal = {SIGPLAN Not.},
month = {mar},
pages = {419–431},
numpages = {13},
keywords = {cloud computing, fault tolerance, bug detection, timing bugs, distributed systems}
}

@article{10.1145/2699412,
author = {Sethumadhavan, Simha and Waksman, Adam and Suozzo, Matthew and Huang, Yipeng and Eum, Julianna},
title = {Trustworthy Hardware from Untrusted Components},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {58},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/2699412},
doi = {10.1145/2699412},
abstract = {This defense-in-depth approach uses static analysis and runtime mechanisms to detect and silence hardware backdoors.},
journal = {Commun. ACM},
month = {aug},
pages = {60–71},
numpages = {12}
}

@inproceedings{10.1145/2967973.2968597,
author = {Stulova, Nataliia and Morales, Jos\'{e} F. and Hermenegildo, Manuel V.},
title = {Reducing the Overhead of Assertion Run-Time Checks via Static Analysis},
year = {2016},
isbn = {9781450341486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2967973.2968597},
doi = {10.1145/2967973.2968597},
abstract = {In order to aid in the process of detecting incorrect program behaviors, a number of approaches have been proposed which include a combination of language-level constructs (such as procedure-level assertions/contracts, program-point assertions, gradual types, etc.) and associated tools (such as code analyzers and run-time verification frameworks). However, it is often the case that these constructs and tools are not used to their full extent in practice due to a number of limitations such as excessive run-time overhead and/or limited expressiveness. Verification frameworks that combine static and dynamic techniques offer the potential to bridge this gap. In this paper we explore the effectiveness of abstract interpretation in detecting parts of program specifications that can be statically simplified to true or false, as well as the impact of such analysis in reducing the cost of the run-time checks required for the remaining parts of these specifications. Starting with a semantics for programs with assertion checking, and for assertion simplification based on static analysis information, we propose and study a number of practical assertion checking modes, each of which represents a trade-off between code annotation depth, execution time slowdown, and program safety. We also propose techniques for taking advantage of the run-time checking semantics to improve the precision of the analysis. Finally, we study experimentally the performance of these techniques. Our experiments illustrate the benefits and costs of each of the assertion checking modes proposed as well as the benefit of analysis for these scenarios.},
booktitle = {Proceedings of the 18th International Symposium on Principles and Practice of Declarative Programming},
pages = {90–103},
numpages = {14},
keywords = {horn clauses, verification, logic programming, abstract interpretation, assertions, run-time checking},
location = {Edinburgh, United Kingdom},
series = {PPDP '16}
}

@inproceedings{10.1145/3407023.3407052,
author = {Kalysch, Anatoli and Deutel, Mark and M\"{u}ller, Tilo},
title = {Template-Based Android Inter Process Communication Fuzzing},
year = {2020},
isbn = {9781450388337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407023.3407052},
doi = {10.1145/3407023.3407052},
abstract = {Fuzzing is a test method in vulnerability assessments that calls the interfaces of a program in order to find bugs in its input processing. Automatically generated inputs, based on a set of templates and randomness, are sent to a program at a high rate, collecting crashes for later investigation. We apply fuzz testing to the inter process communication (IPC) on Android in order to find bugs in the mechanisms how Android apps communicate with each other. The sandboxing principle on Android usually ensures that apps can only communicate to other apps via programmatic interfaces. Unlike traditional operating systems, two Android apps running in the same user context are not able to access the data of each other (security) or quit the other app (safety).Our IPC fuzzer for Android detects the structure of data sent within Intents between apps by disassembling and analyzing an app's bytecode. It relies on multiple mutation engines for input generation and supports post-mortem analysis for a detailed insight into crashes. We tested 1488 popular apps from the Google Play-Store, enabling us to crash 450 apps with intents that could be sent from any unprivileged app on the same device, thus undermining the safety guarantees given by Android. We show that any installed app on a device could easily crash a series of other apps, effectively rendering them useless. Even worse, we discovered flaws in popular frameworks like Unity, the Google Services API, and the Adjust SDK. Comparing our implementation to previous research shows improvements in the depth and diversity of our detected crashes.},
booktitle = {Proceedings of the 15th International Conference on Availability, Reliability and Security},
articleno = {32},
numpages = {6},
keywords = {fuzzing, inter-process communication, Android security},
location = {Virtual Event, Ireland},
series = {ARES '20}
}

@proceedings{10.1145/2998181,
title = {CSCW '17: Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing},
year = {2017},
isbn = {9781450343350},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Welcome to CSCW 2017, the ACM 2017 Conference on Computer Supported Cooperative Work and Social Computing! We are excited to welcome the CSCW community back to Portland, Oregon, where the second CSCW conference was held in 1988. Both Portland and CSCW have matured a great deal during the intervening 29 years. We hope that you will find that Portland provides a stimulating environment for our conference.CSCW is the premier venue for presenting research in the design and use of technologies that affect groups, organizations, communities, and networks. Bringing together top researchers and practitioners from academia and industry, CSCW explores the technical, social, material, and theoretical challenges of designing technology to support collaborative work and life activities. CSCW welcomes a diverse range of topics and research methodologies. Studies often involve the development and application of novel technologies and/or ethnographic studies that inform design practice or theory. The mission of the conference is to share research that advances the state of human knowledge and improves both the design of systems and the ways they are used. The diversity of work in our conference program reflects the diversity of technology use in people's work, social, and civic lives as well as the geographic and cultural diversity of contributors.As many of you know, CSCW follows a rigorous "revise and resubmit" review process that uses peer review to improve submitted papers while maintaining a high-quality threshold for final acceptance. We also help prepare the next generation of reviewers with a mentorship program in which students review papers under the guidance of an experienced reviewer. This year we have the largest CSCW program ever. We had 530 submitted papers and 183 were accepted for presentation at the conference. The program also includes 4 papers published in ACM Transactions on Human- Computer Interaction (TOCHI). In addition, we will feature 14 workshops, 56 posters, 12 demos, and 3 panels.Lili Cheng of Microsoft Research will open the conference, speaking on "Conversational AI &amp; Lessons Learned." Our closing plenary will feature Jorge Cham, the creator of PhD Comics, who will talk about, "The Science Gap." We also welcome Paul Luff and Christian Heath from King's College as the recipients of this year's CSCW Lasting Impact award for their influential 1998 paper, "Mobility in Collaboration."},
location = {Portland, Oregon, USA}
}

@inproceedings{10.1145/3372782.3406276,
author = {Papamitsiou, Zacharoula and Giannakos, Michail and Simon, - and Luxton-Reilly, Andrew},
title = {Computing Education Research Landscape through an Analysis of Keywords},
year = {2020},
isbn = {9781450370929},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372782.3406276},
doi = {10.1145/3372782.3406276},
abstract = {Authors of academic papers are generally required to nominate several keywords that characterize the paper, but are rarely offered guidance on how to select those keywords. We analyzed the keywords in the past 15 years of selected computing education publications: the 1274 papers published in the proceedings of ICER and ITiCSE, including the ITiCSE working group reports. As well as the keywords assigned by the authors, we mined the abstracts of these papers to extract a separate list of keywords. Our work has two goals: to frame the thematic landscape of the field, using keywords that communicate the work conducted; and to detect differences between the human judgement and interpretation of keywords and the machine 'intelligence' on handling those keywords, with respect to the clusters of thematic topics identified in each case. The analysis shows that the field is dominated by learning approaches (e.g., active learning, collaborative learning), aspects of programming (e.g., debugging, misconceptions), computational thinking, feedback, and assessment, while other areas that have attracted attention include academic integrity (e.g., plagiarism) and diversity (e.g., female students, underrepresented groups). It was observed that the keywords chosen by authors are often too general to provide information about the paper (e.g., 'concerns', 'course', 'fun', 'justice'). We elaborate on the findings and begin a discussion on how authors can improve the communication of their research and make access to it more transparent.},
booktitle = {Proceedings of the 2020 ACM Conference on International Computing Education Research},
pages = {102–112},
numpages = {11},
keywords = {computing education, keywords, bibliometrics, dominant themes},
location = {Virtual Event, New Zealand},
series = {ICER '20}
}

@inproceedings{10.1145/3368235.3368838,
author = {Harsh, Piyush and Ribera Laszkowski, Juan Francisco and Edmonds, Andy and Quang Thanh, Tran and Pauls, Michael and Vlaskovski, Radoslav and Avila-Garc\'{\i}a, Orlando and Pages, Enric and Gort\'{a}zar Bellas, Francisco and Gallego Carrillo, Micael},
title = {Cloud Enablers For Testing Large-Scale Distributed Applications},
year = {2019},
isbn = {9781450370448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368235.3368838},
doi = {10.1145/3368235.3368838},
abstract = {Testing large-scale distributed systems (also known as testing in the large) is a challenge that spreads across different technical domains and areas of expertise. Current methods and tools provide some minimal guarantees in relation to the correctness of their functional properties and have serious limitations when evaluating their extra-functional properties in realistic conditions, such as scalability, availability and performance efficiency. Cloud Testing and more specifically "testing in the cloud'' has arisen to tackle those challenges. In this new paradigm, cloud-based environment and infrastructure are used to run realistic end-to-end and/or system-level tests, collect test data and analyse them. In this paper we present a set of cloud-native services to take from the tester the responsibility of managing the resources and complementary services required to simulate realistic operational conditions and production environments. Specifically, they provide cloud testing capabilities such as logs and measurements collection from both testing jobs and system under test; test data analytics and visualization; provisioning and operation of additional services and processes to replicate realistic production ecosystems; support to scalability and diversity of underlying testing infrastructure; and replication of the operational conditions of the software under test through its instrumentation. We present the architecture of the cloud testing solution and the detailed design of each of the services; we also evaluate their relative contribution to satisfy different needs in the context of test execution.},
booktitle = {Proceedings of the 12th IEEE/ACM International Conference on Utility and Cloud Computing Companion},
pages = {35–42},
numpages = {8},
keywords = {large-scale distributed systems, scalability, cloud testing, continuous testing, continuous integration, reliability, testing},
location = {Auckland, New Zealand},
series = {UCC '19 Companion}
}

@inproceedings{10.1145/3362789.3362921,
author = {Winter, Alexander and Pedro, Erendiro and undefinedlasko, Julia and Battaglini, Julien and Faelker, M\"{a}ike and Kivipelto, Ronald and Duarte, Abel J. and Malheiro, Benedita and Ribeiro, Cristina and Justo, Jorge and Silva, Manuel F. and Ferreira, Paulo and Guedes, Pedro},
title = {Waste to Fungi: An EPS@ISEP 2019 Project},
year = {2019},
isbn = {9781450371919},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3362789.3362921},
doi = {10.1145/3362789.3362921},
abstract = {This paper describes the journey of a multinational and multidisciplinary team enrolled in the European Project Semester (EPS) at the Instituto Superior de Engenharia do Porto (ISEP) during the spring semester of 2019. The team embraced the idea of repurposing coffee leftovers to cultivate oyster mushrooms and benefited from the background diversity of the team members as well as from newly acquired marketing, sustainability and design ethics skills to consolidate and strengthen the overall feasibility of the project. The project was set to design, develop and test grey oyster mushroom growth kits with an automated monitoring system, using coffee grounds as growing substrate and complying with the applicable regulations and pre-defined requirements. The ulterior aims of the project were to reconnect people with the food they eat and to disseminate sustainable food production processes, which are not only healthy but environmentally friendly. To achieve these goals, the team developed a circular economy business model where grey oyster mushroom growth kits reuse coffee grounds as growing beds and food buckets as containers. The designed growth kits include a controlled fruiting chamber with an integrated monitoring system. This allows easy domestic cultivation, monitoring through a smart phone. Moreover, the proposed solution contemplates information sharing on the mushroom cultivation process, monitoring system and recipes as well as the maintenance of a dedicated discussion forum. Tests have been conducted to test the concept, cultivation process, monitoring system and fruiting chamber from the incubation of mycelium all the way to the harvesting. Results show the feasibility of creating a business based on the devised concept.},
booktitle = {Proceedings of the Seventh International Conference on Technological Ecosystems for Enhancing Multiculturality},
pages = {115–122},
numpages = {8},
keywords = {Circular Economy, Education for Sustainability, Project-based Learning, Engineering Capstone Project, European Project Semester},
location = {Le\'{o}n, Spain},
series = {TEEM'19}
}

