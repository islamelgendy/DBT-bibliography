@inproceedings{10.1145/3412841.3442020,
author = {Hosseini, Seyedrebvar and Turhan, Burak},
title = {A Comparison of Similarity Based Instance Selection Methods for Cross Project Defect Prediction},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442020},
doi = {10.1145/3412841.3442020},
abstract = {Context: Previous studies have shown that training data instance selection based on nearest neighborhood (NN) information can lead to better performance in cross project defect prediction (CPDP) by reducing heterogeneity in training datasets. However, neighborhood calculation is computationally expensive and approximate methods such as Locality Sensitive Hashing (LSH) can be as effective as exact methods. Aim: We aim at comparing instance selection methods for CPDP, namely LSH, NN-filter, and Genetic Instance Selection (GIS). Method: We conduct experiments with five base learners, optimizing their hyper parameters, on 13 datasets from PROMISE repository in order to compare the performance of LSH with benchmark instance selection methods NN-Filter and GIS. Results: The statistical tests show six distinct groups for F-measure performance. The top two group contains only LSH and GIS benchmarks whereas the bottom two groups contain only NN-Filter variants. LSH and GIS favor recall more than precision. In fact, for precision performance only three significantly distinct groups are detected by the tests where the top group is comprised of NN-Filter variants only. Recall wise, 16 different groups are identified where the top three groups contain only LSH methods, four of the next six are GIS only and the bottom five contain only NN-Filter. Finally, NN-Filter benchmarks never outperform the LSH counterparts with the same base learner, tuned or non-tuned. Further, they never even belong to the same rank group, meaning that LSH is always significantly better than NN-Filter with the same learner and settings. Conclusions: The increase in performance and the decrease in computational overhead and runtime make LSH a promising approach. However, the performance of LSH is based on high recall and in environments where precision is considered more important NN-Filter should be considered.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1455–1464},
numpages = {10},
keywords = {approximate near neighbour, instance selection, locality sensitive hashing, cross project defect prediction, search based optimisation},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1145/2739480.2754696,
author = {Shamshiri, Sina and Rojas, Jos\'{e} Miguel and Fraser, Gordon and McMinn, Phil},
title = {Random or Genetic Algorithm Search for Object-Oriented Test Suite Generation?},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754696},
doi = {10.1145/2739480.2754696},
abstract = {Achieving high structural coverage is an important aim in software testing. Several search-based techniques have proved successful at automatically generating tests that achieve high coverage. However, despite the well- established arguments behind using evolutionary search algorithms (e.g., genetic algorithms) in preference to random search, it remains an open question whether the benefits can actually be observed in practice when generating unit test suites for object-oriented classes. In this paper, we report an empirical study on the effects of using a genetic algorithm (GA) to generate test suites over generating test suites incrementally with random search, by applying the EvoSuite unit test suite generator to 1,000 classes randomly selected from the SF110 corpus of open source projects. Surprisingly, the results show little difference between the coverage achieved by test suites generated with evolutionary search compared to those generated using random search. A detailed analysis reveals that the genetic algorithm covers more branches of the type where standard fitness functions provide guidance. In practice, however, we observed that the vast majority of branches in the analyzed projects provide no such guidance.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1367–1374},
numpages = {8},
keywords = {genetic algorithms, search based software engineering, object oriented unit testing, search based testing, automated test generation, random testing},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/3433210.3437528,
author = {Shen, Shiqi and Kolluri, Aashish and Dong, Zhen and Saxena, Prateek and Roychoudhury, Abhik},
title = {Localizing Vulnerabilities Statistically From One Exploit},
year = {2021},
isbn = {9781450382878},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3433210.3437528},
doi = {10.1145/3433210.3437528},
abstract = {Automatic vulnerability diagnosis can help security analysts identify and, therefore, quickly patch disclosed vulnerabilities. The vulnerability localization problem is to automatically find a program point at which the "root cause" of the bug can be fixed. This paper employs a statistical localization approach to analyze a given exploit. Our main technical contribution is a novel procedure to systematically construct a test-suite which enables high-fidelity localization. We build our techniques in a tool called VulnLoc which automatically pinpoints vulnerability locations, given just one exploit, with high accuracy. VulnLoc does not make any assumptions about the availability of source code, test suites, or specialized knowledge of the type of vulnerability. It identifies actionable locations in its Top-5 outputs, where a correct patch can be applied, for about 88% of 43 CVEs arising in large real-world applications we study. These include 6 different classes of security flaws. Our results highlight the under-explored power of statistical analyses, when combined with suitable test-generation techniques.},
booktitle = {Proceedings of the 2021 ACM Asia Conference on Computer and Communications Security},
pages = {537–549},
numpages = {13},
keywords = {vulnerability localization, directed fuzzing},
location = {Virtual Event, Hong Kong},
series = {ASIA CCS '21}
}

@inproceedings{10.1145/3361149.3361165,
author = {Dias, Jo\~{a}o Pedro and Ferreira, Hugo Sereno and Sousa, Tiago Boldt},
title = {Testing and Deployment Patterns for the Internet-of-Things},
year = {2019},
isbn = {9781450362061},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3361149.3361165},
doi = {10.1145/3361149.3361165},
abstract = {As with every software, Internet-of-Things (IoT) systems have their own life-cycle, from conception to construction, deployment, and operation. However, the testing requirements from these systems are slightly different due to their inherent coupling with hardware and human factors. Hence, the procedure of delivering new software versions in a continuous integration/delivery fashion must be adopted. Based on existent solutions (and inspired in other closely-related domains), we describe two common strategies that developers can use for testing IoT systems, (1) Testbed and (2) Simulation-based Testing, as well as one recurrent solution for its deployment (3) Middleman Update.},
booktitle = {Proceedings of the 24th European Conference on Pattern Languages of Programs},
articleno = {16},
numpages = {8},
keywords = {testing, deployment, internet-of-things, orchestration},
location = {Irsee, Germany},
series = {EuroPLop '19}
}

@inproceedings{10.1145/3371238.3371247,
author = {Naith, Qamar and Ciravegna, Fabio},
title = {The Key Considerations In Building A Crowd-Testing Platform For Software Developers},
year = {2019},
isbn = {9781450376402},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371238.3371247},
doi = {10.1145/3371238.3371247},
abstract = {External testing of mobile software on a larger number of mobile devices by several users is often needed to ensure quality. Currently, the evidence as to what extent developers accept large-scale crowd-testing is limited. This paper aims to (1) gauge developers' perspectives with respect to the participation of the public and anonymous crowd testers, with varied experiences; (2) gather the developers' needs that could reduce their concerns of dealing with the public crowd testers and increase the opportunity of using the crowd-testing platforms. An online exploratory survey, conducted to included 50 Android and iOS developers from different countries with diverse experiences. This paper revealed several findings including the information that must be provided by developers and crowd testers for achieving effective crowd-testing process; the factors that can ensure the reliability and accuracy of the results provided by the public crowd testers. The findings conclude that (90%) of developers are potentially willing to perform testing via the public crowd testers worldwide. This on condition that several fundamental features were available which enable them to perform more realistic tests without artificial environments on large numbers of devices. The results also demonstrated that a group of developers does not consider testing as a serious job that they have to pay for, which can affect the gig-economy and global market. We aim at helping the individual or small development teams who have limited resources to perform large-scale testing of their products.},
booktitle = {Proceedings of the 4th International Conference on Crowd Science and Engineering},
pages = {50–57},
numpages = {8},
keywords = {Public and Anonymous Crowd Testers, Gig-economy, Mobile App testing, Large-scale crowd-testing},
location = {Jinan, China},
series = {ICCSE'19}
}

@inproceedings{10.1145/3407023.3407065,
author = {Pucher, Michael and Kudera, Christian and Merzdovnik, Georg},
title = {AVRS: Emulating AVR Microcontrollers for Reverse Engineering and Security Testing},
year = {2020},
isbn = {9781450388337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3407023.3407065},
doi = {10.1145/3407023.3407065},
abstract = {Embedded systems and microcontrollers are becoming more and more popular as the Internet of Things continues to spread. However, while there is a wealth of different methods and tools for analyzing software and firmware for architectures that are common to standard hardware, such as x86 or Arm, other systems have not been scrutinized so closely. One of these widely used architectures are AVR 8-bit microcontrollers, which are also used in projects like the Arduino platform. This lack of tools makes it more difficult to analyze such systems and identify potential security vulnerabilities. To get the most out of modern reverse engineering and debugging techniques such as fuzzing or concolic execution, sophisticated and correct emulators are required for dynamic analysis.The presented work tries to close this gap by introducing AVRS, a lean AVR emulator prototype developed with the goal of reverse engineering. It was implemented to overcome limitations in existing emulators, such as completeness or execution speed, and to provide simple interfaces for interaction with existing program analysis and reverse engineering tools. We provide an analysis of AVRS in relation to existing emulators and show the improvements in speed and completeness. In addition, we have created a setup that leverages AVRS to use fuzz tests to automatically identify errors in AVR firmware. Our results indicate that AVRS is a valuable addition to the arsenal of analysis tools for embedded firmware and can be easily extended to allow the use of existing analysis tools in the domain of AVR microcontrollers.},
booktitle = {Proceedings of the 15th International Conference on Availability, Reliability and Security},
articleno = {23},
numpages = {10},
keywords = {reverse engineering, emulation, AVR, fuzzing, IoT, security analysis, embedded systems},
location = {Virtual Event, Ireland},
series = {ARES '20}
}

@inproceedings{10.1145/1276958.1277175,
author = {Lakhotia, Kiran and Harman, Mark and McMinn, Phil},
title = {A Multi-Objective Approach to Search-Based Test Data Generation},
year = {2007},
isbn = {9781595936974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1276958.1277175},
doi = {10.1145/1276958.1277175},
abstract = {There has been a considerable body of work on search-based test data generation for branch coverage. However, hitherto, there has been no work on multi-objective branch coverage. In many scenarios a single-objective formulation is unrealistic; testers will want to find test sets that meet several objectives simultaneously in order to maximize the value obtained from the inherently expensive process of running the test cases and examining the output they produce. This paper introduces multi-objective branch coverage.The paper presents results from a case study of the twin objectives of branch coverage and dynamic memory consumption for both real and synthetic programs. Several multi-objective evolutionary algorithms are applied. The results show that multi-objective evolutionary algorithms are suitable for this problem, and illustrates the way in which a Pareto optimal search can yield insights into the trade-offs between the two simultaneous objectives.},
booktitle = {Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation},
pages = {1098–1105},
numpages = {8},
keywords = {evolutionary testing, multi-objective genetic algorithms, automated test data generation},
location = {London, England},
series = {GECCO '07}
}

@article{10.1145/1290993.1290996,
author = {Huang, Gang},
title = {Post-Development Software Architecture},
year = {2007},
issue_date = {September 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1290993.1290996},
doi = {10.1145/1290993.1290996},
abstract = {Software architecture (SA) plays an important role in software development. Since the lifecycle stages post development become more and more important and face with many challenges similar to the development, it is a natural idea to introduce or extend SA into the stages post development. In this paper, we present our practices and experiences on applying software architecture into the deployment and maintenance of J2EE (Java 2 Platform Enterprise Edition) applications, including the tool and principles of SA-based J2EE deployment and SA-based J2EE online maintenance. It demonstrates that 1) SA can help to achieve a holistic, fine-grained and automated deployment of large-scale distributed systems by visualizing the structure of the system to be deployed; 2) SA can provide an understandable, operational and global view for online maintenance by organizing the fragmented and trivial management mechanisms; 3) Extending SA into the stages post development makes it possible that the whole lifecycle of a software system can be governed by SA with many benefits, e.g. consistency, traceability, responsiveness, etc.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–9},
numpages = {9},
keywords = {software maintenance, deployment, runtime software architecture, software architecture}
}

@article{10.1145/3133916,
author = {Li, Xia and Zhang, Lingming},
title = {Transforming Programs and Tests in Tandem for Fault Localization},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133916},
doi = {10.1145/3133916},
abstract = { Localizing failure-inducing code is essential for software debugging. Manual fault localization can be quite tedious, error-prone, and time-consuming. Therefore, a huge body of research e orts have been dedicated to automated fault localization. Spectrum-based fault localization, the most intensively studied fault localization approach based on test execution information, may have limited effectiveness, since a code element executed by a failed tests may not necessarily have impact on the test outcome and cause the test failure. To bridge the gap, mutation-based fault localization has been proposed to transform the programs under test to check the impact of each code element for better fault localization. However, there are limited studies on the effectiveness of mutation-based fault localization on sufficient number of real bugs. In this paper, we perform an extensive study to compare mutation-based fault localization techniques with various state-of-the-art spectrum-based fault localization techniques on 357 real bugs from the Defects4J benchmark suite. The study results firstly demonstrate the effectiveness of mutation-based fault localization, as well as revealing a number of guidelines for further improving mutation-based fault localization. Based on the learnt guidelines, we further transform test outputs/messages and test code to obtain various mutation information. Then, we propose TraPT, an automated Learning-to-Rank technique to fully explore the obtained mutation information for effective fault localization. The experimental results show that TraPT localizes 65.12% and 94.52% more bugs within Top-1 than state-of-the-art mutation and spectrum based techniques when using the default setting of LIBSVM. },
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {92},
numpages = {30},
keywords = {Fault localization, Code transformation, Mutation testing}
}

@article{10.1145/3020266,
author = {Su, Ting and Wu, Ke and Miao, Weikai and Pu, Geguang and He, Jifeng and Chen, Yuting and Su, Zhendong},
title = {A Survey on Data-Flow Testing},
year = {2017},
issue_date = {January 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3020266},
doi = {10.1145/3020266},
abstract = {Data-flow testing (DFT) is a family of testing strategies designed to verify the interactions between each program variable’s definition and its uses. Such a test objective of interest is referred to as a def-use pair. DFT selects test data with respect to various test adequacy criteria (i.e., data-flow coverage criteria) to exercise each pair. The original conception of DFT was introduced by Herman in 1976. Since then, a number of studies have been conducted, both theoretically and empirically, to analyze DFT’s complexity and effectiveness. In the past four decades, DFT has been continuously concerned, and various approaches from different aspects are proposed to pursue automatic and efficient data-flow testing. This survey presents a detailed overview of data-flow testing, including challenges and approaches in enforcing and automating it: (1) it introduces the data-flow analysis techniques that are used to identify def-use pairs; (2) it classifies and discusses techniques for data-flow-based test data generation, such as search-based testing, random testing, collateral-coverage-based testing, symbolic-execution-based testing, and model-checking-based testing; (3) it discusses techniques for tracking data-flow coverage; (4) it presents several DFT applications, including software fault localization, web security testing, and specification consistency checking; and (5) it summarizes recent advances and discusses future research directions toward more practical data-flow testing.},
journal = {ACM Comput. Surv.},
month = {mar},
articleno = {5},
numpages = {35},
keywords = {Data-flow testing, data-flow analysis, coverage criteria, test data generation, coverage tracking}
}

@article{10.1145/1507195.1507207,
author = {Medikonda, Ben Swarup and Panchumarthy, Seetha Ramaiah},
title = {A Framework for Software Safety in Safety-Critical Systems},
year = {2009},
issue_date = {March 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1507195.1507207},
doi = {10.1145/1507195.1507207},
abstract = {Software for safety-critical systems must deal with the hazards identified by safety analysis in order to make the system safe, risk-free, and fail-safe. Because human lives may be lost and tremendous economic costs may result if the software fails, the development of high-integrity software adopts practices that impose greater rigor on the software development processes. Software safety is a composite of many factors. Existing software quality models like McCall's and Boehm's and ISO 9126 are inadequate in addressing the software safety issues of real time safety-critical embedded systems. At present there does not exist any standard framework that comprehensively addresses the factors, criteria and metrics (FCM) approach of the quality models in respect of software safety. The safety of a software component must be considered within the context of both the overall system of which it is a component and the environment in which this system operates. It is not useful to investigate the safety of a software component in isolation. This paper proposes a new framework for software safety based on the McCall's software quality model that specifically identifies the criteria corresponding to software safety in safety critical applications. The criteria in the proposed software safety framework pertains to system hazard analysis, completeness of requirements, identification of software-related safety-critical requirements, safety-constraints based design, run-time issues management, and software safety-critical testing. This framework is then applied to a prototype safety-critical system viz. a software--based Railroad Crossing Control System (RCCS) to validate its utility.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–9},
numpages = {9},
keywords = {software safety, software quality, safety-critical system}
}

@inproceedings{10.1145/1368088.1368116,
author = {Yu, Yanbing and Jones, James A. and Harrold, Mary Jean},
title = {An Empirical Study of the Effects of Test-Suite Reduction on Fault Localization},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368116},
doi = {10.1145/1368088.1368116},
abstract = {Fault-localization techniques that utilize information about all test cases in a test suite have been presented. These techniques use various approaches to identify the likely faulty part(s) of a program, based on information about the execution of the program with the test suite. Researchers have begun to investigate the impact that the composition of the test suite has on the effectiveness of these fault-localization techniques. In this paper, we present the first experiment on one aspect of test-suite composition--test-suite reduction. Our experiment studies the impact of the test-suite reduction on the effectiveness of fault-localization techniques. In our experiment, we apply 10 test-suite reduction strategies to test suites for eight subject programs. We then measure the differences between the effectiveness of four existing fault-localization techniques on the unreduced and reduced test suites. We also measure the reduction in test-suite size of the 10 test-suite reduction strategies. Our experiment shows that fault-localization effectiveness varies depending on the test-suite reduction strategy used, and it demonstrates the trade-offs between test-suite reduction and fault-localization effectiveness.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {201–210},
numpages = {10},
keywords = {empirical study, test-suite reduction, fault localization},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/3449639.3459339,
author = {Vogl, Sebastian and Schweikl, Sebastian and Fraser, Gordon},
title = {Encoding the Certainty of Boolean Variables to Improve the Guidance for Search-Based Test Generation},
year = {2021},
isbn = {9781450383509},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449639.3459339},
doi = {10.1145/3449639.3459339},
abstract = {Search-based test generation commonly uses fitness functions based on branch distances, i.e., estimations of how close conditional statements in a program are to evaluating to true or to false. When conditional statements depend on Boolean variables or Boolean-valued methods, the branch distance metric is unable to provide any guidance to the search, causing challenging plateaus in the fitness landscape. A commonly proposed solution is to apply testability transformations, which transform the program in a way that avoids conditional statements from depending on Boolean values. In this paper we introduce the concept of Certainty Booleans, which encode how certain a true or false Boolean value is. Using these Certainty Booleans, a basic testability transformation allows to restore gradients in the fitness landscape for Boolean branches, even when Boolean values are the result of complex interprocedural calculations. Evaluation on a set of complex Java classes and the EvoSuite test generator shows that this testability transformation substantially alters the fitness landscape for Boolean branches, and the altered fitness landscape leads to performance improvements. However, Boolean branches turn out to be much rarer than anticipated, such that the overall effects on code coverage are minimal.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference},
pages = {1088–1096},
numpages = {9},
location = {Lille, France},
series = {GECCO '21}
}

@inbook{10.1145/3238147.3238222,
author = {Dou, Wensheng and Han, Shi and Xu, Liang and Zhang, Dongmei and Wei, Jun},
title = {Expandable Group Identification in Spreadsheets},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238222},
abstract = {Spreadsheets are widely used in various business tasks. Spreadsheet users may put similar data and computations by repeating a block of cells (a unit) in their spreadsheets. We name the unit and all its expanding ones as an expandable group. All units in an expandable group share the same or similar formats and semantics. As a data storage and management tool, expandable groups represent the fundamental structure in spreadsheets. However, existing spreadsheet systems do not recognize any expandable groups. Therefore, other spreadsheet analysis tools, e.g., data integration and fault detection, cannot utilize this structure of expandable groups to perform precise analysis. In this paper, we propose ExpCheck to automatically extract expandable groups in spreadsheets. We observe that continuous units that share the similar formats and semantics are likely to be an expandable group. Inspired by this, we inspect the format of each cell and its corresponding semantics, and further classify them into expandable groups according to their similarity. We evaluate ExpCheck on 120 spreadsheets randomly sampled from the EUSES and VEnron corpora. The experimental results show that ExpCheck is effective. ExpCheck successfully detect expandable groups with F1-measure of 73.1%, significantly outperforming the state-of-the-art techniques (F1-measure of 13.3%).},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {498–508},
numpages = {11}
}

@inproceedings{10.1145/1595696.1595706,
author = {Sherman, Elena and Dwyer, Matthew B. and Elbaum, Sebastian},
title = {Saturation-Based Testing of Concurrent Programs},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595706},
doi = {10.1145/1595696.1595706},
abstract = {Coverage measures help to determine whether a test suite exercises a program adequately according to a testing criterion. Many existing measures, however, are defined over coverage domains that cannot be precisely calculated, rendering them of limited value in assessing the extent of testing activities. To exploit the use of such measures, we formalize saturation-based test adequacy, a form of adequacy focused on the rate at which coverage increases during test suite execution. We define a family of coverage metrics for concurrent program testing that are well-suited to saturation-based adequacy and present a study that explores their cost and effectiveness. The results of this study suggest that saturation-based testing can serve as an effective complement to traditional notions of coverage-based testing.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {53–62},
numpages = {10},
keywords = {test adequacy criteria, coverage, concurrent programs},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/2723742.2723756,
author = {Pati, Jayadeep and Shukla, K. K.},
title = {A Hybrid Technique for Software Reliability Prediction},
year = {2015},
isbn = {9781450334327},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2723742.2723756},
doi = {10.1145/2723742.2723756},
abstract = {Reliability is an important factor of software quality. The accurate prediction of software reliability is a challenging task. There exist many reliability models to predict the reliability based on software testing activities. There are many software reliability growth models (SRGMs) developed to predict the reliability but they have many unrealistic assumptions and they are also environment dependent. The accuracy of the models is also questionable. In this paper we have used a time series approach for software reliability prediction. We have used an ensemble technique called hybrid ARIMA (ARIMA + NN) for prediction of software reliability based on real life data on software failures. This paper also gives a comparative analysis of forecasting performance of hybrid ARIMA, and ARIMA models. Empirical results indicate that a hybrid ARIMA model can improve the prediction accuracy.},
booktitle = {Proceedings of the 8th India Software Engineering Conference},
pages = {139–146},
numpages = {8},
keywords = {Reliability Prediction, Software Reliability Growth Models, ARIMA, Hybrid ARIMA, ANN},
location = {Bangalore, India},
series = {ISEC '15}
}

@inproceedings{10.1145/3377811.3380338,
author = {Liu, Kui and Wang, Shangwen and Koyuncu, Anil and Kim, Kisub and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Wu, Peng and Klein, Jacques and Mao, Xiaoguang and Traon, Yves Le},
title = {On the Efficiency of Test Suite Based Program Repair: A Systematic Assessment of 16 Automated Repair Systems for Java Programs},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380338},
doi = {10.1145/3377811.3380338},
abstract = {Test-based automated program repair has been a prolific field of research in software engineering in the last decade. Many approaches have indeed been proposed, which leverage test suites as a weak, but affordable, approximation to program specifications. Although the literature regularly sets new records on the number of benchmark bugs that can be fixed, several studies increasingly raise concerns about the limitations and biases of state-of-the-art approaches. For example, the correctness of generated patches has been questioned in a number of studies, while other researchers pointed out that evaluation schemes may be misleading with respect to the processing of fault localization results. Nevertheless, there is little work addressing the efficiency of patch generation, with regard to the practicality of program repair. In this paper, we fill this gap in the literature, by providing an extensive review on the efficiency of test suite based program repair. Our objective is to assess the number of generated patch candidates, since this information is correlated to (1) the strategy to traverse the search space efficiently in order to select sensical repair attempts, (2) the strategy to minimize the test effort for identifying a plausible patch, (3) as well as the strategy to prioritize the generation of a correct patch. To that end, we perform a large-scale empirical study on the efficiency, in terms of quantity of generated patch candidates of the 16 open-source repair tools for Java programs. The experiments are carefully conducted under the same fault localization configurations to limit biases. Eventually, among other findings, we note that: (1) many irrelevant patch candidates are generated by changing wrong code locations; (2) however, if the search space is carefully triaged, fault localization noise has little impact on patch generation efficiency; (3) yet, current template-based repair systems, which are known to be most effective in fixing a large number of bugs, are actually least efficient as they tend to generate majoritarily irrelevant patch candidates.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {615–627},
numpages = {13},
keywords = {empirical assessment, efficiency, program repair, patch generation},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3324884.3418917,
author = {Liu, Zhe},
title = {Discovering UI Display Issues with Visual Understanding},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3418917},
doi = {10.1145/3324884.3418917},
abstract = {GUI complexity posts a great challenge to the GUI implementation. According to our pilot study of crowdtesting bug reports, display issues such as text overlap, blurred screen, missing image always occur during GUI rendering on difference devices due to the software or hardware compatibility. They negatively influence the app usability, resulting in poor user experience. To detect these issues, we propose a novel approach, OwlEye, based on deep learning for modelling visual information of the GUI screenshot. Therefore, OwlEye can detect GUIs with display issues and also locate the detailed region of the issue in the given GUI for guiding developers to fix the bug. We manually construct a large-scale labelled dataset with 4,470 GUI screenshots with UI display issues. We develop a heuristics-based data augmentation method and a GAN-based data augmentation method for boosting the performance of our OwlEye. At present, the evaluation demonstrates that our OwlEye can achieve 85% precision and 84% recall in detecting UI display issues, and 90% accuracy in localizing these issues.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1373–1375},
numpages = {3},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3276529,
author = {Desai, Ankush and Phanishayee, Amar and Qadeer, Shaz and Seshia, Sanjit A.},
title = {Compositional Programming and Testing of Dynamic Distributed Systems},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276529},
doi = {10.1145/3276529},
abstract = {A real-world distributed system is rarely implemented as a standalone monolithic system. Instead, it is composed of multiple independent interacting components that together ensure the desired system-level specification. One can scale systematic testing to large, industrial-scale implementations by decomposing the system-level testing problem into a collection of simpler component-level testing problems.  This paper proposes techniques for compositional programming and testing of distributed systems with two central contributions: (1) We propose a module system based on the theory of compositional trace refinement for dynamic systems consisting of asynchronously-communicating state machines, where state machines can be dynamically created, and communication topology of the existing state machines can change at runtime; (2) We present ModP, a programming system that implements our module system to enable compositional reasoning (assume-guarantee) of distributed systems.  We demonstrate the efficacy of our framework by building two practical fault-tolerant distributed systems, a transaction-commit service and a replicated hash-table. ModP helps implement these systems modularly and validate them via compositional testing. We empirically demonstrate that the abstraction-based compositional reasoning approach helps amplify the coverage during testing and scale it to real-world distributed systems. The distributed services built using ModP achieve performance comparable to open-source equivalents.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {159},
numpages = {30},
keywords = {domain-specific language, actors, module system, systematic testing, distributed systems, compositional verification, event-driven programming}
}

@article{10.1145/224145.224154,
author = {Leathrum, J. F. and Liburdy, K. A.},
title = {Automated Testing of POSIX Standards},
year = {1994},
issue_date = {March 1994},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
issn = {1067-9936},
url = {https://doi.org/10.1145/224145.224154},
doi = {10.1145/224145.224154},
journal = {StandardView},
month = {mar},
pages = {55–59},
numpages = {5}
}

