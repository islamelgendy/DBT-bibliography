@inproceedings{10.1145/3460319.3464812,
author = {Zhang, Yakun and Lv, Xiao and Dong, Haoyu and Dou, Wensheng and Han, Shi and Zhang, Dongmei and Wei, Jun and Ye, Dan},
title = {Semantic Table Structure Identification in Spreadsheets},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464812},
doi = {10.1145/3460319.3464812},
abstract = {Spreadsheets are widely used in various business tasks, and contain amounts of valuable data. However, spreadsheet tables are usually organized in a semi-structured way, and contain complicated semantic structures, e.g., header types and relations among headers. Lack of documented semantic table structures, existing data analysis and error detection tools can hardly understand spreadsheet tables. Therefore, identifying semantic table structures in spreadsheet tables is of great importance, and can greatly promote various analysis tasks on spreadsheets. In this paper, we propose Tasi (Table structure identification) to automatically identify semantic table structures in spreadsheets. Based on the contents, styles, and spatial locations in table headers, Tasi adopts a multi-classifier to predict potential header types and relations, and then integrates all header types and relations into consistent semantic table structures. We further propose TasiError, to detect spreadsheet errors based on the identified semantic table structures by Tasi. Our experiments on real-world spreadsheets show that, Tasi can precisely identify semantic table structures in spreadsheets, and TasiError can detect real-world spreadsheet errors with higher precision (75.2%) and recall (82.9%) than existing approaches.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {283–295},
numpages = {13},
keywords = {table structure, error detection, Spreadsheet},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1145/3356773.3356810,
author = {Xie, Xiaoyuan and Poon, Pak-Lok and Pullum, Laura L.},
title = {Workshop Summary: 2019 IEEE / ACM Fourth International Workshop on Metamorphic Testing (MET 2019)},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/3356773.3356810},
doi = {10.1145/3356773.3356810},
abstract = {MET is a relatively new workshop on metamorphic testing for academic researchers and industry practitioners. The first international workshop on MET (MET 2016) was co-located with the 38th International Conference on Software Engineering (ICSE 2016) in Austin TX, USA on May 16, 2016. Since then the workshop has become an annual event. This paper reports on the fourth International Workshop on Metamorphic Testing (MET 2019) held in Montr\'{e}al, Canada on May 26, 2019, as part of the 41st International Conference on Software Engineering (ICSE 2019). We first outline the aims of the workshop, followed by a discussion of its keynote speech and technical program.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {56–59},
numpages = {4},
keywords = {software engineering, metamorphic testing, software testing, software verification and validation}
}

@inproceedings{10.1145/3395363.3397352,
author = {Sharma, Arnab and Wehrheim, Heike},
title = {Higher Income, Larger Loan? Monotonicity Testing of Machine Learning Models},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397352},
doi = {10.1145/3395363.3397352},
abstract = {Today, machine learning (ML) models are increasingly applied in decision making. This induces an urgent need for quality assurance of ML models with respect to (often domain-dependent) requirements. Monotonicity is one such requirement. It specifies a software as ''learned'' by an ML algorithm to give an increasing prediction with the increase of some attribute values. While there exist multiple ML algorithms for ensuring monotonicity of the generated model, approaches for checking monotonicity, in particular of black-box models are largely lacking.  In this work, we propose verification-based testing of monotonicity, i.e., the formal computation of test inputs on a white-box model via verification technology, and the automatic inference of this approximating white-box model from the black-box model under test. On the white-box model, the space of test inputs can be systematically explored by a directed computation of test cases. The empirical evaluation on 90 black-box models shows that verification-based testing can outperform adaptive random testing as well as property-based techniques with respect to effectiveness and efficiency.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {200–210},
numpages = {11},
keywords = {Machine Learning Testing, Decision Tree, Monotonicity},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3395363.3397385,
author = {Ghaleb, Asem and Pattabiraman, Karthik},
title = {How Effective Are Smart Contract Analysis Tools? Evaluating Smart Contract Static Analysis Tools Using Bug Injection},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397385},
doi = {10.1145/3395363.3397385},
abstract = {Security attacks targeting smart contracts have been on the rise, which have led to financial loss and erosion of trust. Therefore, it is important to enable developers to discover security vulnerabilities in smart contracts before deployment. A number of static analysis tools have been developed for finding security bugs in smart contracts. However, despite the numerous bug-finding tools, there is no systematic approach to evaluate the proposed tools and gauge their effectiveness. This paper proposes SolidiFI, an automated and systematic approach for evaluating smart contracts’ static analysis tools. SolidiFI is based on injecting bugs (i.e., code defects) into all potential locations in a smart contract to introduce targeted security vulnerabilities. SolidiFI then checks the generated buggy contract using the static analysis tools, and identifies the bugs that the tools are unable to detect (false-negatives) along with identifying the bugs reported as false-positives. SolidiFI is used to evaluate six widely-used static analysis tools, namely, Oyente, Securify, Mythril, SmartCheck, Manticore and Slither, using a set of 50 contracts injected by 9369 distinct bugs. It finds several instances of bugs that are not detected by the evaluated tools despite their claims of being able to detect such bugs, and all the tools report many false positives.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {415–427},
numpages = {13},
keywords = {bug injection, smart contracts dataset, smart contracts, Ethereum security, solidity code analysis, fault injection, smart contracts security, smart contracts analysis, static analysis tools evaluation, Ethereum},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2610384.2628056,
author = {Weitz, Konstantin and Srisakaokul, Siwakorn and Kim, Gene and Ernst, Michael D.},
title = {A Format String Checker for Java},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2628056},
doi = {10.1145/2610384.2628056},
abstract = { Java supports format strings, but their use is error prone because: Java’s type system does not find any but the most trivial mistakes, Java’s format methods fail silently, and for- mat methods are often executed infrequently.  This paper presents the Format String Checker that is based on the format string type system presented in [3]. The Format String Checker guarantees that calls to Java’s Formatter API will not throw exceptions.  We evaluate the Format String Checker on 6 large and well-maintained open-source projects. Format string bugs are common in practice (we found 104 bugs), and the an- notation burden on the user of our type system is low (on average, for every bug found, only 1.0 annotations need to be written). },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {441–444},
numpages = {4},
keywords = {static analysis, printf, type system, Format string},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inbook{10.1145/3293882.3330557,
author = {Guo, Jiaqi and Li, Shuyue and Lou, Jian-Guang and Yang, Zijiang and Liu, Ting},
title = {Sara: Self-Replay Augmented Record and Replay for Android in Industrial Cases},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330557},
abstract = {Record-and-replay tools are indispensable for quality assurance of mobile applications. Due to its importance, an increasing number of tools are being developed to record and replay user interactions for Android. However, by conducting an empirical study of various existing tools in industrial settings, researchers have revealed a gap between the characteristics requested from industry and the performance of publicly available record-and-replay tools. The study concludes that no existing tools under evaluation are sufficient for industrial applications. In this paper, we present a record-and-replay tool called SARA towards bridging the gap and targeting a wide adoption. Specifically, a dynamic instrumentation technique is used to accommodate rich sources of inputs in the application layer satisfying various constraints requested from industry. A self-replay mechanism is proposed to record more information of user inputs for accurate replaying without degrading user experience. In addition, an adaptive replay method is designed to enable replaying events on different devices with diverse screen sizes and OS versions. Through an evaluation on 53 highly popular industrial Android applications and 265 common usage scenarios, we demonstrate the effectiveness of SARA in recording and replaying rich sources of inputs on the same or different devices.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {90–100},
numpages = {11}
}

@inproceedings{10.1145/1281700.1281714,
author = {Hart, William E. and Berry, Jonathan W. and Heaphy, Robert and Phillips, Cynthia A.},
title = {EXACT: The Experimental Algorithmics Computational Toolkit},
year = {2007},
isbn = {9781595937513},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1281700.1281714},
doi = {10.1145/1281700.1281714},
abstract = {In this paper, we introduce EXACT, the EXperimental Algorithmics Computational Toolkit. EXACT is a software framework for describing, controlling, and analyzing computer experiments. It provides the experimentalist with convenient software tools to ease and organize the entire experimental process, including the description of factors and levels, the design of experiments, the control of experimental runs, the archiving of results, and analysis of results.As a case study for EXACT, we describe its interaction with FAST, the Sandia Framework for Agile Software Testing. EXACT and FAST now manage the nightly testing of several large software projects at Sandia. We also discuss EXACT's advanced features, which include a driver module that controls complex experiments such as comparisons of parallel algorithms.},
booktitle = {Proceedings of the 2007 Workshop on Experimental Computer Science},
pages = {14–es},
keywords = {experimental analysis, experimental design, software testing},
location = {San Diego, California},
series = {ExpCS '07}
}

@inproceedings{10.1145/3460319.3464798,
author = {Li, Yishuai and Pierce, Benjamin C. and Zdancewic, Steve},
title = {Model-Based Testing of Networked Applications},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464798},
doi = {10.1145/3460319.3464798},
abstract = {We present a principled automatic testing framework for application-layer protocols. The key innovation is a domain-specific embedded language for writing nondeterministic models of the behavior of networked servers. These models are defined within the Coq interactive theorem prover, supporting a smooth transition from testing to formal verification.  Given a server model, we show how to automatically derive a tester that probes the server for unexpected behaviors. We address the uncertainties caused by both the server's internal choices and the network delaying messages nondeterministically. The derived tester accepts server implementations whose possible behaviors are a subset of those allowed by the nondeterministic model.  We demonstrate the effectiveness of this framework by using it to specify and test a fragment of the HTTP/1.1 protocol, showing that the automatically derived tester can capture RFC violations in buggy server implementations, including the latest versions of Apache and Nginx.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {529–539},
numpages = {11},
keywords = {HTTP, Model-based testing, network refinement, Coq, interaction trees, nondeterminism},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1145/3372788,
author = {Luo, Chu and Goncalves, Jorge and Velloso, Eduardo and Kostakos, Vassilis},
title = {A Survey of Context Simulation for Testing Mobile Context-Aware Applications},
year = {2020},
issue_date = {January 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3372788},
doi = {10.1145/3372788},
abstract = {Equipped with an abundance of small-scale microelectromechanical sensors, modern mobile devices such as smartphones and smartwatches can now offer context-aware services to users in mobile environments. Although advances in mobile context-aware applications have made our everyday environments increasingly intelligent, these applications are prone to bugs that are highly difficult to reproduce and repair. Compared to conventional computer software, mobile context-aware applications often have more complex structures to process a wide variety of dynamic context data in specific scenarios. Accordingly, researchers have proposed diverse context simulation techniques to enable low-cost and effective tests instead of conducting costly and time-consuming real-world experiments. This article aims to give a comprehensive overview of the state-of-the-art context simulation methods for testing mobile context-aware applications. In particular, this article highlights the technical distinctions and commonalities in previous research conducted across multiple disciplines, particularly at the intersection of software testing, ubiquitous computing, and mobile computing. This article also discusses how each method can be implemented and deployed by testing tool developers and mobile application testers. Finally, this article identifies several unexplored issues and directions for further advancements in this field.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {21},
numpages = {39},
keywords = {multimedia, Mobile devices, software testing, sensors}
}

@inbook{10.1145/3460319.3464843,
author = {Zhang, Xufan and Sun, Ning and Fang, Chunrong and Liu, Jiawei and Liu, Jia and Chai, Dong and Wang, Jiang and Chen, Zhenyu},
title = {Predoo: Precision Testing of Deep Learning Operators},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464843},
abstract = {Deep learning(DL) techniques attract people from various fields with superior performance in making progressive breakthroughs. To ensure the quality of DL techniques, researchers have been working on testing and verification approaches. Some recent studies reveal that the underlying DL operators could cause defects inside a DL model. DL operators work as fundamental components in DL libraries. Library developers still work on practical approaches to ensure the quality of operators they provide. However, the variety of DL operators and the implementation complexity make it challenging to evaluate their quality. Operator testing with limited test cases may fail to reveal hidden defects inside the implementation. Besides, the existing model-to-library testing approach requires extra labor and time cost to identify and locate errors, i.e., developers can only react to the exposed defects. This paper proposes a fuzzing-based operator-level precision testing approach to estimate individual DL operators' precision errors to bridge this gap. Unlike conventional fuzzing techniques, valid shape variable inputs and fine-grained precision error evaluation are implemented. The testing of DL operators is treated as a searching problem to maximize output precision errors. We implement our approach in a tool named Predoo and conduct an experiment on seven DL operators from TensorFlow. The experiment result shows that Predoo can trigger larger precision errors compared to the error threshold declared in the testing scripts from the TensorFlow repository.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {400–412},
numpages = {13}
}

@inproceedings{10.1145/2811681.2811685,
author = {Liu, Huai and Spichkova, Maria and Schmidt, Heinz W. and Sellis, Timos and Duckham, Matt},
title = {Spatio-Temporal Architecture-Based Framework for Testing Services in the Cloud},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811685},
doi = {10.1145/2811681.2811685},
abstract = {Increasingly, various services are deployed and orchestrated in the cloud to form global, large-scale systems. The global distribution, high complexity, and physical separation pose new challenges into the quality assurance of such complex services. One major challenge is that they are intricately connected with the spatial and temporal characteristics of the domains they support. In this paper, we present our visions on the integration of spatial and temporal logic into the system design and quality maintenance of the complex services in the cloud. We suggest that new paradigms should be proposed for designing software architecture that will particularly embed the spatial and temporal properties of the cloud services, and new testing methodologies should be developed based on architecture including spatio-temporal aspects. We also discuss several potential directions in the relevant research.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {18–22},
numpages = {5},
keywords = {software testing, spatio-temporal logic, Software architecture},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/3092703.3092722,
author = {Ore, John-Paul and Detweiler, Carrick and Elbaum, Sebastian},
title = {Lightweight Detection of Physical Unit Inconsistencies without Program Annotations},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092722},
doi = {10.1145/3092703.3092722},
abstract = { Systems interacting with the physical world operate on quantities measured with physical units. When unit operations in a program are inconsistent with the physical units' rules, those systems may suffer. Existing approaches to support unit consistency in programs can impose an unacceptable burden on developers. In this paper, we present a lightweight static analysis approach focused on physical unit inconsistency detection that requires no end-user program annotation, modification, or migration. It does so by capitalizing on existing shared libraries that handle standardized physical units, common in the cyber-physical domain, to link class attributes of shared libraries to physical units. Then, leveraging rules from dimensional analysis, the approach propagates and infers units in programs that use these shared libraries, and detects inconsistent unit usage. We implement and evaluate the approach in a tool, analyzing 213 open-source systems containing +900,000 LOC, finding inconsistencies in 11% of them, with an 87% true positive rate for a class of inconsistencies detected with high confidence. An initial survey of robot system developers finds that the unit inconsistencies detected by our tool are 'problematic', and we investigate how and when these inconsistencies occur. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {341–351},
numpages = {11},
keywords = {program analysis, physical units, unit consistency, robotic systems, static analysis, dimensional analysis, type checking},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inbook{10.1145/3377812.3382146,
author = {Tufano, Michele and Kimko, Jason and Wang, Shiya and Watson, Cody and Bavota, Gabriele and Di Penta, Massimiliano and Poshyvanyk, Denys},
title = {DeepMutation: A Neural Mutation Tool},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382146},
abstract = {Mutation testing can be used to assess the fault-detection capabilities of a given test suite. To this aim, two characteristics of mutation testing frameworks are of paramount importance: (i) they should generate mutants that are representative of real faults; and (ii) they should provide a complete tool chain able to automatically generate, inject, and test the mutants. To address the first point, we recently proposed an approach using a Recurrent Neural Network Encoder-Decoder architecture to learn mutants from ~787k faults mined from real programs. The empirical evaluation of this approach confirmed its ability to generate mutants representative of real faults. In this paper, we address the second point, presenting DeepMutation, a tool wrapping our deep learning model into a fully automated tool chain able to generate, inject, and test mutants learned from real faults.Video: https://sites.google.com/view/learning-mutation/deepmutation},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {29–32},
numpages = {4}
}

@inproceedings{10.1145/3460319.3464822,
author = {Zhao, Zhe and Chen, Guangke and Wang, Jingyi and Yang, Yiwei and Song, Fu and Sun, Jun},
title = {Attack as Defense: Characterizing Adversarial Examples Using Robustness},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464822},
doi = {10.1145/3460319.3464822},
abstract = {As a new programming paradigm, deep learning has expanded its application to many real-world problems. At the same time, deep learning based software are found to be vulnerable to adversarial attacks. Though various defense mechanisms have been proposed to improve robustness of deep learning software, many of them are ineffective against adaptive attacks. In this work, we propose a novel characterization to distinguish adversarial examples from benign ones based on the observation that adversarial examples are significantly less robust than benign ones. As existing robustness measurement does not scale to large networks, we propose a novel defense framework, named attack as defense (A2D), to detect adversarial examples by effectively evaluating an example’s robustness. A2D uses the cost of attacking an input for robustness evaluation and identifies those less robust examples as adversarial since less robust examples are easier to attack. Extensive experiment results on MNIST, CIFAR10 and ImageNet show that A2D is more effective than recent promising approaches. We also evaluate our defense against potential adaptive attacks and show that A2D is effective in defending carefully designed adaptive attacks, e.g., the attack success rate drops to 0% on CIFAR10.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {42–55},
numpages = {14},
keywords = {defense, neural networks, adversarial examples, Deep learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1145/3447240,
author = {Bertolino, Antonia and Braione, Pietro and Angelis, Guglielmo De and Gazzola, Luca and Kifetew, Fitsum and Mariani, Leonardo and Orr\`{u}, Matteo and Pezz\`{e}, Mauro and Pietrantuono, Roberto and Russo, Stefano and Tonella, Paolo},
title = {A Survey of Field-Based Testing Techniques},
year = {2021},
issue_date = {June 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3447240},
doi = {10.1145/3447240},
abstract = {Field testing refers to testing techniques that operate in the field to reveal those faults that escape in-house testing. Field testing techniques are becoming increasingly popular with the growing complexity of contemporary software systems. In this article, we present the first systematic survey of field testing approaches over a body of 80 collected studies, and propose their categorization based on the environment and the system on which field testing is performed. We discuss four research questions addressing how software is tested in the field, what is tested in the field, which are the requirements, and how field tests are managed, and identify many challenging research directions.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {92},
numpages = {39},
keywords = {field testing, Software testing, in-vivo testing, ex-vivo testing}
}

@inproceedings{10.1145/3324884.3418930,
author = {Olsthoorn, Mitchell and van Deursen, Arie and Panichella, Annibale},
title = {Generating Highly-Structured Input Data by Combining Search-Based Testing and Grammar-Based Fuzzing},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3418930},
doi = {10.1145/3324884.3418930},
abstract = {Software testing is an important and time-consuming task that is often done manually. In the last decades, researchers have come up with techniques to generate input data (e.g., fuzzing) and automate the process of generating test cases (e.g., search-based testing). However, these techniques are known to have their own limitations: search-based testing does not generate highly-structured data; grammar-based fuzzing does not generate test case structures. To address these limitations, we combine these two techniques. By applying grammar-based mutations to the input data gathered by the search-based testing algorithm, it allows us to co-evolve both aspects of test case generation. We evaluate our approach, called G-EvoSuite, by performing an empirical study on 20 Java classes from the three most popular JSON parsers across multiple search budgets. Our results show that the proposed approach on average improves branch coverage for JSON related classes by 15 % (with a maximum increase of 50 %) without negatively impacting other classes.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1224–1228},
numpages = {5},
keywords = {search-based software testing, test case generation, unit testing, grammar-based fuzzing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/1007512.1007537,
author = {Ezick, James},
title = {An Optimizing Compiler for Batches of Temporal Logic Formulas},
year = {2004},
isbn = {1581138202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007512.1007537},
doi = {10.1145/1007512.1007537},
abstract = {Model checking based on validating temporal logic formulas has proven practical and effective for numerous software engineering applications. As systems based on this approach have become more mainstream, a need has arisen to deal effectively with large batches of formulas over a common model. Presently, most systems validate formulas one at a time, with little or no interaction between validation of separate formulas. This is the case despite the fact that, for a wide range of applications, a certain level of redundancy between domain-related formulas can be anticipated.This paper presents an optimizing compiler for batches of temporal logic formulas. A component of the Carnauba model checking system, this compiler addresses the need to handle batches of temporal logic formulas by leveraging the framework common to optimizing programming language compilers. Just as traditional optimizing compilers attempt to exploit redundancy and other solvable properties in a program to reduce the demand on a runtime system, this compiler exploits similar properties in groups of formulas to reduce the demand on a model checking engine. Optimizations are performed via a set of distinct, interchangeable optimization passes operating on a common intermediate representation. The intermediate representation captures the full modal mu-calculus, and the optimization techniques are applicable to any temporal logic subsumed by that logic. The compiler offers a unified framework for expressing some well understood single-formula optimizations as well as numerous inter-formula optimizations that capitalize on redundancy, logical implication, and, optionally, model-specific knowledge. It is capable of working either in place of, or as a preprocessor for, other optimization algorithms. The result is a system that, when applied to a potentially heterogeneous collection of formulas over a common problem domain, is able to measurably reduce the time and space requirements of the subsequent model checking engine.},
booktitle = {Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {183–194},
numpages = {12},
keywords = {temporal logic, model checking, optimizing compiler},
location = {Boston, Massachusetts, USA},
series = {ISSTA '04}
}

@article{10.1145/1013886.1007537,
author = {Ezick, James},
title = {An Optimizing Compiler for Batches of Temporal Logic Formulas},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1013886.1007537},
doi = {10.1145/1013886.1007537},
abstract = {Model checking based on validating temporal logic formulas has proven practical and effective for numerous software engineering applications. As systems based on this approach have become more mainstream, a need has arisen to deal effectively with large batches of formulas over a common model. Presently, most systems validate formulas one at a time, with little or no interaction between validation of separate formulas. This is the case despite the fact that, for a wide range of applications, a certain level of redundancy between domain-related formulas can be anticipated.This paper presents an optimizing compiler for batches of temporal logic formulas. A component of the Carnauba model checking system, this compiler addresses the need to handle batches of temporal logic formulas by leveraging the framework common to optimizing programming language compilers. Just as traditional optimizing compilers attempt to exploit redundancy and other solvable properties in a program to reduce the demand on a runtime system, this compiler exploits similar properties in groups of formulas to reduce the demand on a model checking engine. Optimizations are performed via a set of distinct, interchangeable optimization passes operating on a common intermediate representation. The intermediate representation captures the full modal mu-calculus, and the optimization techniques are applicable to any temporal logic subsumed by that logic. The compiler offers a unified framework for expressing some well understood single-formula optimizations as well as numerous inter-formula optimizations that capitalize on redundancy, logical implication, and, optionally, model-specific knowledge. It is capable of working either in place of, or as a preprocessor for, other optimization algorithms. The result is a system that, when applied to a potentially heterogeneous collection of formulas over a common problem domain, is able to measurably reduce the time and space requirements of the subsequent model checking engine.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {183–194},
numpages = {12},
keywords = {temporal logic, optimizing compiler, model checking}
}

@article{10.1145/3530786,
author = {Ojdanic, Milos and Soremekun, Ezekiel and Degiovanni, Renzo and Papadakis, Mike and Le Traon, Yves},
title = {Mutation Testing in Evolving Systems: Studying the Relevance of Mutants to Code Evolution},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3530786},
doi = {10.1145/3530786},
abstract = {Context:
When software evolves, opportunities for introducing faults appear. Therefore, it is important to test the evolved program behaviors during each evolution cycle. However, while software evolves, its complexity is also evolving, introducing challenges to the testing process. To deal with this issue, testing techniques should be adapted to target the effect of the program changes instead of the entire program functionality. To this end, commit-aware mutation testing, a powerful testing technique, has been proposed. Unfortunately, commit-aware mutation testing is challenging due to the complex program semantics involved. Hence, it is pertinent to understand the characteristics, predictability, and potential of the technique. Objective:
We conduct an exploratory study to investigate the properties of commit-relevant mutants, i.e., the test elements of commit-aware mutation testing, by proposing a general definition and an experimental approach to identify them. We thus, aim at investigating the prevalence, location, and comparative advantages of commit-aware mutation testing over time (i.e., the program evolution). We also investigate the predictive power of several commit-related features in identifying and selecting commit-relevant mutants to understand the essential properties for its best-effort application case. Method:
Our commit-relevant definition relies on the notion of observational slicing, approximated by higher-order mutation. Specifically, our approach utilizes the impact of mutants, effects of one mutant on another in capturing and analyzing the implicit interactions between the changed and unchanged code parts. The study analyses millions of mutants (over 10 million), 288 commits, five (5) different open-source software projects involving over 68,213 CPU days of computation and sets a ground truth where we perform our analysis. Results:
Our analysis shows that commit-relevant mutants are located mainly outside of program commit change (81%), suggesting a limitation in previous work. We also note that effective selection of commit-relevant mutants has the potential of reducing the number of mutants by up to 93%. In addition, we demonstrate that commit relevant mutation testing is significantly more effective and efficient than state-of-the-art baselines, i.e., random mutant selection and analysis of only mutants within the program change. In our analysis of the predictive power of mutants and commit-related features (e.g., number of mutants within a change, mutant type, and commit size) in predicting commit-relevant mutants, we found that most proxy features do not reliably predict commit-relevant mutants. Conclusion:
This empirical study highlights the properties of commit-relevant mutants and demonstrates the importance of identifying and selecting commit-relevant mutants when testing evolving software systems.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
keywords = {Software Testing, Mutation Testing, Continuous Integration, Evolving-Systems}
}

@inbook{10.1145/3238147.3238202,
author = {Ma, Lei and Juefei-Xu, Felix and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Chen, Chunyang and Su, Ting and Li, Li and Liu, Yang and Zhao, Jianjun and Wang, Yadong},
title = {DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238202},
abstract = {Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {120–131},
numpages = {12}
}

@inproceedings{10.1145/3092703.3098230,
author = {Casalnuovo, Casey and Suchak, Yagnik and Ray, Baishakhi and Rubio-Gonz\'{a}lez, Cindy},
title = {GitcProc: A Tool for Processing and Classifying GitHub Commits},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098230},
doi = {10.1145/3092703.3098230},
abstract = { Sites such as GitHub have created a vast collection of software artifacts that researchers interested in understanding and improving software systems can use. Current tools for processing such GitHub data tend to target project metadata and avoid source code processing, or process source code in a manner that requires significant effort for each language supported. This paper presents GitcProc, a lightweight tool based on regular expressions and source code blocks, which downloads projects and extracts their project history, including fine-grained source code information and development time bug fixes. GitcProc can track changes to both single-line and block source code structures and associate these changes to the surrounding function context with minimal set up required from users. We demonstrate GitcProc's ability to capture changes in multiple languages by evaluating it on C, C++, Java, and Python projects, and show it finds bug fixes and the context of source code changes effectively with few false positives. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {396–399},
numpages = {4},
keywords = {Information Extraction, Language Independence, Git Mining Tool},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

