@inproceedings{10.1145/3461648.3463856,
author = {Young, May and Hu, Alan J. and Lemieux, Guy G. F.},
title = {Cache Abstraction for Data Race Detection in Heterogeneous Systems with Non-Coherent Accelerators},
year = {2021},
isbn = {9781450384728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461648.3463856},
doi = {10.1145/3461648.3463856},
abstract = {Embedded systems are becoming increasingly complex and heterogeneous, featuring multiple processor cores (which might themselves be heterogeneous) as well as specialized hardware accelerators, all accessing shared memory. Many accelerators are non-coherent (i.e., do not support hardware cache coherence) because it reduces hardware complexity, cost, and power consumption, while potentially offering superior performance. However, the disadvantage of non-coherence is that the software must explicitly synchronize between accelerators and processors, and this synchronization is notoriously error-prone.  We propose an analysis technique to find data races in software for heterogeneous systems that include non-coherent accelerators. Our approach builds on classical results for data race detection, but the challenge turns out to be analyzing cache behavior rather than the behavior of the non-coherent accelerators. Accordingly, our central contribution is a novel, sound (data-race-preserving) abstraction of cache behavior. We prove our abstraction sound, and then to demonstrate the precision of our abstraction, we implement it in a simple dynamic race detector for a system with a processor and a massively parallel accelerator provided by a commercial FPGA-based accelerator vendor. On eleven software examples provided by the vendor, the tool had zero false positives and was able to detect previously unknown data races in 2 of the 11 examples.},
booktitle = {Proceedings of the 22nd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems},
pages = {151–162},
numpages = {12},
keywords = {Data Race, Memory Coherence, Caching, Hardware Accelerator},
location = {Virtual, Canada},
series = {LCTES 2021}
}

@inproceedings{10.1145/2396716.2396725,
author = {Johansen, Martin F. and Haugen, \O{}ystein and Fleurey, Franck},
title = {Bow Tie Testing: A Testing Pattern for Product Lines},
year = {2011},
isbn = {9781450313025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2396716.2396725},
doi = {10.1145/2396716.2396725},
abstract = {Verification of highly configurable systems poses a significant challenge, the challenge of knowing that every configuration works when there often are intractably many different configurations. When a homogeneous abstraction layer has many mutually exclusive alternative implementations, we might, according to the polymorphic server test pattern, test these implementations using one test suite targeted towards the abstraction layer which is then run for each concrete implementation of the abstraction layer. But, the pattern does not handle interaction testing. Combinatorial interaction testing is one of the more promising techniques for doing interaction testing of a software product line. The bow tie testing pattern describes how the configurations which differ only in the implementation layer require one test suite. In addition, comparing the execution results of one product with another provides for a test oracle. The pattern reduces the effort of testing a highly configurable system without reducing the error detection capabilities provided by ordinary combinatorial interaction testing. We present an example of a subset of the Eclipse IDE product line, and show that only 20 test suites is required to test 41 products, a significant reduction.},
booktitle = {Proceedings of the 16th European Conference on Pattern Languages of Programs},
articleno = {9},
numpages = {13},
keywords = {software product lines},
location = {Irsee, Germany},
series = {EuroPLoP '11}
}

@inproceedings{10.1145/3237009.3237015,
author = {Aumayr, Dominik and Marr, Stefan and B\'{e}ra, Cl\'{e}ment and Boix, Elisa Gonzalez and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Efficient and Deterministic Record &amp; Replay for Actor Languages},
year = {2018},
isbn = {9781450364249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3237009.3237015},
doi = {10.1145/3237009.3237015},
abstract = {With the ubiquity of parallel commodity hardware, developers turn to high-level concurrency models such as the actor model to lower the complexity of concurrent software. However, debugging concurrent software is hard, especially for concurrency models with a limited set of supporting tools. Such tools often deal only with the underlying threads and locks, which obscures the view on e.g. actors and messages and thereby introduces additional complexity.To improve on this situation, we present a low-overhead record &amp; replay approach for actor languages. It allows one to debug concurrency issues deterministically based on a previously recorded trace. Our evaluation shows that the average run-time overhead for tracing on benchmarks from the Savina suite is 10% (min. 0%, max. 20%). For Acme-Air, a modern web application, we see a maximum increase of 1% in latency for HTTP requests and about 1.4 MB/s of trace data. These results are a first step towards deterministic replay debugging of actor systems in production.},
booktitle = {Proceedings of the 15th International Conference on Managed Languages &amp; Runtimes},
articleno = {15},
numpages = {14},
keywords = {replay, actors, determinism, debugging, tracing, concurrency},
location = {Linz, Austria},
series = {ManLang '18}
}

@inproceedings{10.1109/ASE.2015.78,
author = {Filieri, Antonio and P\u{a}s\u{a}reanu, Corina S. and Yang, Guowei},
title = {Quantification of Software Changes through Probabilistic Symbolic Execution},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.78},
doi = {10.1109/ASE.2015.78},
abstract = {Characterizing software changes is fundamental for software maintenance. However existing techniques are imprecise leading to unnecessary maintenance efforts. We introduce a novel approach that computes a precise numeric characterization of program changes, which quantifies the likelihood of reaching target program events (e.g., assert violations or successful termination) and how that evolves with each program update, together with the percentage of inputs impacted by the change.This precise characterization leads to a natural ranking of different program changes based on their probability of execution and their impact on target events. The approach is based on model counting over the constraints collected with a symbolic execution of the program, and exploits the similarity between program versions to reduce cost and improve the quality of analysis results.We implemented our approach in the Symbolic PathFinder tool and illustrate it on several Java case studies, including the evaluation of different program repairs, mutants used in testing, or incremental analysis after a change.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {703–708},
numpages = {6},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/1162678.1162681,
author = {Le, Franck and Lee, Sihyung and Wong, Tina and Kim, Hyong S. and Newcomb, Darrell},
title = {Minerals: Using Data Mining to Detect Router Misconfigurations},
year = {2006},
isbn = {159593569X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1162678.1162681},
doi = {10.1145/1162678.1162681},
abstract = {Recent studies have shown that router misconfigurations are common and have dramatic consequences for the operations of networks. Not only can misconfigurations compromise the security of a single network, they can even cause global disruptions in Internet connectivity. Several solutions have been proposed that can detect a number of problems in real configuration files. However, these solutions share a common limitation: they are rule-based. Rules are assumed to be known beforehand, and violations of these rules are deemed misconfigurations. As policies typically differ among networks, rule-based approaches are limited in the scope of mistakes they can detect. In this paper, we address the problem of router misconfigurations using data mining. We apply association rules mining to the configuration files of routers across an administrative domain to discover local, network-specific policies. Deviations from these local policies are potential misconfigurations. We have evaluated our scheme on configuration files from a large state-wide network provider, a large university campus and a high-performance research network, and found promising results. We discovered a number of errors that were confirmed and later corrected by the network engineers. These errors would have been difficult to detect with current rule-based approaches.},
booktitle = {Proceedings of the 2006 SIGCOMM Workshop on Mining Network Data},
pages = {293–298},
numpages = {6},
keywords = {association rules mining, routers, network misconfiguration, static analysis},
location = {Pisa, Italy},
series = {MineNet '06}
}

@inproceedings{10.1145/3338906.3341464,
author = {Ginelli, Davide},
title = {Failure-Driven Program Repair},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341464},
doi = {10.1145/3338906.3341464},
abstract = {Program repair techniques can dramatically reduce the cost of program debugging by automatically generating program fixes. Although program repair has been already successful with several classes of faults, it also turned out to be quite limited in the complexity of the fixes that can be generated.  This Ph.D. thesis addresses the problem of cost-effectively generating fixes of higher complexity by investigating how to exploit failure information to directly shape the repair process. In particular, this thesis proposes Failure-Driven Program Repair, which is a novel approach to program repair that exploits its knowledge about both the possible failures and the corresponding repair strategies, to produce highly specialized repair tasks that can effectively generate non-trivial fixes.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1156–1159},
numpages = {4},
keywords = {software defects, Automatic program repair, automatic debugging},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inbook{10.1145/3368089.3409744,
author = {Baranov, Eduard and Legay, Axel and Meel, Kuldeep S.},
title = {Baital: An Adaptive Weighted Sampling Approach for Improved t-Wise Coverage},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409744},
abstract = {The rise of highly configurable complex software and its widespread usage requires design of efficient testing methodology. t-wise coverage is a leading metric to measure the quality of the testing suite and the underlying test generation engine. While uniform sampling-based test generation is widely believed to be the state of the art approach to achieve t-wise coverage in presence of constraints on the set of configurations, such a scheme often fails to achieve high t-wise coverage in presence of complex constraints. In this work, we propose a novel approach Baital, based on adaptive weighted sampling using literal weighted functions, to generate test sets with high t-wise coverage. We demonstrate that our approach reaches significantly higher t-wise coverage than uniform sampling. The novel usage of literal weighted sampling leaves open several interesting directions, empirical as well as theoretical, for future research.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1114–1126},
numpages = {13}
}

@inproceedings{10.1145/98949.99159,
author = {Sarkar, Manojit},
title = {Runtime Debuggers for Parallel and Distributed Systems: A Uniform Design Approach},
year = {1990},
isbn = {0897913566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/98949.99159},
doi = {10.1145/98949.99159},
booktitle = {Proceedings of the 28th Annual Southeast Regional Conference},
pages = {333–336},
numpages = {4},
location = {Greenville, South Carolina},
series = {ACM-SE 28}
}

@inproceedings{10.1145/62115.62120,
author = {Bruegge, Bernd},
title = {Program Development for a Systolic Array},
year = {1988},
isbn = {0897912764},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/62115.62120},
doi = {10.1145/62115.62120},
abstract = {The primary objective of the Warp programming environment (WPE) is to simplify the use of Warp, a high-performance programmable linear systolic array connected to a general-purpose workstation host. WPE permits the development of distributed applications that access Warp either locally from the host or remotely from a large number of workstations connected to a local area network. Its audience includes the user who calls routines from a library, the programmer who develops new algorithms for Warp, as well as the implementor who writes support software. Since the linear arrangement of the cells in the array restricts direct input and output with the host to the boundary cells, a source language debugger is important for program development on Warp. This paper presents the Warp debugger and its relation to the other components of the Warp programming environment.},
booktitle = {Proceedings of the ACM/SIGPLAN Conference on Parallel Programming: Experience with Applications, Languages and Systems},
pages = {31–41},
numpages = {11},
location = {New Haven, Connecticut, USA},
series = {PPEALS '88}
}

@article{10.1145/62116.62120,
author = {Bruegge, Bernd},
title = {Program Development for a Systolic Array},
year = {1988},
issue_date = {Sept. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/62116.62120},
doi = {10.1145/62116.62120},
abstract = {The primary objective of the Warp programming environment (WPE) is to simplify the use of Warp, a high-performance programmable linear systolic array connected to a general-purpose workstation host. WPE permits the development of distributed applications that access Warp either locally from the host or remotely from a large number of workstations connected to a local area network. Its audience includes the user who calls routines from a library, the programmer who develops new algorithms for Warp, as well as the implementor who writes support software. Since the linear arrangement of the cells in the array restricts direct input and output with the host to the boundary cells, a source language debugger is important for program development on Warp. This paper presents the Warp debugger and its relation to the other components of the Warp programming environment.},
journal = {SIGPLAN Not.},
month = {jan},
pages = {31–41},
numpages = {11}
}

@inproceedings{10.1145/2970276.2970320,
author = {Tang, Hongyin and Wu, Guoquan and Wei, Jun and Zhong, Hua},
title = {Generating Test Cases to Expose Concurrency Bugs in Android Applications},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970320},
doi = {10.1145/2970276.2970320},
abstract = { Mobile systems usually support an event-based model of concurrent programming. This model, although advantageous to maintain responsive user interfaces, may lead to subtle concurrency errors due to unforeseen threads interleaving coupled with non-deterministic reordering of asynchronous events. These bugs are very difficult to reproduce even by the same user action sequences that trigger them, due to the undetermined schedules of underlying events and threads. In this paper, we proposed RacerDroid, a novel technique that aims to expose concurrency bugs in android applications by actively controlling event schedule and thread interleaving, given the test cases that have potential data races. By exploring the state model of the application constructed dynamically, our technique starts first to generate a test case that has potential data races based on the results obtained from existing static or dynamic race detection technique. Then it reschedules test cases execution by actively controlling event dispatching and thread interleaving to determine whether such potential races really lead to thrown exceptions or assertion violations. Our preliminary experiments show that RacerDroid is effective, and it confirms real data races, while at the same time eliminates false warnings for Android apps found in the wild. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {648–653},
numpages = {6},
keywords = {mobile application, android, record/replay, data race, testing},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3377811.3380402,
author = {Dong, Zhen and B\"{o}hme, Marcel and Cojocaru, Lucia and Roychoudhury, Abhik},
title = {Time-Travel Testing of Android Apps},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380402},
doi = {10.1145/3377811.3380402},
abstract = {Android testing tools generate sequences of input events to exercise the state space of the app-under-test. Existing search-based techniques systematically evolve a population of event sequences so as to achieve certain objectives such as maximal code coverage. The hope is that the mutation of fit event sequences leads to the generation of even fitter sequences. However, the evolution of event sequences may be ineffective. Our key insight is that pertinent app states which contributed to the original sequence's fitness may not be reached by a mutated event sequence. The original path through the state space is truncated at the point of mutation.In this paper, we propose instead to evolve a population of states which can be captured upon discovery and resumed when needed. The hope is that generating events on a fit program state leads to the transition to even fitter states. For instance, we can quickly deprioritize testing the main screen state which is visited by most event sequences, and instead focus our limited resources on testing more interesting states that are otherwise difficult to reach.We call our approach time-travel testing because of this ability to travel back to any state that has been observed in the past. We implemented time-travel testing into TimeMachine, a time-travel enabled version of the successful, automated Android testing tool Monkey. In our experiments on a large number of open- and closed source Android apps, TimeMachine outperforms the state-of-the-art search-based/model-based Android testing tools Sapienz and Stoat, both in terms of coverage achieved and crashes found.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {481–492},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/2670967.2670971,
author = {Gama, Kiev and Donsez, Didier},
title = {Deployment and Activation of Faulty Components at Runtime for Testing Self-Recovery Mechanisms},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/2670967.2670971},
doi = {10.1145/2670967.2670971},
abstract = {The hypotheses on potential sources of error in an application can be specified in a fault model, which helps testing scenarios that are likely to be buggy. Based on a fault model, we developed custom fault detection mechanisms for providing self-recovery behavior in a component platform when third-party components behave inappropriately. In order to perform the tests for validating such mechanisms, it would be necessary to use a technique for fault injection so we could simulate faulty behavior. However such a technique may not be appropriate for a component-based approach. The behavior of systems tested with faults injected in the interface level (e.g., passing invalid parameters) would not represent actual application usage, thus significantly differing from cases where faults are injected in the component level (e.g. emulation of internal component errors). This paper presents our approach for testing self-adaptive mechanism, involving a general model for fault deployment and fault activation. Faulty components deployed at runtime represent faulty behaviors specified in the fault model. These faults are remotely activated through test probes that help testing the effectiveness of the platform's self-adaptive mechanisms that are fired upon the detection of the specified faulty behavior.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {sep},
pages = {44–54},
numpages = {11},
keywords = {testing, self-recovery, fault model}
}

@inproceedings{10.1145/325737.325827,
author = {Miller, Christopher A. and Mitchell, Christine and Lakinsmith, Patty and Onken, Reiner and Penner, Robin and Shalin, Valerie},
title = {Intelligent User Interfaces for Correspondence Domains (Panel Session): Moving IUIs off the Desktop},
year = {2000},
isbn = {1581131348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/325737.325827},
doi = {10.1145/325737.325827},
abstract = {This paper is about the elicitation of the requirements for an intelligent interface for a software test development environment that will accommodate the physically challenged (PC). This research explores the use of eye-tracking mechanisms and digital manipulative user interfaces that are especially enhanced for the PC. In addition these devices provide assistance for the knowledge elicitation phase for an Intelligent User Interface to such an environment. It was never a stated objective of PCTA (Physically Challenged Test Assistant) to include any intelligent augmentation of the environment. It was challenge enough to get a paraplegic to operate the software test environment. However, in the process of evaluating the data collected in the evaluation of the user interface it was discovered that empirical data existed to predict some of the impasses that occur in the software development and more uniquely in the software testing process.},
booktitle = {Proceedings of the 5th International Conference on Intelligent User Interfaces},
pages = {181–186},
numpages = {6},
location = {New Orleans, Louisiana, USA},
series = {IUI '00}
}

@inbook{10.1145/3238147.3238170,
author = {Fan, Lingling and Su, Ting and Chen, Sen and Meng, Guozhu and Liu, Yang and Xu, Lihua and Pu, Geguang},
title = {Efficiently Manifesting Asynchronous Programming Errors in Android Apps},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238170},
abstract = {Android, the #1 mobile app framework, enforces the single-GUI-thread model, in which a single UI thread manages GUI rendering and event dispatching. Due to this model, it is vital to avoid blocking the UI thread for responsiveness. One common practice is to offload long-running tasks into async threads. To achieve this, Android provides various async programming constructs, and leaves evelopers themselves to obey the rules implied by the model. However, as our study reveals, more than 25% apps violate these rules and introduce hard-to-detect, fail-stop errors, which we term as aysnc programming errors (APEs). To this end, this paper introduces APEChecker, a technique to automatically and efficiently manifest APEs. The key idea is to characterize APEs as specific fault patterns, and synergistically combine static analysis and dynamic UI exploration to detect and verify such errors. Among the 40 real-world Android apps, APEChecker unveils and processes 61 APEs, of which 51 are confirmed (83.6% hit rate). Specifically, APEChecker detects 3X more APEs than the state-of-art testing tools (Monkey, Sapienz and Stoat), and reduces testing time from half an hour to a few minutes. On a specific type of APEs, APEChecker confirms 5X more errors than the data race detection tool, EventRacer, with very few false alarms.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {486–497},
numpages = {12}
}

@inbook{10.1145/3468264.3468609,
author = {Kim, Dong Jae and Yang, Bo and Yang, Jinqiu and Chen, Tse-Hsun (Peter)},
title = {How Disabled Tests Manifest in Test Maintainability Challenges?},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468609},
abstract = {Software testing is an essential software quality assurance practice. Testing helps expose faults earlier, allowing developers to repair the code and reduce future maintenance costs. However, repairing (i.e., making failing tests pass) may not always be done immediately. Bugs may require multiple rounds of repairs and even remain unfixed due to the difficulty of bug-fixing tasks. To help test maintenance, along with code comments, the majority of testing frameworks (e.g., JUnit and TestNG) have also introduced annotations such as @Ignore to disable failing tests temporarily. Although disabling tests may help alleviate maintenance difficulties, they may also introduce technical debt. With the faster release of applications in modern software development, disabling tests may become the salvation for many developers to meet project deliverables. In the end, disabled tests may become outdated and a source of technical debt, harming long-term maintenance. Despite its harmful implications, there is little empirical research evidence on the prevalence, evolution, and maintenance of disabling tests in practice. To fill this gap, we perform the first empirical study on test disabling practice. We develop a tool to mine 122K commits and detect 3,111 changes that disable tests from 15 open-source Java systems. Our main findings are: (1) Test disabling changes are 19% more common than regular test refactorings, such as renames and type changes. (2) Our life-cycle analysis shows that 41% of disabled tests are never brought back to evaluate software quality, and most disabled tests stay disabled for several years. (3)We unveil the motivations behind test disabling practice and the associated technical debt by manually studying evolutions of 349 unique disabled tests, achieving a 95% confidence level and a 5% confidence interval. Finally, we present some actionable implications for researchers and developers.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1045–1055},
numpages = {11}
}

@inproceedings{10.1145/1629911.1630102,
author = {Uzelac, Vladimir and Milenkovic, Aleksandar},
title = {A Real-Time Program Trace Compressor Utilizing Double Move-to-Front Method},
year = {2009},
isbn = {9781605584973},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629911.1630102},
doi = {10.1145/1629911.1630102},
abstract = {This paper introduces a new unobtrusive and cost-effective method for the capture and compression of program execution traces in real-time, which is based on a double move-to-front transformation. We explore its effectiveness and describe a cost-effective hardware implementation. The proposed trace compressor requires only 0.12 bits per instruction of trace port bandwidth, at the cost of 25K gates.},
booktitle = {Proceedings of the 46th Annual Design Automation Conference},
pages = {738–743},
numpages = {6},
keywords = {compression, program trace, debugging},
location = {San Francisco, California},
series = {DAC '09}
}

@inproceedings{10.5555/2685048.2685068,
author = {Yuan, Ding and Luo, Yu and Zhuang, Xin and Rodrigues, Guilherme Renna and Zhao, Xu and Zhang, Yongle and Jain, Pranay U. and Stumm, Michael},
title = {Simple Testing Can Prevent Most Critical Failures: An Analysis of Production Failures in Distributed Data-Intensive Systems},
year = {2014},
isbn = {9781931971164},
publisher = {USENIX Association},
address = {USA},
abstract = {Large, production quality distributed systems still fail periodically, and do so sometimes catastrophically, where most or all users experience an outage or data loss. We present the result of a comprehensive study investigating 198 randomly selected, user-reported failures that occurred on Cassandra, HBase, Hadoop Distributed File System (HDFS), Hadoop MapReduce, and Redis, with the goal of understanding how one or multiple faults eventually evolve into a user-visible failure. We found that from a testing point of view, almost all failures require only 3 or fewer nodes to reproduce, which is good news considering that these services typically run on a very large number of nodes. However, multiple inputs are needed to trigger the failures with the order between them being important. Finally, we found the error logs of these systems typically contain sufficient data on both the errors and the input events that triggered the failure, enabling the diagnose and the reproduction of the production failures.We found the majority of catastrophic failures could easily have been prevented by performing simple testing on error handling code - the last line of defense - even without an understanding of the software design. We extracted three simple rules from the bugs that have lead to some of the catastrophic failures, and developed a static checker, Aspirator, capable of locating these bugs. Over 30% of the catastrophic failures would have been prevented had Aspirator been used and the identified bugs fixed. Running Aspirator on the code of 9 distributed systems located 143 bugs and bad practices that have been fixed or confirmed by the developers.},
booktitle = {Proceedings of the 11th USENIX Conference on Operating Systems Design and Implementation},
pages = {249–265},
numpages = {17},
location = {Broomfield, CO},
series = {OSDI'14}
}

@inproceedings{10.1145/3377811.3380377,
author = {Johnson, Brittany and Brun, Yuriy and Meliou, Alexandra},
title = {Causal Testing: Understanding Defects' Root Causes},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380377},
doi = {10.1145/3377811.3380377},
abstract = {Understanding the root cause of a defect is critical to isolating and repairing buggy behavior. We present Causal Testing, a new method of root-cause analysis that relies on the theory of counterfactual causality to identify a set of executions that likely hold key causal information necessary to understand and repair buggy behavior. Using the Defects4J benchmark, we find that Causal Testing could be applied to 71% of real-world defects, and for 77% of those, it can help developers identify the root cause of the defect. A controlled experiment with 37 developers shows that Causal Testing improves participants' ability to identify the cause of the defect from 80% of the time with standard testing tools to 86% of the time with Causal Testing. The participants report that Causal Testing provides useful information they cannot get using tools such as JUnit. Holmes, our prototype, open-source Eclipse plugin implementation of Causal Testing, is available at http://holmes.cs.umass.edu/.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {87–99},
numpages = {13},
keywords = {software debugging, theory of counterfactual causality, causal testing, Holmes, automated test generation, causality, test fuzzing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3357766.3359538,
author = {Raselimo, Moeketsi and Fischer, Bernd},
title = {Spectrum-Based Fault Localization for Context-Free Grammars},
year = {2019},
isbn = {9781450369817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3357766.3359538},
doi = {10.1145/3357766.3359538},
abstract = {We describe and evaluate the first spectrum-based fault localization method aimed at finding faulty rules in a context-free grammar. It takes as input a test suite and a modified parser for the grammar that can collect grammar spectra, i.e., the sets of rules used in attempts to parse the individual test cases, and returns as output a ranked list of suspicious rules. We show how grammar spectra can be collected for both LL and LR parsers, and how the ANTLR and CUP parser generators can be modified and used to automate the collection of the grammar spectra. We evaluate our method over grammars with seeded faults as well as real world grammars and student grammars submitted in compiler engineering courses that contain real faults. The results show that our method ranks the seeded faults within the top five rules in more than half of the cases and can pinpoint them in 10%–40% of the cases. On average, it ranks the faults at around 25% of all rules, and better than 15% for a very large test suite. It also allowed us to identify deviations and faults in the real world and student grammars.},
booktitle = {Proceedings of the 12th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {15–28},
numpages = {14},
keywords = {Spectrum-based fault localization},
location = {Athens, Greece},
series = {SLE 2019}
}

@inproceedings{10.5555/2486788.2486803,
author = {Whalen, Michael and Gay, Gregory and You, Dongjiang and Heimdahl, Mats P. E. and Staats, Matt},
title = {Observable Modified Condition/Decision Coverage},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { In many critical systems domains, test suite adequacy is currently measured using structural coverage metrics over the source code. Of particular interest is the modified condition/decision coverage (MC/DC) criterion required for, e.g., critical avionics systems. In previous investigations we have found that the efficacy of such test suites is highly dependent on the structure of the program under test and the choice of variables monitored by the oracle. MC/DC adequate tests would frequently exercise faulty code, but the effects of the faults would not propagate to the monitored oracle variables.  In this report, we combine the MC/DC coverage metric with a notion of observability that helps ensure that the result of a fault encountered when covering a structural obligation propagates to a monitored variable; we term this new coverage criterion Observable MC/DC (OMC/DC). We hypothesize this path requirement will make structural coverage metrics 1.) more effective at revealing faults, 2.) more robust to changes in program structure, and 3.) more robust to the choice of variables monitored. We assess the efficacy and sensitivity to program structure of OMC/DC as compared to masking MC/DC using four subsystems from the civil avionics domain and the control logic of a microwave. We have found that test suites satisfying OMC/DC are significantly more effective than test suites satisfying MC/DC, revealing up to 88% more faults, and are less sensitive to program structure and the choice of monitored variables. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {102–111},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

