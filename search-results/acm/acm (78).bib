@inproceedings{10.1145/3287324.3287394,
author = {Smith, Rebecca and Rixner, Scott},
title = {The Error Landscape: Characterizing the Mistakes of Novice Programmers},
year = {2019},
isbn = {9781450358903},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287324.3287394},
doi = {10.1145/3287324.3287394},
abstract = {The software development process often follows a circuitous path, littered with mistakes and backtracks. This is particularly true for novice programmers, who typically navigate through a variety of errors en route to their final solution. This paper presents a quantitative analysis of a large dataset of Python programs written by novice students. The analysis paints a multifaceted picture of the errors that students encounter, providing insight into the distribution, duration, and evolution of these errors. Ultimately, this paper aims to incite further conversation on the mistakes made by novice programmers, and to inform the decisions instructors make as they help students overcome these mistakes.},
booktitle = {Proceedings of the 50th ACM Technical Symposium on Computer Science Education},
pages = {538–544},
numpages = {7},
keywords = {education, runtime errors, syntax errors, novice programmers},
location = {Minneapolis, MN, USA},
series = {SIGCSE '19}
}

@inproceedings{10.1145/1595637.1595649,
author = {Subramonian, Venkita},
title = {Towards Automated Functional Testing of Converged Applications},
year = {2009},
isbn = {9781605587677},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595637.1595649},
doi = {10.1145/1595637.1595649},
abstract = {There is a growing demand for IP based multimedia services that encompass usage of multiple user interfaces including web and telephony. The complexity of such converged applications requires sophisticated development tools and techniques. While standards such as SIP and HTTP Servlets enable the application developer to develop and deploy converged applications, there is a growing need for tools and techniques that can help with functional testing of converged applications. This paper makes the following contributions - (1) identifies key challenges including concurrency and coordination in functional testing of converged applications (2) describes our solution to address these challenges and (3) describes the impact of our solution based on experience gained from its use in functional testing of a real-world converged conferencing application.},
booktitle = {Proceedings of the 3rd International Conference on Principles, Systems and Applications of IP Telecommunications},
articleno = {9},
numpages = {12},
keywords = {telecommunications, VoIP applications, testing, converged applications},
location = {Atlanta, Georgia},
series = {IPTComm '09}
}

@inproceedings{10.1145/2647908.2655972,
author = {Meinicke, Jens and Th\"{u}m, Thomas and Schr\"{o}ter, Reimar and Benduhn, Fabian and Saake, Gunter},
title = {An Overview on Analysis Tools for Software Product Lines},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655972},
doi = {10.1145/2647908.2655972},
abstract = {A software product line is a set of different software products that share commonalities. For a selection of features, specialized products of one domain can be generated automatically from domain artifacts. However, analyses of software product lines need to handle a large number of products that can be exponential in the number of features. In the last decade, many approaches have been proposed to analyze software product lines efficiently. For some of these approaches tool support is available. Based on a recent survey on analysis for software product lines, we provide a first overview on such tools. While our discussion is limited to analysis tools, we provide an accompanying website covering further tools for product-line development. We compare tools according to their analysis and implementation strategy to identify underrepresented areas. In addition, we want to ease the reuse of existing tools for researchers and students, and to simplify research transfer to practice.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {94–101},
numpages = {8},
keywords = {tool support, software product lines, non-functional properties, theorem proving, type checking, sampling, static analysis, testing, model checking, code metrics},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3487570,
author = {Xiong, Yingfei and Wang, Bo},
title = {L2S: A Framework for Synthesizing the Most Probable Program under a Specification},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3487570},
doi = {10.1145/3487570},
abstract = {In many scenarios, we need to find the most likely program that meets a specification under a local context, where the local context can be an incomplete program, a partial specification, natural language description, and so on. We call such a problem program estimation. In this article, we propose a framework, LingLong Synthesis Framework (L2S), to address this problem. Compared with existing work, our work is novel in the following aspects. (1) We propose a theory of expansion rules to describe how to decompose a program into choices. (2) We propose an approach based on abstract interpretation to efficiently prune off the program sub-space that does not satisfy the specification. (3) We prove that the probability of a program is the product of the probabilities of choosing expansion rules, regardless of the choosing order. (4) We reduce the program estimation problem to a pathfinding problem, enabling existing pathfinding algorithms to solve this problem.L2S has been applied to program generation and program repair. In this article, we report our instantiation of this framework for synthesizing conditional expressions (L2S-Cond) and repairing conditional statements (L2S-Hanabi). The experiments on L2S-Cond show that each option enabled by L2S, including the expansion rules, the pruning technique, and the use of different pathfinding algorithms, plays a major role in the performance of the approach. The default configuration of L2S-Cond correctly predicts nearly 60% of the conditional expressions in the top 5 candidates. Moreover, we evaluate L2S-Hanabi on 272 bugs from two real-world Java defects benchmarks, namely Defects4J and Bugs.jar. L2S-Hanabi correctly fixes 32 bugs with a high precision of 84%. In terms of repairing conditional statement bugs, L2S-Hanabi significantly outperforms all existing approaches in both precision and recall.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
articleno = {34},
numpages = {45},
keywords = {expansion rules, Program estimation, program synthesis, program repair}
}

@inproceedings{10.5555/1356802.1356905,
author = {Tang, Shan and Xu, Qiang},
title = {A Debug Probe for Concurrently Debugging Multiple Embedded Cores and Inter-Core Transactions in NoC-Based Systems},
year = {2008},
isbn = {9781424419227},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {Existing SoC debug techniques mainly target bus-based systems. They are not readily applicable to the emerging system that use Network-on-Chip (NoC) as on-chip communication scheme. In this paper, we present the detailed design of a novel debug probe (DP) inserted between the core under debug (CUD) and the NoC. With embedded configurable triggers, delay control and timestamping mechanism, the proposed DP is very effective for inter-core transaction analysis as well as controlling embedded cores' debug processes. Experimental results show the functionalities of the proposed DP and its area overhead1.},
booktitle = {Proceedings of the 2008 Asia and South Pacific Design Automation Conference},
pages = {416–421},
numpages = {6},
location = {Seoul, Korea},
series = {ASP-DAC '08}
}

@inproceedings{10.1145/503376.503400,
author = {Jettmar, Eva and Nass, Clifford},
title = {Adaptive Testing: Effects on User Performance},
year = {2002},
isbn = {1581134533},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/503376.503400},
doi = {10.1145/503376.503400},
abstract = {This study examines the effects of interface adaptation on user performance in HCI and CMC. No studies to date have explored the psychological effects of a combination of software performance monitoring and adaptation. This combination is the focus of the present study. Two competing possible effects of adaptive interfaces are presented: 1) Social facilitation, according to which users with high task confidence should perform better, and users with low task confidence should perform less well because their performance is monitored by the interface; and 2) "choking", according to which users with high task confidence should perform less well, and users with low task confidence should perform better because the interface adapts to their performance. A 2 (adaptive vs. non-adaptive) x 2 (high user task confidence vs. low task confidence) x 2 (HCI vs. CMC) laboratory experiment was conducted. Results indicate that for CMC, the social facilitation explanation holds true, while results for HCI were consistent with the "choking" explanation. Implications for the theory and design of adaptive interfaces are discussed},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {129–134},
numpages = {6},
keywords = {social facilitation, adaptive testing, performance monitoring, adaptive software, CMC, intelligent interfaces},
location = {Minneapolis, Minnesota, USA},
series = {CHI '02}
}

@inproceedings{10.5555/563780.563781,
author = {McDermid, John A},
title = {Software Safety: Where's the Evidence?},
year = {2001},
isbn = {090992581X},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Standards for safety critical software usually either mandate or recommend development and assessment techniques which are deemed appropriate to reduce the risk of flaws in the software contributing to accidents. These recommendations are usually broken down into a number of "levels" of rigour, with the highest levels being applied where the consequences of failure, or risk, are most severe. The paper discusses the extent to which it is possible to find evidence that there is a genuine variation in risk with level, i.e. that the principles in the standards are sound, and questions some of the assumptions underlying these standards.The paper then goes on to discuss the potential advantages of using product-based evidence to demonstrate safety of software, as opposed to relying on process prescription. It outlines current work on developing and applying "evidence frameworks" as alternatives to the process-based approach, and identifies some of the challenges in gaining widespread acceptance of such approaches.Finally the paper discusses the ALARP principle, and what would be necessary to show that risks associated with safety-critical software have been reduced ALARP. The paper concludes that there are some fundamental difficulties with applying the ALARP principle to software, which neither the process nor evidence-based approaches to demonstrating software safety can readily resolve.},
booktitle = {Proceedings of the Sixth Australian Workshop on Safety Critical Systems and Software - Volume 3},
pages = {1–6},
numpages = {6},
keywords = {safety critical software, the ALARP principle, software safety evidence},
location = {Brisbane, Australia},
series = {SCS '01}
}

@inproceedings{10.5555/319568.319624,
author = {Redwine, Samuel T. and Riddle, William E.},
title = {Software Technology Maturation},
year = {1985},
isbn = {0818606207},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {We have reviewed the growth and propagation of a variety of software technologies in an attempt to discover natural characteristics of the process as well as principles and techniques useful in transitioning modern software technology into widespread use. What we have looked at is the technology maturation process, the process by which a piece of technology is first conceived, then shaped into something usable, and finally “marketed” to the point that it is found in the repertoire of a majority of professionals.A major interest is the time required for technology maturation — and our conclusion is that technology maturation generally takes much longer than popularly thought, especially for major technology areas. But our prime interest is in determining what actions, if any, can accelerate the maturation of technology, in particular that part of maturation that has to do with transitioning the technology into widespread use. Our observations concerning maturation facilitators and inhibitors are the major subject of this paper.},
booktitle = {Proceedings of the 8th International Conference on Software Engineering},
pages = {189–200},
numpages = {12},
location = {London, England},
series = {ICSE '85}
}

@inproceedings{10.5555/2819261.2819279,
author = {Su, Fang-Hsiang and Bell, Jonathan and Murphy, Christian and Kaiser, Gail},
title = {Dynamic Inference of Likely Metamorphic Properties to Support Differential Testing},
year = {2015},
publisher = {IEEE Press},
abstract = {Metamorphic testing is an advanced technique to test programs without a true test oracle such as machine learning applications. Because these programs have no general oracle to identify their correctness, traditional testing techniques such as unit testing may not be helpful for developers to detect potential bugs. This paper presents a novel system, Kabu, which can dynamically infer properties of methods' states in programs that describe the characteristics of a method before and after transforming its input. These Metamorphic Properties (MPs) are pivotal to detecting potential bugs in programs without test oracles, but most previous work relies solely on human effort to identify them and only considers MPs between input parameters and output result (return value) of a program or method. This paper also proposes a testing concept, Metamorphic Differential Testing (MDT). By detecting different sets of MPs between different versions for the same method, Kabu reports potential bugs for human review. We have performed a preliminary evaluation of Kabu by comparing the MPs detected by humans with the MPs detected by Kabu. Our preliminary results are promising: Kabu can find more MPs than human developers, and MDT is effective at detecting function changes in methods.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {55–59},
numpages = {5},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.1145/2983990.2984000,
author = {Hanappi, Oliver and Hummer, Waldemar and Dustdar, Schahram},
title = {Asserting Reliable Convergence for Configuration Management Scripts},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984000},
doi = {10.1145/2983990.2984000},
abstract = { The rise of elastically scaling applications that frequently deploy new machines has led to the adoption of DevOps practices across the cloud engineering stack. So-called configuration management tools utilize scripts that are based on declarative resource descriptions and make the system converge to the desired state. It is crucial for convergent configurations to be able to gracefully handle transient faults, e.g., network outages when downloading and installing software packages. In this paper we introduce a conceptual framework for asserting reliable convergence in configuration management. Based on a formal definition of configuration scripts and their resources, we utilize state transition graphs to test whether a script makes the system converge to the desired state under different conditions. In our generalized model, configuration actions are partially ordered, often resulting in prohibitively many possible execution orders. To reduce this problem space, we define and analyze a property called preservation, and we show that if preservation holds for all pairs of resources, then convergence holds for the entire configuration. Our implementation builds on Puppet, but the approach is equally applicable to other frameworks like Chef, Ansible, etc. We perform a comprehensive evaluation based on real world Puppet scripts and show the effectiveness of the approach. Our tool is able to detect all idempotence and convergence related issues in a set of existing Puppet scripts with known issues as well as some hitherto undiscovered bugs in a large random sample of scripts. },
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {328–343},
numpages = {16},
keywords = {DevOps, Configuration Management, Testing, Idempotence, Convergence, Puppet, Declarative Language, System Configuration Scripts},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984000,
author = {Hanappi, Oliver and Hummer, Waldemar and Dustdar, Schahram},
title = {Asserting Reliable Convergence for Configuration Management Scripts},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984000},
doi = {10.1145/3022671.2984000},
abstract = { The rise of elastically scaling applications that frequently deploy new machines has led to the adoption of DevOps practices across the cloud engineering stack. So-called configuration management tools utilize scripts that are based on declarative resource descriptions and make the system converge to the desired state. It is crucial for convergent configurations to be able to gracefully handle transient faults, e.g., network outages when downloading and installing software packages. In this paper we introduce a conceptual framework for asserting reliable convergence in configuration management. Based on a formal definition of configuration scripts and their resources, we utilize state transition graphs to test whether a script makes the system converge to the desired state under different conditions. In our generalized model, configuration actions are partially ordered, often resulting in prohibitively many possible execution orders. To reduce this problem space, we define and analyze a property called preservation, and we show that if preservation holds for all pairs of resources, then convergence holds for the entire configuration. Our implementation builds on Puppet, but the approach is equally applicable to other frameworks like Chef, Ansible, etc. We perform a comprehensive evaluation based on real world Puppet scripts and show the effectiveness of the approach. Our tool is able to detect all idempotence and convergence related issues in a set of existing Puppet scripts with known issues as well as some hitherto undiscovered bugs in a large random sample of scripts. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {328–343},
numpages = {16},
keywords = {DevOps, Idempotence, Puppet, System Configuration Scripts, Testing, Configuration Management, Convergence, Declarative Language}
}

@inproceedings{10.1145/3338906.3338967,
author = {Mordahl, Austin and Oh, Jeho and Koc, Ugur and Wei, Shiyi and Gazzillo, Paul},
title = {An Empirical Study of Real-World Variability Bugs Detected by Variability-Oblivious Tools},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338967},
doi = {10.1145/3338906.3338967},
abstract = {Many critical software systems developed in C utilize compile-time configurability. The many possible configurations of this software make bug detection through static analysis difficult. While variability-aware static analyses have been developed, there remains a gap between those and state-of-the-art static bug detection tools. In order to collect data on how such tools may perform and to develop real-world benchmarks, we present a way to leverage configuration sampling, off-the-shelf “variability-oblivious” bug detectors, and automatic feature identification techniques to simulate a variability-aware analysis. We instantiate our approach using four popular static analysis tools on three highly configurable, real-world C projects, obtaining 36,061 warnings, 80% of which are variability warnings. We analyze the warnings we collect from these experiments, finding that most results are variability warnings of a variety of kinds such as NULL dereference. We then manually investigate these warnings to produce a benchmark of 77 confirmed true bugs (52 of which are variability bugs) useful for future development of variability-aware analyses.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {50–61},
numpages = {12},
keywords = {static analysis, configurable C software, variability bugs},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/125490.125493,
author = {Pancake, Cherri M.},
title = {Where Are We Headed?},
year = {1991},
issue_date = {Nov. 1991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {34},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/125490.125493},
doi = {10.1145/125490.125493},
journal = {Commun. ACM},
month = {nov},
pages = {52–64},
numpages = {13}
}

@inproceedings{10.1145/1370847.1370854,
author = {Myers, Brad A. and Ko, Amy J. and Park, Sun Young and Stylos, Jeffrey and LaToza, Thomas D. and Beaton, Jack},
title = {More Natural End-User Software Engineering},
year = {2008},
isbn = {9781605580340},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370847.1370854},
doi = {10.1145/1370847.1370854},
abstract = {The "Natural Programming" project at Carnegie Mellon University has been working for more than 10 years to make programming more "natural", or closer to the way people think. We have addressed the needs of all kinds of programmers: novices, professionals and end-user programmers. Many studies were performed which provided new insights and led to new models of programmers. From these insights and models, we created new programming languages and environments. Evaluations of the resulting systems have shown that they are effective and successful. This paper provides an overview of the entire 10-year Natural Programming project, but focuses on our new results since WEUSE-III in Dagstuhl.},
booktitle = {Proceedings of the 4th International Workshop on End-User Software Engineering},
pages = {30–34},
numpages = {5},
keywords = {designer, interactive behaviors, natural programming, programming by demonstration, authoring, survey, end-user software engineering},
location = {Leipzig, Germany},
series = {WEUSE '08}
}

@inproceedings{10.1145/3358960.3379132,
author = {Bulej, Lubom\'{\i}r and Hork\'{y}, Vojt\v{e}ch and Tuma, Petr and Farquet, Fran\c{c}ois and Prokopec, Aleksandar},
title = {Duet Benchmarking: Improving Measurement Accuracy in the Cloud},
year = {2020},
isbn = {9781450369916},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3358960.3379132},
doi = {10.1145/3358960.3379132},
abstract = {We investigate the duet measurement procedure, which helps improve the accuracy of performance comparison experiments conducted on shared machines by executing the measured artifacts in parallel and evaluating their relative performance together, rather than individually. Specifically, we analyze the behavior of the procedure in multiple cloud environments and use experimental evidence to answer multiple research questions concerning the assumption underlying the procedure. We demonstrate improvements in accuracy ranging from 2.3x to 12.5x (5.03x on average) for the tested ScalaBench (and DaCapo) workloads, and from 23.8x to 82.4x (37.4x on average) for the SPEC CPU 2017 workloads.},
booktitle = {Proceedings of the ACM/SPEC International Conference on Performance Engineering},
pages = {100–107},
numpages = {8},
keywords = {software performance, performance measurement, performance testing, cloud computing},
location = {Edmonton AB, Canada},
series = {ICPE '20}
}

@inproceedings{10.1145/336512.336556,
author = {Lutz, Robyn R.},
title = {Software Engineering for Safety: A Roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336556},
doi = {10.1145/336512.336556},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {213–226},
numpages = {14},
keywords = {software engineering, software safety, future directions},
location = {Limerick, Ireland},
series = {ICSE '00}
}

@inproceedings{10.1145/2656106.2656113,
author = {Wagstaff, Harry and Spink, Tom and Franke, Bj\"{o}rn},
title = {Automated ISA Branch Coverage Analysis and Test Case Generation for Retargetable Instruction Set Simulators},
year = {2014},
isbn = {9781450330503},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2656106.2656113},
doi = {10.1145/2656106.2656113},
abstract = {Processor design tools integrate in their workflows generators for instruction set simulators (Iss) from architecture descriptions. However, it is difficult to validate the correctness of these simulators. Isa coverage analysis is insufficient to isolate modelling faults, which might only be exposed in corner cases. We present a novel Isa branch coverage analysis, which considers every possible execution path within an instruction and, on demand, generates new test cases to cover the missing paths. We have applied this analysis to industry standard Eembc and Spec Cpu2006 benchmarks and show that for an Arm v5T model neither of these benchmark suites provides a sufficient Isa coverage to exercise every path through each instruction of the whole instruction set.},
booktitle = {Proceedings of the 2014 International Conference on Compilers, Architecture and Synthesis for Embedded Systems},
articleno = {15},
numpages = {10},
location = {New Delhi, India},
series = {CASES '14}
}

@inproceedings{10.1109/CGO53902.2022.9741273,
author = {Kurhe, Vaibhav Kiran and Karia, Pratik and Gupta, Shubhani and Rose, Abhishek and Bansal, Sorav},
title = {Automatic Generation of Debug Headers through BlackBox Equivalence Checking},
year = {2022},
isbn = {9781665405843},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CGO53902.2022.9741273},
doi = {10.1109/CGO53902.2022.9741273},
abstract = {Modern compiler optimization pipelines are large and complex, and it is rather cumbersome and error-prone for compiler developers to preserve debugging information across optimization passes. An optimization can add, remove, or reorder code and variables, which makes it difficult to associate the generated code statements and values with the source code statements and values. Moreover recent proposals for automatic generation of optimizations (e.g., through search algorithms) have not previously considered the preservation of debugging information.We demonstrate the application of a blackbox equivalence checker to automatically populate the debugging information in the debug headers of the optimized executables compiled from C programs. A blackbox equivalence checker can automatically compute equivalence proofs between the original source code and the optimized executable code without the knowledge of the exact transformations performed by the compiler/optimizer. We present an algorithm that uses these formal equivalence proofs to improve the executable's debugging headers. We evaluate this approach on benchmarks derived from the Testsuite of Vectorizing Compilers (TSVC) compiled through three different compilers: GCC, Clang/LLVM, and ICC. We demonstrate significant improvements in the debuggability of the optimized executable code in these experiments. The benefits of these improvements can be transparently realized through any standard debugger, such as GDB, to debug the updated executable.},
booktitle = {Proceedings of the 20th IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {144–154},
numpages = {11},
location = {Virtual Event, Republic of Korea},
series = {CGO '22}
}

@inproceedings{10.5555/2486788.2486966,
author = {Hauptmann, Benedikt and Junker, Maximilian and Eder, Sebastian and Heinemann, Lars and Vaas, Rudolf and Braun, Peter},
title = {Hunting for Smells in Natural Language Tests},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Tests are central artifacts of software systems and play a crucial role for software quality. In system testing, a lot of test execution is performed manually using tests in natural language. However, those test cases are often poorly written without best practices in mind. This leads to tests which are not maintainable, hard to understand and inefficient to execute.  For source code and unit tests, so called code smells and test smells have been established as indicators to identify poorly written code. We apply the idea of smells to natural language tests by defining a set of common Natural Language Test Smells (NLTS). Furthermore, we report on an empirical study analyzing the extent in more than 2800 tests of seven industrial test suites. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1217–1220},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1145/2858651,
author = {Thomson, Paul and Donaldson, Alastair F. and Betts, Adam},
title = {Concurrency Testing Using Controlled Schedulers: An Empirical Study},
year = {2016},
issue_date = {March 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {4},
issn = {2329-4949},
url = {https://doi.org/10.1145/2858651},
doi = {10.1145/2858651},
abstract = {We present an independent empirical study on concurrency testing using controlled schedulers. We have gathered 49 buggy concurrent software benchmarks, drawn from public code bases, which we call SCTBench. We applied a modified version of an existing concurrency testing tool to SCTBench, testing five controlled scheduling techniques: depth-first search, preemption bounding, delay bounding, a controlled random scheduler, and probabilistic concurrency testing (PCT). We attempt to answer several research questions: Which technique performs the best, in terms of bug finding ability? How effective are the two main schedule bounding techniques—preemption bounding and delay bounding—at finding bugs? What challenges are associated with applying concurrency testing techniques to existing code? Can we classify certain benchmarks as trivial or nontrivial? Overall, we found that PCT (with parameter d = 3) was the most effective technique in terms of bug finding; it found all bugs found by the other techniques, plus an additional three, and it missed only one bug. Surprisingly, we found that the naive controlled random scheduler, which randomly chooses one thread to execute at each scheduling point, performs well, finding more bugs than preemption bounding and just two fewer bugs than delay bounding. Our findings confirm that delay bounding is superior to preemption bounding and that schedule bounding is superior to an unbounded depth-first search. The majority of bugs in SCTBench can be exposed using a small schedule bound (1--2), supporting previous claims, although one benchmark requires five preemptions. We found that the need to remove nondeterminism and control all synchronization (as is required for systematic concurrency testing) can be nontrivial. There were eight distinct programs that could not easily be included in out study, such as those that perform network and interprocess communication. We report various properties about the benchmarks tested, such as the fact that the bugs in 18 benchmarks were exposed 50% of the time when using random scheduling. We note that future work should not use the benchmarks that we classify as trivial when presenting new techniques, other than as a minimum baseline. We have made SCTBench and our tools publicly available for reproducibility and use in future work.},
journal = {ACM Trans. Parallel Comput.},
month = {feb},
articleno = {23},
numpages = {37},
keywords = {Concurrency, systematic concurrency testing, stateless model checking, context bounding}
}

@inproceedings{10.1145/2998392.2998397,
author = {Ofenbeck, Georg and Rompf, Tiark and P\"{u}schel, Markus},
title = {RandIR: Differential Testing for Embedded Compilers},
year = {2016},
isbn = {9781450346481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2998392.2998397},
doi = {10.1145/2998392.2998397},
abstract = { This paper describes RandIR, a tool for differential testing of compilers using random instances of a given intermediate representation (IR). RandIR assumes no fixed target language but instead supports extensible IR-definitions through an internal IR-independent representation of operations. This makes it particularly well suited to test embedded compilers for multi-stage programming, which is our main use case. The ideas underlying our work, however, are more generally applicable.  RandIR is able to automatically simplify failing instances of a test, a technique commonly referred to as shrinking. This enables testing with large random IR samples, thus increasing the odds of detecting a buggy behavior, while still being able to simplify failing instances to human-readable code. },
booktitle = {Proceedings of the 2016 7th ACM SIGPLAN Symposium on Scala},
pages = {21–30},
numpages = {10},
keywords = {compiler testing, test-case minimization, automated random testing, compiler defect, embedded compilers, bug reporting},
location = {Amsterdam, Netherlands},
series = {SCALA 2016}
}

