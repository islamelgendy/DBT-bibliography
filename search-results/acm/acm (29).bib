@inproceedings{10.1109/ICSE-SEET.2019.00014,
author = {Silvis-Cividjian, Natalia},
title = {Teaching Internet of Things (IoT) Literacy: A Systems Engineering Approach},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET.2019.00014},
doi = {10.1109/ICSE-SEET.2019.00014},
abstract = {The Internet of Things (IoT) invades our world with billions of smart, interconnected devices, all programmed to make our lives easier. For educators, teaching such a vast and dynamic field is both a necessity and a challenge. IoT-relevant topics such as programming, hardware, networking and artificial intelligence are already covered in core computing curricula. Does this mean that fresh graduates are well prepared to tackle complex IoT problems? Unfortunately, nothing could be further from the truth. The problem is that IoT devices are complex systems, where software, hardware, and humans interact with each other. From this interaction, unique behavior and hazardous situations can emerge that might easily stay undetected, unless systems are analyzed as a whole. This paper presents two differently flavored courses that teach IoT using a holistic, system-centric approach. The first is a broad introduction to Pervasive Computing, focused on the intelligence of "Things". The second is an advanced course that zooms on the process of testing a software-intensive system. The key characteristics of our approach are: (1) teaching only the bare essentials (topics needed for end-to-end engineering of a smart system), (2) a strong, hands-on project component, using microcontroller-based miniature systems, inspired by real-life, and (3) a rich partnership with industry and academic idea incubators. Positive student evaluations gathered during the last five years demonstrate that such an approach brings engagement, self-confidence and realism in IoT classrooms. We believe that this success can be replicated in other courses, by shifting the focus on different IoT-relevant aspects.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering Education and Training},
pages = {50–61},
numpages = {12},
keywords = {software engineering, insulin pump, academia-industry cooperation, software testing, systems engineering, microcontrollers, pervasive computing, autonomous vehicles, computer science education, robotic kits, embedded systems, safety-critical systems},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEET '19}
}

@inproceedings{10.1145/2968219.2968586,
author = {Luo, Chu and Kuutila, Miikka and Klakegg, Simon and Ferreira, Denzil and Flores, Huber and Goncalves, Jorge and Kostakos, Vassilis and M\"{a}ntyl\"{a}, Mika},
title = {How to Validate Mobile Crowdsourcing Design? Leveraging Data Integration in Prototype Testing},
year = {2016},
isbn = {9781450344623},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2968219.2968586},
doi = {10.1145/2968219.2968586},
abstract = {Mobile crowdsourcing applications often run in dynamic environments. Due to limited time and budget, developers of mobile crowdsourcing applications sometimes cannot completely test their prototypes in real world situations. We describe a data integration technique for developers to validate their design in prototype testing. Our approach constructs the intended context by combining real-time, historical and simulated data. With correct context-aware design, mobile crowdsourcing applications presenting crowdsourcing questions in relevant context to users are likely to obtain high response quality.},
booktitle = {Proceedings of the 2016 ACM International Joint Conference on Pervasive and Ubiquitous Computing: Adjunct},
pages = {1448–1453},
numpages = {6},
keywords = {mobile crowdsourcing, ambient intelligence, ubiquitous computing, smartphones, software testing and debugging},
location = {Heidelberg, Germany},
series = {UbiComp '16}
}

@inproceedings{10.1145/1147249.1147252,
author = {Kolb, Ronny and Muthig, Dirk},
title = {Making Testing Product Lines More Efficient by Improving the Testability of Product Line Architectures},
year = {2006},
isbn = {1595934596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1147249.1147252},
doi = {10.1145/1147249.1147252},
abstract = {Product line engineering is a recent approach to software development that has shown to enable organizations to achieve significant reductions in development and maintenance cost as well as time-to-market of increasingly complex software systems. Yet, the testing process has not kept up with these reductions and the relative cost for testing product lines is actually becoming higher than in traditional single system development. Also, testing often cannot keep pace with accelerated development in product line engineering due to technical and organizational issues. This paper advocates that testing of product lines can be made more efficient and effective by considering testability already during architectural design. It explores the relationship between testability and product line architecture and discusses the importance of high testability for reducing product line testing effort and achieving required coverage criteria. The paper also outlines a systematic approach that will support product line organizations in improving and evaluating testability of product lines at the architectural level.},
booktitle = {Proceedings of the ISSTA 2006 Workshop on Role of Software Architecture for Testing and Analysis},
pages = {22–27},
numpages = {6},
keywords = {testability, testing, architecture, design, software product line, evaluation},
location = {Portland, Maine},
series = {ROSATEA '06}
}

@inproceedings{10.1145/2884781.2884791,
author = {Henard, Christopher and Papadakis, Mike and Harman, Mark and Jia, Yue and Le Traon, Yves},
title = {Comparing White-Box and Black-Box Test Prioritization},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884791},
doi = {10.1145/2884781.2884791},
abstract = {Although white-box regression test prioritization has been well-studied, the more recently introduced black-box prioritization approaches have neither been compared against each other nor against more well-established white-box techniques. We present a comprehensive experimental comparison of several test prioritization techniques, including well-established white-box strategies and more recently introduced black-box approaches. We found that Combinatorial Interaction Testing and diversity-based techniques (Input Model Diversity and Input Test Set Diameter) perform best among the black-box approaches. Perhaps surprisingly, we found little difference between black-box and white-box performance (at most 4% fault detection rate difference). We also found the overlap between black- and white-box faults to be high: the first 10% of the prioritized test suites already agree on at least 60% of the faults found. These are positive findings for practicing regression testers who may not have source code available, thereby making white-box techniques inapplicable. We also found evidence that both black-box and white-box prioritization remain robust over multiple system releases.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {523–534},
numpages = {12},
keywords = {white-box, regression testing, black-box},
location = {Austin, Texas},
series = {ICSE '16}
}

@inbook{10.1145/3430665.3456368,
author = {Bai, Gina R. and Smith, Justin and Stolee, Kathryn T.},
title = {How Students Unit Test: Perceptions, Practices, and Pitfalls},
year = {2021},
isbn = {9781450382144},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430665.3456368},
abstract = {Unit testing is reported as one of the skills that graduating students lack, yet it is an essential skill for professional software developers. Understanding the challenges students face during testing can help inform practices for software testing education. To that end, we conduct an exploratory study to reveal students' perceptions of unit testing and challenges students encounter when practicing unit testing. We surveyed 54 students from two universities and gave them two testing tasks, one involving black-box test design and one involving white-box test implementation. For the tasks, we used two software projects from prior work in studying test-first development among software developers. We quantitatively analyzed the survey responses and test code properties, and qualitatively identified the mistakes and smells in the test code. We further report on our experience running this study with students.Our results regarding student perceptions show that students believe code coverage is the most important outcome for test suites. For testing practices, most students were ineffective in finding known defects. This may be due to the task design and/or challenges with understanding the source code. For testing pitfalls, we identified six test smells from student-written test code; the most common were ignoring setups in the test code and testing happy path only. These results suggest the students needed more introduction to these common testing concepts and practices in advance of the study activity. Through this experience, we have identified testing concepts that require emphasis for more effective future studies on testing behavior among students.},
booktitle = {Proceedings of the 26th ACM Conference on Innovation and Technology in Computer Science Education V. 1},
pages = {248–254},
numpages = {7}
}

@inbook{10.1145/3368089.3409694,
author = {Ben Khadra, M. Ammar and Stoffel, Dominik and Kunz, Wolfgang},
title = {Efficient Binary-Level Coverage Analysis},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409694},
abstract = {Code coverage analysis plays an important role in the software testing process. More recently, the remarkable effectiveness of coverage feedback has triggered a broad interest in feedback-guided fuzzing. In this work, we introduce bcov, a tool for binary-level coverage analysis. Our tool statically instruments x86-64 binaries in the ELF format without compiler support. We implement several techniques to improve efficiency and scale to large real-world software. First, we bring Agrawal’s probe pruning technique to binary-level instrumentation and effectively leverage its superblocks to reduce overhead. Second, we introduce sliced microexecution, a robust technique for jump table analysis which improves CFG precision and enables us to instrument jump table entries. Additionally, smaller instructions in x86-64 pose a challenge for inserting detours. To address this challenge, we aggressively exploit padding bytes and systematically host detours in neighboring basic blocks.  We evaluate bcov on a corpus of 95 binaries compiled from eight popular and well-tested packages like FFmpeg and LLVM. Two instrumentation policies, with different edge-level precision, are used to patch all functions in this corpus - over 1.6 million functions. Our precise policy has average performance and memory overheads of 14% and 22% respectively. Instrumented binaries do not introduce any test regressions. The reported coverage is highly accurate with an average F-score of 99.86%. Finally, our jump table analysis is comparable to that of IDA Pro on gcc binaries and outperforms it on clang binaries.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1153–1164},
numpages = {12}
}

@inproceedings{10.1109/ASE.2019.00147,
author = {Soto, Mauricio},
title = {Improving Patch Quality by Enhancing Key Components of Automatic Program Repair},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00147},
doi = {10.1109/ASE.2019.00147},
abstract = {The error repair process in software systems is, historically, a resource-consuming task that relies heavily in developer manual effort. Automatic program repair approaches enable the repair of software with minimum human interaction, therefore, mitigating the burden from developers. However, a problem automatically generated patches commonly suffer is generating low-quality patches (which overfit to one program specification, thus not generalizing to an independent oracle evaluation). This work proposes a set of mechanisms to increase the quality of plausible patches including an analysis of test suite behavior and their key characteristics for automatic program repair, analyzing developer behavior to inform the mutation operator selection distribution, and a study of patch diversity as a means to create consolidated higher quality fixes.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1230–1233},
numpages = {4},
keywords = {Automatic Program Repair, patch quality},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2591062.2591106,
author = {Wang, Yurong and Person, Suzette and Elbaum, Sebastian and Dwyer, Matthew B.},
title = {A Framework to Advise Tests Using Tests},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591106},
doi = {10.1145/2591062.2591106},
abstract = { Tests generated by different approaches can form a rich body of information about the system under test (SUT), which can then be used to amplify the power of test suites. Diversity in test representations, however, creates an obstacle to extracting and using this information. In this work, we introduce a test advice framework which enables extraction and application of information contained in existing tests to help improve other tests or test generation techniques. Our framework aims to 1) define a simple, yet expressive test case language so that different types of tests can be represented using a unified language, and 2) define an advice extraction function that enables the elicitation and application of the information encoded in a set of test cases. Preliminary results show how test advice can be used to generate amplified test suites with higher code coverage and improved mutants killed scores over the original test suite. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {440–443},
numpages = {4},
keywords = {Automated test generation, regression testing},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1109/ASE.2013.6693085,
author = {Campos, Jos\'{e} and Abreu, Rui and Fraser, Gordon and d'Amorim, Marcelo},
title = {Entropy-Based Test Generation for Improved Fault Localization},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693085},
doi = {10.1109/ASE.2013.6693085},
abstract = {Spectrum-based Bayesian reasoning can effectively rank candidate fault locations based on passing/failing test cases, but the diagnostic quality highly depends on the size and diversity of the underlying test suite. As test suites in practice often do not exhibit the necessary properties, we present a technique to extend existing test suites with new test cases that optimize the diagnostic quality. We apply probability theory concepts to guide test case generation using entropy, such that the amount of uncertainty in the diagnostic ranking is minimized. Our ENTBUG prototype extends the search-based test generation tool EVOSUITE to use entropy in the fitness function of its underlying genetic algorithm, and we applied it to seven real faults. Empirical results show that our approach reduces the entropy of the diagnostic ranking by 49% on average (compared to using the original test suite), leading to a 91% average reduction of diagnosis candidates needed to inspect to find the true faulty one.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {257–267},
numpages = {11},
keywords = {test case generation, fault localization},
location = {Silicon Valley, CA, USA},
series = {ASE'13}
}

@inproceedings{10.1145/3377811.3380391,
author = {Gerasimou, Simos and Eniser, Hasan Ferit and Sen, Alper and Cakan, Alper},
title = {Importance-Driven Deep Learning System Testing},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380391},
doi = {10.1145/3377811.3380391},
abstract = {Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {702–713},
numpages = {12},
keywords = {safety-critical systems, deep learning systems, test adequacy},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2908812.2908816,
author = {Mariani, Thain\'{a} and Guizzo, Giovani and Vergilio, Silvia R. and Pozo, Aurora T.R.},
title = {Grammatical Evolution for the Multi-Objective Integration and Test Order Problem},
year = {2016},
isbn = {9781450342063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908812.2908816},
doi = {10.1145/2908812.2908816},
abstract = {Search techniques have been successfully applied for solving different software testing problems. However, choosing, implementing and configuring a search technique can be hard tasks. To reduce efforts spent in such tasks, this paper presents an offline hyper-heuristic named GEMOITO, based on Grammatical Evolution (GE). The goal is to automatically generate a Multi-Objective Evolutionary Algorithm (MOEA) to solve the Integration and Test Order (ITO) problem. The MOEAs are distinguished by components and parameters values, described by a grammar. The proposed hyper-heuristic is compared to conventional MOEAs and to a selection hyper-heuristic used in related work. Results show that GEMOITO can generate MOEAs that are statistically better or equivalent to the compared algorithms.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference 2016},
pages = {1069–1076},
numpages = {8},
keywords = {hyper-heuristic, evolutionary algorithm, search based software engineering, grammatical evolution, multi-objective},
location = {Denver, Colorado, USA},
series = {GECCO '16}
}

@inproceedings{10.1145/1401827.1401838,
author = {Lo, David and Khoo, Siau-Cheng and Liu, Chao},
title = {Mining Past-Time Temporal Rules from Execution Traces},
year = {2008},
isbn = {9781605580548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401827.1401838},
doi = {10.1145/1401827.1401838},
abstract = {Specification mining is a process of extracting specifications, often from program execution traces. These specifications can in turn be used to aid program understanding, monitoring and verification. There are a number of dynamic-analysis-based specification mining tools in the literature, however none so far extract past time temporal expressions in the form of rules stating: "whenever a series of events occurs, previously another series of events has happened". Rules of this format are commonly found in practice and useful for various purposes. Most rule-based specification mining tools only mine future-time temporal expression. Many past-time temporal rules like "whenever a resource is used, it was allocated before" are asymmetric as the other direction does not holds. Hence, there is a need to mine past-time temporal rules.In this paper, we describe an approach to mine significant rules of the above format occurring above a certain statistical thresholds from program execution traces. The approach start from a set of traces, each being a sequence of events (i.e., method invocations) and resulting in a set of significant rules obeying minimum thresholds of support and confidence. A rule compaction mechanism is employed to reduce the number of reported rules significantly. Experiments on traces of JBoss Application Server shows the utility of our approach in inferring interesting past-time temporal rules.},
booktitle = {Proceedings of the 2008 International Workshop on Dynamic Analysis: Held in Conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2008)},
pages = {50–56},
numpages = {7},
keywords = {data mining, past-time temporal rules, dynamic analysis, specification mining},
location = {Seattle, Washington},
series = {WODA '08}
}

@inproceedings{10.1145/3377811.3380436,
author = {Chen, Lingchao and Hassan, Foyzul and Wang, Xiaoyin and Zhang, Lingming},
title = {Taming Behavioral Backward Incompatibilities via Cross-Project Testing and Analysis},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380436},
doi = {10.1145/3377811.3380436},
abstract = {In modern software development, software libraries play a crucial role in reducing software development effort and improving software quality. However, at the same time, the asynchronous upgrades of software libraries and client software projects often result in incompatibilities between different versions of libraries and client projects. When libraries evolve, it is often very challenging for library developers to maintain the so-called backward compatibility and keep all their external behavior untouched, and behavioral backward incompatibilities (BBIs) may occur. In practice, the regression test suites of library projects often fail to detect all BBIs. Therefore, in this paper, we propose DeBBI to detect BBIs via cross-project testing and analysis, i.e., using the test suites of various client projects to detect library BBIs. Since executing all the possible client projects can be extremely time consuming, DeBBI transforms the problem of cross-project BBI detection into a traditional information retrieval (IR) problem to execute the client projects with higher probability to detect BBIs earlier. Furthermore, DeBBI considers project diversity and test relevance information for even faster BBI detection. The experimental results show that DeBBI can reduce the end-to-end testing time for detecting the first and average unique BBIs by 99.1% and 70.8% for JDK compared to naive cross-project BBI detection. Also, DeBBI has been applied to other popular 3rd-party libraries. To date, DeBBI has detected 97 BBI bugs with 19 already confirmed as previously unknown bugs.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {112–124},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3428264,
author = {Livinskii, Vsevolod and Babokin, Dmitry and Regehr, John},
title = {Random Testing for C and C++ Compilers with YARPGen},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428264},
doi = {10.1145/3428264},
abstract = {Compilers should not crash and they should not miscompile applications. Random testing is an effective method for finding compiler bugs that have escaped other kinds of testing. This paper presents Yet Another Random Program Generator (YARPGen), a random test-case generator for C and C++ that we used to find and report more than 220 bugs in GCC, LLVM, and the Intel® C++ Compiler. Our research contributions include a method for generating expressive programs that avoid undefined behavior without using dynamic checks, and generation policies, a mechanism for increasing diversity of generated code and for triggering more optimizations. Generation policies decrease the testing time to find hard-to-trigger compiler bugs and, for the kinds of scalar optimizations YARPGen was designed to stress-test, increase the number of times these optimizations are applied by the compiler by an average of 20% for LLVM and 40% for GCC. We also created tools for automating most of the common tasks related to compiler fuzzing; these tools are also useful for fuzzers other than ours.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {196},
numpages = {25},
keywords = {random testing, random program generation, compiler defect, compiler testing, automated testing}
}

@inproceedings{10.1145/3125659.3125670,
author = {Chung, Sam},
title = {Object-Oriented Programming with DevOps},
year = {2017},
isbn = {9781450351003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125659.3125670},
doi = {10.1145/3125659.3125670},
abstract = {DevOps is an emerging culture that emphasizes continuous collaboration between software developers and IT operators through continuous standard process with automated tools for continuous delivery. DevOps participants take diverse roles to support its values - continuous collaboration, continuous process, and continuous delivery. A development team needs to be familiar with user cases, Object-Oriented Analysis (OOA), Object-Oriented Design (OOD), Object-Oriented Programming (OOP), and software testing. A quality assurance team must know use cases, abuse cases, software testing, and penetration testing. An operation team requires understanding deployment of Application Programming Interface (API) documents and executable components, and monitoring them and sharing their monitoring outcomes with both development and quality assurance teams.},
booktitle = {Proceedings of the 18th Annual Conference on Information Technology Education},
pages = {65},
numpages = {1},
keywords = {ebp, oop with devops, reengineering, oop, devops},
location = {Rochester, New York, USA},
series = {SIGITE '17}
}

@inproceedings{10.1145/1868048.1868049,
author = {McMinn, Phil and Stevenson, Mark and Harman, Mark},
title = {Reducing Qualitative Human Oracle Costs Associated with Automatically Generated Test Data},
year = {2010},
isbn = {9781450301381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868048.1868049},
doi = {10.1145/1868048.1868049},
abstract = {Due to the frequent non-existence of an automated oracle, test cases are often evaluated manually in practice. However, this fact is rarely taken into account by automatic test data generators, which seek to maximise a program's structural coverage only. The test data produced tends to be of a poor fit with the program's operational profile. As a result, each test case takes longer for a human to check, because the scenarios that arbitrary-looking data represent require time and effort to understand. This short paper proposes methods to extracting knowledge from programmers, source code and documentation and its incorporation into the automatic test data generation process so as to inject the realism required to produce test cases that are quick and easy for a human to comprehend and check. The aim is to reduce the so-called qualitative human oracle costs associated with automatic test data generation. The potential benefits of such an approach are demonstrated with a simple case study.},
booktitle = {Proceedings of the First International Workshop on Software Test Output Validation},
pages = {1–4},
numpages = {4},
location = {Trento, Italy},
series = {STOV '10}
}

@inproceedings{10.1145/2489280.2489288,
author = {Weiss, Johannes and Mandl, Peter and Schill, Alexander},
title = {Introducing the QCEP-Testing System for Executable Acceptance Test Driven Development of Complex Event Processing Applications},
year = {2013},
isbn = {9781450321617},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489280.2489288},
doi = {10.1145/2489280.2489288},
abstract = { In this paper we introduce the open source testing system QCEP-TS for realizing executable acceptance test driven development (EATDD) of complex event processing (CEP) applications. We will motivate the need of EATDD in this domain and argue that state of the art solutions do not address this topic or have substantial weaknesses. We show the design approach of our testing system including a vendor independent testing syntax. We also state relevant challenges for testing CEP applications and discuss implementation details of our solution. On the basis of a real world scenario we demonstrate first practical experiences of our solution. QCEP-TS is available as open source project at http://sourceforge.net/p/qcep-ts. QCEP (Quality Assurance of Complex Event Processing) is a collaboration project of the Munich University of Applied Sciences, the Technical University of Dresden and the UniCredit Business Integrated Solutions S.C.p.A. },
booktitle = {Proceedings of the 2013 International Workshop on Joining AcadeMiA and Industry Contributions to Testing Automation},
pages = {13–18},
numpages = {6},
keywords = {Test Driven Development, Acceptance testing, Complex Event Processing},
location = {Lugano, Switzerland},
series = {JAMAICA 2013}
}

@inproceedings{10.1145/2660252.2661293,
author = {Fraser, Steven and Mancl, Dennis and Namioka, Aki and Salama, Roberto and Wirfs-Brock, Allen},
title = {East Meets West: The Influences of Geography on Software Production},
year = {2014},
isbn = {9781450332088},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2660252.2661293},
doi = {10.1145/2660252.2661293},
abstract = {How do software development practices differ from coast-to-coast? What should practitioners learn about the influences of geography -- and why is it important? Each community of software professionals has its own technical biases: preferred programming languages, software tools, design paradigms, software testing approaches, and techniques for collaboration within a working group. Conferences like SPLASH provide an opportunity to compare notes, to learn from the successes (and failures) of others, to learn about new technologies, and to learn about how other groups communicate and collaborate. This panel will focus on the diversity of software development practices in North America and the broader influences of geography.},
booktitle = {Proceedings of the Companion Publication of the 2014 ACM SIGPLAN Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {41–42},
numpages = {2},
keywords = {organizational learning, geography},
location = {Portland, Oregon, USA},
series = {SPLASH '14}
}

@inproceedings{10.5555/3437539.3437762,
author = {Luo, Zhengxiong and Zuo, Feilong and Shen, Yuheng and Jiao, Xun and Chang, Wanli and Jiang, Yu},
title = {ICS Protocol Fuzzing: Coverage Guided Packet Crack and Generation},
year = {2020},
isbn = {9781450367257},
publisher = {IEEE Press},
abstract = {Industrial Control System (ICS) protocols play an essential role in building communications among system components. Recently, many severe vulnerabilities, such as Stuxnet and DragonFly, exposed in ICS protocols have affected a wide distribution of devices. Therefore, it is of vital importance to ensure their correctness. However, the vulnerability detection efficiency of traditional techniques such as fuzzing is challenged by the complexity and diversity of the protocols.In this paper, we propose to equip the traditional protocol fuzzing with coverage-guided packet crack and generation. We collect the coverage information during the testing procedure, save those valuable packets that trigger new path coverage and crack them into pieces, based on which, we can construct higher-quality new packets for further testing. For evaluation, we build Peach* on top of Peach, which is one of the most widely used protocol fuzzers, and conduct experiments on several ICS protocols such as Modbus and DNP3. Results show that, compared with the original Peach, Peach* achieves the same code coverage and bug detection numbers at the speed of 1.2X-25X. It also gains final increase with 8.35%-36.84% more paths within 24 hours and has exposed 9 previously unknown vulnerabilities.},
booktitle = {Proceedings of the 57th ACM/EDAC/IEEE Design Automation Conference},
articleno = {223},
numpages = {6},
keywords = {ICS protocol, vulnerability detection, fuzzing},
location = {Virtual Event, USA},
series = {DAC '20}
}

@inproceedings{10.1145/3107091.3107094,
author = {Groce, Alex and Flikkema, Paul and Holmes, Josie},
title = {Towards Automated Composition of Heterogeneous Tests for Cyber-Physical Systems},
year = {2017},
isbn = {9781450351126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107091.3107094},
doi = {10.1145/3107091.3107094},
abstract = { A key trait of modern cyber-physical systems (CPS) is complexity due to the number of components and layers in these systems. Unlike in traditional software development, where the device layer is essentially completely abstracted away by an operating system, CPS components include low-power edge nodes, gateways, and servers that together provide sensing, actuation, communication, model and state inference, and autonomous or user-driven control. Moreover, the CPS design process involves implementation of these functions at different levels of abstraction, from high-level computational models to bare-mental implementations. Unfortunately, even when advanced testing or verification methods are applied only to low level system aspects, those efforts are separated from high-level tests of a CPS, which are often produced by a different team, and do not stress the low-level system. Effective automated test composition would make it possible to automatically produce integration/system tests for CPS, even with extremely heterogeneous aspects, where individual elements have effective tests but the interactions between the sub-systems are untested. Because of the size of the search space involved and the complexity of modeling and designing CPS, we also propose in the long term a move towards system architectures to support testing across both system layers and levels of abstraction. },
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems},
pages = {12–15},
numpages = {4},
keywords = {test composition, software architecture, cyber-physical systems, testability},
location = {Santa Barbara, CA, USA},
series = {TECPS 2017}
}

