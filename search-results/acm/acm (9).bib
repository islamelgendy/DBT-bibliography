@inproceedings{10.1145/3422392.3422483,
author = {Melo, Silvana M. and Moreira, Veronica X. S. and Paschoal, Leo Natan and Souza, Simone R. S.},
title = {Testing Education: A Survey on a Global Scale},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422483},
doi = {10.1145/3422392.3422483},
abstract = {[Background]: The software industry has a high demand for professionals in the software quality area. However, computing students, in general, leave the university with little or no knowledge in software testing. [Aim] Although previous studies have investigated how software testing is covered in universities, they focus on pedagogical approaches, trends, and technologies for improving the educational process, and do not evaluate in-depth the contents explored. This article addresses a worldwide perspective on the way educators both deal with the software testing subject, materials and details covered, support mechanisms and teaching practices applied, and challenges imposed, and evaluate instruments. [Method] A survey with questions was conducted with software testing professors from April to June 2020 towards investigating the way the subject is taught in the classroom. [Results] Most higher education institutions address the fundamental software testing techniques and criteria, with more completeness in specific than general software engineering courses. Additional findings and discussions are presented in detail throughout the paper. [Conclusion] This study provides a comprehensive overview of the software testing education field from educators' perspectives. We strongly believe it is useful for the understanding of the actual scenario of the topic, provides foundations and directions to new research, guides educators into subjects' conduction and identification of potential weaknesses and solutions to recurrent problems.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {554–563},
numpages = {10},
keywords = {Computing Education and Training, Survey, Software Testing},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2931037.2931049,
author = {B. Le, Tien-Duy and Lo, David and Le Goues, Claire and Grunske, Lars},
title = {A Learning-to-Rank Based Fault Localization Approach Using Likely Invariants},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931049},
doi = {10.1145/2931037.2931049},
abstract = { Debugging is a costly process that consumes much of developer time and energy. To help reduce debugging effort, many studies have proposed various fault localization approaches. These approaches take as input a set of test cases (some failing, some passing) and produce a ranked list of program elements that are likely to be the root cause of the failures (i.e., failing test cases). In this work, we propose Savant, a new fault localization approach that employs a learning-to-rank strategy, using likely invariant diffs and suspiciousness scores as features, to rank methods based on their likelihood to be a root cause of a failure. Savant has four steps: method clustering &amp; test case selection, invariant mining, feature extraction, and method ranking. At the end of these four steps, Savant produces a short ranked list of potentially buggy methods. We have evaluated Savant on 357 real-life bugs from 5 programs from the Defects4J benchmark. Out of these bugs, averaging over 100 repeated trials with different seeds to randomly break ties, we find that on average Savant can identify correct buggy methods for 63.03, 101.72, and 122 bugs at top 1, 3, and 5 positions in the ranked lists that Savant produces. We have compared Savant against several state-of-the-art fault localization baselines that work on program spectra. We show that Savant can successfully locate 57.73%, 56.69%, and 43.13% more bugs at top 1, top 3, and top 5 positions than the best performing baseline, respectively. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {177–188},
numpages = {12},
keywords = {Program Invariant, Learning to Rank, Automated Debugging},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3395363.3397383,
author = {Peng, Qianyang and Shi, August and Zhang, Lingming},
title = {Empirically Revisiting and Enhancing IR-Based Test-Case Prioritization},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397383},
doi = {10.1145/3395363.3397383},
abstract = {Test-case prioritization (TCP) aims to detect regression bugs faster via reordering the tests run. While TCP has been studied for over 20 years, it was almost always evaluated using seeded faults/mutants as opposed to using real test failures. In this work, we study the recent change-aware information retrieval (IR) technique for TCP. Prior work has shown it performing better than traditional coverage-based TCP techniques, but it was only evaluated on a small-scale dataset with a cost-unaware metric based on seeded faults/mutants. We extend the prior work by conducting a much larger and more realistic evaluation as well as proposing enhancements that substantially improve the performance. In particular, we evaluate the original technique on a large-scale, real-world software-evolution dataset with real failures using both cost-aware and cost-unaware metrics under various configurations. Also, we design and evaluate hybrid techniques combining the IR features, historical test execution time, and test failure frequencies. Our results show that the change-aware IR technique outperforms stateof-the-art coverage-based techniques in this real-world setting, and our hybrid techniques improve even further upon the original IR technique. Moreover, we show that flaky tests have a substantial impact on evaluating the change-aware TCP techniques based on real test failures.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {324–336},
numpages = {13},
keywords = {continuous integration, information retrieval, Test-case prioritization},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3460319.3464806,
author = {Sun, Jingling and Su, Ting and Li, Junxin and Dong, Zhen and Pu, Geguang and Xie, Tao and Su, Zhendong},
title = {Understanding and Finding System Setting-Related Defects in Android Apps},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464806},
doi = {10.1145/3460319.3464806},
abstract = {Android, the most popular mobile system, offers a number of user-configurable system settings (e.g., network, location, and permission) for controlling devices and apps. Even popular, well-tested apps may fail to properly adapt their behaviors to diverse setting changes, thus frustrating their users. However, there exists no effort to systematically investigate such defects. To this end, we conduct the first empirical study to understand the characteristics of these setting-related defects (in short as "setting defects"), which reside in apps and are triggered by system setting changes. We devote substantial manual effort (over three person-months) to analyze 1,074 setting defects from 180 popular apps on GitHub. We investigate their impact, root causes, and consequences. We find that setting defects have a wide, diverse impact on apps' correctness, and the majority of these defects (≈70.7%) cause non-crash (logic) failures, and thus could not be automatically detected by existing app testing techniques due to the lack of strong test oracles. Motivated and guided by our study, we propose setting-wise metamorphic fuzzing, the first automated testing approach to effectively detect setting defects without explicit oracles. Our key insight is that an app's behavior should, in most cases, remain consistent if a given setting is changed and later properly restored, or exhibit expected differences if not restored. We realize our approach in SetDroid, an automated, end-to-end GUI testing tool, for detecting both crash and non-crash setting defects. SetDroid has been evaluated on 26 popular, open-source apps and detected 42 unique, previously unknown setting defects in 24 apps. To date, 33 have been confirmed and 21 fixed. We also apply SetDroid on five highly popular industrial apps, namely WeChat, QQMail, TikTok, CapCut, and AlipayHK, all of which each have billions of monthly active users. SetDroid successfully detects 17 previously unknown setting defects in these apps' latest releases, and all defects have been confirmed and fixed by the app vendors. The majority of SetDroid-detected defects (49 out of 59) cause non-crash failures, which could not be detected by existing testing tools (as our evaluation confirms). These results demonstrate SetDroid's strong effectiveness and practicality.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {204–215},
numpages = {12},
keywords = {Setting, Android, Testing, Empirical study},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inbook{10.1145/3293882.3330561,
author = {Golagha, Mojdeh and Lehnhoff, Constantin and Pretschner, Alexander and Ilmberger, Hermann},
title = {Failure Clustering without Coverage},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330561},
abstract = {Developing and integrating software in the automotive industry is a complex task and requires extensive testing. An important cost factor in testing and debugging is the time required to analyze failing tests. In the context of regression testing, usually, large numbers of tests fail due to a few underlying faults. Clustering failing tests with respect to their underlying faults can, therefore, help in reducing the required analysis time. In this paper, we propose a clustering technique to group failing hardware-in-the-loop tests based on non-code-based features, retrieved from three different sources. To effectively reduce the analysis effort, the clustering tool selects a representative test for each cluster. Instead of analyzing all failing tests, testers only inspect the representative tests to find the underlying faults. We evaluated the effectiveness and efficiency of our solution in a major automotive company using 86 regression test runs, 8743 failing tests, and 1531 faults. The results show that utilizing our clustering tool, testers can reduce the analysis time more than 60% and find more than 80% of the faults only by inspecting the representative tests.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {134–145},
numpages = {12}
}

@inproceedings{10.1145/1868048.1868050,
author = {Kim-Park, Dae S. and de la Riva, Claudio and Tuya, Javier},
title = {An Automated Test Oracle for XML Processing Programs},
year = {2010},
isbn = {9781450301381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868048.1868050},
doi = {10.1145/1868048.1868050},
abstract = {XML processing programs play an important role in the achievement of XML data querying, manipulation, and construction operations to compose XML data structures for very diverse purposes regarding information representation, storing and exchange on XML-based systems. Testing of XML processing programs is a challenging task since the test input and output data involved in the test executions may be complex and large in volume, which makes it difficult to determine the correctness of the execution results. However, existing approaches on XML-based testing pay scarce attention to the specification and automation of so-called test oracles in charge of judging the execution results from XML processing programs. This paper deals with the definition of an automated test oracle for XML processing programs which operates with differentiated levels of specification. The oracle automation is achieved by transforming these specification levels into program code, and the resulting oracle implementation is evaluated through an experimental study that reveals promising results.},
booktitle = {Proceedings of the First International Workshop on Software Test Output Validation},
pages = {5–12},
numpages = {8},
keywords = {XML-based testing, test oracles, XML queries, software testing},
location = {Trento, Italy},
series = {STOV '10}
}

@inbook{10.1145/3474624.3477066,
author = {Fernandes, Daniel and Machado, Ivan and Maciel, Rita},
title = {Handling Test Smells in Python: Results from a Mixed-Method Study},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3477066},
abstract = {Software testing is an activity in the software development process that looks for defects. Automated testing is composed of code that allows run software testing scenarios more quickly, avoiding manual rework. However, testers are likely to employ practices that might negatively impact test quality regarding maintainability, understandability, and effectiveness when writing test code. Such bad practices are also known as test smells. Although test smells are a language-independent concept, different programming languages could present different occurrence standards. Therefore, studies in one programming language may not be generalizable. This study aims to investigate how test smells occurrence in Python test files. Python became the most widely used programming language globally in 2020. However, most research on test code quality only considers the Java language. To accomplish our goals, we built a dataset with 5,303 test files from 90 Python projects collected from GitHub repositories to understand and analyze strategies for handling test smells in Python. This analysis allowed us to propose four new test smells, discussing their potential problems. We carried out a preliminary evaluation with 40 Python developers to validate their thoughts on the proposed test smells. These results are part of an ongoing research project aiming to propose a foundation to better support automation tests in Python. },
booktitle = {Brazilian Symposium on Software Engineering},
pages = {84–89},
numpages = {6}
}

@inproceedings{10.1145/2610384.2610411,
author = {Galindo, Jos\'{e} A. and Alf\'{e}rez, Mauricio and Acher, Mathieu and Baudry, Benoit and Benavides, David},
title = {A Variability-Based Testing Approach for Synthesizing Video Sequences},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610411},
doi = {10.1145/2610384.2610411},
abstract = { A key problem when developing video processing software is the difficulty to test different input combinations. In this paper, we present VANE, a variability-based testing approach to derive video sequence variants. The ideas of VANE are i) to encode in a variability model what can vary within a video sequence; ii) to exploit the variability model to generate testable configurations; iii) to synthesize variants of video sequences corresponding to configurations. VANE computes T-wise covering sets while optimizing a function over attributes. Also, we present a preliminary validation of the scalability and practicality of VANE in the context of an industrial project involving the test of video processing algorithms. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {293–303},
numpages = {11},
keywords = {Variability, Video analysis, Combinatorial testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3213846.3213858,
author = {Dwarakanath, Anurag and Ahuja, Manish and Sikand, Samarth and Rao, Raghotham M. and Bose, R. P. Jagadeesh Chandra and Dubash, Neville and Podder, Sanjay},
title = {Identifying Implementation Bugs in Machine Learning Based Image Classifiers Using Metamorphic Testing},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213858},
doi = {10.1145/3213846.3213858},
abstract = {We have recently witnessed tremendous success of Machine Learning (ML) in practical applications. Computer vision, speech recognition and language translation have all seen a near human level performance. We expect, in the near future, most business applications will have some form of ML. However, testing such applications is extremely challenging and would be very expensive if we follow today's methodologies. In this work, we present an articulation of the challenges in testing ML based applications. We then present our solution approach, based on the concept of Metamorphic Testing, which aims to identify implementation bugs in ML based image classifiers. We have developed metamorphic relations for an application based on Support Vector Machine and a Deep Learning based application. Empirical validation showed that our approach was able to catch 71% of the implementation bugs in the ML applications.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {118–128},
numpages = {11},
keywords = {Metamorphic Testing, Testing Machine Learning based applications},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/1572272.1572282,
author = {Schuler, David and Dallmeier, Valentin and Zeller, Andreas},
title = {Efficient Mutation Testing by Checking Invariant Violations},
year = {2009},
isbn = {9781605583389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1572272.1572282},
doi = {10.1145/1572272.1572282},
abstract = {Mutation testing measures the adequacy of a test suite by seeding artificial defects (mutations) into a program. If a mutation is not detected by the test suite, this usually means that the test suite is not adequate. However, it may also be that the mutant keeps the program's semantics unchanged-and thus cannot be detected by any test. Such equivalent mutants have to be eliminated manually, which is tedious.We assess the impact of mutations by checking dynamic invariants. In an evaluation of our JAVALANCHE framework on seven industrial-size programs, we found that mutations that violate invariants are significantly more likely to be detectable by a test suite. As a consequence, mutations with impact on invariants should be focused upon when improving test suites. With less than 3% of equivalent mutants, our approach provides an efficient, precise, and fully automatic measure of the adequacy of a test suite.},
booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
pages = {69–80},
numpages = {12},
keywords = {dynamic invariants, mutation testing},
location = {Chicago, IL, USA},
series = {ISSTA '09}
}

@inproceedings{10.1145/2483760.2483782,
author = {Zhang, Lingming and Marinov, Darko and Khurshid, Sarfraz},
title = {Faster Mutation Testing Inspired by Test Prioritization and Reduction},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483782},
doi = {10.1145/2483760.2483782},
abstract = { Mutation testing is a well-known but costly approach for determining test adequacy. The central idea behind the approach is to generate mutants, which are small syntactic transformations of the program under test, and then to measure for a given test suite how many mutants it kills. A test t is said to kill a mutant m of program p if the output of t on m is different from the output of t on p. The effectiveness of mutation testing in determining the quality of a test suite relies on the ability to apply it using a large number of mutants. However, running many tests against many mutants is time consuming. We present a family of techniques to reduce the cost of mutation testing by prioritizing and reducing tests to more quickly determine the sets of killed and non-killed mutants. Experimental results show the effectiveness and efficiency of our techniques. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {235–245},
numpages = {11},
keywords = {Mutation testing, Test Prioritization, Test Reduction},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@article{10.1145/3507903,
author = {J\'{u}nior, Misael C. and Amalfitano, Domenico and Garc\'{e}s, Lina and Fasolino, Anna Rita and Andrade, Stev\~{a}o A. and Delamaro, M\'{a}rcio},
title = {Dynamic Testing Techniques of Non-Functional Requirements in Mobile Apps: A Systematic Mapping Study},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3507903},
doi = {10.1145/3507903},
abstract = {Context: The mobile app market is continually growing offering solutions to almost all aspects of people’s lives, e.g., healthcare, business, entertainment, as well as the stakeholders’ demand for apps that are more secure, portable, easy to use, among other non-functional requirements (NFRs). Therefore, manufacturers should guarantee that their mobile apps achieve high-quality levels. A good strategy is to include software testing and quality assurance activities during the whole life cycle of such solutions. Problem: Systematically warranting NFRs is not an easy task for any software product. Software engineers must take important decisions before adopting testing techniques and automation tools to support such endeavors. Proposal: To provide to the software engineers with a broad overview of existing dynamic techniques and automation tools for testing mobile apps regarding NFRs. Methods: We planned and conducted a Systematic Mapping Study (SMS) following well-established guidelines for executing secondary studies in software engineering. Results: We found 56 primary studies and characterized their contributions based on testing strategies, testing approaches, explored mobile platforms, and the proposed tools. Conclusions: The characterization allowed us to identify and discuss important trends and opportunities that can benefit both academics and practitioners.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {dec},
keywords = {systematic mapping, Software testing, mobile apps, dynamic testing techniques, non-functional requirements}
}

@inproceedings{10.1145/3395363.3397375,
author = {Liu, Muyang and Li, Ke and Chen, Tao},
title = {DeepSQLi: Deep Semantic Learning for Testing SQL Injection},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397375},
doi = {10.1145/3395363.3397375},
abstract = {Security is unarguably the most serious concern for Web applications, to which SQL injection (SQLi) attack is one of the most devastating attacks. Automatically testing SQLi vulnerabilities is of ultimate importance, yet is unfortunately far from trivial to implement. This is because the existence of a huge, or potentially infinite, number of variants and semantic possibilities of SQL leading to SQLi attacks on various Web applications. In this paper, we propose a deep natural language processing based tool, dubbed DeepSQLi, to generate test cases for detecting SQLi vulnerabilities. Through adopting deep learning based neural language model and sequence of words prediction, DeepSQLi is equipped with the ability to learn the semantic knowledge embedded in SQLi attacks, allowing it to translate user inputs (or a test case) into a new test case, which is se- mantically related and potentially more sophisticated. Experiments are conducted to compare DeepSQLi with SQLmap, a state-of-the-art SQLi testing automation tool, on six real-world Web applications that are of different scales, characteristics and domains. Empirical results demonstrate the effectiveness and the remarkable superiority of DeepSQLi over SQLmap, such that more SQLi vulnerabilities can be identified by using a less number of test cases, whilst running much faster.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {286–297},
numpages = {12},
keywords = {Web security, SQL injection, natural language processing, deep learning, test case generation},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2771783.2771802,
author = {Aquino, Andrea and Bianchi, Francesco A. and Chen, Meixian and Denaro, Giovanni and Pezz\`{e}, Mauro},
title = {Reusing Constraint Proofs in Program Analysis},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771802},
doi = {10.1145/2771783.2771802},
abstract = { Symbolic analysis techniques have largely improved over the years, and are now approaching an industrial maturity level. One of the main limitations to the scalability of symbolic analysis is the impact of constraint solving that is still a relevant bottleneck for the applicability of symbolic techniques, despite the dramatic improvements of the last decades. In this paper we discuss a novel approach to deal with the constraint solving bottleneck. Starting from the observation that constraints may recur during the analysis of the same as well as different programs, we investigate the advantages of complementing constraint solving with searching for the satisfiability proof of a constraint in a repository of constraint proofs. We extend recent proposals with powerful simplifications and an original canonical form of the constraints that reduce syntactically different albeit equivalent constraints to the same form, and thus facilitate the search for equivalent constraints in large repositories. The experimental results we attained indicate that the proposed approach improves over both similar solutions and state of the art constraint solvers. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {305–315},
numpages = {11},
keywords = {constraint canonicalization, Constraint solving for symbolic program analysis, proof reuse},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/3395363.3404366,
author = {Grieco, Gustavo and Song, Will and Cygan, Artur and Feist, Josselin and Groce, Alex},
title = {Echidna: Effective, Usable, and Fast Fuzzing for Smart Contracts},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404366},
doi = {10.1145/3395363.3404366},
abstract = {Ethereum smart contracts---autonomous programs that run on a blockchain---often control transactions of financial and intellectual property. Because of the critical role they play, smart contracts need complete, comprehensive, and effective test generation. This paper introduces an open-source smart contract fuzzer called Echidna that makes it easy to automatically generate tests to detect violations in assertions and custom properties. Echidna is easy to install and does not require a complex configuration or deployment of contracts to a local blockchain. It offers responsive feedback, captures many property violations, and its default settings are calibrated based on experimental data. To date, Echidna has been used in more than 10 large paid security audits, and feedback from those audits has driven the features and user experience of Echidna, both in terms of practical usability (e.g., smart contract frameworks like Truffle and Embark) and test generation strategies. Echidna aims to be good at finding real bugs in smart contracts, with minimal user effort and maximal speed.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {557–560},
numpages = {4},
keywords = {fuzzing, test generation, smart contracts},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2338965.2336764,
author = {Zhang, Pingyu and Elbaum, Sebastian and Dwyer, Matthew B.},
title = {Compositional Load Test Generation for Software Pipelines},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336764},
doi = {10.1145/2338965.2336764},
abstract = { Load tests validate whether a system’s performance is acceptable under extreme conditions. Traditional load testing approaches are black-box, inducing load by increasing the size or rate of the input. Symbolic execution based load testing techniques complement traditional approaches by enabling the selection of precise input values. However, as the programs under analysis or their required inputs increase in size, the analyses required by these techniques either fail to scale up or sacrifice test effectiveness. We propose a new approach that addresses this limitation by performing load test generation compositionally. It uses existing symbolic execution based techniques to analyze the performance of each system component in isolation, summarizes the results of those analyses, and then performs an analysis across those summaries to generate load tests for the whole system. In its current form, the approach can be applied to any system that is structured in the form of a software pipeline. A study of the approach revealed that it can generate effective load tests for Unix and XML pipelines while outperforming state-of-the-art techniques. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {89–99},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/1292414.1292418,
author = {Chen, T. Y. and Huang, De Hao and Kuo, F.-C},
title = {Adaptive Random Testing by Balancing},
year = {2007},
isbn = {9781595938817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1292414.1292418},
doi = {10.1145/1292414.1292418},
abstract = {Adaptive Random Testing (ART) is an effective improvement of Random Testing (RT). It is based on the observation that failure-causing inputs tend to be clustered together. ART, therefore, proposes to have randomly selected test cases being more evenly spread throughout the input domain by employing the location information of the successful test cases (those that have been executed but do not reveal failures). Based on this intuition, several ART methods have been developed. However, the fault-detection capability of some ART methods is compromised in high dimensional input domains. To improve the fault-detection capability in high dimensional input domains, this paper proposes an innovative ART method using the notion of balancing. Simulation results show that the new method has improved the fault-detection capability in high dimensional input domains.},
booktitle = {Proceedings of the 2nd International Workshop on Random Testing: Co-Located with the 22nd IEEE/ACM International Conference on Automated Software Engineering (ASE 2007)},
pages = {2–9},
numpages = {8},
keywords = {random testing, software testing, adaptive random testing, balancing},
location = {Atlanta, Georgia},
series = {RT '07}
}

@inproceedings{10.1145/2897010.2897011,
author = {Fischer, Stefan and Lopez-Herrejon, Roberto E. and Ramler, Rudolf and Egyed, Alexander},
title = {A Preliminary Empirical Assessment of Similarity for Combinatorial Interaction Testing of Software Product Lines},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897011},
doi = {10.1145/2897010.2897011},
abstract = {Extensive work on Search-Based Software Testing for Software Product Lines has been published in the last few years. Salient among them is the use of similarity as a surrogate metric for t-wise coverage whenever higher strengths are needed or whenever the size of the test suites is infeasible because of technological or budget limitations. Though promising, this metric has not been assessed with real fault data. In this paper, we address this limitation by using Drupal, a widely used open source web content management system, as an industry-strength case study for which both variability information and fault data have been recently made available. Our preliminary assessment corroborates some of the previous findings but also raises issues on some assumptions and claims made. We hope our work encourages further empirical evaluations of Combinatorial Interaction Testing approaches for Software Product Lines.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {15–18},
numpages = {4},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1145/3460319.3469082,
author = {Martin-Lopez, Alberto and Segura, Sergio and Ruiz-Cort\'{e}s, Antonio},
title = {RESTest: Automated Black-Box Testing of RESTful Web APIs},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3469082},
doi = {10.1145/3460319.3469082},
abstract = {Testing RESTful APIs thoroughly is critical due to their key role in software integration. Existing tools for the automated generation of test cases in this domain have shown great promise, but their applicability is limited as they mostly rely on random inputs, i.e., fuzzing. In this paper, we present RESTest, an open source black-box testing framework for RESTful web APIs. Based on the API specification, RESTest supports the generation of test cases using different testing techniques such as fuzzing and constraint-based testing, among others. RESTest is developed as a framework and can be easily extended with new test case generators and test writers for different programming languages. We evaluate the tool in two scenarios: offline and online testing. In the former, we show how RESTest can efficiently generate realistic test cases (test inputs and test oracles) that uncover bugs in real-world APIs. In the latter, we show RESTest's capabilities as a continuous testing and monitoring framework. Demo video: https://youtu.be/1f_tjdkaCKo.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {682–685},
numpages = {4},
keywords = {black-box testing, web APIs, REST},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3460319.3464837,
author = {Ren, Meng and Yin, Zijing and Ma, Fuchen and Xu, Zhenyang and Jiang, Yu and Sun, Chengnian and Li, Huizhong and Cai, Yan},
title = {Empirical Evaluation of Smart Contract Testing: What is the Best Choice?},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464837},
doi = {10.1145/3460319.3464837},
abstract = {Security of smart contracts has attracted increasing attention in recent years. Many researchers have devoted themselves to devising testing tools for vulnerability detection. Each published tool has demonstrated its effectiveness through a series of evaluations on their own experimental scenarios. However, the inconsistency of evaluation settings such as different data sets or performance metrics, may result in biased conclusion.  In this paper, based on an empirical evaluation of widely used smart contract testing tools, we propose a unified standard to eliminate the bias in the assessment process. First, we collect 46,186 source-available smart contracts from four influential organizations. This comprehensive dataset is open to the public and involves different code characteristics, vulnerability patterns and application scenarios. Then we propose a 4-step evaluation process and summarize the difference among relevant work in these steps. We use nine representative tools to carry out extensive experiments. The results demonstrate that different choices of experimental settings could significantly affect tool performance and lead to misleading or even opposite conclusions. Finally, we generalize some problems of existing testing tools, and propose some possible directions for further improvement.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {566–579},
numpages = {14},
keywords = {observations and solutions, evaluation, smart contract testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

