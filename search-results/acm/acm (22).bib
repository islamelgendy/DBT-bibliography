@inproceedings{10.1145/3366428.3380768,
author = {Peng, Chao and Rajan, Ajitha},
title = {Automated Test Generation for OpenCL Kernels Using Fuzzing and Constraint Solving},
year = {2020},
isbn = {9781450370257},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3366428.3380768},
doi = {10.1145/3366428.3380768},
abstract = {Graphics Processing Units (GPUs) are massively parallel processors offering performance acceleration and energy efficiency unmatched by current processors (CPUs) in computers. These advantages along with recent advances in the programmability of GPUs have made them attractive for general-purpose computations. Despite the advances in programmability, GPU kernels are hard to code and analyse due to the high complexity of memory sharing patterns, striding patterns for memory accesses, implicit synchronisation, and combinatorial explosion of thread interleavings. Existing few techniques for testing GPU kernels use symbolic execution for test generation that incur a high overhead, have limited scalability and do not handle all data types.We propose a test generation technique for OpenCL kernels that combines mutation-based fuzzing and selective constraint solving with the goal of being fast, effective and scalable. Fuzz testing for GPU kernels has not been explored previously. Our approach for fuzz testing randomly mutates input kernel argument values with the goal of increasing branch coverage. When fuzz testing is unable to increase branch coverage with random mutations, we gather path constraints for uncovered branch conditions and invoke the Z3 constraint solver to generate tests for them.In addition to the test generator, we also present a schedule amplifier that simulates multiple work-group schedules, with which to execute each of the generated tests. The schedule amplifier is designed to help uncover inter work-group data races. We evaluate the effectiveness of the generated tests and schedule amplifier using 217 kernels from open source projects and industry standard benchmark suites measuring branch coverage and fault finding. We find our test generation technique achieves close to 100% coverage and mutation score for majority of the kernels. Overhead incurred in test generation is small (average of 0.8 seconds). We also confirmed our technique scales easily to large kernels, and can support all OpenCL data types, including complex data structures.},
booktitle = {Proceedings of the 13th Annual Workshop on General Purpose Processing Using Graphics Processing Unit},
pages = {61–70},
numpages = {10},
keywords = {software testing, test case generation, fuzz testing, data race, OpenCL, constraint solving, GPU},
location = {San Diego, California},
series = {GPGPU '20}
}

@inproceedings{10.1145/3387940.3392265,
author = {Devroey, Xavier and Panichella, Sebastiano and Gambi, Alessio},
title = {Java Unit Testing Tool Competition: Eighth Round},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392265},
doi = {10.1145/3387940.3392265},
abstract = {We report on the results of the eighth edition of the Java unit testing tool competition. This year, two tools, EvoSuite and Randoop, were executed on a benchmark with (i) new classes under test, selected from open-source software projects, and (ii) the set of classes from one project considered in the previous edition. We relied on an updated infrastructure for the execution of the different tools and the subsequent coverage and mutation analysis based on Docker containers. We considered two different time budgets for test case generation: one an three minutes. This paper describes our methodology and statistical analysis of the results, presents the results achieved by the contestant tools and highlights the challenges we faced during the competition.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {545–548},
numpages = {4},
keywords = {benchmark, JUnit, software testing, tool competition, test case generation, Java, unit testing},
location = {Seoul, Republic of Korea},
series = {ICSEW'20}
}

@article{10.1145/3363824,
author = {Godefroid, Patrice},
title = {Fuzzing: Hack, Art, and Science},
year = {2020},
issue_date = {February 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/3363824},
doi = {10.1145/3363824},
abstract = {Reviewing software testing techniques for finding security vulnerabilities.},
journal = {Commun. ACM},
month = {jan},
pages = {70–76},
numpages = {7}
}

@inproceedings{10.1109/ASE.2019.00037,
author = {Chen, Junjie and Wang, Guancheng and Hao, Dan and Xiong, Yingfei and Zhang, Hongyu and Zhang, Lu},
title = {History-Guided Configuration Diversification for Compiler Test-Program Generation},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00037},
doi = {10.1109/ASE.2019.00037},
abstract = {Compilers, like other software systems, contain bugs, and compiler testing is the most widely-used way to assure compiler quality. A critical task of compiler testing is to generate test programs that could effectively and efficiently discover bugs. Though we can configure test generators such as Csmith to control the features of the generated programs, it is not clear what test configuration is effective. In particular, an effective test configuration needs to generate test programs that are bug-revealing, i.e., likely to trigger bugs, and diverse, i.e., able to discover different types of bugs. It is not easy to satisfy both properties. In this paper, we propose a novel test-program generation approach, called HiCOND, which utilizes historical data for configuration diversification to solve this challenge. HiCOND first infers the range for each option in a test configuration where bug-revealing test programs are more likely to be generated based on historical data. Then, it identifies a set of test configurations that can lead to diverse test programs through a search method (particle swarm optimization). Finally, based on the set of test configurations for compiler testing, HiCOND generates test programs, which are likely to be bug-revealing and diverse. We have conducted experiments on two popular compilers GCC and LLVM, and the results confirm the effectiveness of our approach. For example, HiCOND detects 75.00%, 133.33%, and 145.00% more bugs than the three existing approaches, respectively. Moreover, HiCOND has been successfully applied to actual compiler testing in a global IT company and detected 11 bugs during the practical evaluation.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {305–316},
numpages = {12},
keywords = {history, search, compiler testing, configuration},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/2818639,
author = {Mcminn, Phil and Wright, Chris J. and Kapfhammer, Gregory M.},
title = {The Effectiveness of Test Coverage Criteria for Relational Database Schema Integrity Constraints},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2818639},
doi = {10.1145/2818639},
abstract = {Despite industry advice to the contrary, there has been little work that has sought to test that a relational database's schema has correctly specified integrity constraints. These critically important constraints ensure the coherence of data in a database, defending it from manipulations that could violate requirements such as “usernames must be unique” or “the host name cannot be missing or unknown.” This article is the first to propose coverage criteria, derived from logic coverage criteria, that establish different levels of testing for the formulation of integrity constraints in a database schema. These range from simple criteria that mandate the testing of successful and unsuccessful INSERT statements into tables to more advanced criteria that test the formulation of complex integrity constraints such as multi-column PRIMARY KEYs and arbitrary CHECK constraints. Due to different vendor interpretations of the structured query language (SQL) specification with regard to how integrity constraints should actually function in practice, our criteria crucially account for the underlying semantics of the database management system (DBMS). After formally defining these coverage criteria and relating them in a subsumption hierarchy, we present two approaches for automatically generating tests that satisfy the criteria. We then describe the results of an empirical study that uses mutation analysis to investigate the fault-finding capability of data generated when our coverage criteria are applied to a wide variety of relational schemas hosted by three well-known and representative DBMSs—HyperSQL, PostgreSQL, and SQLite. In addition to revealing the complementary fault-finding capabilities of the presented criteria, the results show that mutation scores range from as low as just 12% of mutants being killed with the simplest of criteria to 96% with the most advanced.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {8},
numpages = {49},
keywords = {schema testing, search-based software engineering, mutation analysis, relational database schemas, automatic test data generation, integrity constraints, coverage criteria, Software testing}
}

@inproceedings{10.5555/2755753.2755873,
author = {H\"{o}ller, Andrea and Kajtazovic, Nermin and Rauter, Tobias and R\"{o}mer, Kay and Kreiner, Christian},
title = {Evaluation of Diverse Compiling for Software-Fault Detection},
year = {2015},
isbn = {9783981537048},
publisher = {EDA Consortium},
address = {San Jose, CA, USA},
abstract = {Although software fault prevention techniques improve continually, faults remain in every complex software system. Thus safety-critical embedded systems need mechanisms to tolerate software faults. Typically, these systems use static redundancy to detect hardware faults during operation. However, the reliability of a redundant system not only depends on the reliability of each version, but also on the dissimilarity between them. Thus, researchers have investigated ways to automatically add cost-efficient diversity to software to increase the efficiency of redundancy strategies. One of these automated software diversification methods is diverse compiling, which exploits the diversity introduced by different compilers and different optimization flags. Today, diverse compiling is used to improve the hardware fault tolerance and to avoid common defects from compilers.However, in this paper we show that diverse compiling also enhances the software fault tolerance by increasing the chance of finding defects in the source code of the executed software during runtime. More precisely, the memory is organized differently, when using different compilers and compiler flags. This enhances the chance of detecting memory-related software bugs, such as missing memory initialization, during runtime. Here we experimentally quantify the efficiency of diverse compiling for software fault tolerance and we show that diverse compiling can help to detect up to about 70% of memory-related software bugs.},
booktitle = {Proceedings of the 2015 Design, Automation &amp; Test in Europe Conference &amp; Exhibition},
pages = {531–536},
numpages = {6},
location = {Grenoble, France},
series = {DATE '15}
}

@inproceedings{10.1145/3214239.3214243,
author = {Sevilla, Michael A. and Maltzahn, Carlos},
title = {Popper Pitfalls: Experiences Following a Reproducibility Convention},
year = {2018},
isbn = {9781450358613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3214239.3214243},
doi = {10.1145/3214239.3214243},
abstract = {We describe the four publications we have tried to make reproducible and discuss how each paper has changed our workflows, practices, and collaboration policies. The fundamental insight is that paper artifacts must be made reproducible from the start of the project; artifacts are too difficult to make reproducible when the papers are (1) already published and (2) authored by researchers that are not thinking about reproducibility. In this paper, we present the best practices adopted by our research laboratory, which was sculpted by the pitfalls we have identified for the Popper convention. We conclude with a "call-to-arms" for the community focused on enhancing reproducibility initiatives for academic conferences, industry environments, and national laboratories. We hope that our experiences will shape a best practices guide for future reproducible papers.},
booktitle = {Proceedings of the First International Workshop on Practical Reproducible Evaluation of Computer Systems},
articleno = {4},
numpages = {5},
keywords = {Software Testing, Performance Engineering},
location = {Tempe, AZ, USA},
series = {P-RECS'18}
}

@inproceedings{10.1145/3324884.3416643,
author = {Derakhshanfar, Pouria and Devroey, Xavier and Zaidman, Andy and van Deursen, Arie and Panichella, Annibale},
title = {Good Things Come in Threes: Improving Search-Based Crash Reproduction with Helper Objectives},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416643},
doi = {10.1145/3324884.3416643},
abstract = {Writing a test case reproducing a reported software crash is a common practice to identify the root cause of an anomaly in the software under test. However, this task is usually labor-intensive and time-taking. Hence, evolutionary intelligence approaches have been successfully applied to assist developers during debugging by generating a test case reproducing reported crashes. These approaches use a single fitness function called Crash Distance to guide the search process toward reproducing a target crash. Despite the reported achievements, these approaches do not always successfully reproduce some crashes due to a lack of test diversity (premature convergence). In this study, we introduce a new approach, called MO-HO, that addresses this issue via multi-objectivization. In particular, we introduce two new Helper-Objectives for crash reproduction, namely test length (to minimize) and method sequence diversity (to maximize), in addition to Crash Distance. We assessed MO-HO using five multi-objective evolutionary algorithms (NSGA-II, SPEA2, PESA-II, MOEA/D, FEMO) on 124 non-trivial crashes stemming from open-source projects. Our results indicate that SPEA2 is the best-performing multi-objective algorithm for MO-HO. We evaluated this best-performing algorithm for MO-HO against the state-of-the-art: single-objective approach (Single-Objective Search) and decomposition-based multi-objectivization approach (De-MO). Our results show that MO-HO reproduces five crashes that cannot be reproduced by the current state-of-the-art. Besides, MO-HO improves the effectiveness (+10% and +8% in reproduction ratio) and the efficiency in 34.6% and 36% of crashes (i.e., significantly lower running time) compared to Single-Objective Search and De-MO, respectively. For some crashes, the improvements are very large, being up to +93.3% for reproduction ratio and -92% for the required running time.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {211–223},
numpages = {13},
keywords = {multi-objective evolutionary algorithms, search-based software testing, crash reproduction},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/1555860.1555862,
author = {Masri, Wes and Abou-Assi, Rawad and El-Ghali, Marwa and Al-Fatairi, Nour},
title = {An Empirical Study of the Factors That Reduce the Effectiveness of Coverage-Based Fault Localization},
year = {2009},
isbn = {9781605586540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555860.1555862},
doi = {10.1145/1555860.1555862},
abstract = {Coverage-based fault localization techniques typically assign a suspiciousness rank to the statements in a program following an analysis of the coverage of certain types of program elements by the failing and passing runs. The effectiveness of existing techniques has been limited despite the fact that researchers have explored various suspiciousness metrics, ranking strategies, and types of program elements. This work aims at identifying the factors that impair coverage-based fault localization. Specifically, we conducted an empirical study in which we assessed the prevalence of the following scenarios: 1) the condition for failure is met but the program does not fail; 2) the faulty statement is executed but the program does not fail; 3) the failure is correlated with a combination of more than one program element possibly of different types; 4) a large number of program elements occurred in all failing runs but in no passing runs.The study was conducted using 148 seeded versions of ten Java programs which included three releases of NanoXML, and seven programs from the Siemens test suite that were converted to Java. The results showed that most of the above scenarios occur frequently.},
booktitle = {Proceedings of the 2nd International Workshop on Defects in Large Software Systems: Held in Conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2009)},
pages = {1–5},
numpages = {5},
keywords = {coincidental correctness, genetic algorithm, combinations of program elements, coverage-based fault localization, Siemens test suite for Java},
location = {Chicago, Illinois},
series = {DEFECTS '09}
}

@inproceedings{10.5555/1769168.1769175,
author = {Chan, W. K. and Chen, T. Y. and Cheung, S. C. and Tse, T. H. and Zhang, Zhenyu},
title = {Towards the Testing of Power-Aware Software Applications for Wireless Sensor Networks},
year = {2007},
isbn = {9783540732297},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {The testing of programs in wireless sensor networks (WSN) is an important means to assure quality but is a challenging process. As pervasive computing has been identified as a notable trend in computing, investigations on effective software testing techniques for WSN are essential. In particular, energy is a crucial and scarce resource in WSN nodes. Programs running correctly but failing to meet the energy constraintsmay still be problematic. As such, testing techniques for power-aware applications are useful; otherwise, the quickly depleted device batteries will need frequent replacements, hence challenging the effectiveness of automation. Since current testing techniques do not consider the issue of energy constraints, their automation in the WSN domain warrants further investigation.This paper proposes a novel power-aware technique built on top of the notion of metamorphic testing to alleviate both the test oracle issue and the power-awareness issue. It tests the functions of programs in WSN nodes that are in close proximity, and uses the data consolidation criteria of data aggregation in programs as the basis for verifying test results. The power-aware transmissions of intermediate and final test data as well as the computation required for verification of test results are directly supported by the WSN programs. Our proposed technique has been strategically designed to blend in with the special features of the WSN environment.},
booktitle = {Proceedings of the 12th International Conference on Reliable Software Technologies},
pages = {84–99},
numpages = {16},
keywords = {WSN application, test automation, metamorphic testing, test oracle, software testing, power awareness, wireless sensor network},
location = {Geneva, Switzerland},
series = {Ada-Europe'07}
}

@inproceedings{10.1109/ICSE.2019.00115,
author = {Jabbarvand, Reyhaneh and Lin, Jun-Wei and Malek, Sam},
title = {Search-Based Energy Testing of Android},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00115},
doi = {10.1109/ICSE.2019.00115},
abstract = {The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efficiently use the device's battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app's energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present Cobweb, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app's energy behavior, Cobweb generates a test suite that can effectively find energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efficiently test energy behavior of apps, but also its superiority over prior techniques by finding a wider and more diverse set of energy defects.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {1119–1130},
numpages = {12},
keywords = {software testing, energy testing, Android},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@article{10.1145/2347696.2347707,
author = {Mahajan, Manish and Kumar, Sumit and Porwal, Rabins},
title = {Applying Genetic Algorithm to Increase the Efficiency of a Data Flow-Based Test Data Generation Approach},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2347696.2347707},
doi = {10.1145/2347696.2347707},
abstract = {The success or failure of the entire software development process relies on the software testing component which is responsible for ensuring that the software that is released is free from bugs. One of the major labor intensive activities of software testing is the generation of the test data for the purpose of applying the testing methodologies. Many approaches have been tried and tested for automating the process of generating the test data. Meta-heuristics have been applied extensively for improving the efficiency of the process. This paper analyses the effectiveness of applying genetic algorithms for generating test data automatically using data flow testing approach. An incremental coverage measurement method is used to improve the convergence.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–5},
numpages = {5},
keywords = {def-use, testing, data flow, genetic algorithms, coverage criteria}
}

@inbook{10.1145/3313831.3376835,
author = {Chen, Yan and Pandey, Maulishree and Song, Jean Y. and Lasecki, Walter S. and Oney, Steve},
title = {Improving Crowd-Supported GUI Testing with Structural Guidance},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376835},
abstract = {Crowd testing is an emerging practice in Graphical User Interface (GUI) testing, where developers recruit a large number of crowd testers to test GUI features. It is often easier and faster than a dedicated quality assurance team, and its output is more realistic than that of automated testing. However, crowds of testers working in parallel tend to focus on a small set of commonly-used User Interface (UI) navigation paths, which can lead to low test coverage and redundant effort. In this paper, we introduce two techniques to increase crowd testers' coverage: interactive event-flow graphs and GUI-level guidance. The interactive event-flow graphs track and aggregate every tester's interactions into a single directed graph that visualizes the cases that have already been explored. Crowd testers can interact with the graphs to find new navigation paths and increase the coverage of the created tests. We also use the graphs to augment the GUI (GUI-level guidance) to help testers avoid only exploring common paths. Our evaluation with 30 crowd testers on 11 different test pages shows that the techniques can help testers avoid redundant effort while also increasing untrained testers' coverage by 55%. These techniques can help us develop more robust software that works in more mission-critical settings not only by performing more thorough testing with the same effort that has been put in before but also by integrating them into different parts of the development pipeline to make more reliable software in the early development stage.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13}
}

@inproceedings{10.1145/3127005.3127008,
author = {Coppola, Riccardo and Morisio, Maurizio and Torchiano, Marco},
title = {Scripted GUI Testing of Android Apps: A Study on Diffusion, Evolution and Fragility},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127008},
doi = {10.1145/3127005.3127008},
abstract = {Background. Evidence suggests that mobile applications are not thoroughly tested as their desktop counterparts. In particular GUI testing is generally limited. Like web-based applications, mobile apps suffer from GUI test fragility, i.e. GUI test classes failing due to minor modifications in the GUI, without the application functionalities being altered.Aims. The objective of our study is to examine the diffusion of GUI testing on Android, and the amount of changes required to keep test classes up to date, and in particular the changes due to GUI test fragility. We define metrics to characterize the modifications and evolution of test classes and test methods, and proxies to estimate fragility-induced changes.Method. To perform our experiments, we selected six widely used open-source tools for scripted GUI testing of mobile applications previously described in the literature. We have mined the repositories on GitHub that used those tools, and computed our set of metrics.Results. We found that none of the considered GUI testing frameworks achieved a major diffusion among the open-source Android projects available on GitHub. For projects with GUI tests, we found that test suites have to be modified often, specifically 5%--10% of developers' modified LOCs belong to tests, and that a relevant portion (60% on average) of such modifications are induced by fragility.Conclusions. Fragility of GUI test classes constitute a relevant concern, possibly being an obstacle for developers to adopt automated scripted GUI tests. This first evaluation and measure of fragility of Android scripted GUI testing can constitute a benchmark for developers, and the basis for the definition of a taxonomy of fragility causes, and actionable guidelines to mitigate the issue.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {22–32},
numpages = {11},
keywords = {Software Maintenance, Software Evolution, Mobile Development, GUI Testing, Automated Software Testing},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1145/1555860.1555864,
author = {Ayewah, Nathaniel and Pugh, William},
title = {Using Checklists to Review Static Analysis Warnings},
year = {2009},
isbn = {9781605586540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1555860.1555864},
doi = {10.1145/1555860.1555864},
abstract = {Static analysis tools find silly mistakes, confusing code, bad practices and property violations. But software developers and organizations may or may not care about all these warnings, depending on how they impact code behavior and other factors. In the past, we have tried to identify important warnings by asking users to rate them as severe, low impact or not a bug. In this paper, we observe that the user's rating may be more complicated depending on whether the warning is feasible, changes code behavior, occurs in deployed code and other factors. To better model this, we ask users to review warnings using a checklist which enables more detailed reviews. We find that reviews are consistent across users and across checklist questions, though some users may disagree about whether to fix or filter out certain bug classes.},
booktitle = {Proceedings of the 2nd International Workshop on Defects in Large Software Systems: Held in Conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2009)},
pages = {11–15},
numpages = {5},
keywords = {false positives, software quality, bugs, static analysis, software defects, FindBugs, bug patterns, Java},
location = {Chicago, Illinois},
series = {DEFECTS '09}
}

@inproceedings{10.5555/563801.563826,
author = {Randolph, Nick and Morris, John and Lee, Gareth},
title = {A Generalised Spreadsheet Verification Methodology},
year = {2002},
isbn = {0909925828},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {Although spreadsheets have been around for over thirty years, we are only just realising their importance. Most companies use spreadsheets in their decision-making processes, but rarely employ any form of testing. This paper shows how an "all-uses" test adequacy technique can be integrated into Microsoft's Excel. The modular technique adopted makes the implementation spreadsheet package independent. It also includes a user interface, to assist developers specify test cases and a technique for recording test cases and session information. In particular it presents a systematic technique for constructing test cases. As a key problem with spreadsheet development is the inexperience of developers, this paper describes an easy to use tool that will improve the standard of spreadsheets developed.},
booktitle = {Proceedings of the Twenty-Fifth Australasian Conference on Computer Science - Volume 4},
pages = {215–222},
numpages = {8},
keywords = {errors, software testing, verification, spreadsheets},
location = {Melbourne, Victoria, Australia},
series = {ACSC '02}
}

@inproceedings{10.1145/1401827.1401833,
author = {Groce, Alex and Joshi, Rajeev},
title = {Random Testing and Model Checking: Building a Common Framework for Nondeterministic Exploration},
year = {2008},
isbn = {9781605580548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1401827.1401833},
doi = {10.1145/1401827.1401833},
abstract = {Two popular forms of dynamic analysis, random testing and explicit-state software model checking, are perhaps best viewed as search strategies for exploring the state spaces introduced by nondeterminism in program inputs. We present an approach that enables this nondeterminism to be expressed in the SPIN model checker's PROMELA language, and then lets users generate either model checkers or random testers from a single harness for a tested C program. Our approach makes it easy to compare model checking and random testing for models with precisely the same input ranges and probabilities and allows us to mix random testing with model checking's exhaustive exploration of non-determinism. The PROMELA language, as intended in its design, serves as a convenient notation for expressing nondeterminism and mixing random choices with nondeterministic choices. We present and discuss a comparison of random testing and model checking. The results derive from using our framework to test a C program with an effectively infinite state space, a module in JPL's next Mars rover mission. More generally, we show how the ability of the SPIN model checker to call C code can be used to extend SPIN's features, and hope to inspire others to use the same methods to implement dynamic analyses that can make use of efficient state storage, matching, and backtracking.},
booktitle = {Proceedings of the 2008 International Workshop on Dynamic Analysis: Held in Conjunction with the ACM SIGSOFT International Symposium on Software Testing and Analysis (ISSTA 2008)},
pages = {22–28},
numpages = {7},
keywords = {test frameworks, random testing, dynamic analysis, model checking},
location = {Seattle, Washington},
series = {WODA '08}
}

@inproceedings{10.1145/1414004.1414060,
author = {Guo, Yuepu and Sampath, Sreedevi},
title = {Web Application Fault Classification - an Exploratory Study},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414060},
doi = {10.1145/1414004.1414060},
abstract = {Controlled experiments in web application testing use seeded faults to evaluate the effectiveness of the testing technique. However, the classes of seeded faults are not always experimentally supported by real-world fault data. In this paper, we conduct an exploratory study on two large open source web systems to identify a fault classification that is representative of and supported by real world faults. Through our study we provide support to several categories of an existing web application fault classification, and identify one new fault category and six new sub-categories. Researchers and experimenters will find the proposed fault classification useful when evaluating techniques for testing web applications.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {303–305},
numpages = {3},
keywords = {web applications, software testing, fault classification},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.1109/ICSE.2019.00082,
author = {Choi, Jaeseung and Jang, Joonun and Han, Choongwoo and Cha, Sang Kil},
title = {Grey-Box Concolic Testing on Binary Code},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00082},
doi = {10.1109/ICSE.2019.00082},
abstract = {We present grey-box concolic testing, a novel path-based test case generation method that combines the best of both white-box and grey-box fuzzing. At a high level, our technique systematically explores execution paths of a program under test as in white-box fuzzing, a.k.a. concolic testing, while not giving up the simplicity of grey-box fuzzing: it only uses a lightweight instrumentation, and it does not rely on an SMT solver. We implemented our technique in a system called Eclipser, and compared it to the state-of-the-art grey-box fuzzers (including AFLFast, LAF-intel, Steelix, and VUzzer) as well as a symbolic executor (KLEE). In our experiments, we achieved higher code coverage and found more bugs than the other tools.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {736–747},
numpages = {12},
keywords = {fuzzing, concolic testing, software testing},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/3458817.3476147,
author = {Rahman, Md Hasanur and Shamji, Aabid and Guo, Shengjian and Li, Guanpeng},
title = {PEPPA-X: Finding Program Test Inputs to Bound Silent Data Corruption Vulnerability in HPC Applications},
year = {2021},
isbn = {9781450384421},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3458817.3476147},
doi = {10.1145/3458817.3476147},
abstract = {Transient hardware faults have become prevalent due to the shrinking size of transistors, leading to silent data corruptions (SDCs). Therefore, HPC applications need to be evaluated (e.g., via fault injections) and protected to meet the reliability target. In the evaluation, the target programs exercise with a set of given inputs which are usually from program benchmark suite. However, these inputs rarely manifest the SDC vulnerabilities, leading to over-optimistic assessment and unexpectedly higher failure rates in production. We propose Peppa-X, which efficiently identifies the test inputs that estimate the bound of program SDC resiliency. Our key insight is that the SDC sensitivity distribution in a program often remains stationary across input space. Thereby, we can guide the search of SDC-bound inputs by a sampled distribution. Our evaluation shows that Peppa-X can identify the SDC-bound input of a program that existing methods cannot find even with 5x more search time.},
booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
articleno = {80},
numpages = {13},
keywords = {error resilience, program analysis, high performance computing, silent data corruption, fault injection, error propagation, software testing, input fuzzing},
location = {St. Louis, Missouri},
series = {SC '21}
}

