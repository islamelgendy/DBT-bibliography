@article{10.1145/1409360.1409381,
author = {Rinard, Martin C.},
title = {Technical Perspective<br><br>Patching Program Errors},
year = {2008},
issue_date = {December 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {12},
issn = {0001-0782},
url = {https://doi.org/10.1145/1409360.1409381},
doi = {10.1145/1409360.1409381},
journal = {Commun. ACM},
month = {dec},
pages = {86},
numpages = {1}
}

@inproceedings{10.1109/ASE.2011.6100092,
author = {Malburg, Jan and Fraser, Gordon},
title = {Combining Search-Based and Constraint-Based Testing},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100092},
doi = {10.1109/ASE.2011.6100092},
abstract = {Many modern automated test generators are based on either meta-heuristic search techniques or use constraint solvers. Both approaches have their advantages, but they also have specific drawbacks: Search-based methods get stuck in local optima and degrade when the search landscape offers no guidance; constraint-based approaches, on the other hand, can only handle certain domains efficiently. In this paper we describe a method that integrates both techniques and delivers the best of both worlds. On a high-level view, our method uses a genetic algorithm to generate tests, but the twist is that during evolution a constraint solver is used to ensure that mutated offspring efficiently explores different control flow. Experiments on 20 case study examples show that on average the combination improves branch coverage by 28% over search-based techniques and by 13% over constraint-based techniques.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {436–439},
numpages = {4},
series = {ASE '11}
}

@inproceedings{10.1145/1882291.1882330,
author = {Xu, Zhihong and Kim, Yunho and Kim, Moonzoo and Rothermel, Gregg and Cohen, Myra B.},
title = {Directed Test Suite Augmentation: Techniques and Tradeoffs},
year = {2010},
isbn = {9781605587912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1882291.1882330},
doi = {10.1145/1882291.1882330},
abstract = {Test suite augmentation techniques are used in regression testing to identify code elements affected by changes and to generate test cases to cover those elements. Our preliminary work suggests that several factors influence the cost and effectiveness of test suite augmentation techniques. These include the order in which affected elements are considered while generating test cases, the manner in which existing regression test cases and newly generated test cases are used, and the algorithm used to generate test cases. In this work, we present the results of an empirical study examining these factors, considering two test case generation algorithms (concolic and genetic). The results of our experiment show that the primary factor affecting augmentation is the test case generation algorithm utilized; this affects both cost and effectiveness. The manner in which existing and newly generated test cases are utilized also has a substantial effect on efficiency but a lesser effect on effectiveness. The order in which affected elements are considered turns out to have relatively few effects when using concolic test case generation, but more substantial effects when using genetic test case generation.},
booktitle = {Proceedings of the Eighteenth ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {257–266},
numpages = {10},
keywords = {genetic algorithms, empirical studies, test suite augmentation, concolic testing, regression testing},
location = {Santa Fe, New Mexico, USA},
series = {FSE '10}
}

@inproceedings{10.5555/1883978.1883982,
author = {Monperrus, Martin and Bruch, Marcel and Mezini, Mira},
title = {Detecting Missing Method Calls in Object-Oriented Software},
year = {2010},
isbn = {3642141064},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
abstract = {When using object-oriented frameworks it is easy to overlook certain important method calls that are required at particular places in code. In this paper, we provide a comprehensive set of empirical facts on this problem, starting from traces of missing method calls in a bug repository. We propose a new system, which automatically detects them during both software development and quality assurance phases. The evaluation shows that it has a low false positive rate (&lt;5%) and that it is able to find missing method calls in the source code of the Eclipse IDE.},
booktitle = {Proceedings of the 24th European Conference on Object-Oriented Programming},
pages = {2–25},
numpages = {24},
location = {Maribor, Slovenia},
series = {ECOOP'10}
}

@inproceedings{10.1145/3236024.3236048,
author = {Yi, Qiuping and Huang, Jeff},
title = {Concurrency Verification with Maximal Path Causality},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236048},
doi = {10.1145/3236024.3236048},
abstract = {We present a technique that systematically explores the state spaces of concurrent programs across both the schedule space and the input space. The cornerstone is a new model called Maximal Path Causality (MPC), which captures all combinations of thread schedules and program inputs that reach the same path as one equivalency class, and generates a unique schedule+input combination to explore each path. Moreover, the exploration for different paths can be easily parallelized. Our extensive evaluation on both popular concurrency benchmarks and real-world C/C++ applications shows that MPC significantly improves the performance of existing techniques.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {366–376},
numpages = {11},
keywords = {Concurrency, Verification, Dynamic Symbolic Execution},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/ASE.2015.89,
author = {Choudhary, Shauvik Roy and Gorla, Alessandra and Orso, Alessandro},
title = {Automated Test Input Generation for Android: Are We There Yet?},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.89},
doi = {10.1109/ASE.2015.89},
abstract = {Like all software, mobile applications ("apps") must be adequately tested to gain confidence that they behave correctly. Therefore, in recent years, researchers and practitioners alike have begun to investigate ways to automate apps testing. In particular, because of Android's open source nature and its large share of the market, a great deal of research has been performed on input generation techniques for apps that run on the Android operating systems. At this point in time, there are in fact a number of such techniques in the literature, which differ in the way they generate inputs, the strategy they use to explore the behavior of the app under test, and the specific heuristics they use. To better understand the strengths and weaknesses of these existing approaches, and get general insight on ways they could be made more effective, in this paper we perform a thorough comparison of the main existing test input generation tools for Android. In our comparison, we evaluate the effectiveness of these tools, and their corresponding techniques, according to four metrics: ease of use, ability to work on multiple platforms, code coverage, and ability to detect faults. Our results provide a clear picture of the state of the art in input generation for Android apps and identify future research directions that, if suitably investigated, could lead to more effective and efficient testing tools for Android.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {429–440},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/2695664.2695782,
author = {Vieira, Vaninha and Holl, Konstantin and Hassel, Michael},
title = {A Context Simulator as Testing Support for Mobile Apps},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695782},
doi = {10.1145/2695664.2695782},
abstract = {Context-aware mobile applications gain more and more influence on our daily life. Since mobile devices are equipped with various sensors to detect their environment, it is possible to receive and process information from beyond application and device borders. Within the development, these context-aware applications have to be verified to assure that they do not cause any failures. This contribution outlines challenges of testing context-aware mobile applications relating to their context factors and present our approach for a context simulator that provides support for modeling and simulation of context in different levels: physical and logical context, situations and scenarios. The simulator supports test case derivation and enables test case execution for several context sources as part of testing mobile applications.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {535–541},
numpages = {7},
keywords = {mobile application testing, context-awareness, context modeling, context simulation, quality assurance},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/2814270.2814319,
author = {Le, Vu and Sun, Chengnian and Su, Zhendong},
title = {Finding Deep Compiler Bugs via Guided Stochastic Program Mutation},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814319},
doi = {10.1145/2814270.2814319},
abstract = { Compiler testing is important and challenging. Equivalence Modulo Inputs (EMI) is a recent promising approach for compiler validation. It is based on mutating the unexecuted statements of an existing program under some inputs to produce new equivalent test programs w.r.t. these inputs. Orion is a simple realization of EMI by only randomly deleting unexecuted statements. Despite its success in finding many bugs in production compilers, Orion’s effectiveness is still limited by its simple, blind mutation strategy. To more effectively realize EMI, this paper introduces a guided, advanced mutation strategy based on Bayesian optimization. Our goal is to generate diverse programs to more thoroughly exercise compilers. We achieve this with two techniques: (1) the support of both code deletions and insertions in the unexecuted regions, leading to a much larger test program space; and (2) the use of an objective function that promotes control-flow-diverse programs for guiding Markov Chain Monte Carlo (MCMC) optimization to explore the search space. Our technique helps discover deep bugs that require elaborate mutations. Our realization, Athena, targets C compilers. In 19 months, Athena has found 72 new bugs — many of which are deep and important bugs — in GCC and LLVM. Developers have confirmed all 72 bugs and fixed 68 of them. },
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {386–399},
numpages = {14},
keywords = {Compiler testing, Markov Chain Monte Carlo, automated testing, equivalent program variants},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814319,
author = {Le, Vu and Sun, Chengnian and Su, Zhendong},
title = {Finding Deep Compiler Bugs via Guided Stochastic Program Mutation},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814319},
doi = {10.1145/2858965.2814319},
abstract = { Compiler testing is important and challenging. Equivalence Modulo Inputs (EMI) is a recent promising approach for compiler validation. It is based on mutating the unexecuted statements of an existing program under some inputs to produce new equivalent test programs w.r.t. these inputs. Orion is a simple realization of EMI by only randomly deleting unexecuted statements. Despite its success in finding many bugs in production compilers, Orion’s effectiveness is still limited by its simple, blind mutation strategy. To more effectively realize EMI, this paper introduces a guided, advanced mutation strategy based on Bayesian optimization. Our goal is to generate diverse programs to more thoroughly exercise compilers. We achieve this with two techniques: (1) the support of both code deletions and insertions in the unexecuted regions, leading to a much larger test program space; and (2) the use of an objective function that promotes control-flow-diverse programs for guiding Markov Chain Monte Carlo (MCMC) optimization to explore the search space. Our technique helps discover deep bugs that require elaborate mutations. Our realization, Athena, targets C compilers. In 19 months, Athena has found 72 new bugs — many of which are deep and important bugs — in GCC and LLVM. Developers have confirmed all 72 bugs and fixed 68 of them. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {386–399},
numpages = {14},
keywords = {Markov Chain Monte Carlo, Compiler testing, automated testing, equivalent program variants}
}

@inproceedings{10.1145/2330163.2330333,
author = {Wilkerson, Josh L. and Tauritz, Daniel R. and Bridges, James M.},
title = {Multi-Objective Coevolutionary Automated Software Correction},
year = {2012},
isbn = {9781450311779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330163.2330333},
doi = {10.1145/2330163.2330333},
abstract = {For a given program, testing, locating the errors identified, and correcting those errors is a critical, yet expensive process. The field of Search Based Software Engineering (SBSE) addresses these phases by formulating them as search problems. The Coevolutionary Automated Software Correction (CASC) system targets the correction phase by coevolving test cases and programs at the source code level. This paper presents the latest version of the CASC system featuring multi-objective optimization and an enhanced representation language. Results are presented demonstrating CASC's ability to successfully correct five seeded bugs in two non-trivial programs from the Siemens test suite. Additionally, evidence is provided substantiating the hypothesis that multi-objective optimization is beneficial to SBSE.},
booktitle = {Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation},
pages = {1229–1236},
numpages = {8},
keywords = {sbse, nsga-ii, multi-objective optimization, coevolution, genetic programming, automated program correction, fitness sharing},
location = {Philadelphia, Pennsylvania, USA},
series = {GECCO '12}
}

@inproceedings{10.1145/2901739.2901747,
author = {G\'{o}mez, Mar\'{\i}a and Rouvoy, Romain and Adams, Bram and Seinturier, Lionel},
title = {Mining Test Repositories for Automatic Detection of UI Performance Regressions in Android Apps},
year = {2016},
isbn = {9781450341868},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2901739.2901747},
doi = {10.1145/2901739.2901747},
abstract = {The reputation of a mobile app vendor is crucial to survive amongst the ever increasing competition. However this reputation largely depends on the quality of the apps, both functional and non-functional. One major non-functional requirement of mobile apps is to guarantee smooth UI interactions, since choppy scrolling or navigation caused by performance problems on a mobile device's limited hardware resources, is highly annoying for end-users. The main research challenge of automatically identifying UI performance problems on mobile devices is that the performance of an app highly varies depending on its context---i.e., the hardware and software configurations on which it runs.This paper presents Dune, an approach to automatically detect UI performance degradations in Android apps while taking into account context differences. First, Dune builds an ensemble model of the UI performance metrics of an app from a repository of historical test runs that are known to be acceptable, for different configurations of context. Then, Dune uses this model to flag UI performance deviations (regressions and optimizations) in new test runs. We empirically evaluate Dune on real UI performance defects reported in two Android apps, and one manually injected defect in a third app. We demonstrate that this toolset can be successfully used to spot UI performance regressions at a fine granularity.},
booktitle = {Proceedings of the 13th International Conference on Mining Software Repositories},
pages = {13–24},
numpages = {12},
location = {Austin, Texas},
series = {MSR '16}
}

@inproceedings{10.5555/3370272.3370293,
author = {Peruma, Anthony and Almalki, Khalid and Newman, Christian D. and Mkaouer, Mohamed Wiem and Ouni, Ali and Palomba, Fabio},
title = {On the Distribution of Test Smells in Open Source Android Applications: An Exploratory Study},
year = {2019},
publisher = {IBM Corp.},
address = {USA},
abstract = {The impact of bad programming practices, such as code smells, in production code has been the focus of numerous studies in software engineering. Like production code, unit tests are also affected by bad programming practices which can have a negative impact on the quality and maintenance of a software system. While several studies addressed code and test smells in desktop applications, there is little knowledge of test smells in the context of mobile applications. In this study, we extend the existing catalog of test smells by identifying and defining new smells and survey over 40 developers who confirm that our proposed smells are bad programming practices in test suites. Additionally, we perform an empirical study on the occurrences and distribution of the proposed smells on 656 open-source Android applications (apps). Our findings show a widespread occurrence of test smells in apps. We also show that apps tend to exhibit test smells early in their lifetime with different degrees of co-occurrences on different smell types. This empirical study demonstrates that test smells can be used as an indicator for necessary preventive software maintenance for test suites.},
booktitle = {Proceedings of the 29th Annual International Conference on Computer Science and Software Engineering},
pages = {193–202},
numpages = {10},
keywords = {test smells, unit test, software maintenance, software quality},
location = {Toronto, Ontario, Canada},
series = {CASCON '19}
}

@inproceedings{10.1145/2889160.2889227,
author = {Park, Joonyoung and Lim, Inho and Ryu, Sukyoung},
title = {Battles with False Positives in Static Analysis of JavaScript Web Applications in the Wild},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889227},
doi = {10.1145/2889160.2889227},
abstract = {Now that HTML5 technologies are everywhere from web services to various platforms, assuring quality of web applications becomes very important. While web application developers use syntactic checkers and type-related bug detectors, extremely dynamic features and diverse execution environments of web applications make it particularly difficult to statically analyze them leading to too many false positives. Recently, researchers have developed static analyzers for JavaScript web applications addressing quirky JavaScript language semantics and browser environments, but they lack empirical studies on the practicality of such analyzers.In this paper, we collect 30 JavaScript web applications in the wild, analyze them using SAFE, the state-of-the-art JavaScript static analyzer with bug detection, and investigate false positives in the analysis results. After manually inspecting them, we classify 7 reasons that cause the false positives: W3C APIs, browser-specific APIs, JavaScript library APIs, dynamic file loading, dynamic code generation, asynchronous calls, and others. Among them, we identify 4 cases which are the sources of false positives that we can practically reduce. Rather than striving for sound analysis with unrealistic assumptions, we choose to be intentionally unsound to analyze web applications in the real world with less false positives. Our evaluation shows that the approach effectively reduces false positives in statically analyzing web applications in the wild.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {61–70},
numpages = {10},
keywords = {web applications, JavaScript, static analysis, false positives},
location = {Austin, Texas},
series = {ICSE '16}
}

@inbook{10.1145/3238147.3240464,
author = {Bugariu, Alexandra and W\"{u}stholz, Valentin and Christakis, Maria and M\"{u}ller, Peter},
title = {Automatically Testing Implementations of Numerical Abstract Domains},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240464},
abstract = {Static program analyses are routinely applied as the basis of code optimizations and to detect safety and security issues in software systems. For their results to be reliable, static analyses should be sound (i.e., should not produce false negatives) and precise (i.e., should report a low number of false positives). Even though it is possible to prove properties of the design of a static analysis, ensuring soundness and precision for its implementation is challenging. Complex algorithms and sophisticated optimizations make static analyzers difficult to implement and test. In this paper, we present an automatic technique to test, among other properties, the soundness and precision of abstract domains, the core of all static analyzers based on abstract interpretation. In order to cover a wide range of test data and input states, we construct inputs by applying sequences of abstract-domain operations to representative domain elements, and vary the operations through gray-box fuzzing. We use mathematical properties of abstract domains as test oracles. Our experimental evaluation demonstrates the effectiveness of our approach. We detected several previously unknown soundness and precision errors in widely-used abstract domains. Our experiments also show that our approach is more effective than dynamic symbolic execution and than fuzzing the test inputs directly.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {768–778},
numpages = {11}
}

@article{10.1145/3353401.3353405,
author = {Wolverton, Colleen Carraher and Cenfetelli, Ronald},
title = {An Exploration of the Drivers of Non-Adoption Behavior: A Discriminant Analysis Approach},
year = {2019},
issue_date = {August 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0095-0033},
url = {https://doi.org/10.1145/3353401.3353405},
doi = {10.1145/3353401.3353405},
abstract = {While there has been a substantial amount of attention within the information systems research community towards understanding the phenomenon of adoption, much less is known about non-adoption. This study examines the factors surrounding the decision to not adopt a technology and whether certain factors exert differing effects on individuals in particular ways such that concurrent factors could be identified to develop a classification of the specific types of non-adoption behavior. Utilizing inhibitor theory and the symbolic adoption model as a foundational framework for the different types of non-adoption, we posit that different types of non-adoption exist which is demonstrated by determining the perceptions towards technology that coalesce around different types of non-adoption. We conducted a two-phase investigation into non-adoption with two goals in mind: 1) identify and explore specific factors of the IT that are associated with the rejection decision and are distinct from the adoption decision, and 2) determine the extent to which these factors (along with traditional enablers) differentiate between different types of non-adoption. The results from a discriminant function analysis (DFA) indicate the coalescence of specific perceptual variables according to the types of non-adoption behavior, specifically, the discriminatory power of differing perceptions of IT between trial rejecters, symbolic rejecters, trial accepters, symbolic adopters, and adopters. The implications for research and implications for practice are discussed.},
journal = {SIGMIS Database},
month = {jul},
pages = {38–65},
numpages = {28},
keywords = {it rejection, discriminant analysis., non-adoption, it adoption, symbolic adoption model}
}

@article{10.1145/3428212,
author = {Sotiropoulos, Thodoris and Chaliasos, Stefanos and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {A Model for Detecting Faults in Build Specifications},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428212},
doi = {10.1145/3428212},
abstract = {Incremental and parallel builds are crucial features of modern build systems. Parallelism enables fast builds by running independent tasks simultaneously, while incrementality saves time and computing resources by processing the build operations that were affected by a particular code change. Writing build definitions that lead to error-free incremental and parallel builds is a challenging task. This is mainly because developers are often unable to predict the effects of build operations on the file system and how different build operations interact with each other. Faulty build scripts may seriously degrade the reliability of automated builds, as they cause build failures, and non-deterministic and incorrect outputs.  To reason about arbitrary build executions, we present BuildFS, a generally-applicable model that takes into account the specification (as declared in build scripts) and the actual behavior (low-level file system operation) of build operations. We then formally define different types of faults related to incremental and parallel builds in terms of the conditions under which a file system operation violates the specification of a build operation. Our testing approach, which relies on the proposed model, analyzes the execution of single full build, translates it into BuildFS, and uncovers faults by checking for corresponding violations.  We evaluate the effectiveness, efficiency, and applicability of our approach by examining 612 Make and Gradle projects. Notably, thanks to our treatment of build executions, our method is the first to handle JVM-oriented build systems. The results indicate that our approach is (1) able to uncover several important issues (247 issues found in 47 open-source projects have been confirmed and fixed by the upstream developers), and (2) much faster than a state-of-the-art tool for Make builds (the median and average speedup is 39X and 74X respectively).},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {144},
numpages = {30},
keywords = {JVM-based builds, Gradle, parallel builds, incremental builds, Make}
}

@inproceedings{10.1145/1287624.1287647,
author = {Harman, Mark and Hassoun, Youssef and Lakhotia, Kiran and McMinn, Phil and Wegener, Joachim},
title = {The Impact of Input Domain Reduction on Search-Based Test Data Generation},
year = {2007},
isbn = {9781595938114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287624.1287647},
doi = {10.1145/1287624.1287647},
abstract = {There has recently been a great deal of interest in search-based test data generation, with many local and global search algorithms being proposed. However, to date, there has been no investigation ofthe relationship between the size of the input domain (the search space) and performance of search-based algorithms. Static analysis can be used to remove irrelevant variables for a given test data generation problem, thereby reducing the search space size. This paper studies the effect of this domain reduction, presenting results from the application of local and global search algorithms to real world examples. This provides evidence to support the claimthat domain reduction has implications for practical search-based test data generation.},
booktitle = {Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {155–164},
numpages = {10},
keywords = {evolutionary testing, hill climbing, automated test data generation, search space reduction, genetic algorithms, input domain reduction},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE '07}
}

@inbook{10.1145/3387940.3392182,
author = {Asad, Moumita and Ganguly, Kishan Kumar and Sakib, Kazi},
title = {Impact of Similarity on Repairing Small Programs: A Case Study on QuixBugs Benchmark},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3392182},
abstract = {Similarity analysis plays an important role in automated program repair by finding the correct solution earlier. However, the effectiveness of similarity is mostly validated using common benchmark Defects4J which consists of 6 large projects. To mitigate the threat of generalizability, this study examines the performance of similarity in repairing small programs. For this purpose, existing syntactic and semantic similarity based approaches, as well as a new technique of combining both similarities, are used. These approaches are evaluated using QuixBugs, a dataset of diverse type bugs from 40 small programs. These techniques fix bugs faster by validating fewer patches than random patch selection based approach. Thus, it proves the effectiveness of similarity in repairing small programs.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {21–22},
numpages = {2}
}

@article{10.1145/3470006,
author = {Nikanjam, Amin and Braiek, Houssem Ben and Morovati, Mohammad Mehdi and Khomh, Foutse},
title = {Automatic Fault Detection for Deep Learning Programs Using Graph Transformations},
year = {2021},
issue_date = {January 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3470006},
doi = {10.1145/3470006},
abstract = {Nowadays, we are witnessing an increasing demand in both corporates and academia for exploiting Deep Learning (DL) to solve complex real-world problems. A DL program encodes the network structure of a desirable DL model and the process by which the model learns from the training dataset. Like any software, a DL program can be faulty, which implies substantial challenges of software quality assurance, especially in safety-critical domains. It is therefore crucial to equip DL development teams with efficient fault detection techniques and tools. In this article, we propose NeuraLint, a model-based fault detection approach for DL programs, using meta-modeling and graph transformations. First, we design a meta-model for DL programs that includes their base skeleton and fundamental properties. Then, we construct a graph-based verification process that covers 23 rules defined on top of the meta-model and implemented as graph transformations to detect faults and design inefficiencies in the generated models (i.e., instances of the meta-model). First, the proposed approach is evaluated by finding faults and design inefficiencies in 28 synthesized examples built from common problems reported in the literature. Then NeuraLint successfully finds 64 faults and design inefficiencies in 34 real-world DL programs extracted from Stack Overflow posts and GitHub repositories. The results show that NeuraLint effectively detects faults and design issues in both synthesized and real-world examples with a recall of 70.5% and a precision of 100%. Although the proposed meta-model is designed for feedforward neural networks, it can be extended to support other neural network architectures such as recurrent neural networks. Researchers can also expand our set of verification rules to cover more types of issues in DL programs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {14},
numpages = {27},
keywords = {model-based verification, fault detection, Graph transformations, deep learning}
}

@inproceedings{10.1145/3183519.3183554,
author = {Zhang, Chengyu and Yan, Yichen and Zhou, Hanru and Yao, Yinbo and Wu, Ke and Su, Ting and Miao, Weikai and Pu, Geguang},
title = {Smartunit: Empirical Evaluations for Automated Unit Testing of Embedded Software in Industry},
year = {2018},
isbn = {9781450356596},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183519.3183554},
doi = {10.1145/3183519.3183554},
abstract = {In this paper, we aim at the automated unit coverage-based testing for embedded software. To achieve the goal, by analyzing the industrial requirements and our previous work on automated unit testing tool CAUT, we rebuild a new tool, SmartUnit, to solve the engineering requirements that take place in our partner companies. SmartUnit is a dynamic symbolic execution implementation, which supports statement, branch, boundary value and MC/DC coverage.SmartUnit has been used to test more than one million lines of code in real projects. For confidentiality motives, we select three in-house real projects for the empirical evaluations. We also carry out our evaluations on two open source database projects, SQLite and PostgreSQL, to test the scalability of our tool since the scale of the embedded software project is mostly not large, 5K-50K lines of code on average. From our experimental results, in general, more than 90% of functions in commercial embedded software achieve 100% statement, branch, MC/DC coverage, more than 80% of functions in SQLite achieve 100% MC/DC coverage, and more than 60% of functions in PostgreSQL achieve 100% MC/DC coverage. Moreover, SmartUnit is able to find the runtime exceptions at the unit testing level. We also have reported exceptions like array index out of bounds and divided-by-zero in SQLite. Furthermore, we analyze the reasons of low coverage in automated unit testing in our setting and give a survey on the situation of manual unit testing with respect to automated unit testing in industry.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Software Engineering in Practice},
pages = {296–305},
numpages = {10},
keywords = {embedded system, automated unit testing, dynamic symbolic execution},
location = {Gothenburg, Sweden},
series = {ICSE-SEIP '18}
}

@inproceedings{10.1145/2797433.2797462,
author = {Falessi, Davide and Kruchten, Philippe},
title = {Five Reasons for Including Technical Debt in the Software Engineering Curriculum},
year = {2015},
isbn = {9781450333931},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2797433.2797462},
doi = {10.1145/2797433.2797462},
abstract = {Technical Debt is a useful metaphor to explain some of the difficulties of software evolution. The concept of Technical Debt is gaining importance from a scientific perspective, as the number of related papers, special issues, and international events grow over the years. From a practical perspective, the number of tools related to Technical Debt and their industrial adoption grow as well. Despite this high interest, Technical Debt is not yet included in the software engineering curriculum and hence the greater part of students graduating in computer science or software engineering does not know about Technical Debt. The aim of this paper is to discuss the inclusion of Technical Debt in the software engineering curriculum. We claim that Technical Debt should be treated as a first class entity the same as Requirements Engineering, Software Design and Architecture, and Software Testing. We support our claim by presenting five reasons why Technical Debt should be included in the software engineering curriculum.},
booktitle = {Proceedings of the 2015 European Conference on Software Architecture Workshops},
articleno = {28},
numpages = {4},
keywords = {technical debt, Software engineering curriculum},
location = {Dubrovnik, Cavtat, Croatia},
series = {ECSAW '15}
}

