@inproceedings{10.1145/3460319.3464824,
author = {Messaoudi, Salma and Shin, Donghwan and Panichella, Annibale and Bianculli, Domenico and Briand, Lionel C.},
title = {Log-Based Slicing for System-Level Test Cases},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464824},
doi = {10.1145/3460319.3464824},
abstract = {Regression testing is arguably one of the most important activities in software testing. However, its cost-effectiveness and usefulness can be largely impaired by complex system test cases that are poorly designed (e.g., test cases containing multiple test scenarios combined into a single test case) and that require a large amount of time and resources to run. One way to mitigate this issue is decomposing such system test cases into smaller, separate test cases---each of them with only one test scenario and with its corresponding assertions---so that the execution time of the decomposed test cases is lower than the original test cases, while the test effectiveness of the original test cases is preserved. This decomposition can be achieved with program slicing techniques, since test cases are software programs too. However, existing static and dynamic slicing techniques exhibit limitations when (1) the test cases use external resources, (2) code instrumentation is not a viable option, and (3) test execution is expensive.  In this paper, we propose a novel approach, called DS3 (Decomposing System teSt caSe), which automatically decomposes a complex system test case into separate test case slices. The idea is to use test case execution logs, obtained from past regression testing sessions, to identify "hidden" dependencies in the slices generated by static slicing. Since logs include run-time information about the system under test, we can use them to extract access and usage of global resources and refine the slices generated by static slicing.  We evaluated DS3 in terms of slicing effectiveness and compared it with a vanilla static slicing tool. We also compared the slices obtained by DS3 with the corresponding original system test cases, in terms of test efficiency and effectiveness. The evaluation results on one proprietary system and one open-source system show that DS3 is able to accurately identify the dependencies related to the usage of global resources, which vanilla static slicing misses. Moreover, the generated test case slices are, on average, 3.56 times faster than original system test cases and they exhibit no significant loss in terms of fault detection effectiveness.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {517–528},
numpages = {12},
keywords = {log, system level testing, program slicing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3266003.3266012,
author = {da Silva, Henrique Neves and Mattiello, Guilherme Ricken and Endo, Andre Takeshi and de Souza, \'{E}rica Ferreira and de Souza, Simone do Rocio Senger},
title = {Evaluating the Impact of Different Testers on Model-Based Testing},
year = {2018},
isbn = {9781450365550},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266003.3266012},
doi = {10.1145/3266003.3266012},
abstract = {Context: Model-Based Testing (MBT) is an approach that allows testers to represent the behavior of the system under test as models, specifying inputs and their expected outputs. From such models, existing tools might be employed to generate test cases automatically. While MBT represents a promising step towards the automation of test case generation, the quality of the model designed by the tester may impact, either positively or negatively, on its ability to reveal faults (i.e., the test effectiveness). Objective: In this context, we conducted a preliminary experiment to evaluate the impact caused by different testers when designing a test model for the same functionality. Method: In the experiment, the participants used Event Sequence Graphs and its supporting tool FourMA to create test models for two mobile apps: arXiv-mobile and WhoHasMyStuff. From the test models, test cases were generated using FourMA and concretized by means of the Robotium framework. In order to measure the impact of different testers, we employed code coverage (namely, instruction and branch coverage) as an estimation of test effectiveness. Results: Based on the results obtained, we observe high variation of code coverage among the testers. No tester was capable of producing a test model that subsumes all other testers' models with respect to code coverage. Moreover, factor learning seems not to reduce the code coverage variation. The relation between model size, modeling time, and code coverage were inconclusive. Conclusion: We conclude that further research effort on the MBT's modeling step is required to not only reduce the variation between testers, but also improving its effectiveness.},
booktitle = {Proceedings of the III Brazilian Symposium on Systematic and Automated Software Testing},
pages = {57–66},
numpages = {10},
keywords = {Event Sequence Graph, Automated Tests, Mobile Apps, Empirical Study, Model-Based Testing, Android},
location = {SAO CARLOS, Brazil},
series = {SAST '18}
}

@inproceedings{10.1145/3356317.3356322,
author = {Souza, Mariana and Villanes, Isabel K. and Dias-Neto, Arilo Claudio and Endo, Andre T.},
title = {On the Exploratory Testing of Mobile Apps},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356322},
doi = {10.1145/3356317.3356322},
abstract = {While the literature acknowledges that mobile apps present different testing challenges and automated solutions have been pursued, it lacks a better understanding of how pervasive practices of manual testing (namely Exploratory Testing - ET) can be more effectively applied. This paper aims to investigate the use of ET in mobile apps. With this study, we intend to have a better understanding of how exploratory testing is employed, its effectiveness, and its usage in an ample and diverse range of apps. To do so, we conducted two studies. The first study was conducted for the purpose of applying ET to apps with diverse contexts and available on Google Play in order to analyze whether testers actually explore all possible scenarios that apps may display. The second study, also applied the ET, however in two apps that were developed by a software development company; this study has the objective of applying the ET in order to identify bugs of different levels, that often cannot be revealed using other techniques. As expected the first study revealed that there are several test scenarios that are not exploited by the testers, yet the 40 participants revealed on average 5 bugs in 1.5h of test sessions. The second study revealed 64 bugs and 21 issues in two apps. Such revealed bugs are of different criticality and category. ET has shown to be a promising technique to uncover bugs, though test professionals can be better guided to explore their apps and search for bugs in scenarios related to mobile specific events.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {42–51},
numpages = {10},
keywords = {Exploratory Testing, Manual Tests, Android, Mobile Applications},
location = {Salvador, Brazil},
series = {SAST 2019}
}

@inproceedings{10.1145/1363686.1363871,
author = {Bueno, Paulo M. S. and Wong, W. Eric and Jino, Mario},
title = {Automatic Test Data Generation Using Particle Systems},
year = {2008},
isbn = {9781595937537},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1363686.1363871},
doi = {10.1145/1363686.1363871},
abstract = {The simulated repulsion algorithm, which is based on particle systems, is used for the automatic generation of diversity oriented test sets (DOTS). These test sets are generated by taking randomly generated test sets and iteratively improving their diversity (the level of variability among values for the test data) towards DOTS. The results of a simulation performed to evaluate characteristics of DOTS indicate improvement, with respect to fault detection, of these test sets over the standard random test sets.},
booktitle = {Proceedings of the 2008 ACM Symposium on Applied Computing},
pages = {809–814},
numpages = {6},
keywords = {software testing, simulated annealing, diversity oriented test data generation, self-organization, test data generation, genetic algorithms, random testing, simulated repulsion},
location = {Fortaleza, Ceara, Brazil},
series = {SAC '08}
}

@inproceedings{10.1145/1146238.1146240,
author = {Walcott, Kristen R. and Soffa, Mary Lou and Kapfhammer, Gregory M. and Roos, Robert S.},
title = {TimeAware Test Suite Prioritization},
year = {2006},
isbn = {1595932631},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1146238.1146240},
doi = {10.1145/1146238.1146240},
abstract = {Regression test prioritization is often performed in a time constrained execution environment in which testing only occurs for a fixed time period. For example, many organizations rely upon nightly building and regression testing of their applications every time source code changes are committed to a version control repository. This paper presents a regression test prioritization technique that uses a genetic algorithm to reorder test suites in light of testing time constraints. Experiment results indicate that our prioritization approach frequently yields higher average percentage of faults detected (APFD) values, for two case study applications, when basic block level coverage is used instead of method level coverage. The experiments also reveal fundamental trade offs in the performance of time-aware prioritization. This paper shows that our prioritization technique is appropriate for many regression testing environments and explains how the baseline approach can be extended to operate in additional time constrained testing circumstances.},
booktitle = {Proceedings of the 2006 International Symposium on Software Testing and Analysis},
pages = {1–12},
numpages = {12},
keywords = {genetic algorithms, coverage testing, test prioritization},
location = {Portland, Maine, USA},
series = {ISSTA '06}
}

@inproceedings{10.1145/3468264.3468619,
author = {Lin, Yun and Ong, You Sheng and Sun, Jun and Fraser, Gordon and Dong, Jin Song},
title = {Graph-Based Seed Object Synthesis for Search-Based Unit Testing},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468619},
doi = {10.1145/3468264.3468619},
abstract = {Search-based software testing (SBST) generates tests using search algorithms guided by measurements gauging how far a test case is away from exercising a coverage goal. The effectiveness of SBST largely depends on the continuity and monotonicity of the fitness landscape decided by these measurements and the search operators. Unfortunately, the fitness landscape is challenging when the function under test takes object inputs, as classical measurement hardly provide guidance for constructing legitimate object inputs. To overcome this problem, we propose test seeds, i.e., test code skeletons of legitimate objects which enable the use of classical measurements. Given a target branch in a function under test, we first statically analyze the function to build an object construction graph that captures the relation between the operands of the target method and the states of their relevant object inputs. Based on the graph, we synthesize test template code where each "slot" is a mutation point for the search algorithm. This approach can be seamlessly integrated with existing SBST algorithms, and we implemented EvoObj on top of EvoSuite. Our experiments show that EvoObj outperforms EvoSuite with statistical significance on 2750 methods over 103 open source Java projects using state-of-the-art SBST algorithms.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1068–1080},
numpages = {13},
keywords = {code synthesis, object oriented, software testing, search-based},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/2338965.2336777,
author = {Iqbal, Muhammad Zohaib and Arcuri, Andrea and Briand, Lionel},
title = {Empirical Investigation of Search Algorithms for Environment Model-Based Testing of Real-Time Embedded Software},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336777},
doi = {10.1145/2338965.2336777},
abstract = { System testing of real-time embedded systems (RTES) is a challenging task and only a fully automated testing approach can scale up to the testing requirements of industrial RTES. One such approach, which offers the advantage for testing teams to be black-box, is to use environment models to automatically generate test cases and oracles and an environment simulator to enable earlier and more practical testing. In this paper, we propose novel heuristics for search-based, RTES system testing which are based on these environment models. We evaluate the fault detection effectiveness of two search-based algorithms, i.e., Genetic Algorithms and (1+1) Evolutionary Algorithm, when using these novel heuristics and their combinations. Preliminary experiments on 13 carefully selected, non-trivial artificial problems, show that, under certain conditions, these novel heuristics are effective at bringing the environment into a state exhibiting a system fault. The heuristic combination that showed the best overall performance on the artificial problems was applied on an industrial case study where it showed consistent results. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {199–209},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inbook{10.1145/3293882.3330570,
author = {Lam, Wing and Godefroid, Patrice and Nath, Suman and Santhiar, Anirudh and Thummalapenta, Suresh},
title = {Root Causing Flaky Tests in a Large-Scale Industrial Setting},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330570},
abstract = {In today’s agile world, developers often rely on continuous integration pipelines to help build and validate their changes by executing tests in an efficient manner. One of the significant factors that hinder developers’ productivity is flaky tests—tests that may pass and fail with the same version of code. Since flaky test failures are not deterministically reproducible, developers often have to spend hours only to discover that the occasional failures have nothing to do with their changes. However, ignoring failures of flaky tests can be dangerous, since those failures may represent real faults in the production code. Furthermore, identifying the root cause of flakiness is tedious and cumbersome, since they are often a consequence of unexpected and non-deterministic behavior due to various factors, such as concurrency and external dependencies.  As developers in a large-scale industrial setting, we first describe our experience with flaky tests by conducting a study on them. Our results show that although the number of distinct flaky tests may be low, the percentage of failing builds due to flaky tests can be substantial. To reduce the burden of flaky tests on developers, we describe our end-to-end framework that helps identify flaky tests and understand their root causes. Our framework instruments flaky tests and all relevant code to log various runtime properties, and then uses a preliminary tool, called RootFinder, to find differences in the logs of passing and failing runs. Using our framework, we collect and publicize a dataset of real-world, anonymized execution logs of flaky tests. By sharing the findings from our study, our framework and tool, and a dataset of logs, we hope to encourage more research on this important problem.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101–111},
numpages = {11}
}

@inproceedings{10.1145/3356317.3356320,
author = {da Silva, Henrique Neves and Farah, Paulo Roberto and Mendon\c{c}a, Willian Douglas Ferrari and Vergilio, Silvia Regina},
title = {Assessing Android Test Data Generation Tools via Mutation Testing},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356320},
doi = {10.1145/3356317.3356320},
abstract = {A growing number of test data generation techniques and tools for Android applications (apps) has been proposed in the last years. As a consequence, a demand for evaluations comparing such tools has emerged. However, we find few studies only dedicated to this subject and there is a lack of studies considering the mutation score, spite of this is a measure largely used and recognized as effective to assess the quality of the test suites. A possible reason is the fact that Android mutation testing has been recently addressed in the literature, and most tools do not support the analyses of mutants in comparison with the original one, nor provide the score. To fulfill with these gaps, this work presents results from the evaluation of three state-of-the-art tools for Android apps: Monkey, Stoat and APE, regarding the ability of the test data generated by them to reveal faults described by the mutation operators of the tool MDroid+. To this end, we implemented a mechanism to automatically capture screenshots of apps execution and calculate the score, which is also described in the paper. In addition to this, we also evaluate aspects related to code coverage and runtime. Stoat reached the best general mean score, but Monkey takes significantly less time to execute, without great differences in the score and coverage.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {32–41},
numpages = {10},
keywords = {Android apps, Mutation score, Mobile test},
location = {Salvador, Brazil},
series = {SAST 2019}
}

@inproceedings{10.1145/2931037.2931056,
author = {Alipour, Mohammad Amin and Groce, Alex and Gopinath, Rahul and Christi, Arpit},
title = {Generating Focused Random Tests Using Directed Swarm Testing},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931056},
doi = {10.1145/2931037.2931056},
abstract = { Random testing can be a powerful and scalable method for finding faults in software. However, sophisticated random testers usually test a whole program, not individual components. Writing random testers for individual components of complex programs may require unreasonable effort. In this paper we present a novel method, directed swarm testing, that uses statistics and a variation of random testing to produce random tests that focus on only part of a program, increasing the frequency with which tests cover the targeted code. We demonstrate the effectiveness of this technique using real-world programs and test systems (the YAFFS2 file system, GCC, and Mozilla's SpiderMonkey JavaScript engine), and discuss various strategies for directed swarm testing. The best strategies can improve coverage frequency for targeted code by a factor ranging from 1.1-4.5x on average, and from nearly 3x to nearly 9x in the best case. For YAFFS2, directed swarm testing never decreased coverage, and for GCC and SpiderMonkey coverage increased for over 99% and 73% of targets, respectively, using the best strategies. Directed swarm testing improves detection rates for real SpiderMonkey faults, when the code in the introducing commit is targeted. This lightweight technique is applicable to existing industrial-strength random testers. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {70–81},
numpages = {12},
keywords = {regression testing, random testing, swarm testing},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3213846.3213859,
author = {Strandberg, Per Erik and Ostrand, Thomas J. and Weyuker, Elaine J. and Sundmark, Daniel and Afzal, Wasif},
title = {Automated Test Mapping and Coverage for Network Topologies},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213859},
doi = {10.1145/3213846.3213859},
abstract = {Communication devices such as routers and switches play a critical role in the reliable functioning of embedded system networks. Dozens of such devices may be part of an embedded system network, and they need to be tested in conjunction with various computational elements on actual hardware, in many different configurations that are representative of actual operating networks. An individual physical network topology can be used as the basis for a test system that can execute many test cases, by identifying the part of the physical network topology that corresponds to the configuration required by each individual test case. Given a set of available test systems and a large number of test cases, the problem is to determine for each test case, which of the test systems are suitable for executing the test case, and to provide the mapping that associates the test case elements (the logical network topology) with the appropriate elements of the test system (the physical network topology).  We studied a real industrial environment where this problem was originally handled by a simple software procedure that was very slow in many cases, and also failed to provide thorough coverage of each network's elements. In this paper, we represent both the test systems and the test cases as graphs, and develop a new prototype algorithm that a) determines whether or not a test case can be mapped to a subgraph of the test system, b) rapidly finds mappings that do exist, and c) exercises diverse sets of network nodes when multiple mappings exist for the test case. The prototype has been implemented and applied to over 10,000 combinations of test cases and test systems, and reduced the computation time by a factor of more than 80 from the original procedure. In addition, relative to a meaningful measure of network topology coverage, the mappings achieved an increased level of thoroughness in exercising the elements of each test system.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {73–83},
numpages = {11},
keywords = {test coverage, network topology, testing, subgraph isomorphism},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/2338965.2336790,
author = {R\"{o}βler, Jeremias and Fraser, Gordon and Zeller, Andreas and Orso, Alessandro},
title = {Isolating Failure Causes through Test Case Generation},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336790},
doi = {10.1145/2338965.2336790},
abstract = { Manual debugging is driven by experiments—test runs that narrow down failure causes by systematically confirming or excluding individual factors. The BUGEX approach leverages test case generation to systematically isolate such causes from a single failing test run—causes such as properties of execution states or branches taken that correlate with the failure. Identifying these causes allows for deriving conclusions as: “The failure occurs whenever the daylight savings time starts at midnight local time.” In our evaluation, a prototype of BUGEX precisely pinpointed important failure explaining facts for six out of seven real-life bugs. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {309–319},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/3128473.3128479,
author = {Lima, Jackson A. Prado and Vergilio, Silvia R.},
title = {A Multi-Objective Optimization Approach for Selection of Second Order Mutant Generation Strategies},
year = {2017},
isbn = {9781450353021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128473.3128479},
doi = {10.1145/3128473.3128479},
abstract = {The use of Higher-Order Mutants (HOMs) presents some advantages concerning the traditional use of First-Order Mutants (FOMs). HOMs can better simulate real and subtle faults, reduce the number of generated mutants and test cases, and so on. However, the HOM space is potentially huge, and an efficient strategy to generate the best HOMs is fundamental. In the literature different strategies were proposed and evaluated, mainly to generate Second-Order Mutants (SOMs), but none has been proved to perform better in different situations. Due to this, the selection of the best strategy is an important task. Most times a lot of experiments need to be conducted. To help the tester in this task and to allow the use of HOMs in practice, this paper proposes a hyper-heuristic approach. Such approach is based on NSGA-II and uses the selection method Choice Function to automatically choose among different Low-Level Heuristics (LLHs), which, in this case, are search-operators related to existing SOM generation strategies. The performance of each LLH is related to some objectives such as the number of SOMs generated, the capacity to capture subtler faults and replace the constituent FOMs. In comparison with existing strategies, our approach obtained better results considering the used objectives, and statistically equivalent results considering mutation score with respect to the FOMs.},
booktitle = {Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {6},
numpages = {10},
keywords = {Multi-objective optimization, Mutation Testing, Higher-Order Mutation},
location = {Fortaleza, Brazil},
series = {SAST}
}

@inbook{10.1145/3460319.3464829,
author = {Liu, Zixi and Feng, Yang and Chen, Zhenyu},
title = {DialTest: Automated Testing for Recurrent-Neural-Network-Driven Dialogue Systems},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464829},
abstract = {With the tremendous advancement of recurrent neural networks(RNN), dialogue systems have achieved significant development. Many RNN-driven dialogue systems, such as Siri, Google Home, and Alexa, have been deployed to assist various tasks. However, accompanying this outstanding performance, RNN-driven dialogue systems, which are essentially a kind of software, could also produce erroneous behaviors and result in massive losses. Meanwhile, the complexity and intractability of RNN models that power the dialogue systems make their testing challenging. In this paper, we design and implement DialTest, the first RNN-driven dialogue system testing tool. DialTest employs a series of transformation operators to make realistic changes on seed data while preserving their oracle information properly. To improve the efficiency of detecting faults, DialTest further adopts Gini impurity to guide the test generation process. We conduct extensive experiments to validate DialTest. We first experiment it on two fundamental tasks, i.e., intent detection and slot filling, of natural language understanding. The experiment results show that DialTest can effectively detect hundreds of erroneous behaviors for different RNN-driven natural language understanding (NLU) modules of dialogue systems and improve their accuracy via retraining with the generated data. Further, we conduct a case study on an industrial dialogue system to investigate the performance of DialTest under the real usage scenario. The study shows DialTest can detect errors and improve the robustness of RNN-driven dialogue systems effectively.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {115–126},
numpages = {12}
}

@article{10.1145/3182657,
author = {Baldoni, Roberto and Coppa, Emilio and D’elia, Daniele Cono and Demetrescu, Camil and Finocchi, Irene},
title = {A Survey of Symbolic Execution Techniques},
year = {2018},
issue_date = {May 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/3182657},
doi = {10.1145/3182657},
abstract = {Many security and software testing applications require checking whether certain properties of a program hold for any possible usage scenario. For instance, a tool for identifying software vulnerabilities may need to rule out the existence of any backdoor to bypass a program’s authentication. One approach would be to test the program using different, possibly random inputs. As the backdoor may only be hit for very specific program workloads, automated exploration of the space of possible inputs is of the essence. Symbolic execution provides an elegant solution to the problem, by systematically exploring many possible execution paths at the same time without necessarily requiring concrete inputs. Rather than taking on fully specified input values, the technique abstractly represents them as symbols, resorting to constraint solvers to construct actual instances that would cause property violations. Symbolic execution has been incubated in dozens of tools developed over the past four decades, leading to major practical breakthroughs in a number of prominent software reliability applications. The goal of this survey is to provide an overview of the main ideas, challenges, and solutions developed in the area, distilling them for a broad audience.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {50},
numpages = {39},
keywords = {Symbolic execution, static analysis, concolic execution, software testing}
}

@inproceedings{10.1145/2931037.2931057,
author = {Palomba, Fabio and Panichella, Annibale and Zaidman, Andy and Oliveto, Rocco and De Lucia, Andrea},
title = {Automatic Test Case Generation: What If Test Code Quality Matters?},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931057},
doi = {10.1145/2931037.2931057},
abstract = { Test case generation tools that optimize code coverage have been extensively investigated. Recently, researchers have suggested to add other non-coverage criteria, such as memory consumption or readability, to increase the practical usefulness of generated tests. In this paper, we observe that test code quality metrics, and test cohesion and coupling in particular, are valuable candidates as additional criteria. Indeed, tests with low cohesion and/or high coupling have been shown to have a negative impact on future maintenance activities. In an exploratory investigation we show that most generated tests are indeed affected by poor test code quality. For this reason, we incorporate cohesion and coupling metrics into the main loop of search-based algorithm for test case generation. Through an empirical study we show that our approach is not only able to generate tests that are more cohesive and less coupled, but can (i) increase branch coverage up to 10% when enough time is given to the search and (ii) result in statistically shorter tests. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {130–141},
numpages = {12},
keywords = {Evolutionary testing, branch coverage, test code quality, many-objective optimization},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3395363.3397357,
author = {Feng, Yang and Shi, Qingkai and Gao, Xinyu and Wan, Jun and Fang, Chunrong and Chen, Zhenyu},
title = {DeepGini: Prioritizing Massive Tests to Enhance the Robustness of Deep Neural Networks},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397357},
doi = {10.1145/3395363.3397357},
abstract = {Deep neural networks (DNN) have been deployed in many software systems to assist in various classification tasks. In company with the fantastic effectiveness in classification, DNNs could also exhibit incorrect behaviors and result in accidents and losses. Therefore, testing techniques that can detect incorrect DNN behaviors and improve DNN quality are extremely necessary and critical. However, the testing oracle, which defines the correct output for a given input, is often not available in the automated testing. To obtain the oracle information, the testing tasks of DNN-based systems usually require expensive human efforts to label the testing data, which significantly slows down the process of quality assurance.  To mitigate this problem, we propose DeepGini, a test prioritization technique designed based on a statistical perspective of DNN. Such a statistical perspective allows us to reduce the problem of measuring misclassification probability to the problem of measuring set impurity, which allows us to quickly identify possibly-misclassified tests. To evaluate, we conduct an extensive empirical study on popular datasets and prevalent DNN models. The experimental results demonstrate that DeepGini outperforms existing coverage-based techniques in prioritizing tests regarding both effectiveness and efficiency. Meanwhile, we observe that the tests prioritized at the front by DeepGini are more effective in improving the DNN quality in comparison with the coverage-based techniques.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {177–188},
numpages = {12},
keywords = {Deep Learning Testing, Deep Learning, Test Case Prioritization},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3425174.3425211,
author = {Ferreira, Thiago do Nascimento and Vergilio, Silvia Regina and Kessentini, Marouane},
title = {Applying Many-Objective Algorithms to the Variability Test of Software Product Lines},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425211},
doi = {10.1145/3425174.3425211},
abstract = {The problem known as Variability Test of Software Product Line (VTSPL) is related to the selection of the most representative products for the SPL testing. This is an optimization problem because a possible exponential number of products can be derived from the SPL variability model, such as the Feature Model (FM). In the literature many works are dedicated to this research subject, each one applying a different search-based algorithm and using distinct criteria. However, there is no study encompassing all these criteria at the same time. To this end, this paper investigates the use of two Many-Objective Evolutionary Algorithms (MaOEAs). We apply the algorithm NSGA-III, widely used for many-objective algorithms, and the algorithm PCA-NSGA-II, a reduction dimensionality algorithm, which uses the Principal-Component Analysis (PCA) in combination with NSGA-II, to evaluate the objectives used in the literature for the VTSPL problem. PCA-NSGA-II reduces the search space dimensionality by eliminating the redundant objectives. The analysis shows the importance of some objectives such as the number of alive mutants, similarity between products, and unselected features. NSGA-III reaches the best results regarding the quality indicators for all instances, but taking a longer time. Besides, PCA-NSGA-II can find different solutions in the search space that are not found by NSGA-III.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {11–20},
numpages = {10},
keywords = {many-objective problems, dimensionality reduction, Software product line testing},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{10.1145/1390630.1390643,
author = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ball, Thomas},
title = {Finding Errors in .Net with Feedback-Directed Random Testing},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390643},
doi = {10.1145/1390630.1390643},
abstract = {We present a case study in which a team of test engineers at Microsoft applied a feedback-directed random testing tool to a critical component of the .NET architecture. Due to its complexity and high reliability requirements, the component had already been tested by 40 test engineers over five years, using manual testing and many automated testing techniques.Nevertheless, the feedback-directed random testing tool found errors in the component that eluded previous testing, and did so two orders of magnitude faster than a typical test engineer (including time spent inspecting the results of the tool). The tool also led the test team to discover errors in other testing and analysis tools, and deficiencies in previous best-practice guidelines for manual testing. Finally, we identify challenges that random testing faces for continued effectiveness, including an observed decrease in the technique's error detection rate over time.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {87–96},
numpages = {10},
keywords = {random testing},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@inproceedings{10.1145/2338965.2336765,
author = {Nguyen, Cu D. and Marchetto, Alessandro and Tonella, Paolo},
title = {Combining Model-Based and Combinatorial Testing for Effective Test Case Generation},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336765},
doi = {10.1145/2338965.2336765},
abstract = { Model-based testing relies on the assumption that effective adequacy criteria can be defined in terms of model coverage achieved by a set of test paths. However, such test paths are only abstract test cases and input test data must be specified to make them concrete. We propose a novel approach that combines model-based and combinatorial testing in order to generate executable and effective test cases from a model. Our approach starts from a finite state model and applies model-based testing to generate test paths that represent sequences of events to be executed against the system under test. Such paths are transformed to classification trees, enriched with domain input specifications such as data types and partitions. Finally, executable test cases are generated from those trees using t-way combinatorial criteria.  While test cases that satisfy a combinatorial criterion can be generated for each individual test path obtained from the model, we introduce a post-optimization algorithm that can guarantee the combinatorial criterion of choice on the whole set of test paths extracted from the model. The resulting test suite is smaller, but it still satisfies the same adequacy criterion. We developed a tool and used it to evaluate our approach on 6 subject systems of various types and sizes, to study the effectiveness of the generated test suites, the reduction achieved by the post-optimization algorithm, as well as the effort required to produce them. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {100–110},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

