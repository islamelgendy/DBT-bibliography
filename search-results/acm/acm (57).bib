@inproceedings{10.1145/3427796.3427798,
author = {Agape, Andrei-Alexandru and Danceanu, Madalin Claudiu and Hansen, Rene Rydhof and Schmid, Stefan},
title = {P4Fuzz: Compiler Fuzzer ForDependable Programmable Dataplanes},
year = {2021},
isbn = {9781450389334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427796.3427798},
doi = {10.1145/3427796.3427798},
abstract = { Emerging software-defined networks and programmable dataplanes promise to render communication networks more dependable, overcoming today’s manual and error-prone approach to operate networks. Indeed, programmable dataplanes such as P4 provide great opportunities for improving network performance and developing innovative security features, by allowing programmers to reconfigure and tailor switches towards their needs. However, extending programmability to the dataplane also introduces new threat models. In this paper, using a systematic security analysis, we identify a particularly worrisome vulnerability: the automated program compilers which lie at the core of programmable dataplanes. The dataplane compilers introduce a risk of persistent threats which are covert and hard to detect, and may be exploited for large-scale attacks, affecting many devices. Our main contribution is P4Fuzz, a compiler fuzzer to find bugs and vulnerabilities in P4 compilers, in an efficient and automated manner. We discuss the challenges involved in designing such a compiler fuzzer for P4, present our fuzzing and taming algorithms, and report on experiments with our prototype implementation, considering the standard compilers of BMv2, eBPF, and NetFPGA. Our experiments confirm that P4Fuzz is able to generate and test the validity of dozens of P4 programs per minute. Using P4Fuzz, we also successfully found several bugs which have been acknowledged and fixed by the community.},
booktitle = {International Conference on Distributed Computing and Networking 2021},
pages = {16–25},
numpages = {10},
keywords = {software defined networking, p4 compiler, fuzzing},
location = {Nara, Japan},
series = {ICDCN '21}
}

@inproceedings{10.1145/2377978.2377983,
author = {Alexandrov, Alexander and Schiefer, Berni and Poelman, John and Ewen, Stephan and Bodner, Thomas O. and Markl, Volker},
title = {Myriad: Parallel Data Generation on Shared-Nothing Architectures},
year = {2011},
isbn = {9781450314398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377978.2377983},
doi = {10.1145/2377978.2377983},
abstract = {The need for efficient data generation for the purposes of testing and benchmarking newly developed massively-parallel data processing systems has increased with the emergence of Big Data problems. As synthetic data model specifications evolve over time, the data generator programs implementing these models have to be adapted continuously -- a task that often becomes more tedious as the set of model constraints grows. In this paper we present Myriad - a new parallel data generation toolkit. Data generators created with the toolkit can quickly produce very large datasets in a shared-nothing parallel execution environment, while at the same time preserve with cross-partition dependencies, correlations and distributions in the generated data. In addition, we report on our efforts towards a benchmark suite for large-scale parallel analysis systems that uses Myriad for the generation of OLAP-style relational datasets.},
booktitle = {Proceedings of the 1st Workshop on Architectures and Systems for Big Data},
pages = {30–33},
numpages = {4},
keywords = {software engineering, scalable data generation, testing and debugging, scalable data generation myriad parallel data generator toolkit, testing tools},
location = {Galveston Island, Texas, USA},
series = {ASBD '11}
}

@inproceedings{10.1145/2025113.2025125,
author = {Jensen, Simon Holm and Madsen, Magnus and M\o{}ller, Anders},
title = {Modeling the HTML DOM and Browser API in Static Analysis of JavaScript Web Applications},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025125},
doi = {10.1145/2025113.2025125},
abstract = {Developers of JavaScript web applications have little tool support for catching errors early in development. In comparison, an abundance of tools exist for statically typed languages, including sophisticated integrated development environments and specialized static analyses. Transferring such technologies to the domain of JavaScript web applications is challenging. In this paper, we discuss the challenges, which include the dynamic aspects of JavaScript and the complex interactions between JavaScript, HTML, and the browser. From this, we present the first static analysis that is capable of reasoning about the flow of control and data in modern JavaScript applications that interact with the HTML DOM and browser API.One application of such a static analysis is to detect type-related and dataflow-related programming errors. We report on experiments with a range of modern web applications, including Chrome Experiments and IE Test Drive applications, to measure the precision and performance of the technique. The experiments indicate that the analysis is able to show absence of errors related to missing object properties and to identify dead and unreachable code. By measuring the precision of the types inferred for object properties, the analysis is precise enough to show that most expressions have unique types. By also producing precise call graphs, the analysis additionally shows that most invocations in the programs are monomorphic. We furthermore study the usefulness of the analysis to detect spelling errors in the code. Despite the encouraging results, not all problems are solved and some of the experiments indicate a potential for improvement, which allows us to identify central remaining challenges and outline directions for future work.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {59–69},
numpages = {11},
keywords = {program analysis, scripting languages},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/2070821.2070823,
author = {Roychowdhury, Shounak and Khurshid, Sarfraz},
title = {Software Fault Localization Using Feature Selection},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070823},
doi = {10.1145/2070821.2070823},
abstract = {Manually locating and fixing faults can be tedious and hard. Recent years have seen much progress in automated techniques for fault localization. A particularly promising approach is to analyze passing and failing runs to compute how likely each statement is to be faulty. Techniques based on this approach have so far largely focused on either using statistical analysis or similarity based algorithms, which have a natural application in evaluating such runs. We present a novel approach to fault localization using feature selection techniques from machine learning. Our insight is that each additional failing or passing run can provide significantly diverse amount of information, which can help localize faults in code -- the statements with maximum feature diversity information can point to most suspicious lines of code. Experimental results show that our approach outperforms state-of-the-art approaches for localizing faults in most subject programs of the Siemens suite, which have previously been used to evaluate several fault localization techniques.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {statistical debugging, RELIEF, automated debugging, feature selection, fault localization, machine learning},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@article{10.1145/3511887,
author = {Zhang, Huangzhao and Fu, Zhiyi and Li, Ge and Ma, Lei and Zhao, Zhehao and Yang, Hua’an and Sun, Yizhe and Liu, Yang and Jin, Zhi},
title = {Towards Robustness of Deep Program Processing Models—Detection, Estimation, and Enhancement},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511887},
doi = {10.1145/3511887},
abstract = {Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This article initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROTA to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROTM, which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROTT) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness enhancement.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {50},
numpages = {40},
keywords = {adversarial attack, Source code processing, robustness enhancement, big code}
}

@inproceedings{10.1145/3341301.3359662,
author = {Kim, Seulbae and Xu, Meng and Kashyap, Sanidhya and Yoon, Jungyeon and Xu, Wen and Kim, Taesoo},
title = {Finding Semantic Bugs in File Systems with an Extensible Fuzzing Framework},
year = {2019},
isbn = {9781450368735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341301.3359662},
doi = {10.1145/3341301.3359662},
abstract = {File systems are too large to be bug free. Although handwritten test suites have been widely used to stress file systems, they can hardly keep up with the rapid increase in file system size and complexity, leading to new bugs being introduced and reported regularly. These bugs come in various flavors: simple buffer overflows to sophisticated semantic bugs. Although bug-specific checkers exist, they generally lack a way to explore file system states thoroughly. More importantly, no turnkey solution exists that unifies the checking effort of various aspects of a file system under one umbrella.In this paper, we highlight the potential of applying fuzzing to find not just memory errors but, in theory, any type of file system bugs with an extensible fuzzing framework: Hydra. Hydra provides building blocks for file system fuzzing, including input mutators, feedback engines, a libOS-based executor, and a bug reproducer with test case minimization. As a result, developers only need to focus on building the core logic for finding bugs of their own interests. We showcase the effectiveness of Hydra with four checkers that hunt crash inconsistency, POSIX violations, logic assertion failures, and memory errors. So far, Hydra has discovered 91 new bugs in Linux file systems, including one in a verified file system (FSCQ), as well as four POSIX violations.},
booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
pages = {147–161},
numpages = {15},
keywords = {file systems, semantic bugs, fuzzing},
location = {Huntsville, Ontario, Canada},
series = {SOSP '19}
}

@inproceedings{10.5555/2663608.2663624,
author = {Zhang, Jie and Yang, Rui and Chen, Zhenyu and Zhao, Zhihong and Xu, Baowen},
title = {Automated EFSM-Based Test Case Generation with Scatter Search},
year = {2012},
isbn = {9781467318228},
publisher = {IEEE Press},
abstract = {Extended Finite State Machine (EFSM) is widely-used to represent system specifications. Automated test data generation based on EFSM models is still a challenging task due to the complexity of transition paths. In this paper, we introduce a new approach to generate test cases automatically for given transition paths of an EFSM model. An executable EFSM model is used to provide run-time feedback information as fitness function. And then scatter search algorithm is used to search for test data that can trigger given transition paths. Based on the executable model, the expected outputs associated with test data are also collected for construction of test oracles automatically. Finally, test data (inputs) and test oracles (expected outputs) are combined to be test cases. The experimental results show that our approach can effectively generate test cases to exercise the feasible transition paths.},
booktitle = {Proceedings of the 7th International Workshop on Automation of Software Test},
pages = {76–82},
numpages = {7},
keywords = {EFSM, test case generation, transition path, scatter search},
location = {Zurich, Switzerland},
series = {AST '12}
}

@inbook{10.1145/3340496.3342761,
author = {Harty, Julian and M\"{u}ller, Matthias},
title = {Better Android Apps Using Android Vitals},
year = {2019},
isbn = {9781450368582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3340496.3342761},
abstract = {Google provides Android Vitals, a set of reports and tools for Android Developers as part of Google Play Console. Android Vitals can help developers improve their Android apps after an app has been launched by providing information on how their app is performing in key areas such as battery use, performance, and stability (freezes and crashes). Android Vitals also provides various comparisons, including against global bad behavior thresholds, against various peer groups of apps, and across releases of this app.  Developers confirm Android Vitals notifies them of relevant problems and they found it valuable even if they also use crash reporting and mobile analytics.  The underlying data is used by Google to assess the relative quality of Android apps; and the perceived quality may materially affect the visibility of an app in the Google Play Store. Yet little is known about the tools.  This paper outlines various experiences from the developers' perspective of using Android Vitals with several popular Android apps to help open discussions and suggest further research areas. It introduces an open source project, created as part of our work, that enables developers to download pertinent data, particularly crash reports. The data can be analysed both by the development team and others. A particular benefit of this tool is to make the data available outside of the Google platform, which allows developers and (indirectly) researchers to develop additional analysis techniques not currently provided by the platform.},
booktitle = {Proceedings of the 3rd ACM SIGSOFT International Workshop on App Market Analytics},
pages = {26–32},
numpages = {7}
}

@inproceedings{10.1109/GRID.2005.1542741,
author = {Mehmood, R. and Crowcroft, J. and Hand, S. and Smith, S.},
title = {Grid-Level Computing Needs Pervasive Debugging},
year = {2005},
isbn = {0780394925},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/GRID.2005.1542741},
doi = {10.1109/GRID.2005.1542741},
booktitle = {Proceedings of the 6th IEEE/ACM International Workshop on Grid Computing},
pages = {186–193},
numpages = {8},
series = {GRID '05}
}

@inproceedings{10.1145/2305484.2305489,
author = {Reis, Bernardo and Teixeira, Jo\~{a}o Marcelo and Breyer, Felipe and Vasconcelos, Luis Arthur and Cavalcanti, Aline and Ferreira, Andr\'{e} and Kelner, Judith},
title = {Increasing Kinect Application Development Productivity by an Enhanced Hardware Abstraction},
year = {2012},
isbn = {9781450311687},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2305484.2305489},
doi = {10.1145/2305484.2305489},
abstract = {Designing and implementing the interaction behavior for body tracking capable systems requires complex modeling of actions and extensive calibration. Being the most recent and successful device for robust interactive body tracking, Microsoft's Kinect has enabled natural interaction by the use of consumer hardware, providing detailed and powerful information to designers and developers, but little tooling. To fulfill this lack of adequate tools for helping developers in the prototyping and implementation of such interfaces, we present Kina, a toolkit that makes the development not fully conditional to the existence of a sensor. By providing playback capabilities together with an online movement database, it reduces the physical effort found while performing testing activities.},
booktitle = {Proceedings of the 4th ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
pages = {5–14},
numpages = {10},
keywords = {interaction database, application testing, development methodology},
location = {Copenhagen, Denmark},
series = {EICS '12}
}

@inproceedings{10.1145/2000229.2000247,
author = {Yoon, Ilchul and Sussman, Alan and Memon, Atif and Porter, Adam},
title = {Towards Incremental Component Compatibility Testing},
year = {2011},
isbn = {9781450307239},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2000229.2000247},
doi = {10.1145/2000229.2000247},
abstract = {Software components are increasingly assembled from other components. Each component may further depend on others, and each may have multiple active versions. The total number of configurations --- combinations of components and their versions --- deployed by end users can be very large. Component developers, therefore, spend considerable time and effort doing compatibility testing --- determining whether their components can be built correctly for all deployed configurations. In previous work we developed Rachet to support large-scale compatibility testing of components.In this paper, we describe and evaluate methods to enable Rachet to perform incremental compatibility testing. We describe algorithms to compute differences in component compatibilities between current and previous component builds, a formal test adequacy criterion based on covering the differences, and cache-aware configuration sampling and testing methods that attempt to reuse effort from previous testing sessions. We evaluate our approach using the 5-year evolution history of a scientific middleware component. Our results show significant performance improvements over Rachet's previous retest-all approach, making the process of compatibility testing practical for evolving components.},
booktitle = {Proceedings of the 14th International ACM Sigsoft Symposium on Component Based Software Engineering},
pages = {119–128},
numpages = {10},
keywords = {incremental testing, software component, compatibility},
location = {Boulder, Colorado, USA},
series = {CBSE '11}
}

@article{10.1145/1018210.1018212,
author = {Orso, Alessandro and Sinha, Saurabh and Harrold, Mary Jean},
title = {Classifying Data Dependences in the Presence of Pointers for Program Comprehension, Testing, and Debugging},
year = {2004},
issue_date = {April 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1018210.1018212},
doi = {10.1145/1018210.1018212},
abstract = {Understanding data dependences in programs is important for many software-engineering activities, such as program understanding, impact analysis, reverse engineering, and debugging. The presence of pointers can cause subtle and complex data dependences that can be difficult to understand. For example, in languages such as C, an assignment made through a pointer dereference can assign a value to one of several variables, none of which may appear syntactically in that statement. In the first part of this article, we describe two techniques for classifying data dependences in the presence of pointer dereferences. The first technique classifies data dependences based on definition type, use type, and path type. The second technique classifies data dependences based on span. We present empirical results to illustrate the distribution of data-dependence types and spans for a set of real C programs. In the second part of the article, we discuss two applications of the classification techniques. First, we investigate different ways in which the classification can be used to facilitate data-flow testing. We outline an approach that uses types and spans of data dependences to determine the appropriate verification technique for different data dependences; we present empirical results to illustrate the approach. Second, we present a new slicing approach that computes slices based on types of data dependences. Based on the new approach, we define an incremental slicing technique that computes a slice in multiple steps. We present empirical results to illustrate the sizes of incremental slices and the potential usefulness of incremental slicing for debugging.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
pages = {199–239},
numpages = {41},
keywords = {data-flow testing, pointers, program slicing, incremental slicing, program comprehension, debugging, Data dependences}
}

@inproceedings{10.5555/1855711.1855722,
author = {Mickens, James and Elson, Jeremy and Howell, Jon},
title = {Mugshot: Deterministic Capture and Replay for Javascript Applications},
year = {2010},
publisher = {USENIX Association},
address = {USA},
abstract = {Mugshot is a system that captures every event in an executing JavaScript program, allowing developers to deterministically replay past executions of web applications. Replay is useful for a variety of reasons: failure analysis using debugging tools, performance evaluation, and even usability analysis of a GUI. Because Mugshot can replay every execution step that led to a failure, it is far more useful for performing root-cause analysis than today's commonly deployed client-based error reporting systems--core dumps and stack traces can only give developers a snapshot of the system after a failure has occurred.Many logging systems require a specially instrumented execution environment like a virtual machine or a custom program interpreter. In contrast, Mugshot's client-side component is implemented entirely in standard JavaScript, providing event capture on unmodified client browsers. Mugshot imposes low overhead in terms of storage (20-80KB/minute) and computation (slowdowns of about 7% for games with high event rates). This combination of features--a low-overhead library that runs in unmodified browers--makes Mugshot one of the first capture systems that is practical to deploy to every client and run in the common case. With Mugshot, developers can collect widespread traces from programs in the field, gaining a visibility into application execution that is typically only available in a controlled development environment.},
booktitle = {Proceedings of the 7th USENIX Conference on Networked Systems Design and Implementation},
pages = {11},
numpages = {1},
location = {San Jose, California},
series = {NSDI'10}
}

@inproceedings{10.1145/3338906.3338925,
author = {Shi, August and Lam, Wing and Oei, Reed and Xie, Tao and Marinov, Darko},
title = {IFixFlakies: A Framework for Automatically Fixing Order-Dependent Flaky Tests},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338925},
doi = {10.1145/3338906.3338925},
abstract = {Regression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming.  We propose iFixFlakies, a framework for automatically fixing order-dependent tests. The key insight in iFixFlakies is that test suites often already have tests, which we call helpers, whose logic resets or sets the states for order-dependent tests to pass. iFixFlakies searches a test suite for helpers that make the order-dependent tests pass and then recommends patches for the order-dependent tests using code from these helpers. Our evaluation on 110 truly orderdependent tests from a public dataset shows that 58 of them have helpers, and iFixFlakies can fix all 58. We opened pull requests for 56 order-dependent tests (2 of 58 were already fixed), and developers have already accepted pull requests for 21 of them, with all the remaining ones still pending.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {545–555},
numpages = {11},
keywords = {order-dependent test, flaky test, patch generation, automated fixing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3195538.3195544,
author = {Flemstr\"{o}m, Daniel and Gustafsson, Thomas and Kobetski, Avenir},
title = {A Case Study of Interactive Development of Passive Tests},
year = {2018},
isbn = {9781450357494},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195538.3195544},
doi = {10.1145/3195538.3195544},
abstract = {Testing in the active sense is the most common way to perform verification and validation of systems, but testing in the passive sense has one compelling property: independence. Independence from test stimuli and other passive tests opens up for parallel testing and off-line analysis. However, the tests can be difficult to develop since the complete testable state must be expressed using some formalism. We argue that a carefully chosen language together with an interactive work flow, providing immediate feedback, can enable testers to approach passive testing. We have conducted a case study in the automotive domain, interviewing experienced testers. The testers have been introduced to, and had hands-on practice with a tool. The tool is based on Easy Approach to Requirements Syntax (EARS) and provides an interactive work flow for developing and evaluating test results. The case study shows that i) the testers believe passive testing is useful for many of their tests, ii) they see benefits in parallelism and off-line analysis, iii) the interactive work flow is necessary for writing the testable state expression, but iv) when the testable state becomes too complex, then the proposed language is a limitation. However, the language contributes to concise tests, resembling executable requirements.},
booktitle = {Proceedings of the 5th International Workshop on Requirements Engineering and Testing},
pages = {13–20},
numpages = {8},
keywords = {case study, test language, test tool, content analysis, passive testing},
location = {Gothenburg, Sweden},
series = {RET '18}
}

@inproceedings{10.1145/2896971.2896976,
author = {Xie, Xiaoyuan and Li, Jiahao and Wang, Chen and Chen, Tsong Yueh},
title = {Looking for an MR? Try METWiki Today},
year = {2016},
isbn = {9781450341639},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896971.2896976},
doi = {10.1145/2896971.2896976},
abstract = {Metamorphic Testing (MT) has been demonstrated to successfully alleviate oracle problems in many areas, including machine learning, compilers, bioinformatics, etc. However, given a new MT task, it is still very challenging to identify enough Metamorphic Relations (MRs) which are key components of MT. Aiming at this problem, we revisited previous MT applications and realized that they could form a very valuable database. Currently there is a lack of efficient link to get testers access these historical data. Therefore, in this paper, we propose to build METWiki, an MR repository, for collection and retrieval of these MRs. By providing exploration and search facilities, testers can find their desired MRs for reuse, reference, or inference. We also illustrate three sample usages of METWiki, which show important illuminations on how MRs can be obtained in practice.},
booktitle = {Proceedings of the 1st International Workshop on Metamorphic Testing},
pages = {1–4},
numpages = {4},
keywords = {metamorphic testing, MR repository, metamorphic relation, METWiki},
location = {Austin, Texas},
series = {MET '16}
}

@inproceedings{10.1145/2254064.2254126,
author = {Pradel, Michael and Gross, Thomas R.},
title = {Fully Automatic and Precise Detection of Thread Safety Violations},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254126},
doi = {10.1145/2254064.2254126},
abstract = {Concurrent, object-oriented programs often use thread-safe library classes. Existing techniques for testing a thread-safe class either rely on tests using the class, on formal specifications, or on both. Unfortunately, these techniques often are not fully automatic as they involve the user in analyzing the output. This paper presents an automatic testing technique that reveals concurrency bugs in supposedly thread-safe classes. The analysis requires as input only the class under test and reports only true positives. The key idea is to generate tests in which multiple threads call methods on a shared instance of the tested class. If a concurrent test exhibits an exception or a deadlock that cannot be triggered in any linearized execution of the test, the analysis reports a thread safety violation. The approach is easily applicable, because it is independent of hand-written tests and explicit specifications. The analysis finds 15 concurrency bugs in popular Java libraries, including two previously unknown bugs in the Java standard library.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {521–530},
numpages = {10},
keywords = {testing, concurrent test generation, thread safety},
location = {Beijing, China},
series = {PLDI '12}
}

@article{10.1145/2345156.2254126,
author = {Pradel, Michael and Gross, Thomas R.},
title = {Fully Automatic and Precise Detection of Thread Safety Violations},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345156.2254126},
doi = {10.1145/2345156.2254126},
abstract = {Concurrent, object-oriented programs often use thread-safe library classes. Existing techniques for testing a thread-safe class either rely on tests using the class, on formal specifications, or on both. Unfortunately, these techniques often are not fully automatic as they involve the user in analyzing the output. This paper presents an automatic testing technique that reveals concurrency bugs in supposedly thread-safe classes. The analysis requires as input only the class under test and reports only true positives. The key idea is to generate tests in which multiple threads call methods on a shared instance of the tested class. If a concurrent test exhibits an exception or a deadlock that cannot be triggered in any linearized execution of the test, the analysis reports a thread safety violation. The approach is easily applicable, because it is independent of hand-written tests and explicit specifications. The analysis finds 15 concurrency bugs in popular Java libraries, including two previously unknown bugs in the Java standard library.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {521–530},
numpages = {10},
keywords = {thread safety, testing, concurrent test generation}
}

@inproceedings{10.1145/3514221.3517864,
author = {Galhotra, Sainyam and Fariha, Anna and Louren\c{c}o, Raoni and Freire, Juliana and Meliou, Alexandra and Srivastava, Divesh},
title = {DataPrism: Exposing Disconnect between Data and Systems},
year = {2022},
isbn = {9781450392495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3514221.3517864},
doi = {10.1145/3514221.3517864},
abstract = {As data is a central component of many modern systems, the cause of a system malfunction may reside in the data, and, specifically, particular properties of data. E.g., a health-monitoring system that is designed under the assumption that weight is reported in lbs will malfunction when encountering weight reported in kilograms. Like software debugging, which aims to find bugs in the source code or runtime conditions, our goal is to debug data to identify potential sources of disconnect between the assumptions about some data and systems that operate on that data. We propose DataPrism, a framework to identify data properties (profiles) that are the root causes of performance degradation or failure of a data-driven system. Such identification is necessary to repair data and resolve the disconnect between data and systems. Our technique is based on causal reasoning through interventions: when a system malfunctions for a dataset, DataPrism alters the data profiles and observes changes in the system's behavior due to the alteration. Unlike statistical observational analysis that reports mere correlations, DataPrism reports causally verified root causes -- in terms of data profiles -- of the system malfunction. We empirically evaluate DataPrism on seven real-world and several synthetic data-driven systems that fail on certain datasets due to a diverse set of reasons. In all cases, DataPrism identifies the root causes precisely while requiring orders of magnitude fewer interventions than prior techniques.},
booktitle = {Proceedings of the 2022 International Conference on Management of Data},
pages = {217–231},
numpages = {15},
keywords = {debugging, causal testing, data profiles, root-cause identification},
location = {Philadelphia, PA, USA},
series = {SIGMOD/PODS '22}
}

@inproceedings{10.1145/1529282.1529374,
author = {Abreu, Rui and Mayer, Wolfgang and Stumptner, Markus and van Gemund, Arjan J. C.},
title = {Refining Spectrum-Based Fault Localization Rankings},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529374},
doi = {10.1145/1529282.1529374},
abstract = {Spectrum-based fault localization is a statistical technique that aims at helping software developers to find faults quickly by analyzing abstractions of program traces to create a ranking of most probable faulty components (e.g., program statements). Although spectrum-based fault localization has been shown to be effective, its diagnostic accuracy is inherently limited, since the semantics of components are not considered. In particular, components that exhibit identical execution patterns cannot be distinguished. To enhance its diagnostic quality, in this paper, we combine spectrum-based fault localization with a model-based debugging approach based on abstract interpretation within a framework coined Deputo. The model-based approach is used to refine the ranking obtained from the spectrum-based method by filtering out those components that do not explain the observed failures when the program's semantics is considered. We show that this combined approach outperforms the individual approaches and other state-of-the-art automated debugging techniques.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {409–414},
numpages = {6},
keywords = {abstract interpretation, fault localization, program spectra},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@inproceedings{10.1145/3196398.3196472,
author = {Soto, Mauricio and Goues, Claire Le},
title = {Common Statement Kind Changes to Inform Automatic Program Repair},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196472},
doi = {10.1145/3196398.3196472},
abstract = {The search space for automatic program repair approaches is vast and the search for mechanisms to help restrict this search are increasing. We make a granular analysis based on statement kinds to find which statements are more likely to be modified than others when fixing an error. We construct a corpus for analysis by delimiting debugging regions in the provided dataset and recursively analyze the differences between the Simplified Syntax Trees associated with EditEvent's. We build a distribution of statement kinds with their corresponding likelihood of being modified and we validate the usage of this distribution to guide the statement selection. We then build association rules with different confidence thresholds to describe statement kinds commonly modified together for multi-edit patch creation. Finally we evaluate association rule coverage over a held out test set and find that when using a 95% confidence threshold we can create less and more accurate rules that fully cover 93.8% of the testing instances.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {102–105},
numpages = {4},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

