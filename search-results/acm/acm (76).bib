@inproceedings{10.5555/2387880.2387910,
author = {Attariyan, Mona and Chow, Michael and Flinn, Jason},
title = {X-Ray: Automating Root-Cause Diagnosis of Performance Anomalies in Production Software},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {Troubleshooting the performance of production software is challenging. Most existing tools, such as profiling, tracing, and logging systems, reveal what events occurred during performance anomalies. However, users of such toolsmust infer why these events occurred; e.g., that their execution was due to a root cause such as a specific input request or configuration setting. Such inference often requires source code and detailed application knowledge that is beyond system administrators and end users.This paper introduces performance summarization, a technique for automatically diagnosing the root causes of performance problems. Performance summarization instruments binaries as applications execute. It first attributes performance costs to each basic block. It then uses dynamic information flow tracking to estimate the likelihood that a block was executed due to each potential root cause. Finally, it summarizes the overall cost of each potential root cause by summing the per-block cost multiplied by the cause-specific likelihood over all basic blocks. Performance summarization can also be performed differentially to explain performance differences between two similar activities. X-ray is a tool that implements performance summarization. Our results show that X-ray accurately diagnoses 17 performance issues in Apache, lighttpd, Postfix, and PostgreSQL, while adding 2.3% average runtime overhead.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {307–320},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@inproceedings{10.1145/2714576.2714607,
author = {Lee, Wang Hao and Srirangam Ramanujam, Murali and Krishnan, S.P.T.},
title = {On Designing an Efficient Distributed Black-Box Fuzzing System for Mobile Devices},
year = {2015},
isbn = {9781450332453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2714576.2714607},
doi = {10.1145/2714576.2714607},
abstract = {Security researchers who jailbreak iOS devices have usually crowdsourced for system level vulnerabilities [1] for iOS. However, their success has depended on whether a particular device owner encountered a crash in system-level code. To conduct voluntary security testing, black-box fuzzing is one of the ideal low-cost and simple techniques to find system level vulnerabilities for the less technical crowd. However, it is not the most effective method due to the large fuzzing space. At the same time, when fuzzing mobile devices such as today's smartphones, it is extremely time consuming to instrument mobile devices of varying versions of system software across the world. This paper, describes Mobile Vulnerability Discovery Pipeline (MVDP), a semi-automated, vulnerability discovery pipeline for mobile devices. MVDP is a carefully crafted process targeted to produce malicious output that is very likely to crash the target leading to vulnerability discovery. MVDP employs a few novel black-box fuzzing techniques such as distributed fuzzing, parameter selection, mutation position optimisation and selection of good seed files. To date, MVDP has discovered around 1900 unique crashing inputs and helped to identify 7 unique vulnerabilities across various Android and iOS phone models.},
booktitle = {Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security},
pages = {31–42},
numpages = {12},
keywords = {crash analysis, smartphones, zero-day vulnerability, black-box fuzzing},
location = {Singapore, Republic of Singapore},
series = {ASIA CCS '15}
}

@article{10.1145/269012.269020,
author = {Johnson, Philip M.},
title = {Reengineering Inspection},
year = {1998},
issue_date = {Feb. 1998},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/269012.269020},
doi = {10.1145/269012.269020},
journal = {Commun. ACM},
month = {feb},
pages = {49–52},
numpages = {4}
}

@inproceedings{10.1145/2463372.2463550,
author = {Poulding, Simon and Alexander, Robert and Clark, John A. and Hadley, Mark J.},
title = {The Optimisation of Stochastic Grammars to Enable Cost-Effective Probabilistic Structural Testing},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463550},
doi = {10.1145/2463372.2463550},
abstract = {The effectiveness of probabilistic structural testing depends on the characteristics of the probability distribution from which test inputs are sampled at random. Metaheuristic search has been shown to be a practical method of optimising the characteristics of such distributions. However, the applicability of the existing search-based algorithm is limited by the requirement that the software's inputs must be a fixed number of numeric values.In this paper we relax this limitation by means of a new representation for the probability distribution. The representation is based on stochastic context-free grammars but incorporates two novel extensions: conditional production weights and the aggregation of terminal symbols representing numeric values.We demonstrate that an algorithm which combines the new representation with hill-climbing search is able to efficiently derive probability distributions suitable for testing software with structurally-complex input domains.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1477–1484},
numpages = {8},
keywords = {structural testing, statistical testing, stochastic context-free grammars, search-based software engineering, software verification},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.1145/2652524.2652541,
author = {Davies, Steven and Roper, Marc},
title = {What's in a Bug Report?},
year = {2014},
isbn = {9781450327749},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2652524.2652541},
doi = {10.1145/2652524.2652541},
abstract = {Context: Bug reports are the primary means by which users of a system are able to communicate a problem to the developers, and their contents are important - not only to support developers in maintaining the system, but also as the basis of automated tools to assist in the challenging tasks of finding and fixing bugs.Goal: This paper aims to investigate how users report bugs in systems: what information is provided, how frequently, and the consequences of this.Method: The study examined the quality and quantity of information provided in 1600 bugs reports drawn from four open-source projects (Eclipse, Firefox, Apache HTTP, and Facebook API), recorded what information users actually provide, how and when users provide the information, and how this affects the outcome of the bug.Results: Of the recorded sources of information, only observed behaviour and expected results appeared in more than 50% of reports. Those sources deemed highly useful by developers and tools such as stack traces and test cases appeared very infrequently. However, no strong relationship was observed between the provided information and the outcome of the bug.Conclusions: The paper demonstrates a clear mismatch between the information that developers would wish to appear in a bug report, and the information that actually appears. Furthermore, the quality of bug reports has an important impact on research which might rely on extracting this information automatically.},
booktitle = {Proceedings of the 8th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {26},
numpages = {10},
keywords = {software maintenance, bug report, bug repository},
location = {Torino, Italy},
series = {ESEM '14}
}

@inproceedings{10.1109/ICSE.2007.37,
author = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ernst, Michael D. and Ball, Thomas},
title = {Feedback-Directed Random Test Generation},
year = {2007},
isbn = {0769528287},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2007.37},
doi = {10.1109/ICSE.2007.37},
abstract = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
booktitle = {Proceedings of the 29th International Conference on Software Engineering},
pages = {75–84},
numpages = {10},
series = {ICSE '07}
}

@inproceedings{10.1145/1123058.1123060,
author = {Blundell, Colin and Giannakopoulou, Dimitra and Pundefinedsundefinedreanu, Corina S.},
title = {Assume-Guarantee Testing},
year = {2005},
isbn = {1595933719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1123058.1123060},
doi = {10.1145/1123058.1123060},
abstract = {Verification techniques for component-based systems should ideally be able to predict properties of the assembled system through analysis of individual components before assembly. This work introduces such a modular technique in the context of testing. Assume-guarantee testing relies on the (automated) decomposition of key system-level requirements into local component requirements at design time. Developers can verify the local requirements by checking components in isolation; failed checks may indicate violations of system requirements, while valid traces from different components compose via the assume-guarantee proof rule to potentially provide system coverage. These local requirements also form the foundation of a technique for efficient predictive testing of assembled systems: given a correct system run, this technique can predict violations by alternative system runs without constructing those runs. We discuss the application of our approach to testing a multi-threaded NASA application, where we treat threads as components.},
booktitle = {Proceedings of the 2005 Conference on Specification and Verification of Component-Based Systems},
pages = {1–es},
keywords = {verification, predictive analysis, assume-guarantee reasoning, testing},
location = {Lisbon, Portugal},
series = {SAVCBS '05}
}

@article{10.1145/1118537.1123060,
author = {Blundell, Colin and Giannakopoulou, Dimitra and Pundefinedsundefinedreanu, Corina S.},
title = {Assume-Guarantee Testing},
year = {2005},
issue_date = {March 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1118537.1123060},
doi = {10.1145/1118537.1123060},
abstract = {Verification techniques for component-based systems should ideally be able to predict properties of the assembled system through analysis of individual components before assembly. This work introduces such a modular technique in the context of testing. Assume-guarantee testing relies on the (automated) decomposition of key system-level requirements into local component requirements at design time. Developers can verify the local requirements by checking components in isolation; failed checks may indicate violations of system requirements, while valid traces from different components compose via the assume-guarantee proof rule to potentially provide system coverage. These local requirements also form the foundation of a technique for efficient predictive testing of assembled systems: given a correct system run, this technique can predict violations by alternative system runs without constructing those runs. We discuss the application of our approach to testing a multi-threaded NASA application, where we treat threads as components.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–es},
numpages = {8},
keywords = {assume-guarantee reasoning, verification, predictive analysis, testing}
}

@inproceedings{10.1145/74587.74614,
author = {Ott, Linda M. and Thuss, Jeffrey J.},
title = {The Relationship between Slices and Module Cohesion},
year = {1989},
isbn = {0818619414},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/74587.74614},
doi = {10.1145/74587.74614},
booktitle = {Proceedings of the 11th International Conference on Software Engineering},
pages = {198–204},
numpages = {7},
location = {Pittsburgh, Pennsylvania, USA},
series = {ICSE '89}
}

@inproceedings{10.5555/2818754.2818850,
author = {Ghotra, Baljinder and McIntosh, Shane and Hassan, Ahmed E.},
title = {Revisiting the Impact of Classification Techniques on the Performance of Defect Prediction Models},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Defect prediction models help software quality assurance teams to effectively allocate their limited resources to the most defect-prone software modules. A variety of classification techniques have been used to build defect prediction models ranging from simple (e.g., logistic regression) to advanced techniques (e.g., Multivariate Adaptive Regression Splines (MARS)). Surprisingly, recent research on the NASA dataset suggests that the performance of a defect prediction model is not significantly impacted by the classification technique that is used to train it. However, the dataset that is used in the prior study is both: (a) noisy, i.e., contains erroneous entries and (b) biased, i.e., only contains software developed in one setting. Hence, we set out to replicate this prior study in two experimental settings. First, we apply the replicated procedure to the same (known-to-be noisy) NASA dataset, where we derive similar results to the prior study, i.e., the impact that classification techniques have appear to be minimal. Next, we apply the replicated procedure to two new datasets: (a) the cleaned version of the NASA dataset and (b) the PROMISE dataset, which contains open source software developed in a variety of settings (e.g., Apache, GNU). The results in these new datasets show a clear, statistically distinct separation of groups of techniques, i.e., the choice of classification technique has an impact on the performance of defect prediction models. Indeed, contrary to earlier research, our results suggest that some classification techniques tend to produce defect prediction models that outperform others.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {789–800},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/1414004.1414013,
author = {Heckman, Sarah and Williams, Laurie},
title = {On Establishing a Benchmark for Evaluating Static Analysis Alert Prioritization and Classification Techniques},
year = {2008},
isbn = {9781595939715},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1414004.1414013},
doi = {10.1145/1414004.1414013},
abstract = {Benchmarks provide an experimental basis for evaluating software engineering processes or techniques in an objective and repeatable manner. We present the FAULTBENCH v0.1 benchmark, as a contribution to current benchmark materials, for evaluation and comparison of techniques that prioritize and classify alerts generated by static analysis tools. Static analysis tools may generate an overwhelming number of alerts, the majority of which are likely to be false positives (FP). Two FP mitigation techniques, alert prioritization and classification, provide an ordering or classification of alerts, identifying those likely to be anomalies. We evaluate FAULTBENCH using three versions of a FP mitigation technique within the AWARE adaptive prioritization model. Individual FAULTBENCH subjects vary in their optimal FP mitigation techniques. Together, FAULTBENCH subjects provide a precise and general evaluation of FP mitigation techniques.},
booktitle = {Proceedings of the Second ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {41–50},
numpages = {10},
keywords = {benchmark creation, automated static analysis, false positive mitigation, alert classification, alert prioritization},
location = {Kaiserslautern, Germany},
series = {ESEM '08}
}

@inproceedings{10.1145/1450058.1450071,
author = {Alur, Rajeev and Kanade, Aditya and Ramesh, S. and Shashidhar, K. C.},
title = {Symbolic Analysis for Improving Simulation Coverage of Simulink/Stateflow Models},
year = {2008},
isbn = {9781605584683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1450058.1450071},
doi = {10.1145/1450058.1450071},
abstract = {Aimed at verifying safety properties and improving simulation coverage for hybrid systems models of embedded control software, we propose a technique that combines numerical simulation and symbolic methods for computing state-sets. We consider systems with linear dynamics described in the commercial modeling tool Simulink/Stateflow. Given an initial state x, and a discrete-time simulation trajectory, our method computes a set of initial states that are guaranteed to be equivalent to x, where two initial states are considered to be equivalent if the resulting simulation trajectories contain the same discrete components at each step of the simulation. We illustrate the benefits of our method on two case studies. One case study is a benchmark proposed in the literature for hybrid systems verification and another is a Simulink demo model from Mathworks.},
booktitle = {Proceedings of the 8th ACM International Conference on Embedded Software},
pages = {89–98},
numpages = {10},
keywords = {Stateflow, coverage, hybrid systems, simulations, Simulink},
location = {Atlanta, GA, USA},
series = {EMSOFT '08}
}

@inproceedings{10.1145/1236360.1236385,
author = {Fraboulet, Antoine and Chelius, Guillaume and Fleury, Eric},
title = {Worldsens: Development and Prototyping Tools for Application Specific Wireless Sensors Networks},
year = {2007},
isbn = {9781595936387},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1236360.1236385},
doi = {10.1145/1236360.1236385},
abstract = {In this paper we present Worldsens, an integrated environment for development and rapid prototyping of wireless sensor network applications. Our environment relies on software simulation to help the designer during the whole development process. The refinement is done starting from the high level design choices down to the target code implementation, debug and performance analysis. In the early stages of the design, high level parameters, like for example the node sleep and activity periods, can be tuned using WS-Net, an event driven wireless network simulator. WSNet uses models for applications, protocols and radio medium communication with a parameterized accuracy. The second step of the sensor network application design takes place after the hardware implementation choices. This second step relies on the WSim cycle accurate hardware platform simulator. WSim is used to debug the application using the real target binary code. Precise performance evaluation, including real-time analysis at the interrupt level, are made possible at this low simulation level. WSim can be connected to WSNet, in place of the application and protocol models used during the high level simulation to achieve a full distributed application simulation. WSNet and WSNet+WSim allow a continuous refinement from high level estimations down to low level real-time validation. We illustrate the complete application design process using a real life demonstrator that implements a hello protocol for dynamic neighborhood discovery in a wireless sensor network environment.},
booktitle = {Proceedings of the 6th International Conference on Information Processing in Sensor Networks},
pages = {176–185},
numpages = {10},
keywords = {performance, simulation, sensor networks, development},
location = {Cambridge, Massachusetts, USA},
series = {IPSN '07}
}

@inproceedings{10.1145/1394504.1394507,
author = {Jetley, Raoul Praful and Jones, Paul L. and Anderson, Paul},
title = {Static Analysis of Medical Device Software Using CodeSonar},
year = {2008},
isbn = {9781595939241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1394504.1394507},
doi = {10.1145/1394504.1394507},
abstract = {Post-market investigators at the United States Food and Drug Administration may need to review medical device software to assess its integrity. They have to do this with little or no prior knowledge of the software. Historically, the only way to perform such a review has been to manually search the code for potential sources of error --- a process that is both tedious and error-prone.Static analysis tools can improve this process by providing a means for automated error detection. By using symbolic execution techniques to explore execution paths of the software, static analysis provides complete, or almost complete, coverage of the code, and helps detect potentially fatal errors that may not easily be detected through conventional testing methods. Using automated static analysis tools can help reduce the effort involved in analysis and provide a more accurate assessment of the software.In this paper, we discuss CodeSonar, a whole-program interprocedural static analysis tool for C/C++ programs, and illustrate how it was used to facilitate error detection during a post-market investigation.},
booktitle = {Proceedings of the 2008 Workshop on Static Analysis},
pages = {22–29},
numpages = {8},
location = {Tucson, Arizona},
series = {SAW '08}
}

@inproceedings{10.1145/3297156.3297227,
author = {Yang, Chunyu and Liu, Yan and Yu, Jia},
title = {Exploring Violations of Programming Styles: Insights from Open Source Projects},
year = {2018},
isbn = {9781450366069},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297156.3297227},
doi = {10.1145/3297156.3297227},
abstract = {Software project is usually a huge cooperative teamwork, programmers in the project usually have to read the code written by others and understand its implementation. A uniform and clean programming style could ensure the readability and maintainability of the project source code, especially when it becomes a legacy project. However, each programmer has his own programming habit and because of the heavy developing tasks, the programming style of the software project is far from satisfactory. Programming style does not resemble software defects which has a serious effect on program executing. Therefore, many programmers ignore the programming style directly instead of improving it. Programming style should be checked before new features are merged into software projects, just like software testing. Developing with the size of software project, some special programming style rules are violated more seriously, which need be highly focused. Furthermore, one of ultimate targets in software quality engineering is to check the programming style automatically with analysis tools because the software projects usually have an enormous quantity of source code. In this paper, static source code analysis is used for detecting the programming style problems. The source file directly or the class files generated by the compiler are scanned then the abstract syntax tree for the source code is generated. With the help of abstract syntax tree, it is possible to detect code snippets that violate the programming style rules by traverse the tree. Our method employs the static code analysis tools to analyze several Java open source projects, and find that the programming style problems which are violated most. According to our method, each problem is also explained from personal habits, JDK version, and other aspects later. Considering all of the analysis results, a special ruleset that is recommended to pay more attention to in the future software developing is proposed. At last, programming style should be highly valued in software development processes in further project management.},
booktitle = {Proceedings of the 2018 2nd International Conference on Computer Science and Artificial Intelligence},
pages = {185–189},
numpages = {5},
keywords = {Code review, Static source code analysis, Programming style},
location = {Shenzhen, China},
series = {CSAI '18}
}

@inproceedings{10.1145/3468264.3468554,
author = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
title = {Vet: Identifying and Avoiding UI Exploration Tarpits},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468554},
doi = {10.1145/3468264.3468554},
abstract = {Despite over a decade of research, it is still challenging for mobile UI testing tools to achieve satisfactory effectiveness, especially on industrial apps with rich features and large code bases. Our experiences suggest that existing mobile UI testing tools are prone to exploration tarpits, where the tools get stuck with a small fraction of app functionalities for an extensive amount of time. For example, a tool logs out an app at early stages without being able to log back in, and since then the tool gets stuck with exploring the app’s pre-login functionalities (i.e., exploration tarpits) instead of its main functionalities. While tool vendors/users can manually hardcode rules for the tools to avoid specific exploration tarpits, these rules can hardly generalize, being fragile in face of diverted testing environments, fast app iterations, and the demand of batch testing product lines. To identify and resolve exploration tarpits, we propose VET, a general approach including a supporting system for the given specific Android UI testing tool on the given specific app under test (AUT). VET runs the tool on the AUT for some time and records UI traces, based on which VET identifies exploration tarpits by recognizing their patterns in the UI traces. VET then pinpoints the actions (e.g., clicking logout) or the screens that lead to or exhibit exploration tarpits. In subsequent test runs, VET guides the testing tool to prevent or recover from exploration tarpits. From our evaluation with state-of-the-art Android UI testing tools on popular industrial apps, VET identifies exploration tarpits that cost up to 98.6% testing time budget. These exploration tarpits reveal not only limitations in UI exploration strategies but also defects in tool implementations. VET automatically addresses the identified exploration tarpits, enabling each evaluated tool to achieve higher code coverage and improve crash-triggering capabilities.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {83–94},
numpages = {12},
keywords = {UI testing, trace analysis, Android testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3377811.3380357,
author = {Xia, Hao and Zhang, Yuan and Zhou, Yingtian and Chen, Xiaoting and Wang, Yang and Zhang, Xiangyu and Cui, Shuaishuai and Hong, Geng and Zhang, Xiaohan and Yang, Min and Yang, Zhemin},
title = {How Android Developers Handle Evolution-Induced API Compatibility Issues: A Large-Scale Study},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380357},
doi = {10.1145/3377811.3380357},
abstract = {As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them.In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in counter-measure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {886–898},
numpages = {13},
keywords = {Android app analysis, compatibility issues, API evolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2851141.2851149,
author = {Machado, Nuno and Lucia, Brandon and Rodrigues, Lu\'{\i}s},
title = {Production-Guided Concurrency Debugging},
year = {2016},
isbn = {9781450340922},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851141.2851149},
doi = {10.1145/2851141.2851149},
abstract = {Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug, because their root causes imply not only different event orderings, but also changes in the control-flow between failing and non-failing executions. We present Cortex: a system that helps exposing and understanding concurrency bugs that result from schedule-dependent branches, without relying on information from failing executions. Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program. By leveraging this information from production runs, Cortex synthesizes executions to guide the search for failing schedules. Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing executions. Evaluation on popular benchmarks shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions, and takes a practical amount of time.},
booktitle = {Proceedings of the 21st ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
articleno = {29},
numpages = {12},
location = {Barcelona, Spain},
series = {PPoPP '16}
}

@article{10.1145/3016078.2851149,
author = {Machado, Nuno and Lucia, Brandon and Rodrigues, Lu\'{\i}s},
title = {Production-Guided Concurrency Debugging},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3016078.2851149},
doi = {10.1145/3016078.2851149},
abstract = {Concurrency bugs that stem from schedule-dependent branches are hard to understand and debug, because their root causes imply not only different event orderings, but also changes in the control-flow between failing and non-failing executions. We present Cortex: a system that helps exposing and understanding concurrency bugs that result from schedule-dependent branches, without relying on information from failing executions. Cortex preemptively exposes failing executions by perturbing the order of events and control-flow behavior in non-failing schedules from production runs of a program. By leveraging this information from production runs, Cortex synthesizes executions to guide the search for failing schedules. Production-guided search helps cope with the large execution search space by targeting failing executions that are similar to observed non-failing executions. Evaluation on popular benchmarks shows that Cortex is able to expose failing schedules with only a few perturbations to non-failing executions, and takes a practical amount of time.},
journal = {SIGPLAN Not.},
month = {feb},
articleno = {29},
numpages = {12}
}

@article{10.1145/3485464,
author = {Hough, Katherine and Bell, Jonathan},
title = {A Practical Approach for Dynamic Taint Tracking with Control-Flow Relationships},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3485464},
doi = {10.1145/3485464},
abstract = {Dynamic taint tracking, a technique that traces relationships between values as a program executes, has been used to support a variety of software engineering tasks. Some taint tracking systems only consider data flows and ignore control flows. As a result, relationships between some values are not reflected by the analysis. Many applications of taint tracking either benefit from or rely on these relationships being traced, but past works have found that tracking control flows resulted in over-tainting, dramatically reducing the precision of the taint tracking system. In this article, we introduce Conflux, alternative semantics for propagating taint tags along control flows. Conflux aims to reduce over-tainting by decreasing the scope of control flows and providing a heuristic for reducing loop-related over-tainting. We created a Java implementation of Conflux and performed a case study exploring the effect of Conflux on a concrete application of taint tracking, automated debugging. In addition to this case study, we evaluated Conflux’s accuracy using a novel benchmark consisting of popular, real-world programs. We compared Conflux against existing taint propagation policies, including a state-of-the-art approach for reducing control-flow-related over-tainting, finding that Conflux had the highest F1 score on 43 out of the 48 total tests.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {26},
numpages = {43},
keywords = {Taint tracking, dynamic information flow, control flow analysis}
}

@inproceedings{10.1109/ASE.2009.13,
author = {Lin, Yu and Tang, Xucheng and Chen, Yuting and Zhao, Jianjun},
title = {A Divergence-Oriented Approach to Adaptive Random Testing of Java Programs},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.13},
doi = {10.1109/ASE.2009.13},
abstract = {Adaptive Random Testing (ART) is a testing technique which is based on an observation that a test input usually has the same potential as its neighbors in detection of a specific program defect. ART helps to improve the efficiency of random testing in that test inputs are selected evenly across the input spaces. However, the application of ART to object-oriented programs (e.g., C++ and Java) still faces a strong challenge in that the input spaces of object-oriented programs are usually high dimensional, and therefore an even distribution of test inputs in a space as such is difficult to achieve. In this paper, we propose a divergence-oriented approach to adaptive random testing of Java programs to address this challenge. The essential idea of this approach is to prepare for the tested program a pool of test inputs each of which is of significant difference from the others, and then to use the ART technique to select test inputs from the pool for the tested program. We also develop a tool called ARTGen to support this testing approach, and conduct experiment to test several popular opensource Java packages to assess the effectiveness of the approach. The experimental result shows that our approach can generate test cases with high quality.},
booktitle = {Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering},
pages = {221–232},
numpages = {12},
series = {ASE '09}
}

@inproceedings{10.1145/3503222.3507748,
author = {Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung},
title = {HeteroGen: Transpiling C to Heterogeneous HLS Code with Automated Test Generation and Program Repair},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507748},
doi = {10.1145/3503222.3507748},
abstract = {Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code.  An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1017–1029},
numpages = {13},
keywords = {program repair, Heterogeneous applications, test generation},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

