@inproceedings{10.1145/3107091.3107094,
author = {Groce, Alex and Flikkema, Paul and Holmes, Josie},
title = {Towards Automated Composition of Heterogeneous Tests for Cyber-Physical Systems},
year = {2017},
isbn = {9781450351126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107091.3107094},
doi = {10.1145/3107091.3107094},
abstract = { A key trait of modern cyber-physical systems (CPS) is complexity due to the number of components and layers in these systems. Unlike in traditional software development, where the device layer is essentially completely abstracted away by an operating system, CPS components include low-power edge nodes, gateways, and servers that together provide sensing, actuation, communication, model and state inference, and autonomous or user-driven control. Moreover, the CPS design process involves implementation of these functions at different levels of abstraction, from high-level computational models to bare-mental implementations. Unfortunately, even when advanced testing or verification methods are applied only to low level system aspects, those efforts are separated from high-level tests of a CPS, which are often produced by a different team, and do not stress the low-level system. Effective automated test composition would make it possible to automatically produce integration/system tests for CPS, even with extremely heterogeneous aspects, where individual elements have effective tests but the interactions between the sub-systems are untested. Because of the size of the search space involved and the complexity of modeling and designing CPS, we also propose in the long term a move towards system architectures to support testing across both system layers and levels of abstraction. },
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems},
pages = {12–15},
numpages = {4},
keywords = {test composition, software architecture, cyber-physical systems, testability},
location = {Santa Barbara, CA, USA},
series = {TECPS 2017}
}

@inproceedings{10.1145/2889160.2891043,
author = {Su, Ting},
title = {FSMdroid: Guided GUI Testing of Android Apps},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2891043},
doi = {10.1145/2889160.2891043},
abstract = {GUI testing has been an effective means of validating Android apps. Meanwhile, it still faces a strong challenge about how to explore trails, i.e., unfrequented test sequences, as defects tend to reside on these unfrequented trails. This paper introduces FSMdroid, a novel, guided approach to GUI testing of Android apps. The essential idea of FSMdroid is to (1) construct an initial stochastic model for the app under test, (2) iteratively mutate the stochastic model and derive tests. The model mutations are guided by an MCMC sampling method such that the resulting test sequences can be diverse and also achieve high code coverage during testing. We have evaluated FSMdroid on 40 real-world Android apps. Compared with the traditional model-based testing approaches, FSMdroid enhances the diversity of test sequences by 85%, but reduces the number of them by 54%. Furthermore, we uncover 7 app bugs.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {689–691},
numpages = {3},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2786805.2786835,
author = {Chen, Yuting and Su, Zhendong},
title = {Guided Differential Testing of Certificate Validation in SSL/TLS Implementations},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786835},
doi = {10.1145/2786805.2786835},
abstract = { Certificate validation in SSL/TLS implementations is critical for Internet security. There is recent strong effort, namely frankencert, in automatically synthesizing certificates for stress-testing certificate validation. Despite its early promise, it remains a significant challenge to generate effective test certificates as they are structurally complex with intricate syntactic and semantic constraints. This paper tackles this challenge by introducing mucert, a novel, guided technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates, and (2) diversify them by adapting Markov Chain Monte Carlo (MCMC) sampling. The diversified certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We have implemented mucert and extensively evaluated it against frankencert. Our experimental results show that mucert is significantly more cost-effective than frankencert. Indeed, 1K mucerts (i.e., mucert-mutated certificates) yield three times as many distinct discrepancies as 8M frankencerts (i.e., frankencert-synthesized certificates), and 200 mucerts can achieve higher code coverage than 100,000 frankencerts. This improvement is significant as it incurs much cost to test each generated certificate. We have analyzed and reported 20+ latent discrepancies (presumably missed by frankencert), and reported an additional 357 discrepancy-triggering certificates to SSL/TLS developers, who have already confirmed some of our reported issues and are investigating causes of all the reported discrepancies. In particular, our reports have led to bug fixes, active discussions in the community, and proposed changes to relevant IETF’s RFCs. We believe that mucert is practical and effective for helping improve the robustness of SSL/TLS implementations. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {793–804},
numpages = {12},
keywords = {certificate validation, Differential testing, mutation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/1005140.1005159,
author = {Wu, Xintao and Wang, Yongge and Zheng, Yuliang},
title = {Privacy Preserving Database Application Testing},
year = {2003},
isbn = {1581137761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1005140.1005159},
doi = {10.1145/1005140.1005159},
abstract = {Traditionally, application software developers carry out their tests on their own local development databases. However, such local databases usually have only a small number of sample data and hence cannot simulate satisfactorily a live environment, especially in terms of performance and scalability testing. On the other hand, the idea of testing applications over live production databases is increasingly problematic in most situations primarily due to the fact that such use of live production databases has the potential to expose sensitive data to an unauthorized tester and to incorrectly update information in the underlying database. In this paper, we investigate techniques to generate mock databases for application software testing without revealing any confidential information from the live production databases. Specifically, we will design mechanisms to create the deterministic rule set R, non-deterministic rule set N R, and statistic data set S for a live production database. We will then build a security Analyzer which will process the triplet &lt;R',N R',S'&gt; together with security requirements (security policy) and output a new triplet &lt;R',N R',S'&gt; The security Analyzer will guarantee that no confidential information could be inferred from the new triplet &lt;R',N R',S'&gt; The mock database generated from this new triplet can simulate the live environment for testing purpose, while maintaining the privacy of data in the original database.},
booktitle = {Proceedings of the 2003 ACM Workshop on Privacy in the Electronic Society},
pages = {118–128},
numpages = {11},
keywords = {indistinguishability, database application testing, privacy},
location = {Washington, DC},
series = {WPES '03}
}

@inproceedings{10.1145/3382494.3422157,
author = {Coviello, Carmen and Romano, Simone and Scanniello, Giuseppe and Antoniol, Giuliano},
title = {GASSER: Genetic Algorithm for TeSt Suite Reduction},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3422157},
doi = {10.1145/3382494.3422157},
abstract = {Background. Regression testing is a practice that ensures a System Under Test (SUT) still works as expected after changes. The simplest regression testing approach is Retest-all, which consists of re-executing the entire Test Suite (TS) on the new version of the SUT. When SUT and its TS grow in size, applying Retest-all could be expensive. Test Suite Reduction (TSR) approaches would allow overcoming the above-mentioned issues by reducing TSs while preserving their fault-detection capability.Aim. In this paper, we introduce GASSER (Genetic Algorithm for teSt SuitE Reduction), a new approach for TSR based on a multi-objective evolutionary algorithm, namely NSGA-II.Method. GASSER reduces TSs by maximizing statement coverage and diversity of test cases, and by minimizing the size of the reduced TSs.Results. The preliminary study shows that GASSER reduces more the TS size with a small effect on fault-detection capability when compared with traditional approaches.Conclusions. These outcomes highlight the potential benefits of the use of multi-objective evolutionary algorithm in TSR field and pose the basis for future work.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {36},
numpages = {6},
keywords = {Test Suite Reduction, Genetic Algorithm, Regression Testing},
location = {Bari, Italy},
series = {ESEM '20}
}

@article{10.1145/3510416,
author = {Nie, Pengbo and Wan, Chengcheng and Zhu, Jiayu and Lin, Ziyi and Chen, Yuting and Su, Zhendong},
title = {Coverage-Directed Differential Testing of X.509 Certificate Validation in SSL/TLS Implementations},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3510416},
doi = {10.1145/3510416},
abstract = {SSL and TLS are two secure protocols for creating secure connections over the Internet. X.509 certificate validation is important for security and needs to be performed before an SSL/TLS connection is established. Some advanced testing techniques, such as frankencert, have revealed, through randomly mutating Internet accessible certificates, that there exist unexpected, sometimes critical, validation differences among different SSL/TLS implementations. Despite these efforts, X.509 certificate validation still needs to be thoroughly tested as this work shows. This paper tackles this challenge by proposing transcert, a coverage-directed technique to much more effectively test real-world certificate validation code. Our core insight is to (1) leverage easily accessible Internet certificates as seed certificates, and (2) use code coverage to direct certificate mutation towards generating a set of diverse certificates. The generated certificates are then used to reveal discrepancies, thus potential flaws, among different certificate validation implementations. We implement transcert and evaluate it against frankencert, NEZHA, and RFCcert (three advanced fuzzing techniques) on five widely used SSL/TLS implementations. The evaluation results clearly show the strengths of transcert — during 10,000 iterations, transcert reveals 71 unique validation differences, 12 \texttimes{}, 1.4 \texttimes{}, and 7 \texttimes{} as many as those revealed by frankencert, NEZHA and RFCcert, respectively; it also supplements RFCcert in conformance testing of the SSL/TLS implementations against 120 validation rules, 85 of which are exclusively covered by transcert-generated certificates. We identify 17 root causes of validation differences, all of which have been confirmed and eleven have never been reported previously. The transcert-generated X.509 certificates also reveal that the primary goal of certificate chain validation is stated ambiguously in the widely-adopted PKI standard RFC 5280.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Certificate Validation, Certification Mutation, Differential Testing, Coverage Transfer Graph}
}

@inbook{10.1145/3387939.3391606,
author = {Liu, Kun and Zhang, Xiaoyi and Arcaini, Paolo and Ishikawa, Fuyuki and Jiao, Wenpin},
title = {Leveraging Test Logs for Building a Self-Adaptive Path Planner},
year = {2020},
isbn = {9781450379625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387939.3391606},
abstract = {Recent approaches in testing autonomous driving systems (ADS) are able to generate a scenario in which the autonomous car collides, and a different ADS configuration that avoids the collision. However, such test information is too low level to be used by engineers to improve the ADS. In this paper, we consider a path planner component provided by our industry partner, that can be configured through some weights. We propose a technique to automatically re-engineer the path planner in terms of a self-adaptive path planner (SAPP) following the MAPE loop reference architecture. The Knowledge Base (KB) of SAPP contains descriptions of collision scenarios discovered with testing, and the corresponding alternative weights that avoid the collisions. We forecast two main usages of SAPP. First of all, designers are provided with a prototype that should facilitate the re-implementation of the path planner. As second usage, SAPP can be useful for improving the diversity of testing, as performing test case generation on SAPP will guarantee to find dangerous situations different from those used to build SAPP. Preliminary experiments indicate that SAPP can effectively adapt on the base of the solutions stored in KB.},
booktitle = {Proceedings of the IEEE/ACM 15th International Symposium on Software Engineering for Adaptive and Self-Managing Systems},
pages = {57–63},
numpages = {7}
}

@inproceedings{10.1145/1499949.1500143,
author = {Thayer, R. H. and Hinton, E. S.},
title = {Software Reliability: A Method That Works},
year = {1975},
isbn = {9781450379199},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1499949.1500143},
doi = {10.1145/1499949.1500143},
abstract = {Software reliability is receiving increased attention from a broad spectrum of computer users as larger computer programs continue to be implemented in diverse and widespread areas. The reason is fundamental: software reliability has been poor on many large systems and poor on systems which have a high degree of human interaction.},
booktitle = {Proceedings of the May 19-22, 1975, National Computer Conference and Exposition},
pages = {877–883},
numpages = {7},
location = {Anaheim, California},
series = {AFIPS '75}
}

@inproceedings{10.1109/ASE.2011.6100093,
author = {Zhang, Pingyu and Elbaum, Sebastian and Dwyer, Matthew B.},
title = {Automatic Generation of Load Tests},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100093},
doi = {10.1109/ASE.2011.6100093},
abstract = {Load tests aim to validate whether system performance is acceptable under peak conditions. Existing test generation techniques induce load by increasing the size or rate of the input. Ignoring the particular input values, however, may lead to test suites that grossly mischaracterize a system's performance. To address this limitation we introduce a mixed symbolic execution based approach that is unique in how it 1) favors program paths associated with a performance measure of interest, 2) operates in an iterative-deepening beam-search fashion to discard paths that are unlikely to lead to high-load tests, and 3) generates a test suite of a given size and level of diversity. An assessment of the approach shows it generates test suites that induce program response times and memory consumption several times worse than the compared alternatives, it scales to large and complex inputs, and it exposes a diversity of resource consuming program behavior.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {43–52},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.5555/2820116.2820123,
author = {Guaiani, Fabio and Muccini, Henry},
title = {Crowd and Laboratory Testing Can They Co-Exist? An Exploratory Study},
year = {2015},
publisher = {IEEE Press},
abstract = {Crowd testing has gained a great attention in recent years, for its cost-effectiveness, impartiality, diversity, and high device and configuration coverage. Still, a number of challenges hamper its full success, such as lack of standards, limited information on critical features coverage, duplicate defect management, inappropriate reword mechanisms. Our intuition is that combining crowd testing with (a more traditional) laboratory testing, may compensate each other limitations. In order to explore how practitioners look at this possibility, we run a survey with crowd testers to understand their perception on this matter. Preliminary results are illustrated in this work.},
booktitle = {Proceedings of the Second International Workshop on CrowdSourcing in Software Engineering},
pages = {32–37},
numpages = {6},
keywords = {crowd testing, laboratory and crowd testing, exploratory study},
location = {Florence, Italy},
series = {CSI-SE '15}
}

@inproceedings{10.1145/3383219.3383230,
author = {Zhang, Xiao-Yi and Zheng, Zheng},
title = {Exploring the Characteristics of Spectra Distribution and Their Impacts on Fault Localization},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383230},
doi = {10.1145/3383219.3383230},
abstract = {Spectrum-Based Fault Localization (SBFL) follows the basic intuitions that the faulty parts are more likely to be covered by failure-revealing test cases and less likely to be covered by passed test cases. However, due to the diversity of programs and faults, many other characteristics (related to program structure, test suites, and type of faulty components) will influence the practical application of SBFL. For example, a statement can be covered by numerous failure-revealing test cases, and also covered by numerous passed test cases. To get more indicators about the faulty components towards a better application of SBFL, we extend the scope of spectrum-based knowledge from the basic intuitions to the Characteristics of Spectra Distribution (CSDs for short). That is, we explore the relationships between different types of statements and their spectra. Firstly, we introduce the concepts of Failure-Independent, Failure-Related, and Failure-Exclusionary to describe the relationships between different types of statements and their executions. Then, we propose two probabilistic models, with and without the noise of fault interference, respectively, to identify various CSDs for each type of statements. As the analysis results, we introduce a visualization technique to generalize the identified CSDs and provide an overall picture of spectra distribution and its dynamics. Finally, based on our analysis and also the observation of the program spectra of current benchmarks, we design a technique to filter the potential non-faulty statements to improve the accuracy of SBFL.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {100–109},
numpages = {10},
keywords = {software fault localization, spectrum-based fault localization, probabilistic model, spectrum-based characteristics, visualization},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/2786805.2786818,
author = {Matinnejad, Reza and Nejati, Shiva and Briand, Lionel C. and Bruckmann, Thomas},
title = {Effective Test Suites for Mixed Discrete-Continuous Stateflow Controllers},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786818},
doi = {10.1145/2786805.2786818},
abstract = { Modeling mixed discrete-continuous controllers using Stateflow is common practice and has a long tradition in the embedded software system industry. Testing Stateflow models is complicated by expensive and manual test oracles that are not amenable to full automation due to the complex continuous behaviors of such models. In this paper, we reduce the cost of manual test oracles by providing test case selection algorithms that help engineers develop small test suites with high fault revealing power for Stateflow models. We present six test selection algorithms for discrete-continuous Stateflows: An adaptive random test selection algorithm that diversifies test inputs, two white-box coverage-based algorithms, a black-box algorithm that diversifies test outputs, and two search-based black-box algorithms that aim to maximize the likelihood of presence of continuous output failure patterns. We evaluate and compare our test selection algorithms, and find that our three output-based algorithms consistently outperform the coverage- and input-based algorithms in revealing faults in discrete-continuous Stateflow models. Further, we show that our output-based algorithms are complementary as the two search-based algorithms perform best in revealing specific failures with small test suites, while the output diversity algorithm is able to identify different failure types better than other algorithms when test suites are above a certain size. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {84–95},
numpages = {12},
keywords = {structural coverage, failure-based testing, output diversity, Stateflow testing, mixed discrete-continuous behaviors},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3338906.3338911,
author = {Durieux, Thomas and Madeiral, Fernanda and Martinez, Matias and Abreu, Rui},
title = {Empirical Review of Java Program Repair Tools: A Large-Scale Experiment on 2,141 Bugs and 23,551 Repair Attempts},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338911},
doi = {10.1145/3338906.3338911},
abstract = {In the past decade, research on test-suite-based automatic program repair has grown significantly. Each year, new approaches and implementations are featured in major software engineering venues. However, most of those approaches are evaluated on a single benchmark of bugs, which are also rarely reproduced by other researchers. In this paper, we present a large-scale experiment using 11 Java test-suite-based repair tools and 2,141 bugs from 5 benchmarks. Our goal is to have a better understanding of the current state of automatic program repair tools on a large diversity of benchmarks. Our investigation is guided by the hypothesis that the repairability of repair tools might not be generalized across different benchmarks. We found that the 11 tools 1) are able to generate patches for 21% of the bugs from the 5 benchmarks, and 2) have better performance on Defects4J compared to other benchmarks, by generating patches for 47% of the bugs from Defects4J compared to 10-30% of bugs from the other benchmarks. Our experiment comprises 23,551 repair attempts, which we used to find causes of non-patch generation. These causes are reported in this paper, which can help repair tool designers to improve their approaches and tools.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {302–313},
numpages = {12},
keywords = {patch generation, Automatic program repair, benchmark overfitting},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inbook{10.5555/1074100.1074503,
author = {Hellerman, Herbert and Hemmendinger, David},
title = {Interrupt},
year = {2003},
isbn = {0470864125},
publisher = {John Wiley and Sons Ltd.},
address = {GBR},
abstract = {The capability to interrupt a program, an important feature of most modern computer systems, permits systems to respond quickly to external or exceptional events that occur at unpredictable times. Some external events of this type are input arriving from a keyboard, modem (q.v.), or network; a signal that an output device like a printer is ready for data; or a signal generated by an instrument or sensor monitoring some industrial or laboratory process. Exceptional events include invalid memory references during program execution, division by zero, or an attempt to execute an illegal instruction. The response to an interrupt is the invocation of a responding subprogram (q.v.) and, in this respect, an interrupt resembles a subprogram call, but one that is initiated by a hardware device rather than by the main program. The essential characteristic of interrupts is the great diversity of their causes and their unpredictability.},
booktitle = {Encyclopedia of Computer Science},
pages = {928–931},
numpages = {4}
}

@inproceedings{10.1145/3278122.3278130,
author = {Ruland, Sebastian and Luthmann, Lars and B\"{u}rdek, Johannes and Lity, Sascha and Th\"{u}m, Thomas and Lochau, Malte and Ribeiro, M\'{a}rcio},
title = {Measuring Effectiveness of Sample-Based Product-Line Testing},
year = {2018},
isbn = {9781450360456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278122.3278130},
doi = {10.1145/3278122.3278130},
abstract = {Recent research on quality assurance (QA) of configurable software systems (e.g., software product lines) proposes different analysis strategies to cope with the inherent complexity caused by the well-known combinatorial-explosion problem. Those strategies aim at improving efficiency of QA techniques like software testing as compared to brute-force configuration-by-configuration analysis. Sampling constitutes one of the most established strategies, defining criteria for selecting a drastically reduced, yet sufficiently diverse subset of software configurations considered during QA. However, finding generally accepted measures for assessing the impact of sample-based analysis on the effectiveness of QA techniques is still an open issue. We address this problem by lifting concepts from single-software mutation testing to configurable software. Our framework incorporates a rich collection of mutation operators for product lines implemented in C to measure mutation scores of samples, including a novel family-based technique for product-line mutation detection. Our experimental results gained from applying our tool implementation to a collection of subject systems confirms the widely-accepted assumption that pairwise sampling constitutes the most reasonable efficiency/effectiveness trade-off for sample-based product-line testing.},
booktitle = {Proceedings of the 17th ACM SIGPLAN International Conference on Generative Programming: Concepts and Experiences},
pages = {119–133},
numpages = {15},
keywords = {Mutation Testing, Sample-Based Testing, Software Product Lines},
location = {Boston, MA, USA},
series = {GPCE 2018}
}

@article{10.1145/3393934.3278130,
author = {Ruland, Sebastian and Luthmann, Lars and B\"{u}rdek, Johannes and Lity, Sascha and Th\"{u}m, Thomas and Lochau, Malte and Ribeiro, M\'{a}rcio},
title = {Measuring Effectiveness of Sample-Based Product-Line Testing},
year = {2018},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {9},
issn = {0362-1340},
url = {https://doi.org/10.1145/3393934.3278130},
doi = {10.1145/3393934.3278130},
abstract = {Recent research on quality assurance (QA) of configurable software systems (e.g., software product lines) proposes different analysis strategies to cope with the inherent complexity caused by the well-known combinatorial-explosion problem. Those strategies aim at improving efficiency of QA techniques like software testing as compared to brute-force configuration-by-configuration analysis. Sampling constitutes one of the most established strategies, defining criteria for selecting a drastically reduced, yet sufficiently diverse subset of software configurations considered during QA. However, finding generally accepted measures for assessing the impact of sample-based analysis on the effectiveness of QA techniques is still an open issue. We address this problem by lifting concepts from single-software mutation testing to configurable software. Our framework incorporates a rich collection of mutation operators for product lines implemented in C to measure mutation scores of samples, including a novel family-based technique for product-line mutation detection. Our experimental results gained from applying our tool implementation to a collection of subject systems confirms the widely-accepted assumption that pairwise sampling constitutes the most reasonable efficiency/effectiveness trade-off for sample-based product-line testing.},
journal = {SIGPLAN Not.},
month = {nov},
pages = {119–133},
numpages = {15},
keywords = {Sample-Based Testing, Mutation Testing, Software Product Lines}
}

@inbook{10.1145/3368089.3409723,
author = {She, Dongdong and Krishna, Rahul and Yan, Lu and Jana, Suman and Ray, Baishakhi},
title = {MTFuzz: Fuzzing with a Multi-Task Neural Network},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409723},
abstract = {Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation.Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model.In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e.,predicting for different types of coverage). The compact embedding can guide the mutation process by focusing most of the mutations on the parts of the embedding where the gradient is high. MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2\texttimes{} more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {737–749},
numpages = {13}
}

@article{10.1145/151257.151258,
author = {DeMillo, Richard A. and Offutt, A. Jefferson},
title = {Experimental Results from an Automatic Test Case Generator},
year = {1993},
issue_date = {April 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/151257.151258},
doi = {10.1145/151257.151258},
abstract = {Constraint-based testing is a novel way of generating test data to detect specific types of common programming faults. The conditions under which faults will be detected are encoded as mathematical systems of constraints in terms of program symbols. A set of tools, collectively called Godzilla, has been implemented that automatically generates constraint systems and solves them to create test cases for use by the Mothra testing system. Experimental results from using Godzilla show that the technique can produce test data that is very close in terms of mutation adequacy to test data that is produced manually, and at substantially reduced cost. Additionally, these experiments have suggested a new procedure for unit testing, where test cases are viewed as throw-away items rather than scarce resources.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
pages = {109–127},
numpages = {19},
keywords = {constraints, mutation analysis, adequacy}
}

@inproceedings{10.1145/2631890.2631894,
author = {Garn, Bernhard and Kapsalis, Ioannis and Simos, Dimitris E. and Winkler, Severin},
title = {On the Applicability of Combinatorial Testing to Web Application Security Testing: A Case Study},
year = {2014},
isbn = {9781450329330},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2631890.2631894},
doi = {10.1145/2631890.2631894},
abstract = { Case studies for evaluating tools in security testing are powerful. Although they cannot achieve the scientific rigor of formal experiments, the results can provide sufficient information to help professionals judge if a specific technology being evaluated will benefit their organization. This paper reports on a case study done for evaluating and revisiting a recently introduced combinatorial testing methodology used for web application security purposes. It further reports on undertaken practical experiments thus strengthening the applicability of combinatorial testing to web application security testing. },
booktitle = {Proceedings of the 2014 Workshop on Joining AcadeMiA and Industry Contributions to Test Automation and Model-Based Testing},
pages = {16–21},
numpages = {6},
keywords = {Combinatorial testing, penetration testing tools, security testing, web application security},
location = {San Jose, CA, USA},
series = {JAMAICA 2014}
}

@inbook{10.1109/ICSE43902.2021.00046,
author = {Wang, Zan and You, Hanmo and Chen, Junjie and Zhang, Yingyi and Dong, Xuyuan and Zhang, Wenbin},
title = {Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00046},
abstract = {Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {397–409},
numpages = {13}
}

@inproceedings{10.1145/336512.336551,
author = {Littlewood, Bev and Strigini, Lorenzo},
title = {Software Reliability and Dependability: A Roadmap},
year = {2000},
isbn = {1581132530},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/336512.336551},
doi = {10.1145/336512.336551},
booktitle = {Proceedings of the Conference on The Future of Software Engineering},
pages = {175–188},
numpages = {14},
keywords = {dependability modelling and assessment, diversity, reliability engineering, COTS reliability},
location = {Limerick, Ireland},
series = {ICSE '00}
}

