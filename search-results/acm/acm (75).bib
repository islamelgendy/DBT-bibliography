@inproceedings{10.1145/3236024.3236053,
author = {Chen, Junjie and Lou, Yiling and Zhang, Lingming and Zhou, Jianyi and Wang, Xiaoleng and Hao, Dan and Zhang, Lu},
title = {Optimizing Test Prioritization via Test Distribution Analysis},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236053},
doi = {10.1145/3236024.3236053},
abstract = {Test prioritization aims to detect regression faults faster via reordering test executions, and a large number of test prioritization techniques have been proposed accordingly. However, test prioritization effectiveness is usually measured in terms of the average percentage of faults detected concerned with the number of test executions, rather than the actual regression testing time, making it unclear which technique is optimal in actual regression testing time. To answer this question, this paper first conducts an empirical study to investigate the actual regression testing time of various prioritization techniques. The results reveal a number of practical guidelines. In particular, no prioritization technique can always perform optimal in practice.  To achieve the optimal prioritization effectiveness for any given project in practice, based on the findings of this study, we design learning-based Predictive Test Prioritization (PTP). PTP predicts the optimal prioritization technique for a given project based on the test distribution analysis (i.e., the distribution of test coverage, testing time, and coverage per unit time). The results show that PTP correctly predicts the optimal prioritization technique for 46 out of 50 open-source projects from GitHub, outperforming state-of-the-art techniques significantly in regression testing time, e.g., 43.16% to 94.92% improvement in detecting the first regression fault. Furthermore, PTP has been successfully integrated into the practical testing infrastructure of Baidu (a search service provider with over 600M monthly active users), and received positive feedbacks from the testing team of this company, e.g., saving beyond 2X testing costs with negligible overheads.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {656–667},
numpages = {12},
keywords = {Regression Testing, Machine Learning, Test Prioritization},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3067695.3084219,
author = {Paduraru, Ciprian and Melemciuc, Marius-Constantin and Stefanescu, Alin},
title = {A Distributed Implementation Using Apache Spark of a Genetic Algorithm Applied to Test Data Generation},
year = {2017},
isbn = {9781450349390},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3067695.3084219},
doi = {10.1145/3067695.3084219},
abstract = {This paper presents a distributed implementation for a genetic algorithm, using Apache Spark, a fast and popular data processing framework. Our approach is rather general, but in this paper the parallelized genetic algorithm is used for test data generation for executable programs. The viability of the approach is demonstrated on two examples.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1857–1863},
numpages = {7},
keywords = {genetic algorithms, test data generation, distributed implementation, test coverage, apache spark},
location = {Berlin, Germany},
series = {GECCO '17}
}

@inproceedings{10.5555/800244.807312,
author = {Osterweil, Leon J. and Fosdick, Lloyd D.},
title = {Program Testing Techniques Using Simulated Execution},
year = {1976},
publisher = {IEEE Press},
abstract = {Simulation is proving to be a valuable technique in testing computer programs. By simulating different aspects of a program's execution and structure it is possible to detect errors and sometimes demonstrate the absence of certain errors in the program. This presentation will explore three popular testing methodologies which employ simulation techniques. Each methodology is based upon a different type of simulation of the program. The differences in error detection capability resulting from these different choices of simulated execution will be examined. Finally a method for using the best characteristics of each technique in a general validation system will be presented.},
booktitle = {Proceedings of the 4th Symposium on Simulation of Computer Systems},
pages = {171–177},
numpages = {7},
location = {Boulder, Colorado, USA},
series = {ANSS '76}
}

@article{10.1145/1013610.807312,
author = {Osterweil, Leon J. and Fosdick, Lloyd D.},
title = {Program Testing Techniques Using Simulated Execution},
year = {1976},
issue_date = {July 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {0163-6103},
url = {https://doi.org/10.1145/1013610.807312},
doi = {10.1145/1013610.807312},
abstract = {Simulation is proving to be a valuable technique in testing computer programs. By simulating different aspects of a program's execution and structure it is possible to detect errors and sometimes demonstrate the absence of certain errors in the program. This presentation will explore three popular testing methodologies which employ simulation techniques. Each methodology is based upon a different type of simulation of the program. The differences in error detection capability resulting from these different choices of simulated execution will be examined. Finally a method for using the best characteristics of each technique in a general validation system will be presented.},
journal = {SIGSIM Simul. Dig.},
month = {jul},
pages = {171–177},
numpages = {7}
}

@inproceedings{10.5555/2486788.2486930,
author = {Zanetti, Marcelo Serrano and Scholtes, Ingo and Tessone, Claudio Juan and Schweitzer, Frank},
title = {Categorizing Bugs with Social Networks: A Case Study on Four Open Source Software Communities},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Efficient bug triaging procedures are an important precondition for successful collaborative software engineering projects. Triaging bugs can become a laborious task particularly in open source software (OSS) projects with a large base of comparably inexperienced part-time contributors. In this paper, we propose an efficient and practical method to identify valid bug reports which a) refer to an actual software bug, b) are not duplicates and c) contain enough information to be processed right away. Our classification is based on nine measures to quantify the social embeddedness of bug reporters in the collaboration network. We demonstrate its applicability in a case study, using a comprehensive data set of more than 700,000 bug reports obtained from the Bugzilla installation of four major OSS communities, for a period of more than ten years. For those projects that exhibit the lowest fraction of valid bug reports, we find that the bug reporters' position in the collaboration network is a strong indicator for the quality of bug reports. Based on this finding, we develop an automated classification scheme that can easily be integrated into bug tracking platforms and analyze its performance in the considered OSS communities. A support vector machine (SVM) to identify valid bug reports based on the nine measures yields a precision of up to 90.3% with an associated recall of 38.9%. With this, we significantly improve the results obtained in previous case studies for an automated early identification of bugs that are eventually fixed. Furthermore, our study highlights the potential of using quantitative measures of social organization in collaborative software engineering. It also opens a broad perspective for the integration of social awareness in the design of support infrastructures. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1032–1041},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1109/ASE.2003.1240321,
author = {Licata, Daniel R. and Harris, Christopher D. and Krishnamurthi, Shriram},
title = {The Feature Signatures of Evolving Programs},
year = {2003},
isbn = {0769520359},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2003.1240321},
doi = {10.1109/ASE.2003.1240321},
booktitle = {Proceedings of the 18th IEEE International Conference on Automated Software Engineering},
pages = {281–285},
numpages = {5},
location = {Montreal, Quebec, Canada},
series = {ASE'03}
}

@inproceedings{10.1145/1543137.1543161,
author = {Bergvall-K\r{a}reborn, Birgitta and Larsson, Staffan},
title = {A Case Study of Real-World Testing},
year = {2008},
isbn = {9781605581927},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1543137.1543161},
doi = {10.1145/1543137.1543161},
abstract = {In this paper we present the results of a real-world test of a mobile application for public transportation, as well as lessons learnt by carrying out the test. By this we contribute with experience and accumulated knowledge to the area of mobile applications for public transportation and to the area of real-life testing. Through the test we learnt a number of lessons related to the application, related concepts, people's behavior in relation to public transportation, and last but not least in relation to our method.},
booktitle = {Proceedings of the 7th International Conference on Mobile and Ubiquitous Multimedia},
pages = {113–116},
numpages = {4},
keywords = {real-word testing, living lab, public transport, mobile application},
location = {Ume\r{a}, Sweden},
series = {MUM '08}
}

@inproceedings{10.1145/1454630.1454637,
author = {de Paz Alberola, Rodolfo and Pesch, Dirk},
title = {AvroraZ: Extending Avrora with an IEEE 802.15.4 Compliant Radio Chip Model},
year = {2008},
isbn = {9781605582399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1454630.1454637},
doi = {10.1145/1454630.1454637},
abstract = {This paper presents AvroraZ, an extension of the Avrora emulator -- The AVR Simulation and Analysis Framework -- which allows the emulation of the Atmel AVR microcontroller based sensor node platforms with IEEE 802.15.4 compliant radio chips thus allowing emulation of sensor nodes such as Crossbow's MicaZ. AvroraZ is based on design, implementation and verification of several extensions to Avrora: the address recognition algorithm, an indoor radio model, the clear channel assessment (CCA) and link quality indicator (LQI) of the IEEE 802.15.4 standard. We have AvroraZ and demonstrated its correctness in emulating MicaZ code.},
booktitle = {Proceedings of the 3nd ACM Workshop on Performance Monitoring and Measurement of Heterogeneous Wireless and Wired Networks},
pages = {43–50},
numpages = {8},
keywords = {Chipcon cc2420, Avrora, IEEE 802.15.4, wireless sensor networks, sensor node emulation, micaZ},
location = {Vancouver, British Columbia, Canada},
series = {PM2HW2N '08}
}

@inproceedings{10.5555/2486788.2487006,
author = {Jonsson, Leif},
title = {Increasing Anomaly Handling Efficiency in Large Organizations Using Applied Machine Learning},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Maintenance costs can be substantial for large organizations (several hundreds of programmers) with very large and complex software systems. By large we mean lines of code in the range of hundreds of thousands or millions. Our research objective is to improve the process of handling anomaly reports for large organizations. Specifically, we are addressing the problem of the manual, laborious and time consuming process of assigning anomaly reports to the correct design teams and the related issue of localizing faults in the system architecture. In large organizations, with complex systems, this is particularly problematic because the receiver of an anomaly report may not have detailed knowledge of the whole system. As a consequence, anomaly reports may be assigned to the wrong team in the organization, causing delays and unnecessary work. We have so far developed two machine learning prototypes to validate our approach. The latest, a re-implementation and extension, of the first is being evaluated on four large systems at Ericsson AB. Our main goal is to investigate how large software development organizations can significantly improve development efficiency by replacing manual anomaly report assignment and fault localization with machine learning techniques. Our approach focuses on training machine learning systems on anomaly report databases; this is in contrast to many other approaches that are based on test case execution combined with program sampling and/or source code analysis. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {1361–1364},
numpages = {4},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3324884.3416570,
author = {Chen, Junjie and Ma, Haoyang and Zhang, Lingming},
title = {Enhanced Compiler Bug Isolation via Memoized Search},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416570},
doi = {10.1145/3324884.3416570},
abstract = {Compiler bugs can be disastrous since they could affect all the software systems built on the buggy compilers. Meanwhile, diagnosing compiler bugs is extremely challenging since usually limited debugging information is available and a large number of compiler files can be suspicious. More specifically, when compiling a given bug-triggering test program, hundreds of compiler files are usually involved, and can all be treated as suspicious buggy files. To facilitate compiler debugging, in this paper we propose the first reinforcement compiler bug isolation approach via structural mutation, called RecBi. For a given bug-triggering test program, RecBi first augments traditional local mutation operators with structural ones to transform it into a set of passing test programs. Since not all the passing test programs can help isolate compiler bugs effectively, RecBi further leverages reinforcement learning to intelligently guide the process of passing test program generation. Then, RecBi ranks all the suspicious files by analyzing the compiler execution traces of the generated passing test programs and the given failing test program following the practice of compiler bug isolation. The experimental results on 120 real bugs from two most popular C open-source compilers, i.e., GCC and LLVM, show that RecBi is able to isolate about 23%/58%/78% bugs within Top-1/Top-5/Top-10 compiler files, and significantly outperforms the state-of-the-art compiler bug isolation approach by improving 92.86%/55.56%/25.68% isolation effectiveness in terms of Top-1/Top-5/Top-10 results.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {78–89},
numpages = {12},
keywords = {compiler bug isolation, fault localization, reinforcement learning},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/2063239.2063241,
author = {Arnold, Matthew and Vechev, Martin and Yahav, Eran},
title = {QVM: An Efficient Runtime for Detecting Defects in Deployed Systems},
year = {2011},
issue_date = {December 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2063239.2063241},
doi = {10.1145/2063239.2063241},
abstract = {Coping with software defects that occur in the post-deployment stage is a challenging problem: bugs may occur only when the system uses a specific configuration and only under certain usage scenarios. Nevertheless, halting production systems until the bug is tracked and fixed is often impossible. Thus, developers have to try to reproduce the bug in laboratory conditions. Often, the reproduction of the bug takes most of the debugging effort.In this paper we suggest an approach to address this problem by using a specialized runtime environment called Quality Virtual Machine (QVM). QVM efficiently detects defects by continuously monitoring the execution of the application in a production setting. QVM enables the efficient checking of violations of user-specified correctness properties, that is, typestate safety properties, Java assertions, and heap properties pertaining to ownership. QVM is markedly different from existing techniques for continuous monitoring by using a novel overhead manager which enforces a user-specified overhead budget for quality checks. Existing tools for error detection in the field usually disrupt the operation of the deployed system. QVM, on the other hand, provides a balanced trade-off between the cost of the monitoring process and the maintenance of sufficient accuracy for detecting defects. Specifically, the overhead cost of using QVM instead of a standard JVM, is low enough to be acceptable in production environments.We implemented QVM on top of IBM’s J9 Java Virtual Machine and used it to detect and fix various errors in real-world applications.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {2},
numpages = {35},
keywords = {debugging, diagnosis, typestate, Virtual machines, heap assertions}
}

@inproceedings{10.1145/2461381.2461402,
author = {Lim, Roman and Ferrari, Federico and Zimmerling, Marco and Walser, Christoph and Sommer, Philipp and Beutel, Jan},
title = {FlockLab: A Testbed for Distributed, Synchronized Tracing and Profiling of Wireless Embedded Systems},
year = {2013},
isbn = {9781450319591},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2461381.2461402},
doi = {10.1145/2461381.2461402},
abstract = {Testbeds are indispensable for debugging and evaluating wireless embedded systems. While existing testbeds provide ample opportunities for realistic, large-scale experiments, they are limited in their ability to closely observe and control the distributed operation of resource-constrained nodes - access to the nodes is restricted to the serial port. This paper presents FlockLab, a testbed that overcomes this limitation by allowing multiple services to run simultaneously and synchronously against all nodes under test in addition to the traditional serial port service: tracing of GPIO pins to record logical events occurring on a node, actuation of GPIO pins to trigger actions on a node, and high-resolution power profiling. FlockLab's accurate timing information in the low microsecond range enables logical events to be correlated with power samples, thus providing a previously unattained level of visibility into the distributed behavior of wireless embedded systems. In this paper, we describe FlockLab's design, benchmark its performance, and demonstrate its capabilities through several real-world test cases.},
booktitle = {Proceedings of the 12th International Conference on Information Processing in Sensor Networks},
pages = {153–166},
numpages = {14},
keywords = {testbed, GPIO tracing, GPIO actuation, power profiling, wireless sensor network, adjustable power supply},
location = {Philadelphia, Pennsylvania, USA},
series = {IPSN '13}
}

@inproceedings{10.1145/3377813.3381356,
author = {Zhang, Xindong and Zhu, Chenguang and Li, Yi and Guo, Jianmei and Liu, Lihua and Gu, Haobo},
title = {Precfix: Large-Scale Patch Recommendation by Mining Defect-Patch Pairs},
year = {2020},
isbn = {9781450371230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377813.3381356},
doi = {10.1145/3377813.3381356},
abstract = {Patch recommendation is the process of identifying errors in software systems and suggesting suitable fixes for them. Patch recommendation can significantly improve developer productivity by reducing both the debugging and repairing time. Existing techniques usually rely on complete test suites and detailed debugging reports, which are often absent in practical industrial settings. In this paper, we propose Precfix, a pragmatic approach targeting large-scale industrial codebase and making recommendations based on previously observed debugging activities. Precfix collects defect-patch pairs from development histories, performs clustering, and extracts generic reusable patching patterns as recommendations. We conducted experimental study on an industrial codebase with 10K projects involving diverse defect patterns. We managed to extract 3K templates of defect-patch pairs, which have been successfully applied to the entire codebase. Our approach is able to make recommendations within milliseconds and achieves a false positive rate of 22% confirmed by manual review. The majority (10/12) of the interviewed developers appreciated Precfix, which has been rolled out to Alibaba to support various critical businesses.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering in Practice},
pages = {41–50},
numpages = {10},
keywords = {patch recommendation, defect detection, patch generation},
location = {Seoul, South Korea},
series = {ICSE-SEIP '20}
}

@inproceedings{10.1145/2435349.2435379,
author = {Rastogi, Vaibhav and Chen, Yan and Enck, William},
title = {AppsPlayground: Automatic Security Analysis of Smartphone Applications},
year = {2013},
isbn = {9781450318907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2435349.2435379},
doi = {10.1145/2435349.2435379},
abstract = {Today's smartphone application markets host an ever increasing number of applications. The sheer number of applications makes their review a daunting task. We propose AppsPlayground for Android, a framework that automates the analysis smartphone applications. AppsPlayground integrates multiple components comprising different detection and automatic exploration techniques for this purpose. We evaluated the system using multiple large scale and small scale experiments involving real benign and malicious applications. Our evaluation shows that AppsPlayground is quite effective at automatically detecting privacy leaks and malicious functionality in applications.},
booktitle = {Proceedings of the Third ACM Conference on Data and Application Security and Privacy},
pages = {209–220},
numpages = {12},
keywords = {privacy leakage, malware, dynamic analysis, android},
location = {San Antonio, Texas, USA},
series = {CODASPY '13}
}

@inproceedings{10.5555/2738600.2738608,
author = {Zheng, Long and Liao, Xiaofei and He, Bingsheng and Wu, Song and Jin, Hai},
title = {On Performance Debugging of Unnecessary Lock Contentions on Multicore Processors: A Replay-Based Approach},
year = {2015},
isbn = {9781479981618},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {Locks have been widely used as an effective synchronization mechanism among processes and threads. However, we observe that a large number of false inter-thread dependencies (i.e., unnecessary lock contentions) exist during the program execution on multicore processors, thereby incurring significant performance overhead. This paper presents a performance debugging framework, PERFPLAY, to facilitate a comprehensive and in-depth understanding of the performance impact of unnecessary lock contentions. The core technique of our debugging framework is trace replay. Specifically, PERFPLAY records the program execution trace, on the basis of which the unnecessary lock contentions can be identified through trace analysis. We then propose a novel technique of trace transformation to transform these identified unnecessary lock contentions in the original trace into the correct pattern as a new trace free of unnecessary lock contentions. Through replaying both traces, PERFPLAY can quantify the performance impact of unnecessary lock contentions. To demonstrate the effectiveness of our debugging framework, we study five real-world programs and PARSEC benchmarks. Our experimental results demonstrate the significant performance overhead of unnecessary lock contentions, and the effectiveness of PERFPLAY in identifying the performance critical unnecessary lock contentions in real applications.},
booktitle = {Proceedings of the 13th Annual IEEE/ACM International Symposium on Code Generation and Optimization},
pages = {56–67},
numpages = {12},
location = {San Francisco, California},
series = {CGO '15}
}

@inproceedings{10.1145/2737924.2737961,
author = {Isradisaikul, Chinawat and Myers, Andrew C.},
title = {Finding Counterexamples from Parsing Conflicts},
year = {2015},
isbn = {9781450334686},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2737924.2737961},
doi = {10.1145/2737924.2737961},
abstract = { Writing a parser remains remarkably painful. Automatic parser generators offer a powerful and systematic way to parse complex grammars, but debugging conflicts in grammars can be time-consuming even for experienced language designers. Better tools for diagnosing parsing conflicts will alleviate this difficulty. This paper proposes a practical algorithm that generates compact, helpful counterexamples for LALR grammars. For each parsing conflict in a grammar, a counterexample demonstrating the conflict is constructed. When the grammar in question is ambiguous, the algorithm usually generates a compact counterexample illustrating the ambiguity. This algorithm has been implemented as an extension to the CUP parser generator. The results from applying this implementation to a diverse collection of faulty grammars show that the algorithm is practical, effective, and suitable for inclusion in other LALR parser generators. },
booktitle = {Proceedings of the 36th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {555–564},
numpages = {10},
keywords = {shift-reduce parser, Context-free grammar, ambiguous grammar, error diagnosis, lookahead-sensitive path, product parser},
location = {Portland, OR, USA},
series = {PLDI '15}
}

@article{10.1145/2813885.2737961,
author = {Isradisaikul, Chinawat and Myers, Andrew C.},
title = {Finding Counterexamples from Parsing Conflicts},
year = {2015},
issue_date = {June 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2813885.2737961},
doi = {10.1145/2813885.2737961},
abstract = { Writing a parser remains remarkably painful. Automatic parser generators offer a powerful and systematic way to parse complex grammars, but debugging conflicts in grammars can be time-consuming even for experienced language designers. Better tools for diagnosing parsing conflicts will alleviate this difficulty. This paper proposes a practical algorithm that generates compact, helpful counterexamples for LALR grammars. For each parsing conflict in a grammar, a counterexample demonstrating the conflict is constructed. When the grammar in question is ambiguous, the algorithm usually generates a compact counterexample illustrating the ambiguity. This algorithm has been implemented as an extension to the CUP parser generator. The results from applying this implementation to a diverse collection of faulty grammars show that the algorithm is practical, effective, and suitable for inclusion in other LALR parser generators. },
journal = {SIGPLAN Not.},
month = {jun},
pages = {555–564},
numpages = {10},
keywords = {lookahead-sensitive path, Context-free grammar, shift-reduce parser, product parser, error diagnosis, ambiguous grammar}
}

@inproceedings{10.1145/168619.168626,
author = {Jones, Michael B.},
title = {Interposition Agents: Transparently Interposing User Code at the System Interface},
year = {1993},
isbn = {0897916328},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/168619.168626},
doi = {10.1145/168619.168626},
abstract = {Many contemporary operating systems utilize a system call interface between the operating system and its clients. Increasing numbers of systems are providing low-level mechanisms for intercepting and handling system calls in user code. Nonetheless, they typically provide no higher-level tools or abstractions for effectively utilizing these mechanisms. Using them has typically required reimplementation of a substantial portion of the system interface from scratch, making the use of such facilities unwieldy at best.This paper presents a toolkit that substantially increases the ease of interposing user code between clients and instances of the system interface by allowing such code to be written in terms of the high-level objects provided by this interface, rather than in terms of the intercepted system calls themselves. This toolkit helps enable new interposition agents to be written, many of which would not otherwise have been attempted.This toolkit has also been used to construct several agents including: system call tracing tools, file reference tracing tools, and customizable filesystem views. Examples of other agents that could be built include: protected environments for running untrusted binaries, logical devices implemented entirely in user space, transparent data compression and/or encryption agents, transactional software environments, and emulators for other operating system environments.},
booktitle = {Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles},
pages = {80–93},
numpages = {14},
location = {Asheville, North Carolina, USA},
series = {SOSP '93}
}

@article{10.1145/173668.168626,
author = {Jones, Michael B.},
title = {Interposition Agents: Transparently Interposing User Code at the System Interface},
year = {1993},
issue_date = {Dec. 1993},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {5},
issn = {0163-5980},
url = {https://doi.org/10.1145/173668.168626},
doi = {10.1145/173668.168626},
abstract = {Many contemporary operating systems utilize a system call interface between the operating system and its clients. Increasing numbers of systems are providing low-level mechanisms for intercepting and handling system calls in user code. Nonetheless, they typically provide no higher-level tools or abstractions for effectively utilizing these mechanisms. Using them has typically required reimplementation of a substantial portion of the system interface from scratch, making the use of such facilities unwieldy at best.This paper presents a toolkit that substantially increases the ease of interposing user code between clients and instances of the system interface by allowing such code to be written in terms of the high-level objects provided by this interface, rather than in terms of the intercepted system calls themselves. This toolkit helps enable new interposition agents to be written, many of which would not otherwise have been attempted.This toolkit has also been used to construct several agents including: system call tracing tools, file reference tracing tools, and customizable filesystem views. Examples of other agents that could be built include: protected environments for running untrusted binaries, logical devices implemented entirely in user space, transparent data compression and/or encryption agents, transactional software environments, and emulators for other operating system environments.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {dec},
pages = {80–93},
numpages = {14}
}

@inproceedings{10.1145/2638404.2675737,
author = {Saad, Ashraf and Liljenquist, James},
title = {A Multi-Robot Testbed for Robotics Programming Education and Research},
year = {2014},
isbn = {9781450329231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2638404.2675737},
doi = {10.1145/2638404.2675737},
abstract = {We present the design choices we made to develop a multi-robot testbed to advance robotics programming education and research. We addressed the following criteria in developing the testbed in order to increase the likelihood that other computer science educators and researchers will adopt it as well, namely: the ability to program the robots in a high-level programming language; the ability of each robot to run programs using on-board processing capabilities; the ability of each robot to sense the environment in which it operates using on-board sensors; the ability of each robot to communicate with other robots in the testbed; and, the ability to incorporate additional robots in the testbed in a scalable manner. Given its programmability and communication capabilities, we chose Pololu's m3pi as the robot to use for constructing the testbed. We provide details of the various design choices and technical challenges we faced to build the testbed, including: programming m3pi robots in C/C++ to perform basic navigation operations, the graphical user interfaces we developed in Java and C# to track the robots, getting the robots to communicate using the Wixel and XBee wireless serial modules, and a virtual machine that we wrote in Python for the mbed microcontroller in order to give m3pi robots the communication capabilities needed for them to communicate using the XBee wireless serial module while navigating a grid. We conclude by outlining future promising directions to extend the testbed.},
booktitle = {Proceedings of the 2014 ACM Southeast Regional Conference},
articleno = {60},
numpages = {4},
keywords = {Multi Robot Testbed, Computer Science Research, Computer Science Education, Robotics programming},
location = {Kennesaw, Georgia},
series = {ACM SE '14}
}

@inproceedings{10.1145/2025113.2025174,
author = {R\"{o}\ss{}ler, Jeremias},
title = {Understanding Failures through Facts},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025174},
doi = {10.1145/2025113.2025174},
abstract = {Why does my program crash?"--This ever recurring question of software debugging drives the developer during the analysis of the failure. Complex defects are impossible to automatically identify; this can only be left to human judgment. But what we can do is empower the developer to make an informed decision, by helping her understand the failure. To fully comprehend a failure, one may need to consider many different aspects such as the range of the input parameters and the program's structure and runtime behavior. I propose an approach that gathers a variety of such facts from a given failing execution. To examine the correlation of those facts to the failure, it produces additional executions that differ in as few facts as possible. Then the approach creates generalizations and abstractions over the correlating facts. These explain different aspects of the failure and thus help the developer understand and eventually fix the underlying defect.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {404–407},
numpages = {4},
keywords = {statistical debugging, automated debugging, test case generation, failure classification},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.5555/2387880.2387910,
author = {Attariyan, Mona and Chow, Michael and Flinn, Jason},
title = {X-Ray: Automating Root-Cause Diagnosis of Performance Anomalies in Production Software},
year = {2012},
isbn = {9781931971966},
publisher = {USENIX Association},
address = {USA},
abstract = {Troubleshooting the performance of production software is challenging. Most existing tools, such as profiling, tracing, and logging systems, reveal what events occurred during performance anomalies. However, users of such toolsmust infer why these events occurred; e.g., that their execution was due to a root cause such as a specific input request or configuration setting. Such inference often requires source code and detailed application knowledge that is beyond system administrators and end users.This paper introduces performance summarization, a technique for automatically diagnosing the root causes of performance problems. Performance summarization instruments binaries as applications execute. It first attributes performance costs to each basic block. It then uses dynamic information flow tracking to estimate the likelihood that a block was executed due to each potential root cause. Finally, it summarizes the overall cost of each potential root cause by summing the per-block cost multiplied by the cause-specific likelihood over all basic blocks. Performance summarization can also be performed differentially to explain performance differences between two similar activities. X-ray is a tool that implements performance summarization. Our results show that X-ray accurately diagnoses 17 performance issues in Apache, lighttpd, Postfix, and PostgreSQL, while adding 2.3% average runtime overhead.},
booktitle = {Proceedings of the 10th USENIX Conference on Operating Systems Design and Implementation},
pages = {307–320},
numpages = {14},
location = {Hollywood, CA, USA},
series = {OSDI'12}
}

@inproceedings{10.1145/2714576.2714607,
author = {Lee, Wang Hao and Srirangam Ramanujam, Murali and Krishnan, S.P.T.},
title = {On Designing an Efficient Distributed Black-Box Fuzzing System for Mobile Devices},
year = {2015},
isbn = {9781450332453},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2714576.2714607},
doi = {10.1145/2714576.2714607},
abstract = {Security researchers who jailbreak iOS devices have usually crowdsourced for system level vulnerabilities [1] for iOS. However, their success has depended on whether a particular device owner encountered a crash in system-level code. To conduct voluntary security testing, black-box fuzzing is one of the ideal low-cost and simple techniques to find system level vulnerabilities for the less technical crowd. However, it is not the most effective method due to the large fuzzing space. At the same time, when fuzzing mobile devices such as today's smartphones, it is extremely time consuming to instrument mobile devices of varying versions of system software across the world. This paper, describes Mobile Vulnerability Discovery Pipeline (MVDP), a semi-automated, vulnerability discovery pipeline for mobile devices. MVDP is a carefully crafted process targeted to produce malicious output that is very likely to crash the target leading to vulnerability discovery. MVDP employs a few novel black-box fuzzing techniques such as distributed fuzzing, parameter selection, mutation position optimisation and selection of good seed files. To date, MVDP has discovered around 1900 unique crashing inputs and helped to identify 7 unique vulnerabilities across various Android and iOS phone models.},
booktitle = {Proceedings of the 10th ACM Symposium on Information, Computer and Communications Security},
pages = {31–42},
numpages = {12},
keywords = {crash analysis, smartphones, zero-day vulnerability, black-box fuzzing},
location = {Singapore, Republic of Singapore},
series = {ASIA CCS '15}
}

