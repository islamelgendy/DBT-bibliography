@inproceedings{10.1145/3128473.3128478,
author = {Magalh\~{a}es, Claudio and Andrade, Jo\~{a}o and Perrusi, Lucas and Mota, Alexandre},
title = {Evaluating an Automatic Text-Based Test Case Selection Using a Non-Instrumented Code Coverage Analysis},
year = {2017},
isbn = {9781450353021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128473.3128478},
doi = {10.1145/3128473.3128478},
abstract = {During development, systems may be tested several times. In general, a system evolves from change requests, aiming at improving its behavior in terms of new features as well as fixing failures. Thus, selecting the best test plan in terms of the closeness between test cases and the changed code and its dependencies is pursued by industry and academia. In this paper we measure the coverage achieved by an automatic test case selection based on information retrieval that relates change requests and test cases. But instead of using off-the-shelf coverage tools, like JaCoCo, we propose a way of obtaining code coverage of Android apk's without instrumentation. This was a basic requirement of our industrial partner. We performed some experiments on this industrial partner and promising results were obtained.},
booktitle = {Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {5},
numpages = {9},
keywords = {Test case selection and prioritization, Information Retrieval, Code coverage},
location = {Fortaleza, Brazil},
series = {SAST}
}

@inbook{10.1145/3293882.3330551,
author = {White, Thomas D. and Fraser, Gordon and Brown, Guy J.},
title = {Improving Random GUI Testing with Image-Based Widget Detection},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330551},
abstract = {Graphical User Interfaces (GUIs) are amongst the most common user interfaces, enabling interactions with applications through mouse movements and key presses. Tools for automated testing of programs through their GUI exist, however they usually rely on operating system or framework specific knowledge to interact with an application. Due to frequent operating system updates, which can remove required information, and a large variety of different GUI frameworks using unique underlying data structures, such tools rapidly become obsolete, Consequently, for an automated GUI test generation tool, supporting many frameworks and operating systems is impractical. We propose a technique for improving GUI testing by automatically identifying GUI widgets in screen shots using machine learning techniques. As training data, we generate randomized GUIs to automatically extract widget information. The resulting model provides guidance to GUI testing tools in environments not currently supported by deriving GUI widget information from screen shots only. In our experiments, we found that identifying GUI widgets in screen shots and using this information to guide random testing achieved a significantly higher branch coverage in 18 of 20 applications, with an average increase of 42.5% when compared to conventional random testing.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {307–317},
numpages = {11}
}

@inproceedings{10.1145/2771783.2771793,
author = {Gyori, Alex and Shi, August and Hariri, Farah and Marinov, Darko},
title = {Reliable Testing: Detecting State-Polluting Tests to Prevent Test Dependency},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771793},
doi = {10.1145/2771783.2771793},
abstract = { Writing reliable test suites for large object-oriented systems is complex and time consuming. One common cause of unreliable test suites are test dependencies that can cause tests to fail unexpectedly, not exposing bugs in the code under test but in the test code itself. Prior research has shown that the main reason for test dependencies is the ``pollution'' of state shared across tests. We propose a technique, called , for finding tests that pollute the shared state. In a nutshell, finds tests that modify some location on the heap shared across tests or on the file system; a subsequent test could fail if it assumes the shared location to have the initial value before the state was modified. To aid in inspecting the pollutions, provides an access path through the heap that leads to the polluted value or the name of the file that was modified. We implemented a prototype tool for Java and evaluated it on NumOfProjects projects, with a total of NumOfTests tests. Diaper reported PollutingTests , and our inspection found that NumOfTPsSpace of those are relevant pollutions that can easily affect other tests. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {223–233},
numpages = {11},
keywords = {Test Dependency, Flaky Tests, State Pollution},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2610384.2628051,
author = {Schur, Matthias and Roth, Andreas and Zeller, Andreas},
title = {ProCrawl: Mining Test Models from Multi-User Web Applications},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2628051},
doi = {10.1145/2610384.2628051},
abstract = { Today's web applications demand very high release cycles--and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. However, specifying such behavior models and writing the necessary scripts manually is a hard task. We present ProCrawl (Process Crawler), a tool that automatically mines (extended) finite-state machines from (multi-user) web applications and generates executable test scripts. ProCrawl explores the behavior of the application by systematically generating program runs and observing changes on the application's user interface. The resulting models can be directly used for effective model-based testing, in particular regression testing. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {413–416},
numpages = {4},
keywords = {dynamic analysis, model-based testing, Specification mining},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3449726.3463147,
author = {Langdon, William B. and Petke, Justyna and Clark, David},
title = {Dissipative Polynomials},
year = {2021},
isbn = {9781450383516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3449726.3463147},
doi = {10.1145/3449726.3463147},
abstract = {Limited precision floating point computer implementations of large polynomial arithmetic expressions are nonlinear and dissipative. They are not reversible (irreversible, lack conservation), lose information, and so are robust to perturbations (anti-fragile) and resilient to fluctuations. This gives a largely stable locally flat evolutionary neutral fitness search landscape. Thus even with a large number of test cases, both large and small changes deep within software typically have no effect and are invisible externally. Shallow mutations are easier to detect but their RMS error need not be simple.},
booktitle = {Proceedings of the Genetic and Evolutionary Computation Conference Companion},
pages = {1683–1691},
numpages = {9},
keywords = {information funnels, sbse, introns, diversity, software testing, genetic programming, evolvability, correctness attraction, information loss, software robustness, mutational robustness, neutral networks, theory of bloat, entropy},
location = {Lille, France},
series = {GECCO '21}
}

@inproceedings{10.1145/2771783.2771788,
author = {Epitropakis, Michael G. and Yoo, Shin and Harman, Mark and Burke, Edmund K.},
title = {Empirical Evaluation of Pareto Efficient Multi-Objective Regression Test Case Prioritisation},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771788},
doi = {10.1145/2771783.2771788},
abstract = { The aim of test case prioritisation is to determine an ordering of test cases that maximises the likelihood of early fault revelation. Previous prioritisation techniques have tended to be single objective, for which the additional greedy algorithm is the current state-of-the-art. Unlike test suite minimisation, multi objective test case prioritisation has not been thoroughly evaluated. This paper presents an extensive empirical study of the effectiveness of multi objective test case prioritisation, evaluating it on multiple versions of five widely-used benchmark programs and a much larger real world system of over 1 million lines of code. The paper also presents a lossless coverage compaction algorithm that dramatically scales the performance of all algorithms studied by between 2 and 4 orders of magnitude, making prioritisation practical for even very demanding problems. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {234–245},
numpages = {12},
keywords = {Test case prioritization, additional greedy algorithm, multi-objective evolutionary algorithm, coverage compaction},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2610384.2610403,
author = {Appelt, Dennis and Nguyen, Cu Duy and Briand, Lionel C. and Alshahwan, Nadia},
title = {Automated Testing for SQL Injection Vulnerabilities: An Input Mutation Approach},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610403},
doi = {10.1145/2610384.2610403},
abstract = { Web services are increasingly adopted in various domains, from finance and e-government to social media. As they are built on top of the web technologies, they suffer also an unprecedented amount of attacks and exploitations like the Web. Among the attacks, those that target SQL injection vulnerabilities have consistently been top-ranked for the last years. Testing to detect such vulnerabilities before making web services public is crucial. We present in this paper an automated testing approach, namely μ4SQLi, and its underpinning set of mutation operators. μ4SQLi can produce effective inputs that lead to executable and harmful SQL statements. Executability is key as otherwise no injection vulnerability can be exploited. Our evaluation demonstrated that the approach is effective to detect SQL injection vulnerabilities and to produce inputs that bypass application firewalls, which is a common configuration in real world. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {259–269},
numpages = {11},
keywords = {Mutation Testing, SQL Injection, Test Generation},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3213846.3213848,
author = {Cummins, Chris and Petoumenos, Pavlos and Murray, Alastair and Leather, Hugh},
title = {Compiler Fuzzing through Deep Learning},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213848},
doi = {10.1145/3213846.3213848},
abstract = {Random program generation — fuzzing — is an effective technique for discovering bugs in compilers but successful fuzzers require extensive development effort for every language supported by the compiler, and often leave parts of the language space untested. We introduce DeepSmith, a novel machine learning approach to accelerating compiler validation through the inference of generative models for compiler inputs. Our approach infers a learned model of the structure of real world code based on a large corpus of open source code. Then, it uses the model to automatically generate tens of thousands of realistic programs. Finally, we apply established differential testing methodologies on them to expose bugs in compilers. We apply our approach to the OpenCL programming language, automatically exposing bugs with little effort on our side. In 1,000 hours of automated testing of commercial and open source compilers, we discover bugs in all of them, submitting 67 bug reports. Our test cases are on average two orders of magnitude smaller than the state-of-the-art, require 3.03\texttimes{} less time to generate and evaluate, and expose bugs which the state-of-the-art cannot. Our random program generator, comprising only 500 lines of code, took 12 hours to train for OpenCL versus the state-of-the-art taking 9 man months to port from a generator for C and 50,000 lines of code. With 18 lines of code we extended our program generator to a second language, uncovering crashes in Solidity compilers in 12 hours of automated testing.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {95–105},
numpages = {11},
keywords = {Deep Learning, Compiler Fuzzing, Differential Testing},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3092703.3092711,
author = {L\"{o}scher, Andreas and Sagonas, Konstantinos},
title = {Targeted Property-Based Testing},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092711},
doi = {10.1145/3092703.3092711},
abstract = { We introduce targeted property-based testing, an enhanced form of property-based testing that aims to make the input generation component of a property-based testing tool guided by a search strategy rather than being completely random. Thus, this testing technique combines the advantages of both search-based and property-based testing. We demonstrate the technique with the framework we have built, called Target, and show its effectiveness on three case studies. The first of them demonstrates how Target can employ simulated annealing to generate sensor network topologies that form configurations with high energy consumption. The second case study shows how the generation of routing trees for a wireless network equipped with directional antennas can be guided to fulfill different energy metrics. The third case study employs Target to test the noninterference property of information-flow control abstract machine designs, and compares it with a sophisticated hand-written generator for programs of these abstract machines. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {46–56},
numpages = {11},
keywords = {Search-based testing, PropEr, Property-based testing, QuickCheck},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/2610384.2610393,
author = {Pradel, Michael and Huggler, Markus and Gross, Thomas R.},
title = {Performance Regression Testing of Concurrent Classes},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610393},
doi = {10.1145/2610384.2610393},
abstract = { Developers of thread-safe classes struggle with two opposing goals. The class must be correct, which requires synchronizing concurrent accesses, and the class should provide reasonable performance, which is difficult to realize in the presence of unnecessary synchronization. Validating the performance of a thread-safe class is challenging because it requires diverse workloads that use the class, because existing performance analysis techniques focus on individual bottleneck methods, and because reliably measuring the performance of concurrent executions is difficult. This paper presents SpeedGun, an automatic performance regression testing technique for thread-safe classes. The key idea is to generate multi-threaded performance tests and to compare two versions of a class with each other. The analysis notifies developers when changing a thread-safe class significantly influences the performance of clients of this class. An evaluation with 113 pairs of classes from popular Java projects shows that the analysis effectively identifies 13 performance differences, including performance regressions that the respective developers were not aware of. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {13–25},
numpages = {13},
keywords = {Performance measurement, Thread safety, Test generation},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3425174.3425215,
author = {Villanes, Isabel K. and Endo, Andre Takeshi and Dias-Neto, Arilo C.},
title = {Using App Attributes to Improve Mobile Device Selection for Compatibility Testing},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425215},
doi = {10.1145/3425174.3425215},
abstract = {Mobile apps have been developed with the aim of attracting a large and diverse number of users. An impediment factor, especially for the Android platform, is a large number of hardware and software configurations available in the market, so app developers face the challenge of producing a highly compatible app. For compatibility testing, an app can be tested in a subset of devices that sufficiently covers the characteristics of devices adopted by users. Moreover, this subset needs to be extracted from up-to-date sources since new devices and APIs are frequently introduced in the market, while others are deprecated. This paper presents an always-updated, app-specific approach, called DeSeCT, for mobile Device Selection for Compatibility Testing. Our approach is divided into the following steps: first, it crawls the Web to obtain an updated list of devices; second, attributes of the app under test are used to filter unwanted mobile devices; and a multiobjective genetic algorithm is employed to tailor a feature-coverage optimized set of mobile devices. The results provide evidence that DeSeCT can produce a better selection of mobile devices for compatibility testing in comparison with the state-of-the-art. DeSeCT avoids the selection of approximately 15% to 18% of unwanted devices.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {31–39},
numpages = {9},
keywords = {fragmentation, compatibility testing, app attributes, Android},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{10.1145/2771783.2771792,
author = {Mu\c{s}lu, K\i{}van\c{c} and Brun, Yuriy and Meliou, Alexandra},
title = {Preventing Data Errors with Continuous Testing},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771792},
doi = {10.1145/2771783.2771792},
abstract = { Today, software systems that rely on data are ubiquitous, and ensuring the data's quality is an increasingly important challenge as data errors result in annual multi-billion dollar losses. While software debugging and testing have received heavy research attention, less effort has been devoted to data debugging: identifying system errors caused by well-formed but incorrect data. We present continuous data testing (CDT), a low-overhead, delay-free technique that quickly identifies likely data errors. CDT continuously executes domain-specific test queries; when a test fails, CDT unobtrusively warns the user or administrator. We implement CDT in the ConTest prototype for the PostgreSQL database management system. A feasibility user study with 96 humans shows that ConTest was extremely effective in a setting with a data entry application at guarding against data errors: With ConTest, users corrected 98.4% of their errors, as opposed to 40.2% without, even when we injected 40% false positives into ConTest's output. Further, when using ConTest, users corrected data entry errors 3.2 times faster than when using state-of-the-art methods. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {373–384},
numpages = {12},
keywords = {continuous testing, data debugging, data testing},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inbook{10.1145/3293882.3338998,
author = {Li, Chi and Zhou, Min and Gu, Zuxing and Chen, Guang and Wang, Yuexing and Wu, Jiecheng and Gu, Ming},
title = {VBSAC: A Value-Based Static Analyzer for C},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338998},
abstract = {Static analysis has long prevailed as a promising approach to detect program bugs at an early development process to increase software quality. However, such tools face great challenges to balance the false-positive rate and the false-negative rate in practical use. In this paper, we present VBSAC, a value-based static analyzer for C aiming to improve the precision and recall. In our tool, we employ a pluggable value-based analysis strategy. A memory skeleton recorder is designed to maintain the memory objects as a baseline. While traversing the control flow graph, diverse value-based plug-ins analyze the specific abstract domains and share program information to strengthen the computation. Simultaneously, checkers consume the corresponding analysis results to detect bugs. We also provide a user-friendly web interface to help users audit the bug detection results. Evaluation on two widely-used benchmarks shows that we perform better to state-of-the-art bug detection tools by finding 221-339 more bugs and improving F-Score 9.88%-40.32%.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {382–385},
numpages = {4}
}

@inproceedings{10.1145/1831708.1831737,
author = {Kettunen, Vesa and Kasurinen, Jussi and Taipale, Ossi and Smolander, Kari},
title = {A Study on Agility and Testing Processes in Software Organizations},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831737},
doi = {10.1145/1831708.1831737},
abstract = {In this paper, we studied the differences in testing activities between software organizations which apply agile development methods and organizations which take the traditional plan-driven approach. Our focus was on the concepts which allow the software organization to successfully apply agile development methods or plan-driven methods. We also observed the test process enhancements and hindrances, which originate in the selected development method. We interviewed 12 organizations, which were selected to represent different polar types of software production. The interviews were tape-recorded and transcribed for further analysis. The study yielded hypotheses which were derived by applying the qualitative grounded theory method. The results indicated that in practice, agile methods can improve the position of testing through the early involvement of testing activities in development, and also have a positive influence on end-product satisfaction. By applying these results, organizations can improve their processes and avoid pitfalls when transitioning to agile methods.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {231–240},
numpages = {10},
keywords = {case study, agile development, empirical study, test process},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/2771783.2771784,
author = {Gligoric, Milos and Eloussi, Lamyaa and Marinov, Darko},
title = {Practical Regression Test Selection with Dynamic File Dependencies},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771784},
doi = {10.1145/2771783.2771784},
abstract = { Regression testing is important but can be time-intensive. One approach to speed it up is regression test selection (RTS), which runs only a subset of tests. RTS was proposed over three decades ago but has not been widely adopted in practice. Meanwhile, testing frameworks, such as JUnit, are widely adopted and well integrated with many popular build systems. Hence, integrating RTS in a testing framework already used by many projects would increase the likelihood that RTS is adopted. We propose a new, lightweight RTS technique, called Ekstazi, that can integrate well with testing frameworks. Ekstazi tracks dynamic dependencies of tests on files, and unlike most prior RTS techniques, Ekstazi requires no integration with version-control systems. We implemented Ekstazi for Java and JUnit, and evaluated it on 615 revisions of 32 open-source projects (totaling almost 5M LOC) with shorter- and longer-running test suites. The results show that Ekstazi reduced the end-to-end testing time 32% on average, and 54% for longer-running test suites, compared to executing all tests. Ekstazi also has lower end-to-end time than the existing techniques, despite the fact that it selects more tests. Ekstazi has been adopted by several popular open source projects, including Apache Camel, Apache Commons Math, and Apache CXF. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {211–222},
numpages = {12},
keywords = {file dependencies, Regression test selection},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/3460319.3464803,
author = {Yao, Peisen and Huang, Heqing and Tang, Wensheng and Shi, Qingkai and Wu, Rongxin and Zhang, Charles},
title = {Fuzzing SMT Solvers via Two-Dimensional Input Space Exploration},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464803},
doi = {10.1145/3460319.3464803},
abstract = {Satisfiability Modulo Theories (SMT) solvers serve as the core engine of many techniques, such as symbolic execution. Therefore, ensuring the robustness and correctness of SMT solvers is critical. While fuzzing is an efficient and effective method for validating the quality of SMT solvers, we observe that prior fuzzing work only focused on generating various first-order formulas as the inputs but neglected the algorithmic configuration space of an SMT solver, which leads to under-reporting many deeply-hidden bugs. In this paper, we present Falcon, a fuzzing technique that explores both the formula space and the configuration space. Combining the two spaces significantly enlarges the search space and makes it challenging to detect bugs efficiently. We solve this problem by utilizing the correlations between the two spaces to reduce the search space, and introducing an adaptive mutation strategy to boost the search efficiency. During six months of extensive testing, Falcon finds 518 confirmed bugs in CVC4 and Z3, two state-of-the-art SMT solvers, 469 of which have already been fixed. Compared to two state-of-the-art fuzzers, Falcon detects 38 and 44 more bugs and improves the coverage by a large margin in 24 hours of testing.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {322–335},
numpages = {14},
keywords = {SMT solvers, Fuzz testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@article{10.1145/1022494.1022535,
author = {Lindstr\"{o}m, B. and Grindal, M. and Offutt, J.},
title = {Using an Existing Suite of Test Objects: Experience from a Testing Experiment},
year = {2004},
issue_date = {September 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/1022494.1022535},
doi = {10.1145/1022494.1022535},
abstract = {This workshop paper presents lessons learned from a recent experiment to compare several test strategies. The test strategies were compared in terms of the number of tests needed to satisfy them and in terms of faults found. The experimental design and conduct are discussed, and frank assessments of the decisions that were made are provided. The paper closes with a summary of the lessons that were learned.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–3},
numpages = {3},
keywords = {fault seeding, software testing, representativity, independence}
}

@inbook{10.1145/3293882.3339004,
author = {Li, Yuying and Hao, Rui and Feng, Yang and Jones, James A. and Zhang, Xiaofang and Chen, Zhenyu},
title = {CTRAS: A Tool for Aggregating and Summarizing Crowdsourced Test Reports},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3339004},
abstract = {In this paper, we present CTRAS, a tool for automatically aggregating and summarizing duplicate crowdsourced test reports on the fly. CTRAS can automatically detect duplicates based on both textual information and the screenshots, and further aggregates and summarizes the duplicate test reports. CTRAS provides end users with a comprehensive and comprehensible understanding of all duplicates by identifying the main topics across the group of aggregated test reports and highlighting supplementary topics that are mentioned in subgroups of test reports. Also, it provides the classic tool of issue tracking systems, such as the project-report dashboard and keyword searching, and automates their classic functionalities, such as bug triaging and best fixer recommendation, to assist end users in managing and diagnosing test reports. Video: https://youtu.be/PNP10gKIPFs},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {406–409},
numpages = {4}
}

@inbook{10.1145/3460319.3464839,
author = {Xue, Lei and Yan, Yuxiao and Yan, Luyi and Jiang, Muhui and Luo, Xiapu and Wu, Dinghao and Zhou, Yajin},
title = {Parema: An Unpacking Framework for Demystifying VM-Based Android Packers},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464839},
abstract = {Android packers have been widely adopted by developers to protect apps from being plagiarized. Meanwhile, various unpacking tools unpack the apps through direct memory dumping. To defend against these off-the-shelf unpacking tools, packers start to adopt virtual machine (VM) based protection techniques, which replace the original Dalvik bytecode (DCode) with customized bytecode (PCode) in memory. This defeats the unpackers using memory dumping mechanisms. However, little is known about whether such packers can provide enough protection to Android apps. In this paper, we aim to shed light on these questions and take the first step towards demystifying the protections provided to the apps by the VM-based packers. We proposed novel program analysis techniques to investigate existing commercial VM-based packers including a learning phase and a deobfuscation phase.We aim at deobfuscating the VM-protection DCode in three scenarios, recovering original DCode or its semantics with training apps, and restoring the semantics without training apps. We also develop a prototype named Parema to automate much work of the deobfuscation procedure. By applying it to the online VM-based Android packers, we reveal that all evaluated packers do not provide adequate protection and could be compromised.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {152–164},
numpages = {13}
}

@inproceedings{10.1145/2001420.2001464,
author = {Fraser, Gordon and Zeller, Andreas},
title = {Generating Parameterized Unit Tests},
year = {2011},
isbn = {9781450305624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001420.2001464},
doi = {10.1145/2001420.2001464},
abstract = {State-of-the art techniques for automated test generation focus on generating executions that cover program behavior. As they do not generate oracles, it is up to the developer to figure out what a test does and how to check the correctness of the observed behavior. In this paper, we present an approach to generate parameterized unit tests---unit tests containing symbolic pre- and postconditions characterizing test input and test result. Starting from concrete inputs and results, we use test generation and mutation to systematically generalize pre- and postconditions while simplifying the computation steps. Evaluated on five open source libraries, the generated parameterized unit tests are (a) more expressive, characterizing general rather than concrete behavior; (b) need fewer computation steps, making them easier to understand; and (c) achieve a higher coverage than regular unit tests.},
booktitle = {Proceedings of the 2011 International Symposium on Software Testing and Analysis},
pages = {364–374},
numpages = {11},
keywords = {unit testing, test oracles, test case generation, assertions},
location = {Toronto, Ontario, Canada},
series = {ISSTA '11}
}

