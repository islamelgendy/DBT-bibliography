@inproceedings{10.1145/1833398.1833403,
author = {Sroka, Jacek and W\l{}odarczyk, Piotr and Krupa, \L{}ukasz and Hidders, Jan},
title = {DFL Designer: Collection-Oriented Scientific Workflows with Petri Nets and Nested Relational Calculus},
year = {2010},
isbn = {9781450301886},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833398.1833403},
doi = {10.1145/1833398.1833403},
abstract = {In this paper we present DFL designer --- a collection-oriented scientific workflow (COSW) tool based on the DFL notation which combines established formalisms from workflow modeling and databases, namely Petri nets and the nested relational calculus (NRC). COSW tools are used in applied sciences like bioinformatics where structured data is processed with the use of specialized services which are made available online by scientific institutions. They make such data processing experiments easier to conduct by the experimentators and easier to comprehend and repeat by the reviewers. The notations, models and techniques used for the construction of COSW tools are similar to the ones known from workflow modeling, but additional emphasis is put on the data manipulation aspects, e.g., the processing of nested collections of data. DFL designer not only allows design and enactment of complicated COSWs with the use of a huge library of supported bioinformatics services, but also provides a set of features for testing and analyzing workflow specifications that is unique for COSWs, including but not limited to interactive firing of transitions, hierarchical analysis of COSWs and translation of side-effect free COSWs to a query language like NRC.},
booktitle = {Proceedings of the 1st International Workshop on Workflow Approaches to New Data-Centric Science},
articleno = {5},
numpages = {6},
location = {Indianapolis, Indiana},
series = {Wands '10}
}

@inproceedings{10.1145/238020.238036,
author = {Hicks, Leesa and Berman, Francine},
title = {Debugging Heterogeneous Applications with Pangaea},
year = {1996},
isbn = {0897918460},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/238020.238036},
doi = {10.1145/238020.238036},
booktitle = {Proceedings of the SIGMETRICS Symposium on Parallel and Distributed Tools},
pages = {41–50},
numpages = {10},
location = {Philadelphia, Pennsylvania, USA},
series = {SPDT '96}
}

@article{10.1145/2362336.2362351,
author = {Woehrle, Matthias and Lampka, Kai and Thiele, Lothar},
title = {Conformance Testing for Cyber-Physical Systems},
year = {2013},
issue_date = {December 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
issn = {1539-9087},
url = {https://doi.org/10.1145/2362336.2362351},
doi = {10.1145/2362336.2362351},
abstract = {Cyber-Physical Systems (CPS) require a high degree of reliability and robustness. Hence it is important to assert their correctness with respect to extra-functional properties, like power consumption, temperature, etc. In turn the physical quantities may be exploited for assessing system implementations. This article develops a methodology for utilizing measurements of physical quantities for testing the conformance of a running CPS with respect to a formal description of its required behavior allowing to uncover defects. We present foundations and implementations of this approach and demonstrate its usefulness by conformance testing power measurements of a wireless sensor node with a formal model of its power consumption.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {jan},
articleno = {84},
numpages = {23},
keywords = {timed testing, Conformance test, cyber-physical systems}
}

@inproceedings{10.1145/1367497.1367715,
author = {Ding, Li and Tao, Jiao and McGuinness, Deborah L.},
title = {An Initial Investigation on Evaluating Semantic Web Instance Data},
year = {2008},
isbn = {9781605580852},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1367497.1367715},
doi = {10.1145/1367497.1367715},
abstract = {Many emerging semantic web applications include ontologies from one set of authors and instance data from another (often much larger) set of authors. Often ontologies are reused and instance data is integrated in manners unanticipated by their authors. Not surprisingly, many instance data rich applications encounter instance data that is not compatible with the expectations of the original ontology author(s). This line of work focuses on issues related to semantic expectation mismatches in instance data. Our initial results include a customizable and extensible service-oriented evaluation architecture, and a domain implementation called PmlValidator, which checks instance data using the corresponding ontologies and additional style requirements.},
booktitle = {Proceedings of the 17th International Conference on World Wide Web},
pages = {1179–1180},
numpages = {2},
keywords = {instance data evaluation, architecture, semantic web},
location = {Beijing, China},
series = {WWW '08}
}

@inproceedings{10.5555/2818754.2818789,
author = {Saha, Ripon K. and Zhang, Lingming and Khurshid, Sarfraz and Perry, Dewayne E.},
title = {An Information Retrieval Approach for Regression Test Prioritization Based on Program Changes},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Regression testing is widely used in practice for validating program changes. However, running large regression suites can be costly. Researchers have developed several techniques for prioritizing tests such that the higher-priority tests have a higher likelihood of finding bugs. A vast majority of these techniques are based on dynamic analysis, which can be precise but can also have significant overhead (e.g., for program instrumentation and test-coverage collection). We introduce a new approach, REPiR, to address the problem of regression test prioritization by reducing it to a standard Information Retrieval problem such that the differences between two program versions form the query and the tests constitute the document collection. REPiR does not require any dynamic profiling or static program analysis. As an enabling technology we leverage the open-source IR toolkit Indri. An empirical evaluation using eight open-source Java projects shows that REPiR is computationally efficient and performs better than existing (dynamic or static) techniques for the majority of subject systems.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {268–279},
numpages = {12},
keywords = {regression testing, information retrieval, test prioritization},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/2245276.2245418,
author = {Casado, Rub\'{e}n and Tuya, Javier and Younas, Muhammad},
title = {Testing the Reliability of Web Services Transactions in Cooperative Applications},
year = {2012},
isbn = {9781450308571},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2245276.2245418},
doi = {10.1145/2245276.2245418},
abstract = {Web services provide a distributed computing environment wherein service providers and consumers can dynamically interact and cooperate on various tasks in different domains such as ebusiness, education, government and healthcare. Transaction management technology is fundamental to building automated and reliable web services applications. Various models and protocols have been developed for web services transactions. However, they give no attention to the key issue of testing the web services transactions. We propose a novel abstract model for dynamically modeling distinct web services transaction standards and test their reliability in terms of failures. The proposed approach exploits model-based testing techniques in order to automatically generate test scenarios for web service transactions.},
booktitle = {Proceedings of the 27th Annual ACM Symposium on Applied Computing},
pages = {743–748},
numpages = {6},
keywords = {transactions, cooperative applications, testing, web services},
location = {Trento, Italy},
series = {SAC '12}
}

@inproceedings{10.1145/2489850.2489851,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul and Danner, Christian and Wallner, Stefan},
title = {Evolving Systems of Systems: Industrial Challenges and Research Perspectives},
year = {2013},
isbn = {9781450320481},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489850.2489851},
doi = {10.1145/2489850.2489851},
abstract = {An increasing number of software systems today are systems of systems (SoS) comprising decentralized and heterogeneous systems with operational and managerial independence. The evolution of SoS is the rule and not the exception in practice due to frequently changing requirements, technologies, and markets. However, providing adequate support for the evolution of SoS is rather challenging as their behavior often emerges at runtime and is difficult to predict. Hence, SoS must be monitored during simulation and operation to ensure compliance with its requirements. In this position paper, we present challenges for SoS evolution from the domain of industrial automation. We discuss existing approaches supporting SoS evolution and derive research issues. We outline requirements for SoS evolution support and present key capabilities of a flexible monitoring and evolution infrastructure. We conclude with a discussion of research perspectives.},
booktitle = {Proceedings of the First International Workshop on Software Engineering for Systems-of-Systems},
pages = {1–4},
numpages = {4},
keywords = {monitoring, large-scale systems, requirements, model-based, software evolution, systems of systems},
location = {Montpellier, France},
series = {SESoS '13}
}

@inproceedings{10.1145/3493700.3493704,
author = {Saha, Diptikalyan and Aggarwal, Aniya and Hans, Sandeep},
title = {Data Synthesis for Testing Black-Box Machine Learning Models},
year = {2022},
isbn = {9781450385824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3493700.3493704},
doi = {10.1145/3493700.3493704},
abstract = {The increasing usage of machine learning models raises the question of the reliability of these models. The current practice of testing with limited data is often insufficient. In this paper, we provide a framework for automated test data synthesis to test black-box ML/DL models. We address an important challenge of generating realistic user-controllable data with model agnostic coverage criteria to test a varied set of properties, essentially to increase trust in machine learning models. We experimentally demonstrate the effectiveness of our technique. },
booktitle = {5th Joint International Conference on Data Science &amp; Management of Data (9th ACM IKDD CODS and 27th COMAD)},
pages = {110–114},
numpages = {5},
location = {Bangalore, India},
series = {CODS-COMAD 2022}
}

@inproceedings{10.5555/2818754.2818828,
author = {Sadowski, Caitlin and van Gogh, Jeffrey and Jaspan, Ciera and S\"{o}derberg, Emma and Winter, Collin},
title = {Tricorder: Building a Program Analysis Ecosystem},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Static analysis tools help developers find bugs, improve code readability, and ensure consistent style across a project. However, these tools can be difficult to smoothly integrate with each other and into the developer workflow, particularly when scaling to large codebases. We present Tricorder, a program analysis platform aimed at building a data-driven ecosystem around program analysis. We present a set of guiding principles for our program analysis tools and a scalable architecture for an analysis platform implementing these principles. We include an empirical, in-situ evaluation of the tool as it is used by developers across Google that shows the usefulness and impact of the platform.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {598–608},
numpages = {11},
keywords = {program analysis, static analysis},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3377816.3381742,
author = {Terragni, Valerio and Salza, Pasquale and Ferrucci, Filomena},
title = {A Container-Based Infrastructure for Fuzzy-Driven Root Causing of Flaky Tests},
year = {2020},
isbn = {9781450371261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377816.3381742},
doi = {10.1145/3377816.3381742},
abstract = {Intermittent test failures (test flakiness) is common during continuous integration as modern software systems have become inherently non-deterministic. Understanding the root cause of test flakiness is crucial as intermittent test failures might be the result of real non-deterministic defects in the production code, rather than mere errors in the test code. Given a flaky test, existing techniques for root causing test flakiness compare the runtime behavior of its passing and failing executions. They achieve this by repetitively executing the flaky test on an instrumented version of the system under test. This approach has two fundamental limitations: (i) code instrumentation might prevent the manifestation of test flakiness; (ii) when test flakiness is rare passively re-executing a test many times might be inadequate to trigger intermittent test outcomes. To address these limitations, we propose a new idea for root causing test flakiness that actively explores the non-deterministic space without instrumenting code. Our novel idea is to repetitively execute a flaky test, under different execution clusters. Each cluster explores a certain non-deterministic dimension (e.g., concurrency, I/O, and networking) with dedicated software containers and fuzzy-driven resource load generators. The execution cluster that manifests the most balanced (or unbalanced) sets of passing and failing executions is likely to explain the broad type of test flakiness.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: New Ideas and Emerging Results},
pages = {69–72},
numpages = {4},
keywords = {concurrency, root-causing analysis, test flakiness, cloud, non-determinism, fuzzy analysis, software containers},
location = {Seoul, South Korea},
series = {ICSE-NIER '20}
}

@inproceedings{10.5555/2486788.2486855,
author = {Meng, Na and Kim, Miryung and McKinley, Kathryn S.},
title = {LASE: Locating and Applying Systematic Edits by Learning from Examples},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Adding features and fixing bugs often require sys- tematic edits that make similar, but not identical, changes to many code locations. Finding all the relevant locations and making the correct edits is a tedious and error-prone process for developers. This paper addresses both problems using edit scripts learned from multiple examples. We design and implement a tool called LASE that (1) creates a context-aware edit script from two or more examples, and uses the script to (2) automatically identify edit locations and to (3) transform the code. We evaluate LASE on an oracle test suite of systematic edits from Eclipse JDT and SWT. LASE finds edit locations with 99% precision and 89% recall, and transforms them with 91% accuracy. We also evaluate LASE on 37 example systematic edits from other open source programs and find LASE is accurate and effective. Furthermore, we confirmed with developers that LASE found edit locations which they missed. Our novel algorithm that learns from multiple examples is critical to achieving high precision and recall; edit scripts created from only one example produce too many false positives, false negatives, or both. Our results indicate that LASE should help developers in automating systematic editing. Whereas most prior work either suggests edit locations or performs simple edits, LASE is the first to do both for nontrivial program edits. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {502–511},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1145/3391202,
author = {Kim, Seulbae and Xu, Meng and Kashyap, Sanidhya and Yoon, Jungyeon and Xu, Wen and Kim, Taesoo},
title = {Finding Bugs in File Systems with an Extensible Fuzzing Framework},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3391202},
doi = {10.1145/3391202},
abstract = {File systems are too large to be bug free. Although handwritten test suites have been widely used to stress file systems, they can hardly keep up with the rapid increase in file system size and complexity, leading to new bugs being introduced. These bugs come in various flavors: buffer overflows to complicated semantic bugs. Although bug-specific checkers exist, they generally lack a way to explore file system states thoroughly. More importantly, no turnkey solution exists that unifies the checking effort of various aspects of a file system under one umbrella.In this article, to highlight the potential of applying fuzzing to find any type of file system bugs in a generic way, we propose Hydra, an extensible fuzzing framework. Hydra provides building blocks for file system fuzzing, including input mutators, feedback engines, test executors, and bug post-processors. As a result, developers only need to focus on building the core logic for finding bugs of their interests. We showcase the effectiveness of Hydra with four checkers that hunt crash inconsistency, POSIX violations, logic assertion failures, and memory errors. So far, Hydra has discovered 157 new bugs in Linux file systems, including three in verified file systems (FSCQ and Yxv6).},
journal = {ACM Trans. Storage},
month = {may},
articleno = {10},
numpages = {35},
keywords = {fuzzing, File systems, crash consistency, bug finding}
}

@inproceedings{10.1145/1294261.1294264,
author = {Kiciman, Emre and Livshits, Benjamin},
title = {AjaxScope: A Platform for Remotely Monitoring the Client-Side Behavior of Web 2.0 Applications},
year = {2007},
isbn = {9781595935915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294261.1294264},
doi = {10.1145/1294261.1294264},
abstract = {The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While web applications have become larger and more complex, web application developers today have little visibility into the end-to-end behavior of their systems. This paper presents AjaxScope, a dynamic instrumentation platform that enables cross-user monitoring and just-in-time control of web application behavior on end-user desktops. AjaxScope is a proxy that performs on-the-fly parsing and instrumentation of JavaScript code as it is sent to users' browsers. AjaxScope provides facilities for distributed and adaptive instrumentation in order to reduce the client-side overhead, while giving fine-grained visibility into the code-level behavior of web applications. We present a variety of policies demonstrating the power of AjaxScope, ranging from simple error reporting and performance profiling to more complex memory leak detection and optimization analyses. We also apply our prototype to analyze the behavior of over 90 Web 2.0 applications and sites that use large amounts of JavaScript.},
booktitle = {Proceedings of Twenty-First ACM SIGOPS Symposium on Operating Systems Principles},
pages = {17–30},
numpages = {14},
keywords = {software instrumentation, software monitoring, web applications},
location = {Stevenson, Washington, USA},
series = {SOSP '07}
}

@article{10.1145/1323293.1294264,
author = {Kiciman, Emre and Livshits, Benjamin},
title = {AjaxScope: A Platform for Remotely Monitoring the Client-Side Behavior of Web 2.0 Applications},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5980},
url = {https://doi.org/10.1145/1323293.1294264},
doi = {10.1145/1323293.1294264},
abstract = {The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While web applications have become larger and more complex, web application developers today have little visibility into the end-to-end behavior of their systems. This paper presents AjaxScope, a dynamic instrumentation platform that enables cross-user monitoring and just-in-time control of web application behavior on end-user desktops. AjaxScope is a proxy that performs on-the-fly parsing and instrumentation of JavaScript code as it is sent to users' browsers. AjaxScope provides facilities for distributed and adaptive instrumentation in order to reduce the client-side overhead, while giving fine-grained visibility into the code-level behavior of web applications. We present a variety of policies demonstrating the power of AjaxScope, ranging from simple error reporting and performance profiling to more complex memory leak detection and optimization analyses. We also apply our prototype to analyze the behavior of over 90 Web 2.0 applications and sites that use large amounts of JavaScript.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {oct},
pages = {17–30},
numpages = {14},
keywords = {web applications, software instrumentation, software monitoring}
}

@inproceedings{10.1145/2568225.2568236,
author = {Jiang, Yanyan and Gu, Tianxiao and Xu, Chang and Ma, Xiaoxing and Lu, Jian},
title = {CARE: Cache Guided Deterministic Replay for Concurrent Java Programs},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568236},
doi = {10.1145/2568225.2568236},
abstract = { Deterministic replay tools help programmers debug concurrent programs. However, for long-running programs, a replay tool may generate huge log of shared memory access dependences. In this paper, we present CARE, an application-level deterministic record and replay technique to reduce the log size. The key idea of CARE is logging read-write dependences only at per-thread value prediction cache misses. This strategy records only a subset of all exact read-write dependences, and reduces synchronizations protecting memory reads in the instrumented code. Realizing that such record strategy provides only value-deterministic replay, CARE also adopts variable grouping and action prioritization heuristics to synthesize sequentially consistent executions at replay in linear time. We implemented CARE in Java and experimentally evaluated it with recognized benchmarks. Results showed that CARE successfully resolved all missing read-write dependences, producing sequentially consistent replay for all benchmarks. CARE exhibited 1.7--40X (median 3.4X) smaller runtime overhead, and 1.1--309X (median 7.0X) smaller log size against state-of-the-art technique LEAP. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {457–467},
numpages = {11},
keywords = {Cache, Debugging, Concurrency, Replay},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2568225.2568300,
author = {Nguyen, Hung Viet and K\"{a}stner, Christian and Nguyen, Tien N.},
title = {Exploring Variability-Aware Execution for Testing Plugin-Based Web Applications},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568300},
doi = {10.1145/2568225.2568300},
abstract = { In plugin-based systems, plugin conflicts may occur when two or more plugins interfere with one another, changing their expected behaviors. It is highly challenging to detect plugin conflicts due to the exponential explosion of the combinations of plugins (i.e., configurations). In this paper, we address the challenge of executing a test case over many configurations. Leveraging the fact that many executions of a test are similar, our variability-aware execution runs common code once. Only when encountering values that are different depending on specific configurations will the execution split to run for each of them. To evaluate the scalability of variability-aware execution on a large real-world setting, we built a prototype PHP interpreter called Varex and ran it on the popular WordPress blogging Web application. The results show that while plugin interactions exist, there is a significant amount of sharing that allows variability-aware execution to scale to 2^50 configurations within seven minutes of running time. During our study, with Varex, we were able to detect two plugin conflicts: one was recently reported on WordPress forum and another one was not previously discovered. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {907–918},
numpages = {12},
keywords = {Testing, Configurable Code, Software Product Lines, Variability-aware Execution, Plugin-based Web Applications},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1109/ASE.2019.00077,
author = {Zheng, Yan and Xie, Xiaofei and Su, Ting and Ma, Lei and Hao, Jianye and Meng, Zhaopeng and Liu, Yang and Shen, Ruimin and Chen, Yingfeng and Fan, Changjie},
title = {Wuji: Automatic Online Combat Game Testing Using Evolutionary Deep Reinforcement Learning},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00077},
doi = {10.1109/ASE.2019.00077},
abstract = {Game testing has been long recognized as a notoriously challenging task, which mainly relies on manual playing and scripting based testing in game industry. Even until recently, automated game testing still remains to be largely untouched niche. A key challenge is that game testing often requires to play the game as a sequential decision process. A bug may only be triggered until completing certain difficult intermediate tasks, which requires a certain level of intelligence. The recent success of deep reinforcement learning (DRL) sheds light on advancing automated game testing, without human competitive intelligent support. However, the existing DRLs mostly focus on winning the game rather than game testing. To bridge the gap, in this paper, we first perform an in-depth analysis of 1349 real bugs from four real-world commercial game products. Based on this, we propose four oracles to support automated game testing, and further propose Wuji, an on-the-fly game testing framework, which leverages evolutionary algorithms, DRL and multi-objective optimization to perform automatic game testing. Wuji balances between winning the game and exploring the space of the game. Winning the game allows the agent to make progress in the game, while space exploration increases the possibility of discovering bugs. We conduct a large-scale evaluation on a simple game and two popular commercial games. The results demonstrate the effectiveness of Wuji in exploring space and detecting bugs. Moreover, Wuji found 3 previously unknown bugs1, which have been confirmed by the developers, in the commercial games.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {772–784},
numpages = {13},
keywords = {game testing, artificial intelligence, deep reinforcement learning, evolutionary multi-objective optimization},
location = {San Diego, California},
series = {ASE '19}
}

@article{10.1145/3485500,
author = {Chaliasos, Stefanos and Sotiropoulos, Thodoris and Drosos, Georgios-Petros and Mitropoulos, Charalambos and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {Well-Typed Programs Can Go Wrong: A Study of Typing-Related Bugs in JVM Compilers},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {OOPSLA},
url = {https://doi.org/10.1145/3485500},
doi = {10.1145/3485500},
abstract = {Despite the substantial progress in compiler testing, research endeavors have mainly focused on detecting compiler crashes and subtle miscompilations caused by bugs in the implementation of compiler optimizations. Surprisingly, this growing body of work neglects other compiler components, most notably the front-end. In statically-typed programming languages with rich and expressive type systems and modern features, such as type inference or a mix of object-oriented with functional programming features, the process of static typing in compiler front-ends is complicated by a high-density of bugs. Such bugs can lead to the acceptance of incorrect programs (breaking code portability or the type system's soundness), the rejection of correct (e.g. well-typed) programs, and the reporting of misleading errors and warnings.  We conduct, what is to the best of our knowledge, the first empirical study for understanding and characterizing typing-related compiler bugs. To do so, we manually study 320 typing-related bugs (along with their fixes and test cases) that are randomly sampled from four mainstream JVM languages, namely Java, Scala, Kotlin, and Groovy. We evaluate each bug in terms of several aspects, including their symptom, root cause, bug fix's size, and the characteristics of the bug-revealing test cases. Some representative observations indicate that: (1) more than half of the typing-related bugs manifest as unexpected compile-time errors: the buggy compiler wrongly rejects semantically correct programs, (2) the majority of typing-related bugs lie in the implementations of the underlying type systems and in other core components related to operations on types, (3) parametric polymorphism is the most pervasive feature in the corresponding test cases, (4) one third of typing-related bugs are triggered by non-compilable programs.  We believe that our study opens up a new research direction by driving future researchers to build appropriate methods and techniques for a more holistic testing of compilers.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {123},
numpages = {30},
keywords = {compiler testing, compiler bugs, Java, static typing, Scala, Groovy, Kotlin}
}

@inproceedings{10.1145/2641580.2641585,
author = {Ahmed, Iftekhar and Mohan, Nitin and Jensen, Carlos},
title = {The Impact of Automatic Crash Reports on Bug Triaging and Development in Mozilla},
year = {2014},
isbn = {9781450330169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2641580.2641585},
doi = {10.1145/2641580.2641585},
abstract = {Free/Open Source Software projects often rely on users submitting bug reports. However, reports submitted by novice users may lack information critical to developers, and the process may be intimidating and difficult. To gather more and better data, projects deploy automatic crash reporting tools, which capture stack traces and memory dumps when a crash occurs. These systems potentially generate large volumes of data, which may overwhelm developers, and their presence may discourage users from submitting traditional bug reports. In this paper, we examine Mozilla's automatic crash reporting system and how it affects their bug triaging process. We find that fewer than 0.00009% of crash reports end up in a bug report, but as many as 2.33% of bug reports have data from crash reports added. Feedback from developers shows that despite some problems, these systems are valuable. We conclude with a discussion of the pros and cons of automatic crash reporting systems.},
booktitle = {Proceedings of The International Symposium on Open Collaboration},
pages = {1–8},
numpages = {8},
keywords = {Open Bug Reporting, Free/Open Source Software, Debugging, Testing, FOSS, Automatic Crash reporting},
location = {Berlin, Germany},
series = {OpenSym '14}
}

@inproceedings{10.1145/2568225.2568258,
author = {Kaleeswaran, Shalini and Tulsian, Varun and Kanade, Aditya and Orso, Alessandro},
title = {MintHint: Automated Synthesis of Repair Hints},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568258},
doi = {10.1145/2568225.2568258},
abstract = { Being able to automatically repair programs is at the same time a very compelling vision and an extremely challenging task. In this paper, we present MintHint, a novel technique for program repair that is a departure from most of today’s approaches. Instead of trying to fully automate program repair, which is often an unachievable goal, MintHint performs statistical correlation analysis to identify expressions that are likely to occur in the repaired code and generates, using pattern-matching based synthesis, repair hints from these expressions. Intuitively, these hints suggest how to rectify a faulty statement and help developers find a complete, actual repair.  We also present an empirical evaluation of MintHint in two parts. The first part is a user study that shows that, when debugging, developers’ productivity improved manyfold with the use of repair hints—instead of traditional fault localization information alone. The second part consists of applying MintHint to several faults in Unix utilities to further assess the effectiveness of the approach. Our results show that MintHint performs well even in common situations where (1) the repair space searched does not contain the exact repair, and (2) the operational specification obtained from the test cases for repair is incomplete or even imprecise, which can be challenging for approaches aiming at fully automated repair. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {266–276},
numpages = {11},
keywords = {repair hints, program synthesis, Program repair, statistical correlations},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3371307.3371311,
author = {Vinson, Sterling and Stonehirsch, Rachel and Coffman, Joel and Stevens, Jim},
title = {Preventing Zero-Day Exploits of Memory Vulnerabilities with Guard Lines},
year = {2019},
isbn = {9781450377461},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3371307.3371311},
doi = {10.1145/3371307.3371311},
abstract = {Exploitable memory errors are pervasive due to the widespread use of unsafe programming languages, such as C and C++. Despite much research, techniques for detecting memory errors at runtime have seen limited adoption due to high performance overhead, incomplete memory safety, or non-trivial microarchitectural changes.This paper describes Guard Lines, a hardware / software memory error detector that detects common types of spatial and temporal memory errors at runtime without imposing a significant performance penalty (on average only 4%). Guard Lines provides memory safety by defining certain regions of memory as inaccessible "guards," which are created in software during memory allocation. If a program ever accesses guarded memory, the hardware raises an exception indicating a memory safety violation. Guard Lines requires minimal microarchitectural changes, and it uses a novel metadata design to efficiently track the guard locations. This paper describes the design, implementation, security analysis, and performance evaluation of Guard Lines and demonstrates its feasibility to protect real-world applications against exploitable memory vulnerabilities.},
booktitle = {Proceedings of the 9th Workshop on Software Security, Protection, and Reverse Engineering},
articleno = {2},
numpages = {11},
keywords = {buffer overflows, hardware support, memory safety, AddressSanitizer, guard lines},
location = {San Juan, Puerto Rico, USA},
series = {SSPREW9 '19}
}

