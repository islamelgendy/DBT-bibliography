@inproceedings{10.1145/3185089.3185099,
author = {Usman, Asmau and Ibrahim, Noraini and Salihu, Ibrahim Anka},
title = {Test Case Generation from Android Mobile Applications Focusing on Context Events},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185099},
doi = {10.1145/3185089.3185099},
abstract = {Nowadays mobile apps are developed to address more critical areas of people's daily computing needs, which bring concern on the applications' quality. Today's Mobile apps processed not only the traditional GUI events but also accept and react to constantly varying context events which may have an impact on the application's behaviour. To build high quality and more reliable applications, there is a need for effective testing techniques to test apps before release. Most of recent testing technique focuses on GUI events only making it difficult to identify other defects in the changes that can be inclined by the context in which an application runs. This paper proposed an approach for testing mobile apps considering the two sets of events: GUI events which we identified through static analysis of bytecode and context events obtained from analysis of manifest.xml file. Results from the experimental evaluation indicated that our approach is effective in identifying and testing context events.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {25–30},
numpages = {6},
keywords = {Software Testing, Android Permissions, Test Case Generation, Mobile Application, GUI Event, Android, Context Event},
location = {Kuantan, Malaysia},
series = {ICSCA 2018}
}

@inproceedings{10.1145/2420950.2420997,
author = {Collberg, Christian and Martin, Sam and Myers, Jonathan and Nagra, Jasvir},
title = {Distributed Application Tamper Detection via Continuous Software Updates},
year = {2012},
isbn = {9781450313124},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2420950.2420997},
doi = {10.1145/2420950.2420997},
abstract = {We present a new general technique for protecting clients in distributed systems against Remote Man-at-the-end (R-MATE) attacks. Such attacks occur in settings where an adversary has physical access to an untrusted client device and can obtain an advantage from tampering with the hardware itself or the software it contains.In our system, the trusted server overwhelms the analytical abilities of the untrusted client by continuously and automatically generating and pushing to him diverse client code variants. The diversity subsystem employs a set of primitive code transformations that provide an ever-changing attack target for the adversary, making tampering difficult without this being detected by the server.},
booktitle = {Proceedings of the 28th Annual Computer Security Applications Conference},
pages = {319–328},
numpages = {10},
keywords = {diversity, defense-in-depth, tamperproofing, distributed systems, software protection, security, renewability, obfuscation},
location = {Orlando, Florida, USA},
series = {ACSAC '12}
}

@article{10.1145/3339836,
author = {Tramontana, Porfirio and Amalfitano, Domenico and Amatucci, Nicola and Memon, Atif and Fasolino, Anna Rita},
title = {Developing and Evaluating Objective Termination Criteria for Random Testing},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3339836},
doi = {10.1145/3339836},
abstract = {Random testing is a software testing technique through which programs are tested by generating and executing random inputs. Because of its unstructured nature, it is difficult to determine when to stop a random testing process. Faults may be missed if the process is stopped prematurely, and resources may be wasted if the process is run too long. In this article, we propose two promising termination criteria, “All Equivalent” (AEQ) and “All Included in One” (AIO), applicable to random testing. These criteria stop random testing once the process has reached a code-coverage-based saturation point after which additional testing effort is unlikely to provide additional effectiveness. We model and implement them in the context of a general random testing process composed of independent random testing sessions. Thirty-six experiments involving GUI testing and unit testing of Java applications have demonstrated that the AEQ criteria is generally able to stop the process when a code coverage equal or very near to the saturation level is reached, while AIO is able to stop the process earlier in cases it reaches the saturation level of coverage. In addition, the performance of the two criteria has been compared against other termination criteria adopted in the literature.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {17},
numpages = {52},
keywords = {code coverage, saturation effect, Random testing}
}

@inproceedings{10.1145/3194747.3194751,
author = {Dubey, Anshu and Wan, Hui},
title = {Methodology for Building Granular Testing in Multicomponent Scientific Software},
year = {2018},
isbn = {9781450357487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194747.3194751},
doi = {10.1145/3194747.3194751},
abstract = {Computational science and engineering communities develop complex application software with multiple mathematical models that need to interact with one another. Partly due to complexity of verifying scientific software, and partly because of the way incentives work in science, there has been insufficient testing of these codes. With a spotlight on the results produced with scientific software, and increasing awareness of software testing and verification as a critical contributor to the reliability of these results, testing is gaining more attention by the developing teams. However, many science teams struggle to find a good solution for themselves due either to lack of training or lack of resources within the team. In this experience paper we describe test development methodologies utilized in two different scenarios: one explains a methodology for building granular tests where none existed before, while the second demonstrates a methodology for selecting test cases that build confidence in the software through a process similar to scaffolding. The common insight from both the experiences is that testing should be a part of software design from the beginning for better software and scientific productivity.},
booktitle = {Proceedings of the International Workshop on Software Engineering for Science},
pages = {9–15},
numpages = {7},
keywords = {earth system modeling, granular testing, test building, FLASH},
location = {Gothenburg, Sweden},
series = {SE4Science '18}
}

@article{10.1145/43857.43860,
author = {Donnelly, K. F. and Gluck, K. A.},
title = {A Case Study in Test Environment Evolution},
year = {1988},
issue_date = {Jan. 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/43857.43860},
doi = {10.1145/43857.43860},
abstract = {As the demand for increasingly complex software systems grows, and our software development environments become increasingly sophisticated in response, our testing technology must keep pace. This paper describes the evolution of one software testing environment and the genesis of its logical successor.pastel (PICS Automated System Testing Environment for Leap) originated over a decade ago, with an interpreter for a simple testing language used on a single project by a group of about ten people. As the demands of testing very large systems grew, pastel's functionality expanded. Today's pastel includes facilities for creating a test by simply exercising the system under test, for running the test in a variety of modes, and for capturing and automatically analyzing test results. pastel allows application experts to create tests easily; no sophisticated programming skill is required. pastel is now used by projects throughout the Software Technology and Systems Area of Bell Communications Research.pastel is a relatively mature product, an adequate testing system for the monolithic database systems it was intended to exercise. astra, its successor, is now being designed to test systems of interacting systems built on different sizes and flavors of hardware. This paper reviews the evolution of pastel and the preliminary design work and unifying concepts underlying astra.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {22–28},
numpages = {7}
}

@inproceedings{10.5555/2484920.2484944,
author = {Gold, Kevin and Weber, Zachary J. and Priest, Ben and Ziegler, Josh and Sittig, Karen and Streilein, William W. and Mazumder, Mark},
title = {Modeling How Thinking about the Past and Future Impacts Network Traffic with the Gosmr Architecture},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {We present the GOSMR architecture, a modular agent architecture designed to actuate web browsers and other network applications, and demonstrate the importance of modeling how users think about the past and future in accurately modeling network traffic. The architecture separates the hierarchical generation of goals and incentives (Behaviors) from hierarchical implementations of their pursuit (Actions). Cognitive aspects modeled include the hyperbolic discounting of future payoffs, the chance a user forgets a task, and the ability of the user to defer tasks for later. The system also uses a logical grammar to allow agents to communicate Beliefs and delegate Actions. Using records of weekend Virtual Private Network traffic from over three thousand users at a medium-scale enterprise, we provide evidence for the importance of the forgetting, payoff discounting, and procrastinating aspects of the model, showing that agent payoff discounting and lookahead predict the observed spike in Sunday night traffic, while forgetfulness can explain a decline in activity on Saturday where the utility of login should be increasing. We then use the learned parameters from this fitting to actuate agents visiting a social networking website hosted on a virtual machine, and we measure the impact of increasing or decreasing the perceived ease of login on the hourly volume of network traffic at peak times.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {127–134},
numpages = {8},
keywords = {network traffic simulation, hyperbolic discounting, software testing, cognitive architecture, web actuation, virtual agents},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inproceedings{10.1145/3501409.3501564,
author = {Chen, Zhuo and Wang, Yongjun},
title = {JFD: Automatic Java Fuzz Driver Generation},
year = {2021},
isbn = {9781450384322},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3501409.3501564},
doi = {10.1145/3501409.3501564},
abstract = {Java is widely used in many areas. There have been a lot of works on improving the security of Java. In the industry, fuzzing is the most efficient software testing technique to discover real-world vulnerabilities and improve software security. Recent efforts are seen to make library fuzzing more automatically and have performed well in general library fuzzing. However, these tools cannot solve problems in Java library fuzzing appropriately. In this paper, we present JFD, an automatic Java fuzz driver generation system, which can generate fuzz drivers based on consumer programs, that utilize target library programs. Our approach consists of three parts: a static-analysis-based method to analyze call dependencies graphs of target Java library, a value-set-based method to analyze argument dependencies graphs of target Java library, and a method to synthesize fuzz drivers in the style of JQF. We then evaluate JFD on JVM native libraries. JFD can generate appropriate fuzz drivers.},
booktitle = {Proceedings of the 2021 5th International Conference on Electronic Information Technology and Computer Engineering},
pages = {862–867},
numpages = {6},
keywords = {software security, JQF, fuzz driver generation, fuzzing, Java},
location = {Xiamen, China},
series = {EITCE 2021}
}

@inproceedings{10.1145/3229631.3229641,
author = {Jahi\'{c}, Jasmin and Kuhn, Thomas and Jung, Matthias and Wehn, Norbert},
title = {BOSMI: A Framework for Non-Intrusive Monitoring and Testing of Embedded Multithreaded Software on the Logical Level},
year = {2018},
isbn = {9781450364942},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3229631.3229641},
doi = {10.1145/3229631.3229641},
abstract = {Traditional software testing methods are inefficient in cases where data inputs alone do not determine the outcome of a program's execution. In order to verify such software, testing is often complemented by analysis of the execution trace. For monitoring the execution trace, most approaches today insert additional instructions at the binary level, making the monitoring intrusive. Binary instrumentation operate on a low level, making it difficult to properly modify a program's states and to quantify its code coverage.In this paper, we present a framework for testing complex embedded multithreaded software on the logical level. Testing software on this level avoids dependency on concrete compilers and relates the execution to the source code, thus enabling coverage. Our non-intrusive execution monitoring and control is implemented using the LLVM interpreter compiler infrastructure. Instead of forcing thread interleaving, we suggest simulating interleaving effects through non-intrusive changes of shared variables. This makes it possible to test a single thread without executing the full software stack, which is especially useful in situations where the full software stack is not available (e.g., pre-integration testing). We complement existing approaches with new features such as dynamic configuration of monitoring and execution rollback to the checkpoints. Our approach introduces acceptable overhead without any complex setup.},
booktitle = {Proceedings of the 18th International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation},
pages = {131–138},
numpages = {8},
keywords = {LLVM, execution control, concurrency, execution monitoring, coverage},
location = {Pythagorion, Greece},
series = {SAMOS '18}
}

@inproceedings{10.1145/2489295.2493955,
author = {Musson, Robert and Smith, Ross},
title = {Data Science in the Cloud: Analysis of Data from Testing in Production},
year = {2013},
isbn = {9781450321624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489295.2493955},
doi = {10.1145/2489295.2493955},
abstract = { As global demographic, workforce, and technological trends alter the landscape of how software services are delivered, a shift towards testing in production and data science is altering the way organizations can deliver high quality experiences. Data science is the ability to find relevant relationships in the data in order to make decisions regarding the quality or performance of the software. This paper presents the landscape of testing in a global company and advocate a more generalized use of data science for testing the Cloud. It describes the collection and analysis of data and investigates the crucial question of data scientists profile and management. },
booktitle = {Proceedings of the 2013 International Workshop on Testing the Cloud},
pages = {18–20},
numpages = {3},
keywords = {test, data},
location = {Lugano, Switzerland},
series = {TTC 2013}
}

@inproceedings{10.1145/64135.65018,
author = {Clarke, Lori A. and Richardson, Debra J. and Zeil, Steven J.},
title = {TEAM: A Support Environment for Testing, Evaluation, and Analysis},
year = {1988},
isbn = {089791290X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/64135.65018},
doi = {10.1145/64135.65018},
abstract = {Current research indicates that software reliability needs to be achieved through the careful integration of a number of diverse testing and analysis techniques. To address this need, the TEAM environment has been designed to support the integration of and experimentation with an ever growing number of software testing and analysis tools. To achieve this flexibility, we exploit three design principles: component technology so that common underlying functionality is recognized; generic realizations so that these common functions can be instantiated as diversely as possible; and language independence so that tools can work on multiple languages, even allowing some tools to be applicable to different phases of the software lifecycle. The result is an environment that contains building blocks for easily constructing and experimenting with new testing and analysis techniques. Although the first prototype has just recently been implemented, we feel it demonstrates how modularity, genericity, and language independence further extensibility and integration.},
booktitle = {Proceedings of the Third ACM SIGSOFT/SIGPLAN Software Engineering Symposium on Practical Software Development Environments},
pages = {153–162},
numpages = {10},
location = {Boston, Massachusetts, USA},
series = {SDE 3}
}

@article{10.1145/64140.65018,
author = {Clarke, Lori A. and Richardson, Debra J. and Zeil, Steven J.},
title = {TEAM: A Support Environment for Testing, Evaluation, and Analysis},
year = {1988},
issue_date = {Feb. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/64140.65018},
doi = {10.1145/64140.65018},
abstract = {Current research indicates that software reliability needs to be achieved through the careful integration of a number of diverse testing and analysis techniques. To address this need, the TEAM environment has been designed to support the integration of and experimentation with an ever growing number of software testing and analysis tools. To achieve this flexibility, we exploit three design principles: component technology so that common underlying functionality is recognized; generic realizations so that these common functions can be instantiated as diversely as possible; and language independence so that tools can work on multiple languages, even allowing some tools to be applicable to different phases of the software lifecycle. The result is an environment that contains building blocks for easily constructing and experimenting with new testing and analysis techniques. Although the first prototype has just recently been implemented, we feel it demonstrates how modularity, genericity, and language independence further extensibility and integration.},
journal = {SIGPLAN Not.},
month = {nov},
pages = {153–162},
numpages = {10}
}

@article{10.1145/64137.65018,
author = {Clarke, Lori A. and Richardson, Debra J. and Zeil, Steven J.},
title = {TEAM: A Support Environment for Testing, Evaluation, and Analysis},
year = {1988},
issue_date = {November 1988},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/64137.65018},
doi = {10.1145/64137.65018},
abstract = {Current research indicates that software reliability needs to be achieved through the careful integration of a number of diverse testing and analysis techniques. To address this need, the TEAM environment has been designed to support the integration of and experimentation with an ever growing number of software testing and analysis tools. To achieve this flexibility, we exploit three design principles: component technology so that common underlying functionality is recognized; generic realizations so that these common functions can be instantiated as diversely as possible; and language independence so that tools can work on multiple languages, even allowing some tools to be applicable to different phases of the software lifecycle. The result is an environment that contains building blocks for easily constructing and experimenting with new testing and analysis techniques. Although the first prototype has just recently been implemented, we feel it demonstrates how modularity, genericity, and language independence further extensibility and integration.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {153–162},
numpages = {10}
}

@inproceedings{10.1145/2889160.2889212,
author = {Briand, Lionel and Nejati, Shiva and Sabetzadeh, Mehrdad and Bianculli, Domenico},
title = {Testing the Untestable: Model Testing of Complex Software-Intensive Systems},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889212},
doi = {10.1145/2889160.2889212},
abstract = {Increasingly, we are faced with systems that are untestable, meaning that traditional testing methods are expensive, time-consuming or infeasible to apply due to factors such as the systems' continuous interactions with the environment and the deep intertwining of software with hardware.In this paper we outline our vision to enable testing of untestable systems. Our key idea is to frame testing on models rather than operational systems. We refer to such testing as model testing. Our goal is to raise the level of abstraction of testing from operational systems to models of their behaviors and properties. The models that underlie model testing are executable representations of the relevant aspects of a system and its environment, alongside the risks of system failures. Such models necessarily have uncertainties due to complex, dynamic environment behaviors and the unknowns about the system. This makes it crucial for model testing to be uncertainty-aware. We propose to synergistically combine metaheuristic search, increasingly used in traditional software testing, with system and risk models to drive the search for faults that entail the most risk.We expect model testing to bring early and cost-effective automation to the testing of many critical systems that defy existing automation techniques, thus significantly improving the dependability of such systems.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {789–792},
numpages = {4},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3180155.3180183,
author = {Papadakis, Mike and Shin, Donghwan and Yoo, Shin and Bae, Doo-Hwan},
title = {Are Mutation Scores Correlated with Real Fault Detection? A Large Scale Empirical Study on the Relationship between Mutants and Real Faults},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180183},
doi = {10.1145/3180155.3180183},
abstract = {Empirical validation of software testing studies is increasingly relying on mutants. This practice is motivated by the strong correlation between mutant scores and real fault detection that is reported in the literature. In contrast, our study shows that correlations are the results of the confounding effects of the test suite size. In particular, we investigate the relation between two independent variables, mutation score and test suite size, with one dependent variable the detection of (real) faults. We use two data sets, CoreBench and Defects4J, with large C and Java programs and real faults and provide evidence that all correlations between mutation scores and real fault detection are weak when controlling for test suite size. We also find that both independent variables significantly influence the dependent one, with significantly better fits, but overall with relative low prediction power. By measuring the fault detection capability of the top ranked, according to mutation score, test suites (opposed to randomly selected test suites of the same size), we find that achieving higher mutation scores improves significantly the fault detection. Taken together, our data suggest that mutants provide good guidance for improving the fault detection of test suites, but their correlation with fault detection are weak.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {537–548},
numpages = {12},
keywords = {mutation testing, test suite effectiveness, real faults},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3196398.3196473,
author = {Saha, Ripon K. and Lyu, Yingjun and Lam, Wing and Yoshida, Hiroaki and Prasad, Mukul R.},
title = {Bugs.Jar: A Large-Scale, Diverse Dataset of Real-World Java Bugs},
year = {2018},
isbn = {9781450357166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196398.3196473},
doi = {10.1145/3196398.3196473},
abstract = {We present Bugs.jar, a large-scale dataset for research in automated debugging, patching, and testing of Java programs. Bugs.jar is comprised of 1,158 bugs and patches, drawn from 8 large, popular open-source Java projects, spanning 8 diverse and prominent application categories. It is an order of magnitude larger than Defects4J, the only other dataset in its class. We discuss the methodology used for constructing Bugs.jar, the representation of the dataset, several use-cases, and an illustration of three of the use-cases through the application of 3 specific tools on Bugs.jar, namely our own tool, Elixir, and two third-party tools, Ekstazi and JaCoCo.},
booktitle = {Proceedings of the 15th International Conference on Mining Software Repositories},
pages = {10–13},
numpages = {4},
keywords = {large-scale dataset, Java programs, reproducible bugs},
location = {Gothenburg, Sweden},
series = {MSR '18}
}

@inproceedings{10.1145/1138929.1138932,
author = {Vieira, Marlon and Leduc, Johanne and Hasling, Bill and Subramanyan, Rajesh and Kazmeier, Juergen},
title = {Automation of GUI Testing Using a Model-Driven Approach},
year = {2006},
isbn = {1595934081},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1138929.1138932},
doi = {10.1145/1138929.1138932},
abstract = {This paper describes an ongoing research on test case generation based on Unified Modeling Language (UML). The described approach builds on and combines existing techniques for data and graph coverage. It first uses the Category-Partition method to introduce data into the UML model. UML Use Cases and Activity diagrams are used to respectively describe which functionalities should be tested and how to test them. This combination has the potential to create a very large number of test cases. This approach offers two ways to manage the number of tests. First, custom annotations and guards use the Category-Partition data which allows the designer tight control over possible, or impossible, paths. Second, automation allows different configurations for both the data and the graph coverage. The process of modeling UML activity diagrams, annotating them with test data requirements, and generating test scripts from the models is described. The goal of this paper is to illustrate the benefits of our model-based approach for improving automation on software testing. The approach is demonstrated and evaluated based on use cases developed for testing a graphical user interface (GUI).},
booktitle = {Proceedings of the 2006 International Workshop on Automation of Software Test},
pages = {9–14},
numpages = {6},
keywords = {GUI verification, UML, model based testing},
location = {Shanghai, China},
series = {AST '06}
}

@article{10.1145/2559936,
author = {Tappenden, Andrew F. and Miller, James},
title = {Automated Cookie Collection Testing},
year = {2014},
issue_date = {February 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2559936},
doi = {10.1145/2559936},
abstract = {Cookies are used by over 80% of Web applications utilizing dynamic Web application frameworks. Applications deploying cookies must be rigorously verified to ensure that the application is robust and secure. Given the intense time-to-market pressures faced by modern Web applications, testing strategies that are low cost and automatable are required. Automated Cookie Collection Testing (CCT) is presented, and is empirically demonstrated to be a low-cost and highly effective automated testing solution for modern Web applications. Automatable test oracles and evaluation metrics specifically designed for Web applications are presented, and are shown to be significant diagnostic tests. Automated CCT is shown to detect faults within five real-world Web applications. A case study of over 580 test results for a single application is presented demonstrating that automated CCT is an effective testing strategy. Moreover, CCT is found to detect security bugs in a Web application released into full production.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {3},
numpages = {40},
keywords = {test generation, Cookies, adaptive random testing, software testing, test strategies, Web application testing, automated testing}
}

@article{10.1145/1050849.1050865,
author = {Xu, Baowen and Qian, Ju and Zhang, Xiaofang and Wu, Zhongqiang and Chen, Lin},
title = {A Brief Survey of Program Slicing},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1050865},
doi = {10.1145/1050849.1050865},
abstract = {Program slicing is a technique to extract program parts with respect to some special computation. Since Weiser first proposed the notion of slicing in 1979, hundreds of papers have been presented in this area. Tens of variants of slicing have been studied, as well as algorithms to compute them. Different notions of slicing have different properties and different applications. These notions vary from Weiser's syntax-preserving static slicing to amorphous slicing which is not syntax-preserving, and the algorithms can be based on dataflow equations, information-flow relations or dependence graphs.Slicing was first-developed to facilitate debugging, but it is then found helpful in many aspects of the software development life cycle, including program debugging, software testing, software measurement, program comprehension, software maintenance, program parallelization and so on.Over the last two decades, several surveys on program slicing have been presented. However, most of them only reviewed parts of researches on program slicing or have now been out of date. People who are interested in program slicing need more information about the up to date researches. Our survey fills this gap. In this paper, we briefly review most of existing slicing techniques including static slicing, dynamic slicing and the latest slicing techniques. We also discuss the contribution of each work and compare the major difference between them. Researches on slicing are classified by the research hot spots such that people can be kept informed of the overall program slicing researches.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–36},
numpages = {36},
keywords = {dependence analysis, debugging, program analysis, pointer analysis, program slicing}
}

@article{10.1145/3490489,
author = {Xie, Xiaofei and Li, Tianlin and Wang, Jian and Ma, Lei and Guo, Qing and Juefei-Xu, Felix and Liu, Yang},
title = {NPC: <u class="uu">N</u>euron <u class="uu">P</u>ath <u class="uu">C</u>overage via Characterizing Decision Logic of Deep Neural Networks},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3490489},
doi = {10.1145/3490489},
abstract = {Deep learning has recently been widely applied to many applications across different domains, e.g., image classification and audio recognition. However, the quality of Deep Neural Networks (DNNs) still raises concerns in the practical operational environment, which calls for systematic testing, especially in safety-critical scenarios. Inspired by software testing, a number of structural coverage criteria are designed and proposed to measure the test adequacy of DNNs. However, due to the blackbox nature of DNN, the existing structural coverage criteria are difficult to interpret, making it hard to understand the underlying principles of these criteria. The relationship between the structural coverage and the decision logic of DNNs is unknown. Moreover, recent studies have further revealed the non-existence of correlation between the structural coverage and DNN defect detection, which further posts concerns on what a suitable DNN testing criterion should be.In this article, we propose the interpretable coverage criteria through constructing the decision structure of a DNN. Mirroring the control flow graph of the traditional program, we first extract a decision graph from a DNN based on its interpretation, where a path of the decision graph represents a decision logic of the DNN. Based on the control flow and data flow of the decision graph, we propose two variants of path coverage to measure the adequacy of the test cases in exercising the decision logic. The higher the path coverage, the more diverse decision logic the DNN is expected to be explored. Our large-scale evaluation results demonstrate that: The path in the decision graph is effective in characterizing the decision of the DNN, and the proposed coverage criteria are also sensitive with errors, including natural errors and adversarial examples, and strongly correlate with the output impartiality.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {47},
numpages = {27},
keywords = {testing coverage criteria, model interpretation, Deep learning testing}
}

@inproceedings{10.1145/1810295.1810421,
author = {Budnik, Christof J. and Chan, Wing Kwong and Kapfhammer, Gregory M.},
title = {Bridging the Gap Between the Theory and Practice of Software Test Automation},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810421},
doi = {10.1145/1810295.1810421},
abstract = {In software development practice, testing often accounts for as much as 50% of the total development effort. It is therefore imperative to reduce the cost and improve the effectiveness of software testing by automating the testing process. In the past decades, a substantial amount of research effort has been invested into the development and study of automatic test case generation, automatic test oracles, and other (semi-)automated testing techniques. As the theory and practice of software testing becomes more mature, a deeper and more meaningful automation of the testing process is possible. Therefore, the automation of various testing activities is now becoming an integral part of industrial practice. In response to and in support of these exciting developments, the 5th Workshop on the Automation of Software Test provides a publication forum that bridges the gap between the theory and practice of automated testing.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {445–446},
numpages = {2},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/1985793.1985932,
author = {Atkinson, Colin and Hummel, Oliver and Janjic, Werner},
title = {Search-Enhanced Testing (NIER Track)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1985932},
doi = {10.1145/1985793.1985932},
abstract = {The prime obstacle to automated defect testing has always been the generation of "correct" results against which to judge the behavior of the system under test - the "oracle problem". So called "back-to-back" testing techniques that exploit the availability of multiple versions of a system to solve the oracle problem have mainly been restricted to very special, safety critical domains such as military and space applications since it is so expensive to manually develop the additional versions. However, a new generation of software search engines that can find multiple copies of software components at virtually zero cost promise to change this situation. They make it economically feasible to use the knowledge locked in reusable software components to dramatically improve the efficiency of the software testing process. In this paper we outline the basic ingredients of such an approach.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {880–883},
numpages = {4},
keywords = {multi-version programming, back-to-back testing, automated testing, search-driven development},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/2945404.2945405,
author = {Hirzel, Matthias and Klaeren, Herbert},
title = {Code Coverage for Any Kind of Test in Any Kind of Transcompiled Cross-Platform Applications},
year = {2016},
isbn = {9781450344128},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945404.2945405},
doi = {10.1145/2945404.2945405},
abstract = { Code coverage is a widely used measure to determine how thoroughly an application is tested. There are many tools available for different languages. However, to the best of our knowledge, most of them focus on unit testing and ignore end-to-end tests with ui- or web tests. Furthermore, there is no support for determining code coverage of transcompiled cross-platform applications. This kind of application is written in one language, but compiled to and executed in a different programming language. Besides, it may run on a different platform. In this paper, we propose a new code coverage testing method that calculates the code coverage of any kind of test (unit-, integration- or ui-/web-test) for any type of (transcompiled) applications (desktop, web or mobile application). Developers obtain information about which parts of the source code are uncovered by tests. The basis of our approach is generic and may be applied in numerous programming languages based on an abstract syntax tree. We present our approach for any-kind-applications developed in Java and evaluate our tool on a web application created with Google Web Toolkit, on standard desktop applications, and on some small Java applications that use the Swing library to create user interfaces. Our results show that our tool is able to judge the code coverage of any kind of test. In particular, our tool is independent of the unit- or ui-/web test-framework in use. The runtime performance is promising although it is not as fast as already existing tools in the area of unit-testing. },
booktitle = {Proceedings of the 2nd International Workshop on User Interface Test Automation},
pages = {1–10},
numpages = {10},
keywords = {Software quality, Code instrumentation, Web Testing, UI Testing, Unit Testing, Cross-Platform, Code coverage},
location = {Saarbr\"{u}cken, Germany},
series = {INTUITEST 2016}
}

