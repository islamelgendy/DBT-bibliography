@inproceedings{10.1145/1830483.1830739,
author = {Wilkerson, Josh L. and Tauritz, Daniel},
title = {Coevolutionary Automated Software Correction},
year = {2010},
isbn = {9781450300728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1830483.1830739},
doi = {10.1145/1830483.1830739},
abstract = {This paper presents the Coevolutionary Automated Software Correction system, which addresses in an integral and fully automated manner the complete cycle of software artifact testing, error location, and correction phases. It employs a coevolutionary approach where software artifacts and test cases are evolved in tandem. The test cases evolve to better find flaws in the software artifacts and the software artifacts evolve to better behave to specification when exposed to the test cases, thus causing an evolutionary arms race. Experimental results are presented on the same test problem employed in the published results on the previous state-of-the-art automated software correction system.},
booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
pages = {1391–1392},
numpages = {2},
keywords = {search-based testing, automated debugging, repair, genetic programming, coevolution},
location = {Portland, Oregon, USA},
series = {GECCO '10}
}

@inbook{10.1109/ICSE43902.2021.00030,
author = {Luo, Chuan and Lin, Jinkun and Cai, Shaowei and Chen, Xin and He, Bing and Qiao, Bo and Zhao, Pu and Lin, Qingwei and Zhang, Hongyu and Wu, Wei and Rajmohan, Saravanakumar and Zhang, Dongmei},
title = {AutoCCAG: An Automated Approach to Constrained Covering Array Generation},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00030},
abstract = {Combinatorial interaction testing (CIT) is an important technique for testing highly configurable software systems with demonstrated effectiveness in practice. The goal of CIT is to generate test cases covering the interactions of configuration options, under certain hard constraints. In this context, constrained covering arrays (CCAs) are frequently used as test cases in CIT. Constrained Covering Array Generation (CCAG) is an NP-hard combinatorial optimization problem, solving which requires an effective method for generating small CCAs. In particular, effectively solving t-way CCAG with t ≥ 4 is even more challenging. Inspired by the success of automated algorithm configuration and automated algorithm selection in solving combinatorial optimization problems, in this paper, we investigate the efficacy of automated algorithm configuration and automated algorithm selection for the CCAG problem, and propose a novel, automated CCAG approach called AutoCCAG. Extensive experiments on public benchmarks show that AutoCCAG can find much smaller-sized CCAs than current state-of-the-art approaches, indicating the effectiveness of AutoCCAG. More encouragingly, to our best knowledge, our paper reports the first results for CCAG with a high coverage strength (i.e., 5-way CCAG) on public benchmarks. Our results demonstrate that AutoCCAG can bring considerable benefits in testing highly configurable software systems.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {201–212},
numpages = {12}
}

@inproceedings{10.1145/3185089.3185148,
author = {Din, Fakhrud and Zamli, Kamal Z.},
title = {Fuzzy Adaptive Teaching Learning-Based Optimization Strategy for GUI Functional Test Cases Generation},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185148},
doi = {10.1145/3185089.3185148},
abstract = {Graphical User Interface (GUI) visualizes computer programs for the purpose of facilitating interaction between users and various computing devices. Today's computers, smart phones and even small devices such as watches are equipped with GUIs. Unlike command based interaction, GUI uses images, labels, push buttons, radio buttons, etc. for the effective communication of users with a software system. GUI testing is a critical part of software testing as it is the door to the actual functionality of software. For the quality assurance, GUI functional testing of a software validates proper interaction between the interface and the user without considering any coding details. In this paper, a strategy based on fuzzy Adaptive Teaching Learning-based Optimization (ATLBO) algorithm, a variant of the basic Teaching Learning-based Optimization (TLBO) algorithm, for GUI functional testing is proposed. ATLBO utilizes Event-Interaction Graph (EIG) for the generation of quality test cases. The proposed strategy has produced competitive experimental results against the basic TLBO and other test case generation algorithms.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {92–96},
numpages = {5},
keywords = {GUI Functional Testing, Teaching Learning-based Optimization, Mamdani Fuzzy Inference System, Model-based testing},
location = {Kuantan, Malaysia},
series = {ICSCA 2018}
}

@inproceedings{10.1145/2884781.2884879,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding and Analyzing Compiler Warning Defects},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884879},
doi = {10.1145/2884781.2884879},
abstract = {Good compiler diagnostic warnings facilitate software development as they indicate likely programming mistakes or code smells. However, due to compiler bugs, the warnings may be erroneous, superfluous or missing, even for mature production compilers like GCC and Clang. In this paper, we (1) propose the first randomized differential testing technique to detect compiler warning defects and (2) describe our extensive evaluation in finding warning defects in widely-used C compilers.At the high level, our technique starts with generating random programs to trigger compilers to emit a variety of compiler warnings, aligns the warnings from different compilers, and identifies inconsistencies as potential bugs. We develop effective techniques to overcome three specific challenges: (1) How to generate random programs, (2) how to align textual warnings, and (3) how to reduce test programs for bug reporting?Our technique is very effective --- we have found and reported 60 bugs for GCC (38 confirmed, assigned or fixed) and 39 for Clang (14 confirmed or fixed). This case study not only demonstrates our technique's effectiveness, but also highlights the need to continue improving compilers' warning support, an essential, but rather neglected aspect of compilers.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {203–213},
numpages = {11},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/3324884.3416668,
author = {Nguyen, Hoang Lam and Nassar, Nebras and Kehrer, Timo and Grunske, Lars},
title = {MoFuzz: A Fuzzer Suite for Testing Model-Driven Software Engineering Tools},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416668},
doi = {10.1145/3324884.3416668},
abstract = {Fuzzing or fuzz testing is an established technique that aims to discover unexpected program behavior (e.g., bugs, security vulnerabilities, or crashes) by feeding automatically generated data into a program under test. However, the application of fuzzing to test Model-Driven Software Engineering (MDSE) tools is still limited because of the difficulty of existing fuzzers to provide structured, well-typed inputs, namely models that conform to typing and consistency constraints induced by a given meta-model and underlying modeling framework. By drawing from recent advances on both fuzz testing and automated model generation, we present three different approaches for fuzzing MDSE tools: A graph grammar-based fuzzer and two variants of a coverage-guided mutation-based fuzzer working with different sets of model mutation operators. Our evaluation on a set of real-world MDSE tools shows that our approaches can outperform both standard fuzzers and model generators w.r.t. their fuzzing capabilities. Moreover, we found that each of our approaches comes with its own strengths and weaknesses in terms of fault finding capabilities and the ability to cover different aspects of the system under test. Thus the approaches complement each other, forming a fuzzer suite for testing MDSE tools.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1103–1115},
numpages = {13},
keywords = {automated model generation, model-driven software engineering, eclipse modeling framework, modeling tools, fuzzing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3276531,
author = {Selakovic, Marija and Pradel, Michael and Karim, Rezwana and Tip, Frank},
title = {Test Generation for Higher-Order Functions in Dynamic Languages},
year = {2018},
issue_date = {November 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {OOPSLA},
url = {https://doi.org/10.1145/3276531},
doi = {10.1145/3276531},
abstract = {Test generation has proven to provide an effective way of identifying programming errors. Unfortunately, current test generation techniques are challenged by higher-order functions in dynamic languages, such as JavaScript functions that receive callbacks. In particular, existing test generators suffer from the unavailability of statically known type signatures, do not provide functions or provide only trivial functions as inputs, and ignore callbacks triggered by the code under test. This paper presents LambdaTester, a novel test generator that addresses the specific problems posed by higher-order functions in dynamic languages. The approach automatically infers at what argument position a method under test expects a callback, generates and iteratively improves callback functions given as input to this method, and uses novel test oracles that check whether and how callback functions are invoked. We apply LambdaTester to test 43 higher-order functions taken from 13 popular JavaScript libraries. The approach detects unexpected behavior in 12 of the 13 libraries, many of which are missed by a state-of-the-art test generator.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {161},
numpages = {27},
keywords = {dynamic analysis, Higher-order functions, JavaScript, differential testing}
}

@inproceedings{10.1145/3396452.3396459,
author = {Aljedaani, Wajdi and Javed, Yasir and Alenezi, Mamdouh},
title = {Open Source Systems Bug Reports: Meta-Analysis},
year = {2020},
isbn = {9781450374989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3396452.3396459},
doi = {10.1145/3396452.3396459},
abstract = {Bug Tracking System (BTS) is a wealthy source of software development information. They contain many insights about the health status of the software project. Making sense of this information is a big challenge to software development communities. In this work, we perform an investigation on fixing time, components, and platforms related to five open source systems hosted by Bugzilla. The motive is to identify what are the most error-prone components and to allocate the right developer to fix the bug. The results are indicators for how data in BTS should be utilized for decision-making processes. The results reveal a strong relationship between bugs and committers, where it is seen that committer is usually related to fixing a single domain of bugs that also shows their expertise. This study can also help in the automated classification of bug allocation to the right kind of committers instead of manual allocation that will result in a reduction of fixing time and interloping between different committers.},
booktitle = {Proceedings of the 2020 The 3rd International Conference on Big Data and Education},
pages = {43–49},
numpages = {7},
keywords = {bug repository, bug reports, empirical studies, fixing-time},
location = {London, United Kingdom},
series = {ICBDE '20}
}

@inproceedings{10.1109/ICSE43902.2021.00136,
author = {Vidoni, Melina},
title = {Evaluating Unit Testing Practices in R Packages},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00136},
doi = {10.1109/ICSE43902.2021.00136},
abstract = {Testing Technical Debt (TTD) occurs due to shortcuts (non-optimal decisions) taken about testing; it is the test dimension of technical debt. R is a package-based programming ecosystem that provides an easy way to install third-party code, datasets, tests, documentation and examples. This structure makes it especially vulnerable to TTD because errors present in a package can transitively affect all packages and scripts that depend on it. Thus, TTD can effectively become a threat to the validity of all analysis written in R that rely on potentially faulty code. This two-part study provides the first analysis in this area. First, 177 systematically-selected, open-source R packages were mined and analysed to address quality of testing, testing goals, and identify potential TTD sources. Second, a survey addressed how R package developers perceive testing and face its challenges (response rate of 19.4%). Results show that testing in R packages is of low quality; the most common smells are inadequate and obscure unit testing, improper asserts, inexperienced testers and improper test design. Furthermore, skilled R developers still face challenges such as time constraints, emphasis on development rather than testing, poor tool documentation and a steep learning curve.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1523–1534},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inproceedings{10.1145/2093190.2093191,
author = {De Borger, Wouter and Lagaisse, Bert and Joosen, Wouter},
title = {Inspection of Distributed and Composed Systems},
year = {2011},
isbn = {9781450310727},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2093190.2093191},
doi = {10.1145/2093190.2093191},
abstract = {Middleware enables developers to build more complex applications by shielding them from the complexity of the underlying platform and environment. However, when inspecting software at run-time, the abstractions offered by the middleware are no longer visible.To support advanced control of middleware based systems, in a dynamic cloud environment, my goal is to provide the proper tools to enable inspection of complex, composed systems, in terms of the most appropriate abstractions.},
booktitle = {Proceedings of the 8th Middleware Doctoral Symposium},
articleno = {1},
numpages = {3},
keywords = {monitoring, model transformation, complex event processing, debugging, middleware},
location = {Lisbon, Portugal},
series = {MDS '11}
}

@article{10.1145/3428298,
author = {Mukherjee, Suvam and Deligiannis, Pantazis and Biswas, Arpita and Lal, Akash},
title = {Learning-Based Controlled Concurrency Testing},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428298},
doi = {10.1145/3428298},
abstract = {Concurrency bugs are notoriously hard to detect and reproduce. Controlled concurrency testing (CCT) techniques aim to offer a solution, where a scheduler explores the space of possible interleavings of a concurrent program looking for bugs. Since the set of possible interleavings is typically very large, these schedulers employ heuristics that prioritize the search to “interesting” subspaces. However, current heuristics are typically tuned to specific bug patterns, which limits their effectiveness in practice. In this paper, we present QL, a learning-based CCT framework where the likelihood of an action being selected by the scheduler is influenced by earlier explorations. We leverage the classical Q-learning algorithm to explore the space of possible interleavings, allowing the exploration to adapt to the program under test, unlike previous techniques. We have implemented and evaluated QL on a set of microbenchmarks, complex protocols, as well as production cloud services. In our experiments, we found QL to consistently outperform the state-of-the-art in CCT.},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {230},
numpages = {31},
keywords = {Concurrency, Reinforcement Learning, Systematic Testing}
}

@inproceedings{10.1145/3488660.3493803,
author = {Crochet, Christophe and Rousseaux, Tom and Piraux, Maxime and Sambon, Jean-Fran\c{c}ois and Legay, Axel},
title = {Verifying QUIC Implementations Using Ivy},
year = {2021},
isbn = {9781450391351},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3488660.3493803},
doi = {10.1145/3488660.3493803},
abstract = {QUIC is a new transport protocol combining the reliability and congestion control features of TCP with the security features of TLS. One of the main challenges with QUIC is to guarantee that any of its implementation follows the IETF specification. This challenge is particularly appealing as the specification is written in textual language, and hence may contain ambiguities. In a recent work, McMillan and Zuck proposed a formal representation of part of draft-18 of the IETF specification. They also showed that this representation made it possible to efficiently generate tests to stress four implementations of QUIC. Our first contribution is to complete and extend the formal representation from draft-18 to draft-29. Our second contribution is to test seven implementations of both QUIC client and server. Our last contribution is to show that our tool can highlight ambiguities in the QUIC specification, for which we suggest paths to corrections.},
booktitle = {Proceedings of the 2021 Workshop on Evolution, Performance and Interoperability of QUIC},
pages = {35–41},
numpages = {7},
keywords = {testing, Ivy, interoperability, formal specification, draft-29, RFC9000, verification, QUIC},
location = {Virtual Event, Germany},
series = {EPIQ '21}
}

@inproceedings{10.1145/2591062.2591170,
author = {Nederlof, Alex and Mesbah, Ali and Deursen, Arie van},
title = {Software Engineering for the Web: The State of the Practice},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591170},
doi = {10.1145/2591062.2591170},
abstract = { Today’s web applications increasingly rely on client-side code execution. HTML is not just created on the server, but manipulated extensively within the browser through JavaScript code. In this paper, we seek to understand the software engineering implications of this. We look at deviations from many known best practices in such areas of performance, accessibility, and correct structuring of HTML documents. Furthermore, we assess to what extent such deviations manifest themselves through client-side code manipulation only. To answer these questions, we conducted a large scale experiment, involving automated client-enabled crawling of over 4000 web applications, resulting in over 100,000,000 pages analyzed, and close to 1,000,000 unique client-side user interface states. Our findings show that the majority of sites contain a substantial number of problems, making sites unnecessarily slow, inaccessible for the visually impaired, and with layout that is unpredictable due to errors in the dynamically modified DOM trees. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {4–13},
numpages = {10},
keywords = {Web development best practices, Automatic error detection, Crawling, JavaScript},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/1569901.1570253,
author = {Ribeiro, Jos\'{e} Carlos B. and Zenha-Rela, M\'{a}rio Alberto and de Vega, Francisco Fern\'{a}ndez},
title = {An Adaptive Strategy for Improving the Performance of Genetic Programming-Based Approaches to Evolutionary Testing},
year = {2009},
isbn = {9781605583259},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1569901.1570253},
doi = {10.1145/1569901.1570253},
abstract = {This paper proposes an adaptive strategy for enhancing Genetic Programming-based approaches to automatic test case generation. The main contribution of this study is that of proposing an adaptive Evolutionary Testing methodology for promoting the introduction of relevant instructions into the generated test cases by means of mutation; the instructions from which the algorithm can choose are ranked, with their rankings being updated every generation in accordance to the feedback obtained from the individuals evaluated in the preceding generation. The experimental studies developed show that the adaptive strategy proposed improves the algorithm's efficiency considerably, while introducing a negligible computational overhead.},
booktitle = {Proceedings of the 11th Annual Conference on Genetic and Evolutionary Computation},
pages = {1949–1950},
numpages = {2},
keywords = {search-based test case generation, search-based software engineering, adaptive evolutionary algorithms, genetic programming, evolutionary testing},
location = {Montreal, Qu\'{e}bec, Canada},
series = {GECCO '09}
}

@inproceedings{10.1145/1168149.1168166,
author = {Whiting, Mark A. and Cowley, Wendy and Haack, Jereme and Love, Doug and Tratz, Stephen and Varley, Caroline and Wiessner, Kim},
title = {Threat Stream Data Generator: Creating the Known Unknowns for Test and Evaluation of Visual Analytics Tools},
year = {2006},
isbn = {1595935622},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1168149.1168166},
doi = {10.1145/1168149.1168166},
abstract = {We present the Threat Stream Data Generator, an approach and tool for creating synthetic data sets for the test and evaluation of visual analytics tools and environments. We have focused on working with information analysts to understand the characteristics of threat data, to develop scenarios that will allow us to define data sets with known ground truth, to define a process of mapping threat elements in a scenario to expressions in data, and creating a software system to generate the data. We are also developing approaches to evaluating our data sets considering characteristics such as threat subtlety and appropriateness of data for the software to be examined.},
booktitle = {Proceedings of the 2006 AVI Workshop on BEyond Time and Errors: Novel Evaluation Methods for Information Visualization},
pages = {1–3},
numpages = {3},
keywords = {threat stream, data generator, threat},
location = {Venice, Italy},
series = {BELIV '06}
}

@article{10.1145/3355048,
author = {Tian, Cong and Chen, Chu and Duan, Zhenhua and Zhao, Liang},
title = {Differential Testing of Certificate Validation in SSL/TLS Implementations: An RFC-Guided Approach},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3355048},
doi = {10.1145/3355048},
abstract = {Certificate validation in Secure Sockets Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS implementations is correctly implemented. With this motivation, we propose a novel differential testing approach that is based on the standard Request for Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e., certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted, since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations. In addition, by providing seed certificates for mutation approaches with RFCcert, the ability of mutation approaches in finding distinct discrepancies is significantly enhanced.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
articleno = {24},
numpages = {37},
keywords = {SSL/TLS, request for comments, dynamic symbolic execution, Differential testing, certificate validation}
}

@inproceedings{10.1109/ICSE-NIER.2017.8,
author = {Durieux, Thomas and Hamadi, Youssef and Monperrus, Martin},
title = {Production-Driven Patch Generation},
year = {2017},
isbn = {9781538626757},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-NIER.2017.8},
doi = {10.1109/ICSE-NIER.2017.8},
abstract = {We present an original concept for patch generation: we propose to do it directly in production. Our idea is to generate patches on-the-fly based on automated analysis of the failure context. By doing this in production, the repair process has complete access to the system state at the point of failure. We propose to perform live regression testing of the generated patches directly on the production traffic, by feeding a sandboxed version of the application with a copy of the production traffic, the "shadow traffic". Our concept widens the applicability of program repair, because it removes the requirements of having a failing test case.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering: New Ideas and Emerging Results Track},
pages = {23–26},
numpages = {4},
location = {Buenos Aires, Argentina},
series = {ICSE-NIER '17}
}

@inproceedings{10.1145/2884781.2884821,
author = {Devroey, Xavier and Perrouin, Gilles and Papadakis, Mike and Legay, Axel and Schobbens, Pierre-Yves and Heymans, Patrick},
title = {Featured Model-Based Mutation Analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884821},
doi = {10.1145/2884781.2884821},
abstract = {Model-based mutation analysis is a powerful but expensive testing technique. We tackle its high computation cost by proposing an optimization technique that drastically speeds up the mutant execution process. Central to this approach is the Featured Mutant Model, a modelling framework for mutation analysis inspired by the software product line paradigm. It uses behavioural variability models, viz., Featured Transition Systems, which enable the optimized generation, configuration and execution of mutants. We provide results, based on models with thousands of transitions, suggesting that our technique is fast and scalable. We found that it outperforms previous approaches by several orders of magnitude and that it makes higher-order mutation practically applicable.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {655–666},
numpages = {12},
keywords = {variability, featured transition systems, mutation analysis},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2644866.2644885,
author = {Bosch, Mart\'{\i} and Genev\`{e}s, Pierre and Laya\"{\i}ida, Nabil},
title = {Automated Refactoring for Size Reduction of CSS Style Sheets},
year = {2014},
isbn = {9781450329491},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2644866.2644885},
doi = {10.1145/2644866.2644885},
abstract = {Cascading Style Sheets (CSS) is a standard language for stylizing and formatting web documents. Its role in web user experience becomes increasingly important. However, CSS files tend to be designed from a result-driven point of view, without much attention devoted to the CSS file structure as long as it produces the desired results. Furthermore, the rendering intended in the browser is often checked and debugged with a document instance. Style sheets normally apply to a set of documents, therefore modifications added while focusing on a particular instance might affect other documents of the set.We present a first prototype of static CSS semantical analyzer and optimizer that is capable of automatically detecting and removing redundant property declarations and rules. We build on earlier work on tree logics to locate redundancies due to the semantics of selectors and properties. Existing purely syntactic CSS optimizers might be used in conjunction with our tool, for performing complementary (and orthogonal) size reduction, toward the common goal of providing smaller and cleaner CSS files.},
booktitle = {Proceedings of the 2014 ACM Symposium on Document Engineering},
pages = {13–16},
numpages = {4},
keywords = {css, web development, style sheets, debugging},
location = {Fort Collins, Colorado, USA},
series = {DocEng '14}
}

@inproceedings{10.5555/3103196.3103200,
author = {Yun, Wonkyung and Shin, Donghwan and Bae, Doo-Hwan},
title = {Mutation Analysis for System of Systems Policy Testing},
year = {2017},
isbn = {9781538627990},
publisher = {IEEE Press},
abstract = {A System of Systems (SoS) is a set of the constituent systems (CS) which has managerial and operational independence. To address an SoS-level goal that cannot be satisfied by each CS, an SoS policy guides or forces the CSs to collaborate with each other. If there is a fault in the SoS policy, SoS may fail to reach its goal, even if there is no fault in the CSs. Such a call for SoS policy testing leads to an essential question---how can testers evaluate the effectiveness of test cases?In this paper, we suggest a mutation analysis approach for SoS policy testing. Mutation analysis is a systematic way of evaluating test cases using artificial faults called mutants. As a general mutation framework for SoS policy testing, we present an overview of mutation analysis in SoS policy testing as well as the key aspects that must be defined in practice. To demonstrate the applicability of the proposed approach, we provide a case study using a traffic management SoS with the Simulation of Urban Mobility (SUMO) simulator. The results show that the mutation analysis is effective at evaluating fault detection effectiveness of test cases for SoS policies at a reasonable cost.},
booktitle = {Proceedings of the Joint 5th International Workshop on Software Engineering for Systems-of-Systems and 11th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems},
pages = {16–22},
numpages = {7},
keywords = {system of systems testing, simulation of urban mobility (SUMO), system of systems, system of systems policy, mutation analysis},
location = {Buenos Aires, Argentina},
series = {JSOS '17}
}

@inproceedings{10.1145/3135932.3135945,
author = {Lehmann, Daniel},
title = {Automatic Testing of Interactive JavaScript Debuggers},
year = {2017},
isbn = {9781450355148},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3135932.3135945},
doi = {10.1145/3135932.3135945},
abstract = { When debugging programs, we often assume the debugger itself is correct. However, when it is not, it becomes hard to find bugs or lets developers search for bugs that are not even present. We thus propose a new approach to automatic testing of debuggers, inspired by differential testing of compilers. Our approach generates debugger actions to exercise the debugger and records a trace during the debugging session. By comparing traces of different debuggers against each other, we find deviating behavior and bugs. We evaluate our approach on the JavaScript debuggers of Firefox and Chromium and find 16 previously unreported bugs, four of which are already confirmed and fixed. },
booktitle = {Proceedings Companion of the 2017 ACM SIGPLAN International Conference on Systems, Programming, Languages, and Applications: Software for Humanity},
pages = {24–26},
numpages = {3},
keywords = {Firefox, automatic testing, Interactive debuggers, differential testing, Chromium},
location = {Vancouver, BC, Canada},
series = {SPLASH Companion 2017}
}

