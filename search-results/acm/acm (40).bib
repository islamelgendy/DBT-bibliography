@inproceedings{10.1145/2048066.2048082,
author = {Joshi, Pallavi and Gunawi, Haryadi S. and Sen, Koushik},
title = {PREFAIL: A Programmable Tool for Multiple-Failure Injection},
year = {2011},
isbn = {9781450309400},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2048066.2048082},
doi = {10.1145/2048066.2048082},
abstract = {As hardware failures are no longer rare in the era of cloud computing, cloud software systems must "prevail" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures. We integrate PreFail to three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X--200X less time than exhaustive testing.},
booktitle = {Proceedings of the 2011 ACM International Conference on Object Oriented Programming Systems Languages and Applications},
pages = {171–188},
numpages = {18},
keywords = {distributed systems, testing, fault injection},
location = {Portland, Oregon, USA},
series = {OOPSLA '11}
}

@article{10.1145/2076021.2048082,
author = {Joshi, Pallavi and Gunawi, Haryadi S. and Sen, Koushik},
title = {PREFAIL: A Programmable Tool for Multiple-Failure Injection},
year = {2011},
issue_date = {October 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2076021.2048082},
doi = {10.1145/2076021.2048082},
abstract = {As hardware failures are no longer rare in the era of cloud computing, cloud software systems must "prevail" against multiple, diverse failures that are likely to occur. Testing software against multiple failures poses the problem of combinatorial explosion of multiple failures. To address this problem, we present PreFail, a programmable failure-injection tool that enables testers to write a wide range of policies to prune down the large space of multiple failures. We integrate PreFail to three cloud software systems (HDFS, Cassandra, and ZooKeeper), show a wide variety of useful pruning policies that we can write for them, and evaluate the speed-ups in testing time that we obtain by using the policies. In our experiments, our testing approach with appropriate policies found all the bugs that one can find using exhaustive testing while spending 10X--200X less time than exhaustive testing.},
journal = {SIGPLAN Not.},
month = {oct},
pages = {171–188},
numpages = {18},
keywords = {fault injection, distributed systems, testing}
}

@inproceedings{10.1145/3180155.3180174,
author = {Lin, Jun-Wei and Jabbarvand, Reyhaneh and Garcia, Joshua and Malek, Sam},
title = {Nemo: Multi-Criteria Test-Suite Minimization with Integer Nonlinear Programming},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180174},
doi = {10.1145/3180155.3180174},
abstract = {Multi-criteria test-suite minimization aims to remove redundant test cases from a test suite based on some criteria such as code coverage, while trying to optimally maintain the capability of the reduced suite based on other criteria such as fault-detection effectiveness. Existing techniques addressing this problem with integer linear programming claim to produce optimal solutions. However, the multi-criteria test-suite minimization problem is inherently nonlinear, due to the fact that test cases are often dependent on each other in terms of test-case criteria. In this paper, we propose a framework that formulates the multi-criteria test-suite minimization problem as an integer nonlinear programming problem. To solve this problem optimally, we programmatically transform this nonlinear problem into a linear one and then solve the problem using modern linear solvers. We have implemented our framework as a tool, called Nemo, that supports a number of modern linear and nonlinear solvers. We have evaluated Nemo with a publicly available dataset and minimization problems involving multiple criteria including statement coverage, fault-revealing capability, and test execution time. The experimental results show that Nemo can be used to efficiently find an optimal solution for multi-criteria test-suite minimization problems with modern solvers, and the optimal solutions outperform the suboptimal ones by up to 164.29% in terms of the criteria considered in the problem.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {1039–1049},
numpages = {11},
keywords = {integer programming, test-suite minimization},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3242744.3242747,
author = {Mista, Agust\'{\i}n and Russo, Alejandro and Hughes, John},
title = {Branching Processes for QuickCheck Generators},
year = {2018},
isbn = {9781450358354},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242744.3242747},
doi = {10.1145/3242744.3242747},
abstract = {In QuickCheck (or, more generally, random testing), it is challenging to control random data generators' distributions---specially when it comes to user-defined algebraic data types (ADT). In this paper, we adapt results from an area of mathematics known as branching processes, and show how they help to analytically predict (at compile-time) the expected number of generated constructors, even in the presence of mutually recursive or composite ADTs. Using our probabilistic formulas, we design heuristics capable of automatically adjusting probabilities in order to synthesize generators which distributions are aligned with users' demands. We provide a Haskell implementation of our mechanism in a tool called DRaGeN and perform case studies with real-world applications. When generating random values, our synthesized QuickCheck generators show improvements in code coverage when compared with those automatically derived by state-of-the-art tools.},
booktitle = {Proceedings of the 11th ACM SIGPLAN International Symposium on Haskell},
pages = {1–13},
numpages = {13},
keywords = {Branching process, Haskell, Testing, QuickCheck},
location = {St. Louis, MO, USA},
series = {Haskell 2018}
}

@article{10.1145/3299711.3242747,
author = {Mista, Agust\'{\i}n and Russo, Alejandro and Hughes, John},
title = {Branching Processes for QuickCheck Generators},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {7},
issn = {0362-1340},
url = {https://doi.org/10.1145/3299711.3242747},
doi = {10.1145/3299711.3242747},
abstract = {In QuickCheck (or, more generally, random testing), it is challenging to control random data generators' distributions---specially when it comes to user-defined algebraic data types (ADT). In this paper, we adapt results from an area of mathematics known as branching processes, and show how they help to analytically predict (at compile-time) the expected number of generated constructors, even in the presence of mutually recursive or composite ADTs. Using our probabilistic formulas, we design heuristics capable of automatically adjusting probabilities in order to synthesize generators which distributions are aligned with users' demands. We provide a Haskell implementation of our mechanism in a tool called DRaGeN and perform case studies with real-world applications. When generating random values, our synthesized QuickCheck generators show improvements in code coverage when compared with those automatically derived by state-of-the-art tools.},
journal = {SIGPLAN Not.},
month = {sep},
pages = {1–13},
numpages = {13},
keywords = {Branching process, Haskell, QuickCheck, Testing}
}

@inproceedings{10.1145/3236024.3275439,
author = {Tomassi, David A.},
title = {Bugs in the Wild: Examining the Effectiveness of Static Analyzers at Finding Real-World Bugs},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275439},
doi = {10.1145/3236024.3275439},
abstract = {Static analysis is a powerful technique to find software bugs. In past years, a few static analysis tools have become available for developers to find certain kinds of bugs in their programs. However, there is no evidence on how effective the tools are in finding bugs in real-world software. In this paper, we present a preliminary study on the popular static analyzers ErrorProne and SpotBugs. Specifically, we consider 320 real Java bugs from the BugSwarm dataset, and determine which of these bugs can potentially be found by the analyzers, and how many are indeed detected. We find that 30.3% and 40.3% of the bugs are candidates for detection by ErrorProne and SpotBugs, respectively. Our evaluation shows that the analyzers are relatively easy to incorporate into the tool chain of diverse projects that use the Maven build system. However, the analyzers are not as effective detecting the bugs under study, with only one bug successfully detected by SpotBugs.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {980–982},
numpages = {3},
keywords = {static analysis, BugSwarm, bug finding tools},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/1185448.1185480,
author = {Vieira, Francisca Emanuelle and Martins, Francisco and Silva, Rafael and Menezes, Ronaldo and Braga, M\'{a}rcio},
title = {Using Genetic Algorithms to Generate Test Plans for Functionality Testing},
year = {2006},
isbn = {1595933158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1185448.1185480},
doi = {10.1145/1185448.1185480},
abstract = {Like in other fields, computer products (applications, hardware, etc.), before being marketed, require some level of testing to verify whether they meet their design and functional specifications -- called functionality test. The general process of performing functionality test consists in the production of a test plan that is then executed by humans or by automated software tools. The main difficulty in this entire process is the definition of such test plan. How can we know what a good sequence (test plan) is? The rule of thumb is to trust on people who understand the workings of the application being tested and who can decide what should be tested. The danger is that experts, due to their over-confidence on their knowledge, may become blind to issues that should otherwise be easy to see. This paper describes a technique based on genetic algorithms that is able to generate good test plans in an unbiased way and with minimum expert interference.},
booktitle = {Proceedings of the 44th Annual Southeast Regional Conference},
pages = {140–145},
numpages = {6},
location = {Melbourne, Florida},
series = {ACM-SE 44}
}

@inproceedings{10.1145/2635868.2635889,
author = {Dinges, Peter and Agha, Gul},
title = {Solving Complex Path Conditions through Heuristic Search on Induced Polytopes},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635889},
doi = {10.1145/2635868.2635889},
abstract = { Test input generators using symbolic and concolic execution must solve path conditions to systematically explore a program and generate high coverage tests. However, path conditions may contain complicated arithmetic constraints that are infeasible to solve: a solver may be unavailable, solving may be computationally intractable, or the constraints may be undecidable. Existing test generators either simplify such constraints with concrete values to make them decidable, or rely on strong but incomplete constraint solvers. Unfortunately, simplification yields coarse approximations whose solutions rarely satisfy the original constraint. Moreover, constraint solvers cannot handle calls to native library methods. We show how a simple combination of linear constraint solving and heuristic search can overcome these limitations. We call this technique Concolic Walk. On a corpus of 11 programs, an instance of our Concolic Walk algorithm using tabu search generates tests with two- to three-times higher coverage than simplification-based tools while being up to five-times as efficient. Furthermore, our algorithm improves the coverage of two state-of-the-art test generators by 21% and 32%. Other concolic and symbolic testing tools could integrate our algorithm to solve complex path conditions without having to sacrifice any of their own capabilities, leading to higher overall coverage. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {425–436},
numpages = {12},
keywords = {Concolic Testing, Local Search, Non-Linear Constraints},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/3330089.3330121,
author = {Mateen, Ahmed and Zhu, Qingsheng and Afsar, Salman},
title = {Comparitive Analysis of Manual vs Automotive Testing for Software Quality},
year = {2018},
isbn = {9781450361019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3330089.3330121},
doi = {10.1145/3330089.3330121},
abstract = {Success and failure of Software depend upon the quality of a software selection of an appropriate model for development of the product. In previous research many techniques were used to check the quality of software. But it is still a challenge for developers to select which technique may be best suited for quality of software. Quality attribute requirements such as those for performance, security, modifiability, reliability, and usability have a considerable influence on the software architecture of a system. Architects need to understand their designs in terms of quality attributes. Software quality assurance (SQA) consists of a means of monitoring the software engineering processes and methods used to ensure quality. The methods by which this is accomplished are varied and may include ensuring conformance to one or more standards. Both manual and automated testing offer benefits and disadvantages. In manual testing (as the name suggests), test cases are executed manually (by a human, that is) without any support from tools or scripts. But with automated testing, test cases are executed with the assistance of tools, scripts, and software.},
booktitle = {Proceedings of the 7th International Conference on Software Engineering and New Technologies},
articleno = {21},
numpages = {7},
keywords = {Network Retrieval, Radio Access Network, Cloud Computing, 5 G Network, Fog Computing},
location = {Hammamet, Tunisia},
series = {ICSENT 2018}
}

@inproceedings{10.1145/1068009.1068187,
author = {Wappler, Stefan and Lammermann, Frank},
title = {Using Evolutionary Algorithms for the Unit Testing of Object-Oriented Software},
year = {2005},
isbn = {1595930108},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1068009.1068187},
doi = {10.1145/1068009.1068187},
abstract = {As the paradigm of object orientation becomes more and more important for modern IT development projects, the demand for an automated test case generation to dynamically test object-oriented software increases. While search-based test case generation strategies, such as evolutionary testing, are well researched for procedural software, relatively little research has been done in the area of evolutionary object-oriented software testing.This paper presents an approach with which to apply evolutionary algorithms for the automatic generation of test cases for the white-box testing of object-oriented software. Test cases for testing object-oriented software include test programs which create and manipulate objects in order to achieve a certain test goal. Strategies for the encoding of test cases to evolvable data structures as well as ideas about how the objective functions could allow for a sophisticated evaluation are proposed. It is expected that the ideas herein can be adapted for other unit testing methods as well.The approach has been implemented by a prototype for empirical validation. In experiments with this prototype, evolutionary testing outperformed random testing. Evolutionary algorithms could be successfully applied for the white-box testing of object-oriented software.},
booktitle = {Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation},
pages = {1053–1060},
numpages = {8},
keywords = {object-oriented testing, automated test case generation, multi-level optimization, evolutionary testing, chaining approach},
location = {Washington DC, USA},
series = {GECCO '05}
}

@inproceedings{10.1145/3127005.3127006,
author = {Noor, Tanzeem Bin and Hemmati, Hadi},
title = {Studying Test Case Failure Prediction for Test Case Prioritization},
year = {2017},
isbn = {9781450353052},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3127005.3127006},
doi = {10.1145/3127005.3127006},
abstract = {Background: Test case prioritization refers to the process of ranking test cases within a test suite for execution. The goal is ranking fault revealing test cases higher so that in case of limited budget one only executes the top ranked tests and still detects as many bugs as possible. Since the actual fault detection ability of test cases is unknown before execution, heuristics such as "code coverage" of the test cases are used for ranking test cases. Other test quality metrics such as "coverage of the changed parts of the code" and "number of fails in the past"' have also been studied in the literature. Aims: In this paper, we propose using a logistic regression model to predict the failing test cases in the current release based on a set of test quality metrics. Method: We have studied the effect of including our newly proposed quality metric ("similarity-based" metric) into this model for tests prioritization. Results: The results of our experiments on five open source systems show that none of the individual quality metrics of our study outperforms the others in all the projects. Conclusions: However, the ranks given by the regression model are more consistent in prioritizing fault revealing test cases in the current release.},
booktitle = {Proceedings of the 13th International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {2–11},
numpages = {10},
keywords = {Test Case Prioritization, Regression Model, Test Case Quality Metrics},
location = {Toronto, Canada},
series = {PROMISE}
}

@inproceedings{10.1145/3382494.3410694,
author = {Huang, Yuekai and Wang, Junjie and Wang, Song and Liu, Zhe and Hu, Yuanzhe and Wang, Qing},
title = {Quest for the Golden Approach: An Experimental Evaluation of Duplicate Crowdtesting Reports Detection},
year = {2020},
isbn = {9781450375801},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382494.3410694},
doi = {10.1145/3382494.3410694},
abstract = {Background: Given the invisibility and unpredictability of distributed crowdtesting processes, there is a large number of duplicate reports, and detecting these duplicate reports is an important task to help save testing effort. Although, many approaches have been proposed to automatically detect the duplicates, the comparison among them and the practical guidelines to adopt these approaches in crowdtesting remain vague.Aims: We aim at conducting the first experimental evaluation of the commonly-used and state-of-the-art approaches for duplicate detection in crowdtesting reports, and exploring which is the golden approach.Method: We begin with a systematic review of approaches for duplicate detection, and select ten state-of-the-art approaches for our experimental evaluation. We conduct duplicate detection with each approach on 414 crowdtesting projects with 59,289 reports collected from one of the largest crowdtesting platforms.Results: Machine learning based approach, i.e., ML-REP, and deep learning based approach, i.e., DL-BiMPM, are the best two approaches for duplicate reports detection in crowdtesting, while the later one is more sensitive to the size of training data and more time-consuming for model training and prediction.Conclusions: This paper provides new insights and guidelines to select appropriate duplicate detection techniques for duplicate crowdtesting reports detection.},
booktitle = {Proceedings of the 14th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {17},
numpages = {12},
keywords = {duplicate detection, machine learning, deep learning, Crowdtesting, information retrieval},
location = {Bari, Italy},
series = {ESEM '20}
}

@inproceedings{10.5555/1874620.1874995,
author = {Costa, Jos\'{e} C. and Monteiro, Jos\'{e} C.},
title = {A MILP-Based Approach to Path Sensitization of Embedded Software},
year = {2009},
isbn = {9783981080155},
publisher = {European Design and Automation Association},
address = {Leuven, BEL},
abstract = {We propose a new methodology based on Mixed Integer Linear Programming (milp) for determining the input values that will exercise a specified execution path in a program. In order to seamlessly handle variable values, pointers and arrays, and variable aliasing, our method uses memory addresses for data references. This implies a dynamic methodology where all decisions are taken as the program executes. During execution, we gather constraints for the milp problem, whose solution will directly yield the input values for the desired path. We present results that demonstrate the effectiveness of this approach. This methodology was implemented into a fully functional tool that is capable of handling medium sized real programs specified in the c language. Our work is motivated by the complexity of validating embedded systems and uses a similar approach to an existing Hdl functional vector generation. The joint solution of the milp problems will provide a hardware/software co-validation tool.},
booktitle = {Proceedings of the Conference on Design, Automation and Test in Europe},
pages = {1568–1571},
numpages = {4},
location = {Nice, France},
series = {DATE '09}
}

@inproceedings{10.5555/2663370.2663381,
author = {de Oliveira, Diego Caminha B. and Rakamari\'{c}, Zvonimir and Gopalakrishnan, Ganesh and Humphrey, Alan and Meng, Qingyu and Berzins, Martin},
title = {Practical Formal Correctness Checking of Million-Core Problem Solving Environments for HPC},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {While formal correctness checking methods have been deployed at scale in a number of important practical domains, we believe that such an experiment has yet to occur in the domain of high performance computing at the scale of a million CPU cores. This paper presents preliminary results from the Uintah Runtime Verification (URV) project that has been launched with this objective. Uintah is an asynchronous task-graph based problem-solving environment that has shown promising results on problems as diverse as fluid-structure interaction and turbulent combustion at well over 200K cores to date. Uintah has been tested on leading platforms such as Kraken, Keenland, and Titan consisting of multicore CPUs and GPUs, incorporates several innovative design features, and is following a roadmap for development well into the million core regime. The main results from the URV project to date are crystallized in two observations: (1) A diverse array of well-known ideas from light-weight formal methods and testing/observing HPC systems at scale have an excellent chance of succeeding. The real challenges are in finding out exactly which combinations of ideas to deploy, and where. (2) Large-scale problem solving environments for HPC must be designed such that they can be "crashed early" (at smaller scales of deployment) and "crashed often" (have effective ways of input generation and schedule perturbation that cause vulnerabilities to be attacked with higher probability). Furthermore, following each crash, one must "explain well" (given the extremely obscure ways in which an error finally manifests itself, we must develop ways to record information leading up to the crash in informative ways, to minimize off-site debugging burden). Our plans to achieve these goals and to measure our success are described. We also highlight some of the broadly applicable concepts and approaches.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {75–83},
numpages = {9},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@inproceedings{10.1145/2950290.2950322,
author = {Kurtz, Bob and Ammann, Paul and Offutt, Jeff and Delamaro, M\'{a}rcio E. and Kurtz, Mariet and G\"{o}k\c{c}e, Nida},
title = {Analyzing the Validity of Selective Mutation with Dominator Mutants},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950322},
doi = {10.1145/2950290.2950322},
abstract = {Various forms of selective mutation testing have long been accepted as valid approximations to full mutation testing. This paper presents counterevidence to traditional selective mutation. The recent development of dominator mutants and minimal mutation analysis lets us analyze selective mutation without the noise introduced by the redundancy inherent in traditional mutation. We then exhaustively evaluate all small sets of mutation operators for the Proteum mutation system and determine dominator mutation scores and required work for each of these sets on an empirical test bed. The results show that all possible selective mutation approaches have poor dominator mutation scores on at least some of these programs. This suggests that to achieve high performance with respect to full mutation analysis, selective approaches will have to become more sophisticated, possibly by choosing mutants based on the specifics of the artifact under test, that is, specialized selective mutation.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {571–582},
numpages = {12},
keywords = {Mutation analysis, subsumption, dominator mutants},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inbook{10.1145/3368826.3377910,
author = {Verma, Aakanksha and Kalita, Pankaj Kumar and Pandey, Awanish and Roy, Subhajit},
title = {Interactive Debugging of Concurrent Programs under Relaxed Memory Models},
year = {2020},
isbn = {9781450370479},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368826.3377910},
abstract = {Programming environments for sequential programs provide strong debugging support. However, concurrent programs, especially under relaxed memory models, lack powerful interactive debugging tools. In this work, we present Gambit, an interactive debugging environment that uses gdb to run a concrete debugging session on a concurrent program, while employing a symbolic execution on the program trace in the background simultaneously. The symbolic execution is analysed by a theorem prover to answer queries from the programmer on possible scenarios resulting from alternate thread interleavings or due to reorderings on other relaxed memory models. Gambit can help programmers understand complex bug scenarios on alien environments, that is, when the program is deployed in the field under different concurrent schedulers and diverse memory models. While Gambit currently packages three memory models (PSO, TSO and SC) and provides support for constructs like fences and atomic blocks, the framework is extensible, allowing support for other memory models and programming constructs. We have evaluated Gambit on multiple programs and found that Gambit responds to the debug queries quite fast (about a second on an average across our benchmark programs).},
booktitle = {Proceedings of the 18th ACM/IEEE International Symposium on Code Generation and Optimization},
pages = {68–80},
numpages = {13}
}

@article{10.1145/636517.636525,
author = {Wiger, Ulf and Ask, G\"{o}sta and Boortz, Kent},
title = {World-Class Product Certification Using Erlang},
year = {2002},
issue_date = {December 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {12},
issn = {0362-1340},
url = {https://doi.org/10.1145/636517.636525},
doi = {10.1145/636517.636525},
abstract = {It is now ten years ago since the decision was made to apply the functional programming language Erlang to real production projects at Ericsson. In late 1995, development on the Open Telecom Platform (OTP) started, and in mid 1996 the AXD 301 project became the first user of OTP. The AXD 301 Multi-service Switch was released in October 1998, and later became "the heart of ENGINE", Ericsson's leading Voice over Packet solution.In those early days of Erlang programming, high-level tools for development and testing were not really available, and programmers used mainly the Emacs editor and the Erlang shell. Still, anecdotal evidence suggested a 4-10x productivity increase compared to mainstream programming techniques.Through the years, significant progress has been made, especially in the area of automated testing of Erlang programs. The OTP team designed an Erlang-based test suite execution environment, were developers can easily write their own automated test suites, and now performs nightly builds where more than one thousand test cases are executed on ten different platforms. OTP designers can view the outcome in web-based test reports as they come to work the next day. Each corrected bug results in a new test case that is incorporated into the ever-growing test suite. Thus, this world-class middleware is certified to telecom-class quality without a dedicated test team!The AXD 301 project uses OTP's test environment, and executes more than 10,000 automated test cases before each major release. Designers and testers compose their own test suites, and the designers carry out function tests with little or no help from the Integration and Certification team. Each test case can be run both in a simulated environment on the designer's workstation and in the test lab on real hardware. In order to provide stimuli to the system, the testers often design their own traffic generators in Erlang.To analyze the faults that occur, Erlang offers an increasing wealth of debugging options. Beyond the symbolic error messages, which are often sufficient to locate the fault, Erlang developers are able to dynamically turn on tracing on message passing, scheduling events, garbage collections, selected function calls, etc.This paper demonstrates how Erlang's declarative syntax and pattern matching provide an outstanding environment for test suite development.},
journal = {SIGPLAN Not.},
month = {dec},
pages = {25–34},
numpages = {10},
keywords = {testing, Erlang}
}

@article{10.1145/357139.357140,
author = {Gannon, John and McMullin, Paul and Hamlet, Richard},
title = {Data Abstraction, Implementation, Specification, and Testing},
year = {1981},
issue_date = {July 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/357139.357140},
doi = {10.1145/357139.357140},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
pages = {211–223},
numpages = {13}
}

@inproceedings{10.1145/3287098.3287112,
author = {Suri, Venkata Ratnadeep and Rangaswamy, Nimmy and Joshi, Tanmay and Joshi, Meghna and Nanavati, Sneha},
title = {Tool Smiths in Off-Shored Work: Socio-Technical System of Quality Testing in India},
year = {2019},
isbn = {9781450361224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287098.3287112},
doi = {10.1145/3287098.3287112},
abstract = {Today, the consequence of viewing work automation as a face-off between human potential and smart technology denies the role of business acumen, tacit knowledge, market forces and social contexts that shape work environments around the world. Using a socio-technical framework, we examine the consequences arising out of the introduction of automation in the quality testing segment of the IT industry in India. We highlight key socio-economic parameters influencing the decisions to automate a testing environment. Next, by applying an ICTD lens, we analyze the ensuing discourse emerging from the voices of Quality Testers imbuing value into the testing job profile and juxtapose these discourses with current QT work practices. Finally, we highlight the importance of creating better QT work practices in tandem with training strategies that allow IT professionals to draw upon their implicit knowledge, critical thinking, computing skills, and business process knowledge, and combine it with automated QT testing procedures to add more value to QT profession.},
booktitle = {Proceedings of the Tenth International Conference on Information and Communication Technologies and Development},
articleno = {11},
numpages = {10},
keywords = {IT industry, ethnography, ICTD, India, quality testing, automation},
location = {Ahmedabad, India},
series = {ICTD '19}
}

@inproceedings{10.1145/68210.69228,
author = {Forin, Alessandro},
title = {Debugging of Heterogeneous Parallel Systems},
year = {1988},
isbn = {0897912969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/68210.69228},
doi = {10.1145/68210.69228},
abstract = {The Agora system supports the development of heterogeneous parallel programs, e.g. programs written in multiple languages and running on heterogeneous machines. Agora has been used since September 1986 in a large distributed system [1]: Two versions of the application have been demonstrated in one year, contrary to the expectation of two years per one version. The simplicity in debugging is one of the reasons of the productivity speedup gained. This simplicity is due both to the deeper understanding that the debugger has of parallel systems, and to a novel feature: the ability to replay the execution of parallel systems built with Agora. A user is able to exactly repeat for any number of times and at a slower pace an execution that failed. This makes it easy to identify time-dependent errors, which are peculiar to parallel and distributed systems. The debugger can also be customized to support user defined synchronization primitives, which are built on top of the system provided ones. The Agora debugger tackles three set of problems that no parallel debugger in the past has simultaneously addressed: dealing with programming-in-the-large, multiple processes in different languages, and multiple machine architectures.},
booktitle = {Proceedings of the 1988 ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging},
pages = {130–140},
numpages = {11},
location = {Madison, Wisconsin, USA},
series = {PADD '88}
}

@article{10.1145/69215.69228,
author = {Forin, Alessandro},
title = {Debugging of Heterogeneous Parallel Systems},
year = {1988},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/69215.69228},
doi = {10.1145/69215.69228},
abstract = {The Agora system supports the development of heterogeneous parallel programs, e.g. programs written in multiple languages and running on heterogeneous machines. Agora has been used since September 1986 in a large distributed system [1]: Two versions of the application have been demonstrated in one year, contrary to the expectation of two years per one version. The simplicity in debugging is one of the reasons of the productivity speedup gained. This simplicity is due both to the deeper understanding that the debugger has of parallel systems, and to a novel feature: the ability to replay the execution of parallel systems built with Agora. A user is able to exactly repeat for any number of times and at a slower pace an execution that failed. This makes it easy to identify time-dependent errors, which are peculiar to parallel and distributed systems. The debugger can also be customized to support user defined synchronization primitives, which are built on top of the system provided ones. The Agora debugger tackles three set of problems that no parallel debugger in the past has simultaneously addressed: dealing with programming-in-the-large, multiple processes in different languages, and multiple machine architectures.},
journal = {SIGPLAN Not.},
month = {nov},
pages = {130–140},
numpages = {11}
}

@inbook{10.1145/3238147.3238180,
author = {Hu, Jiajun and Wei, Lili and Liu, Yepang and Cheung, Shing-Chi and Huang, Huaxun},
title = {A Tale of Two Cities: How WebView Induces Bugs to Android Applications},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238180},
abstract = {WebView is a widely used Android component that augments a native app with web browser capabilities. It eases the interactions between an app’s native code and web code. However, the interaction mechanism of WebView induces new types of bugs in Android apps. Understanding the characteristics and manifestation of these WebView-induced bugs (ωBugs for short) facilitates the correct usages of WebViews in Android apps. This motivates us to conduct the first empirical study on ωBugs based on those found in popular open-source Android apps. Our study identified the major root causes and consequences of ωBugs and made interesting observations that can be leveraged for detecting and diagnosing ωBugs. Based on the empirical study, we further propose an automated testing technique ωDroid to effectively expose ωBugs in Android apps. In our experiments, ωDroid successfully discovered 30 unique and previously-unknown ωBugs when applied to 146 open-source Android apps. We reported the 30 ωBugs to the corresponding app developers. Out of these 30 ωBugs, 14 were confirmed and 7 of them were fixed. This shows that ωDroid can effectively detect ωBugs that are of the developers’ concern.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {702–713},
numpages = {12}
}

@inproceedings{10.1109/ASE.2015.13,
author = {Di Nardo, Daniel and Pastore, Fabrizio and Arcuri, Andrea and Briand, Lionel},
title = {Evolutionary Robustness Testing of Data Processing Systems Using Models and Data Mutation},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.13},
doi = {10.1109/ASE.2015.13},
abstract = {System level testing of industrial data processing software poses several challenges. Input data can be very large, even in the order of gigabytes, and with complex constraints that define when an input is valid. Generating the right input data to stress the system for robustness properties (e.g. to test how faulty data is handled) is hence very complex, tedious and error prone when done manually. Unfortunately, this is the current practice in industry. In previous work, we defined a methodology to model the structure and the constraints of input data by using UML class diagrams and OCL constraints. Tests were automatically derived to cover predefined fault types in a fault model. In this paper, to obtain more effective system level test cases, we developed a novel search-based test generation tool. Experiments on a real-world, large industrial data processing system show that our automated approach can not only achieve better code coverage, but also accomplishes this using significantly smaller test suites.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {126–137},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

