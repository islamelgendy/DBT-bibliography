@inproceedings{10.1145/2594291.2594334,
author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
title = {Compiler Validation via Equivalence modulo Inputs},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594334},
doi = {10.1145/2594291.2594334},
abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {216–226},
numpages = {11},
keywords = {equivalent program variants, miscompilation, compiler testing, automated testing},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@article{10.1145/2666356.2594334,
author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
title = {Compiler Validation via Equivalence modulo Inputs},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594334},
doi = {10.1145/2666356.2594334},
abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {216–226},
numpages = {11},
keywords = {compiler testing, automated testing, miscompilation, equivalent program variants}
}

@inbook{10.1109/ICSE-SEET52601.2021.00026,
author = {Henley, Austin Z. and Ball, Julian and Klein, Benjamin and Rutter, Aiden and Lee, Dylan},
title = {An Inquisitive Code Editor for Addressing Novice Programmers' Misconceptions of Program Behavior},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00026},
abstract = {Novice programmers face numerous barriers while attempting to learn how to code that may deter them from pursuing a computer science degree or career in software development. In this work, we propose a tool concept to address the particularly challenging barrier of novice programmers holding misconceptions about how their code behaves. Specifically, the concept involves an inquisitive code editor that: (1) identifies misconceptions by periodically prompting the novice programmer with questions about their program's behavior, (2) corrects the misconceptions by generating explanations based on the program's actual behavior, and (3) prevents further misconceptions by inserting test code and utilizing other educational resources. We have implemented portions of the concept as plugins for the Atom code editor and conducted informal surveys with students and instructors. Next steps include deploying the tool prototype to students enrolled in introductory programming courses.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {165–170},
numpages = {6}
}

@inproceedings{10.1109/ICSE.2009.5070516,
author = {Buse, Raymond P. L. and Weimer, Westley},
title = {The Road Not Taken: Estimating Path Execution Frequency Statically},
year = {2009},
isbn = {9781424434534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2009.5070516},
doi = {10.1109/ICSE.2009.5070516},
abstract = {A variety of compilers, static analyses, and testing frameworks rely heavily on path frequency information. Uses for such information range from optimizing transformations to bug finding. Path frequencies are typically obtained through profiling, but that approach is severely restricted: it requires running programs in an indicative environment, and on indicative test inputs. We present a descriptive statistical model of path frequency based on features that can be readily obtained from a program's source code. Our model is over 90% accurate with respect to several benchmarks, and is sufficient for selecting the 5% of paths that account for over half of a program's total runtime. We demonstrate our technique's robustness by measuring its performance as a static branch predictor, finding it to be more accurate than previous approaches on average. Finally, our qualitative analysis of the model provides insight into which source-level features indicate “hot paths.”},
booktitle = {Proceedings of the 31st International Conference on Software Engineering},
pages = {144–154},
numpages = {11},
series = {ICSE '09}
}

@inproceedings{10.1145/3324884.3416590,
author = {Wang, Shangwen and Wen, Ming and Lin, Bo and Wu, Hongjun and Qin, Yihao and Zou, Deqing and Mao, Xiaoguang and Jin, Hai},
title = {Automated Patch Correctness Assessment: How Far Are We?},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416590},
doi = {10.1145/3324884.3416590},
abstract = {Test-based automated program repair (APR) has attracted huge attention from both industry and academia. Despite the significant progress made in recent studies, the overfitting problem (i.e., the generated patch is plausible but overfitting) is still a major and long-standing challenge. Therefore, plenty of techniques have been proposed to assess the correctness of patches either in the patch generation phase or in the evaluation of APR techniques. However, the effectiveness of existing techniques has not been systematically compared and little is known to their advantages and disadvantages. To fill this gap, we performed a large-scale empirical study in this paper. Specifically, we systematically investigated the effectiveness of existing automated patch correctness assessment techniques, including both static and dynamic ones, based on 902 patches automatically generated by 21 APR tools from 4 different categories. Our empirical study revealed the following major findings: (1) static code features with respect to patch syntax and semantics are generally effective in differentiating overfitting patches over correct ones; (2) dynamic techniques can generally achieve high precision while heuristics based on static code features are more effective towards recall; (3) existing techniques are more effective towards certain projects and types of APR techniques while less effective to the others; (4) existing techniques are highly complementary to each other. For instance, a single technique can only detect at most 53.5% of the overfitting patches while 93.3% of them can be detected by at least one technique when the oracle information is available. Based on our findings, we designed an integration strategy to first integrate static code features via learning, and then combine with others by the majority voting strategy. Our experiments show that the strategy can enhance the performance of existing patch correctness assessment techniques significantly.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {968–980},
numpages = {13},
keywords = {program repair, patch correctness, empirical assessment},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3338502.3359760,
author = {Chen, Yongheng and Song, Linhai and Xing, Xinyu and Xu, Fengyuan and Wu, Wenfei},
title = {Automated Finite State Machine Extraction},
year = {2019},
isbn = {9781450368346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338502.3359760},
doi = {10.1145/3338502.3359760},
abstract = {Finite state machine (FSM) is a type of computation models widely used in various software programs. Extracting implemented FSMs has many important applications in the networking, software engineering and security domains. In this paper, we first conduct an empirical study to understand how FSMs are implemented in real-world software. Under the guidance of our study results, we then design a static analysis tool, FSMExtractor, to automatically identify and synthesize implemented FSMs. Evaluation using 160 software programs from three sources shows that FSMExtractor can extract all implemented FSMs and report very few false positives.},
booktitle = {Proceedings of the 3rd ACM Workshop on Forming an Ecosystem Around Software Transformation},
pages = {9–15},
numpages = {7},
keywords = {static analysis, finite state machine},
location = {London, United Kingdom},
series = {FEAST'19}
}

@inproceedings{10.1145/1085130.1085132,
author = {Lindig, Christian},
title = {Random Testing of C Calling Conventions},
year = {2005},
isbn = {1595930507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1085130.1085132},
doi = {10.1145/1085130.1085132},
abstract = {In a C compiler, function calls are difficult to implement correctly because they must respect a platform-specific calling convention. But they are governed by a simple invariant: parameters passed to a function must be received unaltered. A violation of this invariant signals an inconsistency in a compiler. We automatically test the consistency of C compilers using randomly generated programs. An inconsistency manifests itself as an assertion failure when compiling and running the generated code. The generation of programs is type-directed and can be controlled by the user with composable random generators in about 100 lines of Lua. Lua is a scripting language built into our testing tool that drives program generation. Random testing is fully automatic, requires no specification, yet is comparable in effectiveness with specification-based testing from prior work. Using this method, we uncovered 13 new bugs in mature open-source and commercial C compilers.},
booktitle = {Proceedings of the Sixth International Symposium on Automated Analysis-Driven Debugging},
pages = {3–12},
numpages = {10},
keywords = {calling convention, C, composition, compiler, random testing, consistency},
location = {Monterey, California, USA},
series = {AADEBUG'05}
}

@inproceedings{10.1145/2884781.2884796,
author = {Cheung, Shing-Chi and Chen, Wanjun and Liu, Yepang and Xu, Chang},
title = {CUSTODES: Automatic Spreadsheet Cell Clustering and Smell Detection Using Strong and Weak Features},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884796},
doi = {10.1145/2884781.2884796},
abstract = {Various techniques have been proposed to detect smells in spreadsheets, which are susceptible to errors. These techniques typically detect spreadsheet smells through a mechanism based on a fixed set of patterns or metric thresholds. Unlike conventional programs, tabulation styles vary greatly across spreadsheets. Smell detection based on fixed patterns or metric thresholds, which are insensitive to the varying tabulation styles, can miss many smells in one spreadsheet while reporting many spurious smells in another. In this paper, we propose CUSTODES to effectively cluster spreadsheet cells and detect smells in these clusters. The clustering mechanism can automatically adapt to the tabulation styles of each spreadsheet using strong and weak features. These strong and weak features capture the invariant and variant parts of tabulation styles, respectively. As smelly cells in a spreadsheet normally occur in minority, they can be mechanically detected as clusters' outliers in feature spaces. We implemented and applied CUSTODES to 70 spreadsheets files randomly sampled from the EUSES corpus. These spreadsheets contain 1,610 formula cell clusters. Experimental results confirmed that CUSTODES is effective. It successfully detected harmful smells that can induce computation anomalies in spreadsheets with an F-measure of 0.72, outperforming state-of-the-art techniques.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {464–475},
numpages = {12},
keywords = {feature modeling, end-user programming, spreadsheets, smell detection, cell clustering},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/1655925.1656177,
author = {Lee, Chongwon},
title = {Adapting and Adjusting Test Process Reflecting Characteristics of Embedded Software and Industrial Properties Based on Referential Models},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1656177},
doi = {10.1145/1655925.1656177},
abstract = {Test activities are very important for the product quality. Testability Maturity Model (TMM) and Test Process Improvement (TPI) are often applied to improve the test activities. However, in the domain of embedded software, there are many different concerns from that of other software domains to achieve the high quality. This paper deals with the characteristics of embedded software and industrial corporate challenges based on the referential models suggesting the TPI-EI (Test Process Improvement for Embedded software and Industrial characteristics) model. The TPI-EI model reflects the practical improvement strategies and focuses on the evaluation procedures with cost-effectiveness. In order to check if this model is valid, the result of applying this model is explained. The proposed model provides the foundation to find out the insufficient parts of test activities, ultimately improving the test capabilities upon the self-evaluation.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {1372–1377},
numpages = {6},
keywords = {test, TPI, embedded, process, adapting, TMM, industrial},
location = {Seoul, Korea},
series = {ICIS '09}
}

@inproceedings{10.1145/3267204.3267207,
author = {Hartung, Robert and Kulau, Ulf and Lichtblau, Niels and Wolf, Lars C.},
title = {A Flexible Software Framework for Real-World Experiments and Temperature-Controlled Testbeds},
year = {2018},
isbn = {9781450359306},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267204.3267207},
doi = {10.1145/3267204.3267207},
abstract = {Both reliability and robustness are required for applicationsin the field of wireless sensor networks and the internet ofthings. Testbeds are a usefool tool to verify both hardwareand software concepts under realistic conditions. However,existing testbeds often are fitted to specific use cases. Thispaper presents a software framework to build testbeds forvarious applications and scenarios. The module architectureof our framework approach is presented in detail. Communi-cation is based on MQTT which fulfills both the need to beflexible, but also to scale well for larger testbeds. Key require-ments are derived from our existing use cases, an outdoorscenario and an indoor, temperature-controlled testbed. Wepresent our temperature-controlled chambers in detail andcompare their performance to existing testbeds.},
booktitle = {Proceedings of the 12th International Workshop on Wireless Network Testbeds, Experimental Evaluation &amp; Characterization},
pages = {30–37},
numpages = {8},
keywords = {climate chamber, reliability, MQTT, real-world experiments, testbed, wireless sensor networks, temperature control, testbed framework},
location = {New Delhi, India},
series = {WiNTECH '18}
}

@inbook{10.1145/3468264.3468573,
author = {Mansur, Muhammad Numair and Christakis, Maria and W\"{u}stholz, Valentin},
title = {Metamorphic Testing of Datalog Engines},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468573},
abstract = {Datalog is a popular query language with applications in several domains. Like any complex piece of software, Datalog engines may contain bugs. The most critical ones manifest as incorrect results when evaluating queries—we refer to these as query bugs. Given the wide applicability of the language, query bugs may have detrimental consequences, for instance, by compromising the soundness of a program analysis that is implemented and formalized in Datalog. In this paper, we present the first metamorphic-testing approach for detecting query bugs in Datalog engines. We ran our tool on three mature engines and found 13 previously unknown query bugs, some of which are deep and revealed critical semantic issues.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {639–650},
numpages = {12}
}

@inproceedings{10.1145/3281411.3281436,
author = {Zheng, Peng and Benson, Theophilus and Hu, Chengchen},
title = {P4Visor: Lightweight Virtualization and Composition Primitives for Building and Testing Modular Programs},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281436},
doi = {10.1145/3281411.3281436},
abstract = {Programmable data planes, PDPs, enable an unprecedented level of flexibility and have emerged as a promising alternative to existing data planes. Despite the rapid development and prototyping cycles that PDPs promote, the existing PDP ecosystem lacks appropriate abstractions and algorithms to support these rapid testing and deployment life-cycles. In this paper, we propose P4Visor, a lightweight virtualization abstraction that provides testing primitives as a first-order citizen of the PDP ecosystem. P4Visor can efficiently support multiple PDP programs through a combination of compiler optimizations and program analysis-based algorithms. P4Visor s algorithm improves over state-of-the-art techniques by significantly reducing the resource overheads associated with embedding numerous versions of a PDP program into hardware. To demonstrate the efficiency and viability of P4Visor, we implemented and evaluated P4Visor on both a software switch and an FPGA-based hardware switch using fourteen different PDP programs. Our results demonstrate that P4Visor introduces minimal overheads (less than 1%) and is one order of magnitude more efficient than existing PDPs primitives for concurrently supporting multiple programs.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {98–111},
numpages = {14},
keywords = {code merge, testing, programmable data plane},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.1145/2384716.2384770,
author = {Sinha, Vibha Singhal and Mani, Senthil and Mukherjee, Debdoot},
title = {Is Text Search an Effective Approach for Fault Localization: A Practitioners Perspective},
year = {2012},
isbn = {9781450315630},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2384716.2384770},
doi = {10.1145/2384716.2384770},
abstract = {There has been widespread interest in both academia and industry around techniques to help in fault localization. Much of this work leverages static or dynamic code analysis and hence is constrained by the programming language used or presence of test cases. In order to provide more generically applicable techniques, recent work has focused on devising text search based approaches that recommend source files which a developer can modify to fix a bug. Text search may be used for fault localization in either of the following ways. We can search a repository of past bugs with the bug description to find similar bugs and recommend the source files that were modified to fix those bugs. Alternately, we can directly search the code repository to find source files that share words with the bug report text. Few interesting questions come to mind when we consider applying these text-based search techniques in real projects. For example, would searching on past fixed bugs yield better results than searching on code? What is the accuracy one can expect? Would giving preference to code words in the bug report better the search results? In this paper, we apply variants of text-search on four open source projects and compare the impact of different design considerations on search efficacy.},
booktitle = {Proceedings of the 3rd Annual Conference on Systems, Programming, and Applications: Software for Humanity},
pages = {159–158},
keywords = {bug-solving, empirical study},
location = {Tucson, Arizona, USA},
series = {SPLASH '12}
}

@inproceedings{10.1145/1830483.1830735,
author = {Conrad, Alexander P. and Roos, Robert S. and Kapfhammer, Gregory M.},
title = {Empirically Studying the Role of Selection Operators Duringsearch-Based Test Suite Prioritization},
year = {2010},
isbn = {9781450300728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1830483.1830735},
doi = {10.1145/1830483.1830735},
abstract = {Regression test suite prioritization techniques reorder test cases so that, on average, more faults will be revealed earlier in the test suite's execution than would otherwise be possible. This paper presents a genetic algorithm-based test prioritization method that employs a wide variety of mutation, crossover, selection, and transformation operators to reorder a test suite. Leveraging statistical analysis techniques, such as tree model construction through binary recursive partitioning and kernel density estimation, the paper's empirical results highlight the unique role that the selection operators play in identifying an effective ordering of a test suite. The study also reveals that, while truncation selection consistently outperformed the tournament and roulette operators in terms of test suite effectiveness, increasing selection pressure consistently produces the best results within each class of operator. After further explicating the relationship between selection intensity, termination condition, fitness landscape, and the quality of the resulting test suite, this paper demonstrates that the genetic algorithm-based prioritizer is superior to random search and hill climbing and thus suitable for many regression testing environments.},
booktitle = {Proceedings of the 12th Annual Conference on Genetic and Evolutionary Computation},
pages = {1373–1380},
numpages = {8},
keywords = {coverage testing, genetic algorithm, test prioritization},
location = {Portland, Oregon, USA},
series = {GECCO '10}
}

@inproceedings{10.1145/2676723.2677304,
author = {Lee, Keunhong and Kim, Joongi and Moon, Sue},
title = {An Educational Networking Framework for Full Layer Implementation and Testing},
year = {2015},
isbn = {9781450329668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676723.2677304},
doi = {10.1145/2676723.2677304},
abstract = {We present the KENSv2 (KAIST Educational Network System) framework for network protocol implementation. The framework is event-driven to guarantee deterministic behaviour and reproducibility, which in turn delivers ease of debugging and evaluation. Our framework consists of four components: the event generator, the virtual host, the TCP driver and the IP driver. The two drivers are what students have to implement, and we offer to the students the drivers in the binary format for paired testing and debugging. We have developed a test suite that covers three categories of test cases: specification, paired, and logic tests. The framework logs packet transmissions in the PCAP format to allow use of widely available packet analysis tools to inspect logical behaviour of student solutions, such as congestion control. We have designed five step-by-step assignments and evaluated student submissions. With our automated test suite, we have cut down the number of TAs by half for the doubled class size compared to the previous semester, in total 3 TAs with 49 students. We plan to continue using KENSv2 in our undergraduate networking course and expand the test suite.},
booktitle = {Proceedings of the 46th ACM Technical Symposium on Computer Science Education},
pages = {663–668},
numpages = {6},
keywords = {ip, full layer implementation, educational networking framework, tcp, network protocols, automated test suite},
location = {Kansas City, Missouri, USA},
series = {SIGCSE '15}
}

@inproceedings{10.1145/1837274.1837407,
author = {Constantinides, Kypros and Austin, Todd},
title = {Using Introspective Software-Based Testing for Post-Silicon Debug and Repair},
year = {2010},
isbn = {9781450300025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1837274.1837407},
doi = {10.1145/1837274.1837407},
abstract = {As silicon process technology scales deeper into the nanometer regime, hardware defects are becoming more common, to the point of threatening yield rates and product lifetimes. Introspective software mechanisms hold great promise to address these reliability challenges with both low cost and high coverage. To address these challenges, we have developed a novel instruction set enhancement, called Access-Control Extensions (ACE), that can access and control a microprocessor's internal state. Using ACE technology, special firmware can periodically probe the microprocessor during execution to locate run-time faults, repair design errors (even those discovered in the field), and streamline manufacturing tests.},
booktitle = {Proceedings of the 47th Design Automation Conference},
pages = {537–542},
numpages = {6},
location = {Anaheim, California},
series = {DAC '10}
}

@article{10.1145/333630.333634,
author = {Devanbu, Premkumar T.},
title = {Re-Targetability in Software Tools},
year = {1999},
issue_date = {Fall 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/333630.333634},
doi = {10.1145/333630.333634},
abstract = {Software tool construction is a risky business, with uncertain rewards. Many tools never get used. This is a truism: software tools, however brilliantly conceived, well-designed, and meticulously constructed, have little impact unless they are actually adopted by real programmers. While there are no sure-fire ways of ensuring that a tool will be used, experience indicates that retargetability is an important enabler for wide adoption. In this paper, we elaborate on the need for retargetability in software tools, describe some mechanisms that have proven useful in our experience, and outline our future research in the broader area of inter-operability and retargetability.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {sep},
pages = {19–26},
numpages = {8}
}

@inproceedings{10.1145/1294261.1294264,
author = {Kiciman, Emre and Livshits, Benjamin},
title = {AjaxScope: A Platform for Remotely Monitoring the Client-Side Behavior of Web 2.0 Applications},
year = {2007},
isbn = {9781595935915},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294261.1294264},
doi = {10.1145/1294261.1294264},
abstract = {The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While web applications have become larger and more complex, web application developers today have little visibility into the end-to-end behavior of their systems. This paper presents AjaxScope, a dynamic instrumentation platform that enables cross-user monitoring and just-in-time control of web application behavior on end-user desktops. AjaxScope is a proxy that performs on-the-fly parsing and instrumentation of JavaScript code as it is sent to users' browsers. AjaxScope provides facilities for distributed and adaptive instrumentation in order to reduce the client-side overhead, while giving fine-grained visibility into the code-level behavior of web applications. We present a variety of policies demonstrating the power of AjaxScope, ranging from simple error reporting and performance profiling to more complex memory leak detection and optimization analyses. We also apply our prototype to analyze the behavior of over 90 Web 2.0 applications and sites that use large amounts of JavaScript.},
booktitle = {Proceedings of Twenty-First ACM SIGOPS Symposium on Operating Systems Principles},
pages = {17–30},
numpages = {14},
keywords = {software instrumentation, software monitoring, web applications},
location = {Stevenson, Washington, USA},
series = {SOSP '07}
}

@article{10.1145/1323293.1294264,
author = {Kiciman, Emre and Livshits, Benjamin},
title = {AjaxScope: A Platform for Remotely Monitoring the Client-Side Behavior of Web 2.0 Applications},
year = {2007},
issue_date = {December 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {6},
issn = {0163-5980},
url = {https://doi.org/10.1145/1323293.1294264},
doi = {10.1145/1323293.1294264},
abstract = {The rise of the software-as-a-service paradigm has led to the development of a new breed of sophisticated, interactive applications often called Web 2.0. While web applications have become larger and more complex, web application developers today have little visibility into the end-to-end behavior of their systems. This paper presents AjaxScope, a dynamic instrumentation platform that enables cross-user monitoring and just-in-time control of web application behavior on end-user desktops. AjaxScope is a proxy that performs on-the-fly parsing and instrumentation of JavaScript code as it is sent to users' browsers. AjaxScope provides facilities for distributed and adaptive instrumentation in order to reduce the client-side overhead, while giving fine-grained visibility into the code-level behavior of web applications. We present a variety of policies demonstrating the power of AjaxScope, ranging from simple error reporting and performance profiling to more complex memory leak detection and optimization analyses. We also apply our prototype to analyze the behavior of over 90 Web 2.0 applications and sites that use large amounts of JavaScript.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {oct},
pages = {17–30},
numpages = {14},
keywords = {web applications, software instrumentation, software monitoring}
}

@inproceedings{10.1145/2019136.2019143,
author = {Oster, Sebastian and Zink, Marius and Lochau, Malte and Grechanik, Mark},
title = {Pairwise Feature-Interaction Testing for SPLs: Potentials and Limitations},
year = {2011},
isbn = {9781450307895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2019136.2019143},
doi = {10.1145/2019136.2019143},
abstract = {A fundamental problem of testing Software Product Lines (SPLs) is that variability enables the production of a large number of instances and it is difficult to construct and run test cases even for SPLs with a small number of variable features. Interacting features is a foundation of a fault model for SPLs, where faults are likely to be revealed at execution points where features exchange information with other features or influence one another. Therefore, a test adequacy criterion is to cover as many interactions among different features as possible, thus increasing the probability of finding bugs. Our approach combines a combinatorial designs algorithm for pairwise feature generation with model-based testing to reduce the size of the SPL required for comprehensive coverage of interacting features. We implemented our approach and applied it to an SPL from the automotive domain provided by one of our industrial partners. The results suggest that with our approach higher coverage of feature interactions is achieved at a fraction of cost when compared with a baseline approach of testing all feature interactions.},
booktitle = {Proceedings of the 15th International Software Product Line Conference, Volume 2},
articleno = {6},
numpages = {8},
keywords = {product lines, combinatorial testing, reusable test model, feature interaction, feature model, model-based testing},
location = {Munich, Germany},
series = {SPLC '11}
}

@inproceedings{10.1145/3468264.3468556,
author = {Park, Joonyoung and Park, Jihyeok and Youn, Dongjun and Ryu, Sukyoung},
title = {Accelerating JavaScript Static Analysis via Dynamic Shortcuts},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468556},
doi = {10.1145/3468264.3468556},
abstract = {JavaScript has become one of the most widely used programming languages for web development, server-side programming, and even micro-controllers for IoT. However, its extremely functional and dynamic features degrade the performance and precision of static analysis. Moreover, the variety of built-in functions and host environments requires excessive manual modeling of their behaviors. To alleviate these problems, researchers have proposed various ways to leverage dynamic analysis during JavaScript static analysis. However, they do not fully utilize the high performance of dynamic analysis and often sacrifice the soundness of static analysis. In this paper, we present dynamic shortcuts, a new technique to flexibly switch between abstract and concrete execution during JavaScript static analysis in a sound way. It can significantly improve the analysis performance and precision by using highly-optimized commercial JavaScript engines and lessen the modeling efforts for opaque code. We actualize the technique via SAFEDS, an extended combination of SAFE and Jalangi, a static analyzer and a dynamic analyzer, respectively. We evaluated SAFEDS using 269 official tests of Lodash 4 library. Our experiment shows that SAFEDS is 7.81x faster than the baseline static analyzer, and it improves the precision to reduce failed assertions by 12.31% on average for 22 opaque functions.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1129–1140},
numpages = {12},
keywords = {dynamic shortcut, static analysis, dynamic analysis, sealed execution, JavaScript},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.5555/2486788.2486864,
author = {Niu, Nan and Mahmoud, Anas and Chen, Zhangji and Bradshaw, Gary},
title = {Departures from Optimality: Understanding Human Analyst's Information Foraging in Assisted Requirements Tracing},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Studying human analyst's behavior in automated tracing is a new research thrust. Building on a growing body of work in this area, we offer a novel approach to understanding requirements analyst's information seeking and gathering. We model analysts as predators in pursuit of prey --- the relevant traceability information, and leverage the optimality models to characterize a rational decision process. The behavior of real analysts with that of the optimal information forager is then compared and contrasted. The results show that the analysts' information diets are much wider than the theory's predictions, and their residing in low-profitability information patches is much longer than the optimal residence time. These uncovered discrepancies not only offer concrete insights into the obstacles faced by analysts, but also lead to principled ways to increase practical tool support for overcoming the obstacles. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {572–581},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

