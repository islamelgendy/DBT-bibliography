@inproceedings{10.1109/ICSE.2007.23,
author = {Elbaum, Sebastian and Person, Suzette and Dokulil, Jon and Jorde, Matt},
title = {Bug Hunt: Making Early Software Testing Lessons Engaging and Affordable},
year = {2007},
isbn = {0769528287},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2007.23},
doi = {10.1109/ICSE.2007.23},
abstract = {Software testing efforts account for a large part of software development costs. However, as educators, we struggle to properly prepare students to perform software testing activities. This struggle is caused by multiple factors: 1) it is challenging to effectively incorporate software testing into an already over-packed curriculum, 2) ad-hoc efforts to teach testing generally happen too late in the students' career, after bad habits have already been developed, and 3) these efforts lack the necessary institutional consistency and support to be effective. To address these challenges we created Bug Hunt, a web-based tutorial to engage students in learning software testing strategies. In this paper we describe the most interesting aspects of the tutorial including the lessons and feedback mechanisms, and the facilities for instructors to configure the tutorial and obtain automatic student assessment. We also present the lessons learned after two years of deployment.},
booktitle = {Proceedings of the 29th International Conference on Software Engineering},
pages = {688–697},
numpages = {10},
keywords = {Software Testing Education, Web-based Tutorial.},
series = {ICSE '07}
}

@article{10.1145/1713254.1713257,
author = {Ciortea, Liviu and Zamfir, Cristian and Bucur, Stefan and Chipounov, Vitaly and Candea, George},
title = {Cloud9: A Software Testing Service},
year = {2010},
issue_date = {January 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {43},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1713254.1713257},
doi = {10.1145/1713254.1713257},
abstract = {Cloud9 aims to reduce the resource-intensive and laborintensive nature of high-quality software testing. First, Cloud9 parallelizes symbolic execution (an effective, but still poorly scalable test automation technique) to large shared-nothing clusters. To our knowledge, Cloud9 is the first symbolic execution engine that scales to large clusters of machines, thus enabling thorough automated testing of real software in conveniently short amounts of time. Preliminary results indicate one to two orders of magnitude speedup over a state-of-the-art symbolic execution engine. Second, Cloud9 is an on-demand software testing service: it runs on compute clouds, like Amazon EC2, and scales its use of resources over a wide dynamic range, proportionally with the testing task at hand.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {jan},
pages = {5–10},
numpages = {6}
}

@inproceedings{10.1145/3210459.3210475,
author = {Williams, Ashley},
title = {Do Software Engineering Practitioners Cite Research on Software Testing in Their Online Articles? A Preliminary Survey.},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210475},
doi = {10.1145/3210459.3210475},
abstract = {Background: Software engineering (SE) research continues to study the degree to which practitioners perceive that SE research has impact on practice. Such studies typically comprise surveys of practitioner opinions. These surveys could be complemented with other in situ practitioner sources e.g. grey literature.Objective: To investigate whether and how practitioners cite software testing research in their online articles.Method: We conduct 11,200 web searches using a customized Google-based search tool, scrape the pages of 722 unique results, and then analyse the articles for citations to research.Results: We find few citations to research (range 0% - 1% in our datasets) although this is similar to the frequency of citations to practitioner sites (0% - 4%). We find and discuss the only two significant instances of practitioners citing research in our datasets.Conclusion: We conducted a preliminary survey that complements the findings of previous work. But our survey contains a number of threats to validity. Our results should therefore be interpreted as hypotheses to motivate further investigation into the frequency to which practitioners cite research, and into the impact of research on practice.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {151–156},
numpages = {6},
keywords = {research impact, grey literature, research relevance, Evidence},
location = {Christchurch, New Zealand},
series = {EASE'18}
}

@inproceedings{10.1145/1150343.1150396,
author = {Krug, Margrit R. and Moraes, Marcelo S. and Lubaszewski, Marcelo S.},
title = {Using a Software Testing Technique to Identify Registers for Partial Scan Implementation},
year = {2006},
isbn = {1595934790},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1150343.1150396},
doi = {10.1145/1150343.1150396},
abstract = {Scan design has been widely used to ease test generation process for digital circuits. Although full scan approach results in high fault coverage while reducing ATPG effort, it introduces area and performance overheads that are most times unacceptable. Hence, partial scan is a commonly used technique to improve testability of sequential circuits while respecting design constraints. In this paper, we present a method to select sequential elements (flip-flops) to compose a partial scan chain. We use a software engineering technique to identify internal variables or signals of the circuit's behavioral description that have low observability. Experiments demonstrate that our approach achieves a high fault coverage including few flip-flops in the scan chain. Moreover, comparative results show that, for complex circuits, proposed technique is more efficient than some classical methods in selecting flip-flops to compose partial scan.},
booktitle = {Proceedings of the 19th Annual Symposium on Integrated Circuits and Systems Design},
pages = {208–213},
numpages = {6},
keywords = {automatic test generation, hardware testing, hardware description language, partial scan design, testability improvement},
location = {Ouro Preto, MG, Brazil},
series = {SBCCI '06}
}

@inproceedings{10.1109/CESSER-IP.2019.00010,
author = {Santos, Ronnie E. S. and Bener, Ay\c{s}e Ba\c{s}ar and Baldassarre, Maria Teresa and Magalh\~{a}es, Cleyton V. C. and Correia-Neto, Jorge S. and Silva, Fabio Q. B. da},
title = {Mind the Gap: Are Practitioners and Researchers in Software Testing Speaking the Same Language?},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CESSER-IP.2019.00010},
doi = {10.1109/CESSER-IP.2019.00010},
abstract = {Context. Software testing is the area of software engineering focused on determining whether a software meets the planned requirements and on evaluating its quality. Lately, academic researchers have increased their attention in this topic due to the impact of its success on software projects. However, recent studies have discussed that practitioners and researchers might have different views regarding what is important to explore and study in order to improve the software testing process. Goal. This study aims to investigate the differences of interests between academic researchers and practitioners in software testing, pointing out observable convergences and divergences between the two communities. Method. A mixed-method approach based on a mapping study, a quantitative study and a focus group was applied to collect quantitative and qualitative data from professionals and academic sources. Results. Our results confirm the existence of a gap between the two communities and the findings suggest that, while researchers are mainly focused on the proposition of novel tools and techniques, practitioners are more interested in issues related to the evaluation and discussions of existing approaches, tools and techniques. Therefore, academic researchers might consider identify, understand and modify the existing tools and strategies, instead of building new ones. Conclusion. In general, the distinction between the two groups is noticeable and there is only one strong mutual interest between both practitioners and researchers, namely, test automation. Therefore, there is a need for the development of strategies that reduce the gap between academia and industrial practice and bring them closer in order to increase the quality of the software testing processes.},
booktitle = {Proceedings of the Joint 7th International Workshop on Conducting Empirical Studies in Industry and 6th International Workshop on Software Engineering Research and Industrial Practice},
pages = {10–17},
numpages = {8},
keywords = {software testing, software engineering, industry},
location = {Montreal, Quebec, Canada},
series = {CESSER-IP '19}
}

@article{10.1145/3210309,
author = {B\"{o}hme, Marcel},
title = {STADS: Software Testing as Species Discovery},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3210309},
doi = {10.1145/3210309},
abstract = {A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (1)&nbsp;to estimate the total number of feasible program branches, given that only a fraction has been covered so far; (2)&nbsp;to estimate the additional time required to cover 10% more branches (or to estimate the coverage achieved in one more day, respectively); or (3)&nbsp;to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability does not mean that none exists—even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees.In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimations (1)&nbsp;of the total number of species, (2)&nbsp;of the additional sampling effort required to discover 10% more species, or (3)&nbsp;of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias—AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {7},
numpages = {52},
keywords = {discovery probability, reliability, fuzzing, security, measure of confidence, stopping rule, species coverage, measure of progress, Statistical guarantees, extrapolation, code coverage}
}

@inproceedings{10.1145/1292414.1292419,
author = {Bueno, Paulo M. S. and Wong, W. Eric and Jino, Mario},
title = {Improving Random Test Sets Using the Diversity Oriented Test Data Generation},
year = {2007},
isbn = {9781595938817},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1292414.1292419},
doi = {10.1145/1292414.1292419},
abstract = {We present a measure that characterizes the diversity of a test set from the perspective of the input domain of the program under test. By using a metaheuristic algorithm, randomly generated test sets (RTS) are evolved towards Diversity Oriented Test Sets (DOTS), which thoroughly cover the input domain. DOTS are evaluated using a Monte Carlo simulation to assess how testing factors influence their effectiveness and also by the values of data flow coverage and mutation scores attained on simple programs. Results provide understanding on possible gains of using DOTS and on circumstances where RTS can be more effective.},
booktitle = {Proceedings of the 2nd International Workshop on Random Testing: Co-Located with the 22nd IEEE/ACM International Conference on Automated Software Engineering (ASE 2007)},
pages = {10–17},
numpages = {8},
keywords = {genetic algorithms, software testing, test data generation, simulated annealing, data flow testing, simulated repulsion, mutation testing, random testing, diversity oriented test data generation},
location = {Atlanta, Georgia},
series = {RT '07}
}

@inproceedings{10.1145/3239235.3268923,
author = {Santos, Ronnie E. S. and Magalh\~{a}es, Cleyton V. C. and Capretz, Luiz Fernando and Correia-Neto, Jorge S. and da Silva, Fabio Q. B. and Saher, Abdelrahman},
title = {Computer Games Are Serious Business and so is Their Quality: Particularities of Software Testing in Game Development from the Perspective of Practitioners},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3268923},
doi = {10.1145/3239235.3268923},
abstract = {Context. Over the last several decades, computer games started to have a significant impact on society. However, although a computer game is a type of software, the process to conceptualize, produce and deliver a game could involve unusual features. In software testing, for instance, studies demonstrated the hesitance of professionals to use automated testing techniques with games, due to the constant changes in requirements and design, and pointed out the need for creating testing tools that take into account the flexibility required for the game development process. Goal. This study aims to improve the current body of knowledge regarding these theme and point out the existing particularities observed in software testing considering the development of a computer game. Method. A mixed-method approach based on a case study and a survey was applied to collect quantitative and qualitative data from practitioners regarding the particularities of software testing in game development. Results. We analyzed over 70 messages posted on three well-established network of question-and-answer communities and received answers of 38 practitioners, and identified important aspects to be observed in the process of planning, performing and reporting tests games. Conclusion. Considering computer games, software testing must focus not only on the common aspects of a general software, but also, track and investigate issues that could be related to game balance, game physics and entertainment related-aspects to guarantee the quality of computer games and a successful testing process.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {33},
numpages = {10},
keywords = {software testing, game development, mixed-method},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/3395363.3397382,
author = {He, Xiao and Wang, Xingwei and Shi, Jia and Liu, Yi},
title = {Testing High Performance Numerical Simulation Programs: Experience, Lessons Learned, and Open Issues},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397382},
doi = {10.1145/3395363.3397382},
abstract = {High performance numerical simulation programs are widely used to simulate actual physical processes on high performance computers for the analysis of various physical and engineering problems. They are usually regarded as non-testable due to their high complexity. This paper reports our real experience and lessons learned from testing five simulation programs that will be used to design and analyze nuclear power plants. We applied five testing approaches and found 33 bugs. We found that property-based testing and metamorphic testing are two effective methods. Nevertheless, we suffered from the lack of domain knowledge, the high test costs, the shortage of test cases, severe oracle issues, and inadequate automation support. Consequently, the five programs are not exhaustively tested from the perspective of software testing, and many existing software testing techniques and tools are not fully applicable due to scalability and portability issues. We need more collaboration and communication with other communities to promote the research and application of software testing techniques.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {502–515},
numpages = {14},
keywords = {Numerical simulation, Software testing, Experience, High performance computing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3128473.3128480,
author = {Silva-de-Souza, Thiago and Travassos, Guilherme Horta},
title = {Observing Effort Factors in the Test Design &amp; Implementation Process of Web Services Projects},
year = {2017},
isbn = {9781450353021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128473.3128480},
doi = {10.1145/3128473.3128480},
abstract = {Several models of software testing effort estimation take into account intrinsic characteristics for the whole software development, presenting no evidence of how they effectively affect the test effort. Also, most of such estimation models only estimate the effort regarding the execution of tests. However, in software testing projects the activity usually requiring more effort is the test design &amp; implementation one. Better precision in the identification of the effort factors influencing these activities can result in estimates with a higher degree of accuracy. Therefore, this paper presents a study on the factors affecting the effort in performing test design &amp; implementation of large-scale software projects in a Brazilian government company. The study was carried out through observing the factors influencing the effort in two Web Services testing projects. The identified factors were made explicit through a clear description procedure, which took into account the tasks and artifacts comprising the duties regarding test design &amp; implementation activity based on IEEE 29119 standard. It supported the identification and classification of 24 effort factors concerned with a Web Services test design &amp; implementation process into five different categories.},
booktitle = {Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {7},
numpages = {10},
keywords = {Web Services, Software testing, effort drivers, effort factors},
location = {Fortaleza, Brazil},
series = {SAST}
}

@inproceedings{10.1145/2610384.2610415,
author = {Baudry, Benoit and Allier, Simon and Monperrus, Martin},
title = {Tailored Source Code Transformations to Synthesize Computationally Diverse Program Variants},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610415},
doi = {10.1145/2610384.2610415},
abstract = { The predictability of program execution provides attackers a rich source of knowledge who can exploit it to spy or remotely control the program. Moving target defense ad- dresses this issue by constantly switching between many di- verse variants of a program, which reduces the certainty that an attacker can have about the program execution. The ef- fectiveness of this approach relies on the availability of a large number of software variants that exhibit dierent ex- ecutions. However, current approaches rely on the natural diversity provided by o-the-shelf components, which is very limited. In this paper, we explore the automatic synthe- sis of large sets of program variants, called sosies. Sosies provide the same expected functionality as the original pro- gram, while exhibiting dierent executions. They are said to be computationally diverse. This work addresses two objectives: comparing dierent transformations for increasing the likelihood of sosie synthe- sis (densifying the search space for sosies); demonstrating computation diversity in synthesized sosies. We synthesized 30 184 sosies in total, for 9 large, real-world, open source ap- plications. For all these programs we identied one type of program analysis that systematically increases the density of sosies; we measured computation diversity for sosies of 3 programs and found diversity in method calls or data in more than 40% of sosies. This is a step towards controlled massive unpredictability of software. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {149–159},
numpages = {11},
keywords = {Program Transformation, Software Diversity},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/3338906.3338970,
author = {Biagiola, Matteo and Stocco, Andrea and Ricca, Filippo and Tonella, Paolo},
title = {Diversity-Based Web Test Generation},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338970},
doi = {10.1145/3338906.3338970},
abstract = {Existing web test generators derive test paths from a navigational model of the web application, completed with either manually or randomly generated input values. However, manual test data selection is costly, while random generation often results in infeasible input sequences, which are rejected by the application under test. Random and search-based generation can achieve the desired level of model coverage only after a large number of test execution at- tempts, each slowed down by the need to interact with the browser during test execution. In this work, we present a novel web test generation algorithm that pre-selects the most promising candidate test cases based on their diversity from previously generated tests. As such, only the test cases that explore diverse behaviours of the application are considered for in-browser execution. We have implemented our approach in a tool called DIG. Our empirical evaluation on six real-world web applications shows that DIG achieves higher coverage and fault detection rates significantly earlier than crawling-based and search-based web test generators.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {142–153},
numpages = {12},
keywords = {diversity, page object, test generation, web testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/2620000,
author = {Wang, Huai and Chan, W. K. and Tse, T. H.},
title = {Improving the Effectiveness of Testing Pervasive Software via Context Diversity},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {9},
number = {2},
issn = {1556-4665},
url = {https://doi.org/10.1145/2620000},
doi = {10.1145/2620000},
abstract = {Context-aware pervasive software is responsive to various contexts and their changes. A faulty implementation of the context-aware features may lead to unpredictable behavior with adverse effects. In software testing, one of the most important research issues is to determine the sufficiency of a test suite to verify the software under test. Existing adequacy criteria for testing traditional software, however, have not explored the dimension of serial test inputs and have not considered context changes when constructing test suites. In this article, we define the concept of context diversity to capture the extent of context changes in serial inputs and propose three strategies to study how context diversity may improve the effectiveness of the data-flow testing criteria. Our case study shows that the strategy that uses test cases with higher context diversity can significantly improve the effectiveness of existing data-flow testing criteria for context-aware pervasive software. In addition, test suites with higher context diversity are found to execute significantly longer paths, which may provide a clue that reveals why context diversity can contribute to the improvement of effectiveness of test suites.},
journal = {ACM Trans. Auton. Adapt. Syst.},
month = {jul},
articleno = {9},
numpages = {28},
keywords = {Context-aware program, test adequacy, context diversity}
}

@inbook{10.1145/3460319.3469076,
author = {Groce, Alex and Grieco, Gustavo},
title = {Echidna-Parade: A Tool for Diverse Multicore Smart Contract Fuzzing},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3469076},
abstract = {Echidna is a widely used fuzzer for Ethereum Virtual Machine (EVM) compatible blockchain smart contracts that generates transaction sequences of calls to smart contracts. While Echidna is an essentially single-threaded tool, it is possible for multiple Echidna processes to communicate by use of a shared transaction sequence corpus. Echidna provides a very large variety of configuration options, since each smart contract may be best-tested by a non-default configuration, and different faults or coverage targets within a single contract may also have differing ideal configurations. This paper presents echidna-parade, a tool that provides pushbutton multicore fuzzing using Echidna as an underlying fuzzing engine, and automatically provides sophisticated diversification of configurations. Even without using multiple cores, echidna-parade can improve the effectiveness of fuzzing with Echidna, due to the advantages provided by multiple types of test configuration diversity. Using echidna-parade with multiple cores can produce significantly better results than Echidna, in less time.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {658–661},
numpages = {4}
}

@inproceedings{10.5555/2663608.2663637,
author = {De Lucia, Andrea and Di Penta, Massimiliano and Oliveto, Rocco and Panichella, Annibale},
title = {On the Role of Diversity Measures for Multi-Objective Test Case Selection},
year = {2012},
isbn = {9781467318228},
publisher = {IEEE Press},
abstract = {Test case selection has been recently formulated as multi-objective optimization problem trying to satisfy conflicting goals, such as code coverage and computational cost. This paper introduces the concept of asymmetric distance preserving, useful to improve the diversity of non-dominated solutions produced by multi-objective Pareto efficient genetic algorithms, and proposes two techniques to achieve this objective. Results of an empirical study conducted over four programs from the SIR benchmark show how the proposed technique (i) obtains non-dominated solutions having a higher diversity than the previously proposed multi-objective Pareto genetic algorithms; and (ii) improves the convergence speed of the genetic algorithms.},
booktitle = {Proceedings of the 7th International Workshop on Automation of Software Test},
pages = {145–151},
numpages = {7},
keywords = {niched genetic algorithms, empirical studies, search-based software testing, test case selection},
location = {Zurich, Switzerland},
series = {AST '12}
}

@inproceedings{10.5555/3105427.3105437,
author = {Cohen, Myra B.},
title = {The Evolutionary Landscape of SBST: A 10 Year Perspective},
year = {2017},
isbn = {9781538627891},
publisher = {IEEE Press},
abstract = {A key indicator of the health and quality of any evolutionary algorithm is the landscape of its search. By analyzing the landscape one can determine the peaks (local maxima) where significant solutions exist. In this paper we examine the landscape for the history of the International Workshop on Search-Based Software Testing (SBST) within the context of the broader field of search-based software testing. We study the evolution of the field, highlighting key advances during three phases of its ten year history. In 2008 the focus of SBST was inner looking, with advances in existing search techniques, improvements to individual generation techniques, and methods to transform the problem space for search effectiveness. However, diverse seeds of new ideas (such as automated program repair) were already being injected into the population. A few SBST tools existed, but the engineer still required skill and expertise to effectively apply search based approaches. During the middle years, open source tools were created and released, whole test suite generation appeared, and searches hybridized. Tool competitions began and industry started to play a stronger role. As we move to the most recent workshop years and look towards the future, more sophisticated techniques such as those that incorporate hyper-heuristics via learning, and/or balance multiple objectives at once are now common. SBST has become a mainstream topic in the testing community, tools are being commercialized and these tools often hide their inner workings, leading to a future that is optimized towards SBST for all.},
booktitle = {Proceedings of the 10th International Workshop on Search-Based Software Testing},
pages = {47–48},
numpages = {2},
keywords = {SBST, search-based software testing},
location = {Buenos Aires, Argentina},
series = {SBST '17}
}

@inproceedings{10.1145/3195836.3195857,
author = {Menezes, \'{A}lvaro and Prikladnicki, Rafael},
title = {Diversity in Software Engineering},
year = {2018},
isbn = {9781450357258},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3195836.3195857},
doi = {10.1145/3195836.3195857},
abstract = {1 Studies about diversity in Software Engineering (SE) are important to understand the disparity occurring nowadays at information technology workplaces. The goal of this work is to analyze the characteristics of diversity in SE and how to adapt SE practices when we have teams with diversity characteristics. We collected data by conducting a Systematic Literature Review (SLR) and semi-structured interviews aiming to identify what impacts of diversity can be observed in software development teams. We found that there are several challenges and barriers encountered in the work environment, and that inclusion and diversity can affect the software development teams positively.},
booktitle = {Proceedings of the 11th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {45–48},
numpages = {4},
keywords = {software engineering, workplace, team, diversity},
location = {Gothenburg, Sweden},
series = {CHASE '18}
}

@inproceedings{10.1145/2610384.2610413,
author = {Alshahwan, Nadia and Harman, Mark},
title = {Coverage and Fault Detection of the Output-Uniqueness Test Selection Criteria},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610413},
doi = {10.1145/2610384.2610413},
abstract = { This paper studies the whitebox coverage and fault detection achieved by Output Uniqueness, a newly proposed blackbox test criterion, using 6 web applications. We find that output uniqueness exhibits average correlation coefficients of 0.85, 0.83 and 0.97 with statement, branch and path coverage respectively. More interestingly, output uniqueness finds 92% of the real faults found by branch coverage (and a further 47% that remained undetected by such whitebox techniques). These results suggest that output uniqueness may provide a useful surrogate when whitebox techniques are inapplicable and an effective complement where they are. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {181–192},
numpages = {12},
keywords = {Web applications, Software Testing, Whitebox testing, Blackbox testing},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/1273463.1273476,
author = {Ciupa, Ilinca and Leitner, Andreas and Oriol, Manuel and Meyer, Bertrand},
title = {Experimental Assessment of Random Testing for Object-Oriented Software},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273476},
doi = {10.1145/1273463.1273476},
abstract = {Progress in testing requires that we evaluate the effectiveness of testing strategies on the basis of hard experimental evidence, not just intuition or a priori arguments. Random testing, the use of randomly generated test data, is an example of a strategy that the literature often deprecates because of such preconceptions. This view is worth revisiting since random testing otherwise offers several attractive properties: simplicity of implementation, speed of execution, absence of human bias.We performed an intensive experimental analysis of the efficiency of random testing on an existing industrial-grade code base. The use of a large-scale cluster of computers, for a total of 1500 hours of CPU time, allowed a fine-grain analysis of the individual effect of the various parameters involved in the random testing strategy, such as the choice of seed for a random number generator. The results provide insights into the effectiveness of random testing and a number of lessons for testing researchers and practitioners.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {84–94},
numpages = {11},
keywords = {experimental evaluation, random testing, software testing},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@inproceedings{10.1145/3460319.3464795,
author = {Herrera, Adrian and Gunadi, Hendra and Magrath, Shane and Norrish, Michael and Payer, Mathias and Hosking, Antony L.},
title = {Seed Selection for Successful Fuzzing},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464795},
doi = {10.1145/3460319.3464795},
abstract = {Mutation-based greybox fuzzing---unquestionably the most widely-used fuzzing technique---relies on a set of non-crashing seed inputs (a corpus) to bootstrap the bug-finding process. When evaluating a fuzzer, common approaches for constructing this corpus include: (i) using an empty file; (ii) using a single seed representative of the target's input format; or (iii) collecting a large number of seeds (e.g., by crawling the Internet). Little thought is given to how this seed choice affects the fuzzing process, and there is no consensus on which approach is best (or even if a best approach exists).  To address this gap in knowledge, we systematically investigate and evaluate how seed selection affects a fuzzer's ability to find bugs in real-world software. This includes a systematic review of seed selection practices used in both evaluation and deployment contexts, and a large-scale empirical evaluation (over 33 CPU-years) of six seed selection approaches. These six seed selection approaches include three corpus minimization techniques (which select the smallest subset of seeds that trigger the same range of instrumentation data points as a full corpus).  Our results demonstrate that fuzzing outcomes vary significantly depending on the initial seeds used to bootstrap the fuzzer, with minimized corpora outperforming singleton, empty, and large (in the order of thousands of files) seed sets. Consequently, we encourage seed selection to be foremost in mind when evaluating/deploying fuzzers, and recommend that (a) seed choice be carefully considered and explicitly documented, and (b) never to evaluate fuzzers with only a single seed.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {230–243},
numpages = {14},
keywords = {software testing, fuzzing, corpus minimization},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

