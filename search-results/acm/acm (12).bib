@inproceedings{10.1145/1273463.1273479,
author = {Koster, Ken},
title = {Using Portfolio Theory for Better and More Consistent Quality},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273479},
doi = {10.1145/1273463.1273479},
abstract = {The effectiveness of software quality techniques varies. Many uncertain or unpredictable factors influence effectiveness, including human factors, the types of defects in the program, and luck. Compared to using a single quality technique, a diversified portfolio of techniques will typically be more effective and less variable. This work postulates a simple model, adapted from financial Modern Portfolio Theory, for the variability and effectiveness of techniques, singly and in portfolios. Proofs and simulations analyze the model to evaluate factors influencing the success of diversification; the model is checked against data sets from previous work.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {108–117},
numpages = {10},
keywords = {portfolio software quality, effectiveness, variability, economic models, testing, diversification},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@inproceedings{10.1145/2771783.2771798,
author = {Khoshnood, Sepideh and Kusano, Markus and Wang, Chao},
title = {ConcBugAssist: Constraint Solving for Diagnosis and Repair of Concurrency Bugs},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771798},
doi = {10.1145/2771783.2771798},
abstract = { Programmers often have to spend a significant amount of time in- specting the software code and execution traces to identify the cause of a bug. For a multithreaded program, debugging is even more challenging due to the subtle interactions between threads and the often astronomical number of interleavings. In this work, we pro- pose a logical constraint based symbolic analysis method to aid in the diagnosis of concurrency bugs and to recommend repairs. Both diagnosis and repair are formulated as constraint solving prob- lems. Our method, by leveraging the power of satisfiability (SAT) solvers and a bounded model checker, performs a semantic analy- sis of the sequential computation as well as thread interactions. The constraint based analysis is designed for handling critical software with small to medium code size, but complex concurrency control, such as device drivers, implementations of synchronization proto- cols, and concurrent data structures. We have implemented our new method in a software tool and demonstrated its effectiveness in di- agnosing bugs in multithreaded C programs. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {165–176},
numpages = {12},
keywords = {program repair, partial MAX-SAT, error diagnosis, bounded model checking, binate covering, unsatisfiability core, Concurrency},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/1273463.1273469,
author = {Tzoref, Rachel and Ur, Shmuel and Yom-Tov, Elad},
title = {Instrumenting Where It Hurts: An Automatic Concurrent Debugging Technique},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273469},
doi = {10.1145/1273463.1273469},
abstract = {As concurrent and distributive applications are becoming more common and debugging such applications is very difficult, practical tools for automatic debugging of concurrent applications are in demand. In previous work, we applied automatic debugging to noise-based testing of concurrent programs. The idea of noise-based testing is to increase the probability of observing the bugs by adding, using instrumentation, timing "noise" to the execution of the program. The technique of finding a small subset of points that causes the bug to manifest can be used as an automatic debugging technique. Previously, we showed that Delta Debugging can be used to pinpoint the bug location on some small programs.In the work reported in this paper, we create and evaluate two algorithms for automatically pinpointing program locations that are in the vicinity of the bugs on a number of industrial programs. We discovered that the Delta Debugging algorithms do not scale due to the non-monotonic nature of the concurrent debugging problem. Instead we decided to try a machine learning feature selection algorithm. The idea is to consider each instrumentation point as a feature, execute the program many times with different instrumentations, and correlate the features (instrumentation points) with the executions in which the bug was revealed. This idea works very well when the bug is very hard to reveal using instrumentation, correlating to the case when a very specific timing window is needed to reveal the bug. However, in the more common case, when the bugs are easy to find using instrumentation points ranked high by the feature selection algorithm is not high enough. We show that for these cases, the important value is not the absolute value of the evaluation of the feature but the derivative of that value along the program execution path.As a number of groups expressed interest in this research, we built an open infrastructure for automatic debugging algorithms for concurrent applications, based on noise injection based concurrent testing using instrumentation. The infrastructure is described in this paper.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {27–38},
numpages = {12},
keywords = {debugging, concurrency, feature selection},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@inproceedings{10.1145/2931037.2931073,
author = {Chapman, Carl and Stolee, Kathryn T.},
title = {Exploring Regular Expression Usage and Context in Python},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931073},
doi = {10.1145/2931037.2931073},
abstract = { Due to the popularity and pervasive use of regular expressions, researchers have created tools to support their creation, validation, and use. However, little is known about the context in which regular expressions are used, the features that are most common, and how behaviorally similar regular expressions are to one another.  In this paper, we explore the context in which regular expressions are used through a combination of developer surveys and repository analysis. We survey 18 professional developers about their regular expression usage and pain points. Then, we analyze nearly 4,000 open source Python projects from GitHub and extract nearly 14,000 unique regular expression patterns. We map the most common features used in regular expressions to those features supported by four major regex research efforts from industry and academia: brics, Hampi, RE2, and Rex. Using similarity analysis of regular expressions across projects, we identify six common behavioral clusters that describe how regular expressions are often used in practice. This is the first rigorous examination of regex usage and it provides empirical evidence to support design decisions by regex tool builders. It also points to areas of needed future work, such as refactoring regular expressions to increase regex understandability and context-specific tool support for common regex usages. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {282–293},
numpages = {12},
keywords = {repository analysis, developer survey, regular expressions},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3460319.3464826,
author = {Shuai, Ziqi and Chen, Zhenbang and Zhang, Yufeng and Sun, Jun and Wang, Ji},
title = {Type and Interval Aware Array Constraint Solving for Symbolic Execution},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464826},
doi = {10.1145/3460319.3464826},
abstract = {Array constraints are prevalent in analyzing a program with symbolic execution. Solving array constraints is challenging due to the complexity of the precise encoding for arrays. In this work, we propose to synergize symbolic execution and array constraint solving. Our method addresses the difficulties in solving array constraints with novel ideas. First, we propose a lightweight method for pre-checking the unsatisfiability of array constraints based on integer linear programming. Second, observing that encoding arrays at the byte-level introduces many redundant axioms that reduce the effectiveness of constraint solving, we propose type and interval aware axiom generation. Note that the type information of array variables is inferred by symbolic execution, whereas interval information is calculated through the above pre-checking step. We have implemented our methods based on KLEE and its underlying constraint solver STP and conducted large-scale experiments on 75 real-world programs. The experimental results show that our method effectively improves the efficiency of symbolic execution. Our method solves 182.56% more constraints and explores 277.56% more paths on average under the same time threshold.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {361–373},
numpages = {13},
keywords = {symbolic execution, constraint solving, array SMT theory},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3395363.3404540,
author = {Tizpaz-Niari, Saeid and \v{C}ern\'{y}, Pavol and Trivedi, Ashutosh},
title = {Detecting and Understanding Real-World Differential Performance Bugs in Machine Learning Libraries},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3404540},
doi = {10.1145/3395363.3404540},
abstract = {Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {189–199},
numpages = {11},
keywords = {ML Libraries, Testing, Differential Performance Bugs, Debugging},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inbook{10.1145/3293882.3330564,
author = {Cai, Haipeng and Zhang, Ziyi and Li, Li and Fu, Xiaoqin},
title = {A Large-Scale Study of Application Incompatibilities in Android},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330564},
abstract = {The rapid expansion of the Android ecosystem is accompanied by continuing diversification of platforms and devices, resulting in increasing incompatibility issues which damage user experiences and impede app development productivity. In this paper, we conducted a large-scale, longitudinal study of compatibility issues in 62,894 benign apps developed in the past eight years, to understand the symptoms and causes of these issues. We further investigated the incompatibilities that are actually exercised at runtime through the system logs and execution traces of 15,045 apps. Our study revealed that, among others, (1) compatibility issues were prevalent and persistent at both installation and run time, with greater prevalence of run-time incompatibilities, (2) there were no certain Android versions that consistently saw more or less app incompatibilities than others, (3) installation-time incompatibilities were strongly correlated with the minSdkVersion specified in apps, while run-time incompatibilities were most significantly correlated with the underlying platform’s API level, and (4) installation-time incompatibilities were mostly due to apps’ use of architecture-incompatible native libraries, while run-time incompatibilities were mostly due to API changes during SDK evolution. We offered further insights into app incompatibilities, as well as recommendations on dealing with the issues for bother developers and end users of Android apps.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {216–227},
numpages = {12}
}

@inproceedings{10.1145/229000.226313,
author = {Daran, Murial and Th\'{e}venod-Fosse, Pascale},
title = {Software Error Analysis: A Real Case Study Involving Real Faults and Mutations},
year = {1996},
isbn = {0897917871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/229000.226313},
doi = {10.1145/229000.226313},
abstract = {The paper reports on a first experimental comparison of software errors generated by real faults and by 1st-order mutations. The experiments were conducted on a program developed by a student from the industrial specification of a critical software from the civil nuclear field. Emphasis was put on the analysis of errors produced upon activation of 12 real faults by focusing on the mechanisms of error creation, masking, and propagation up to failure occurrence, and on the comparison of these errors with those created by 24 mutations. The results involve a total of 3730 errors recorded from program execution traces: 1458 errors were produced by the real faults, and the 2272 others by the mutations. They are in favor of a suitable consistency between errors generated by mutations and by real faults: 85% of the 2272 errors due to the mutations were also produced by the real faults. Moreover, it was observed that although the studied mutations were simple faults, they can create erroneous behaviors as complex as those identified for the real faults. This lends support to the representativeness of errors due to mutations.},
booktitle = {Proceedings of the 1996 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {158–171},
numpages = {14},
location = {San Diego, California, USA},
series = {ISSTA '96}
}

@article{10.1145/226295.226313,
author = {Daran, Murial and Th\'{e}venod-Fosse, Pascale},
title = {Software Error Analysis: A Real Case Study Involving Real Faults and Mutations},
year = {1996},
issue_date = {May 1996},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {21},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/226295.226313},
doi = {10.1145/226295.226313},
abstract = {The paper reports on a first experimental comparison of software errors generated by real faults and by 1st-order mutations. The experiments were conducted on a program developed by a student from the industrial specification of a critical software from the civil nuclear field. Emphasis was put on the analysis of errors produced upon activation of 12 real faults by focusing on the mechanisms of error creation, masking, and propagation up to failure occurrence, and on the comparison of these errors with those created by 24 mutations. The results involve a total of 3730 errors recorded from program execution traces: 1458 errors were produced by the real faults, and the 2272 others by the mutations. They are in favor of a suitable consistency between errors generated by mutations and by real faults: 85% of the 2272 errors due to the mutations were also produced by the real faults. Moreover, it was observed that although the studied mutations were simple faults, they can create erroneous behaviors as complex as those identified for the real faults. This lends support to the representativeness of errors due to mutations.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {158–171},
numpages = {14}
}

@inproceedings{10.1145/2001420.2001463,
author = {Zhang, Sai and Saff, David and Bu, Yingyi and Ernst, Michael D.},
title = {Combined Static and Dynamic Automated Test Generation},
year = {2011},
isbn = {9781450305624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001420.2001463},
doi = {10.1145/2001420.2001463},
abstract = {In an object-oriented program, a unit test often consists of a sequence of method calls that create and mutate objects, then use them as arguments to a method under test. It is challenging to automatically generate sequences that are legal and behaviorally-diverse, that is, reaching as many different program states as possible.This paper proposes a combined static and dynamic automated test generation approach to address these problems, for code without a formal specification. Our approach first uses dynamic analysis to infer a call sequence model from a sample execution, then uses static analysis to identify method dependence relations based on the fields they may read or write. Finally, both the dynamically-inferred model (which tends to be accurate but incomplete) and the statically-identified dependence information (which tends to be conservative) guide a random test generator to create legal and behaviorally-diverse tests.Our Palus tool implements this testing approach. We compared its effectiveness with a pure random approach, a dynamic-random approach (without a static phase), and a static-random approach (without a dynamic phase) on several popular open-source Java programs. Tests generated by Palus achieved higher structural coverage and found more bugs.Palus is also internally used in Google. It has found 22 previously-unknown bugs in four well-tested Google products.},
booktitle = {Proceedings of the 2011 International Symposium on Software Testing and Analysis},
pages = {353–363},
numpages = {11},
keywords = {static, automated test generation, dynamic analyses},
location = {Toronto, Ontario, Canada},
series = {ISSTA '11}
}

@inproceedings{10.1145/2771783.2771796,
author = {Barr, Earl T. and Harman, Mark and Jia, Yue and Marginean, Alexandru and Petke, Justyna},
title = {Automated Software Transplantation},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771796},
doi = {10.1145/2771783.2771796},
abstract = { Automated transplantation would open many exciting avenues for software development: suppose we could autotransplant code from one system into another, entirely unrelated, system. This paper introduces a theory, an algorithm, and a tool that achieve this. Leveraging lightweight annotation, program analysis identifies an organ (interesting behavior to transplant); testing validates that the organ exhibits the desired behavior during its extraction and after its implantation into a host. While we do not claim automated transplantation is now a solved problem, our results are encouraging: we report that in 12 of 15 experiments, involving 5 donors and 3 hosts (all popular real-world systems), we successfully autotransplanted new functionality and passed all regression tests. Autotransplantation is also already useful: in 26 hours computation time we successfully autotransplanted the H.264 video encoding functionality from the x264 system to the VLC media player; compare this to upgrading x264 within VLC, a task that we estimate, from VLC's version history, took human programmers an average of 20 days of elapsed, as opposed to dedicated, time. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {257–269},
numpages = {13},
keywords = {genetic improvement, Automated software transplantation, autotransplantation},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/186258.186523,
author = {Goldberg, Allen and Wang, T. C. and Zimmerman, David},
title = {Applications of Feasible Path Analysis to Program Testing},
year = {1994},
isbn = {0897916832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/186258.186523},
doi = {10.1145/186258.186523},
abstract = {For certain structural testing criteria a significant proportion of tests instances are infeasible in the sense the semantics of the program implies that test data cannot be constructed that meet the test requirement. This paper describes the design and prototype implementation of a structural testing system that uses a theorem prover to determine feasibility of testing requirements and to optimize the number of test cases required to achieve test coverage. Using this approach, we were able to accurately and efficiently determine path feasibility for moderately-sized program units of production code written in a subset of Ada. On these problems, the computer solutions were obtained much faster and with greater accuracy than manual analysis. The paper describes how we formalize test criteria as control flow graph path expressions; how the criteria are mapped to logic formulas; and how we control the complexity of the inference task. It describes the limitations of the system and proposals for its improvement as well as other applications of the analysis.},
booktitle = {Proceedings of the 1994 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {80–94},
numpages = {15},
location = {Seattle, Washington, USA},
series = {ISSTA '94}
}

@inproceedings{10.1145/2771783.2784772,
author = {Ohmann, Peter},
title = {Making Your Crashes Work for You (Doctoral Symposium)},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2784772},
doi = {10.1145/2771783.2784772},
abstract = { Debugging is difficult and costly. Developers greatly value full traces and complete, reproducible crash recordings, but these are impractical for deployed software. Fortunately, failing applications can leave behind a snapshot of their crashing state in the form of a core dump. Unfortunately, crash data alone often leaves substantial ambiguity in the program's execution. My thesis work aims both to improve the quality of information extracted from core dumps and enhance this readily-available information. My prior work showed that automated postmortem analysis results can be significantly improved by targeted, lightweight, and tunable instrumentation (0-5% run-time overhead). My thesis aims to expand this work in three directions: improved tracing and dump data recovery, expanded postmortem analyses, and improved tracing efficiency based on previously-observed failures. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {428–431},
numpages = {4},
keywords = {Core dumps, postmortem program analysis, debugging},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/3422392.3422508,
author = {Marinho, Euler Horta and Figueiredo, Eduardo},
title = {PLATOOL: A Functional Test Generation Tool for Mobile Applications},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422508},
doi = {10.1145/3422392.3422508},
abstract = {Mobile applications are ubiquitous nowadays and their testing is a central activity for quality assurance. Software testing is considered an important activity in this context. Application testers are faced with several classes of events in this domain including GUI and system events, such as sensor-related events. While GUI events have been systematically explored in mobile application testing literature, system events have received less attention. A possible difficulty faced by mobile application testers is the identification and generation of input data for system events. This paper presents PLATOOL for assisting mobile application testers to deal with common events of the mobile applications during the automation of functional tests. Our preliminary results indicate that PLATOOL is able to generate and execute useful functional tests to support testing of mobile applications.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {548–553},
numpages = {6},
keywords = {software testing, system events, mobile applications, sensor events, GUI events, events},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/3213846.3213875,
author = {Shi, August and Gyori, Alex and Mahmood, Suleman and Zhao, Peiyuan and Marinov, Darko},
title = {Evaluating Test-Suite Reduction in Real Software Evolution},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213875},
doi = {10.1145/3213846.3213875},
abstract = {Test-suite reduction (TSR) speeds up regression testing by removing redundant tests from the test suite, thus running fewer tests in the future builds. To decide whether to use TSR or not, a developer needs some way to predict how well the reduced test suite will detect real faults in the future compared to the original test suite. Prior research evaluated the cost of TSR using only program versions with seeded faults, but such evaluations do not explicitly predict the effectiveness of the reduced test suite in future builds.  We perform the first extensive study of TSR using real test failures in (failed) builds that occurred for real code changes. We analyze 1478 failed builds from 32 GitHub projects that run their tests on Travis. Each failed build can have multiple faults, so we propose a family of mappings from test failures to faults. We use these mappings to compute Failed-Build Detection Loss (FBDL), the percentage of failed builds where the reduced test suite misses to detect all the faults detected by the original test suite. We find that FBDL can be up to 52.2%, which is higher than suggested by traditional TSR metrics. Moreover, traditional TSR metrics are not good predictors of FBDL, making it difficult for developers to decide whether to use reduced test suites.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {84–94},
numpages = {11},
keywords = {Test-suite reduction, regression testing, continuous integration},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/2771783.2771808,
author = {Tan, Tian Huat and Xue, Yinxing and Chen, Manman and Sun, Jun and Liu, Yang and Dong, Jin Song},
title = {Optimizing Selection of Competing Features via Feedback-Directed Evolutionary Algorithms},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771808},
doi = {10.1145/2771783.2771808},
abstract = { Software that support various groups of customers usually require complicated configurations to attain different functionalities. To model the configuration options, feature model is proposed to capture the commonalities and competing variabilities of the product variants in software family or Software Product Line (SPL). A key challenge for deriving a new product is to find a set of features that do not have inconsistencies or conflicts, yet optimize multiple objectives (e.g., minimizing cost and maximizing number of features), which are often competing with each other. Existing works have attempted to make use of evolutionary algorithms (EAs) to address this problem. In this work, we incorporated a novel feedback-directed mechanism into existing EAs. Our empirical results have shown that our method has improved noticeably over all unguided version of EAs on the optimal feature selection. In particular, for case studies in SPLOT and LVAT repositories, the feedback-directed Indicator-Based EA (IBEA) has increased the number of correct solutions found by 72.33% and 75%, compared to unguided IBEA. In addition, by leveraging a pre-computed solution, we have found 34 sound solutions for Linux X86, which contains 6888 features, in less than 40 seconds. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {246–256},
numpages = {11},
keywords = {evolutionary algorithms, SAT solvers, Software product line},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2771783.2771787,
author = {Dahse, Johannes and Holz, Thorsten},
title = {Experience Report: An Empirical Study of PHP Security Mechanism Usage},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771787},
doi = {10.1145/2771783.2771787},
abstract = { The World Wide Web mainly consists of web applications written in weakly typed scripting languages, with PHP being the most popular language in practice. Empirical evidence based on the analysis of vulnerabilities suggests that security is often added as an ad-hoc solution, rather than planning a web application with security in mind during the design phase. Although some best-practice guidelines emerged, no comprehensive security standards are available for developers. Thus, developers often apply their own favorite security mechanisms for data sanitization or validation to prohibit malicious input to a web application. In the context of our development of a new static code analysis tool for vulnerability detection, we studied commonly used input sanitization or validation mechanisms in 25 popular PHP applications. Our analysis of 2.5 million lines of code and over 26 thousand secured data flows provides a comprehensive overview of how developers utilize security mechanisms in practice regarding different markup contexts. In this paper, we discuss these security mechanisms in detail and reveal common pitfalls. For example, we found certain markup contexts and security mechanisms more frequently vulnerable than others. Our empirical study helps researchers, web developers, and tool developers to focus on error-prone markup contexts and security mechanisms in order to detect and mitigate vulnerabilities. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {60–70},
numpages = {11},
keywords = {PHP, input validation, input sanitization, Static analysis},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/1833310.1833327,
author = {Shah, Hina and Harrold, Mary Jean},
title = {Studying Human and Social Aspects of Testing in a Service-Based Software Company: Case Study},
year = {2010},
isbn = {9781605589664},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1833310.1833327},
doi = {10.1145/1833310.1833327},
abstract = {This paper presents a case study that reports the findings of a preliminary ethnographic study (35 days of fieldwork over a period of two months) conducted at a service-based software company. The focus of the study was on understanding the human-dimension and social aspects involved in software testing. In this paper, we present the design of the study, our observations, and the analysis of the findings. We also discuss the differences between the senior and junior participants' attitudes towards testing, how the seniors' attitudes influence the juniors' attitudes, and reasons that seem to motivate juniors to work on testing projects. Additionally, we report our findings about the relationship between enthusiasm and responsibility with ownership, the relationship between the developer and test engineer, the communication gaps faced by test engineers in various situations, and how hierarchical structuring in an organization may influence enthusiasm of the test engineers.},
booktitle = {Proceedings of the 2010 ICSE Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {102–108},
numpages = {7},
keywords = {ethnography, software testing, field study, human factors, motivation, qualitative study, attitudes},
location = {Cape Town, South Africa},
series = {CHASE '10}
}

@inproceedings{10.1145/3194718.3194720,
author = {Kim, Junhwi and Kwon, Minhyuk and Yoo, Shin},
title = {Generating Test Input with Deep Reinforcement Learning},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194720},
doi = {10.1145/3194718.3194720},
abstract = {Test data generation is a tedious and laborious process. Search-based Software Testing (SBST) automatically generates test data optimising structural test criteria using metaheuristic algorithms. In essence, metaheuristic algorithms are systematic trial-and-error based on the feedback of fitness function. This is similar to an agent of reinforcement learning which iteratively decides an action based on the current state to maximise the cumulative reward. Inspired by this analogy, this paper investigates the feasibility of employing reinforcement learning in SBST to replace human designed meta-heuristic algorithms. We reformulate the software under test (SUT) as an environment of reinforcement learning. At the same time, we present GunPowder, a novel framework for SBST which extends SUT to the environment. We train a Double Deep Q-Networks (DDQN) agent with deep neural network and evaluate the effectiveness of our approach by conducting a small empirical study. Finally, we find that agents can learn metaheuristic algorithms for SBST, achieving 100% branch coverage for training functions. Our study sheds light on the future integration of deep neural network and SBST.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {51–58},
numpages = {8},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@inproceedings{10.1145/3395363.3397377,
author = {Jiang, Muhui and Zhou, Yajin and Luo, Xiapu and Wang, Ruoyu and Liu, Yang and Ren, Kui},
title = {An Empirical Study on ARM Disassembly Tools},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397377},
doi = {10.1145/3395363.3397377},
abstract = {With the increasing popularity of embedded devices, ARM is becoming the dominant architecture for them. In the meanwhile, there is a pressing need to perform security assessments for these devices. Due to different types of peripherals, it is challenging to dynamically run the firmware of these devices in an emulated environment. Therefore, the static analysis is still commonly used. Existing work usually leverages off-the-shelf tools to disassemble stripped ARM binaries and (implicitly) assume that reliable disassembling binaries and function recognition are solved problems. However, whether this assumption really holds is unknown. In this paper, we conduct the first comprehensive study on ARM disassembly tools. Specifically, we build 1,896 ARM binaries (including 248 obfuscated ones) with different compilers, compiling options, and obfuscation methods. We then evaluate them using eight state-of-the-art ARM disassembly tools (including both commercial and noncommercial ones) on their capabilities to locate instructions and function boundaries. These two are fundamental ones, which are leveraged to build other primitives. Our work reveals some observations that have not been systematically summarized and/or confirmed. For instance, we find that the existence of both ARM and Thumb instruction sets, and the reuse of the BL instruction for both function calls and branches bring serious challenges to disassembly tools. Our evaluation sheds light on the limitations of state-of-the-art disassembly tools and points out potential directions for improvement. To engage the community, we release the data set, and the related scripts at https://github.com/valour01/arm_disasssembler_study.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {401–414},
numpages = {14},
keywords = {Empirical Study, ARM Architecture, Disassembly Tools},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3213846.3213857,
author = {Li, Li and Bissyand\'{e}, Tegawend\'{e} F. and Wang, Haoyu and Klein, Jacques},
title = {CiD: Automating the Detection of API-Related Compatibility Issues in Android Apps},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213857},
doi = {10.1145/3213846.3213857},
abstract = {The Android Application Programming Interface provides the necessary building blocks for app developers to harness the functionalities of the Android devices, including for interacting with services and accessing hardware. This API thus evolves rapidly to meet new requirements for security, performance and advanced features, creating a race for developers to update apps. Unfortunately, given the extent of the API and the lack of automated alerts on important changes, Android apps are suffered from API-related compatibility issues. These issues can manifest themselves as runtime crashes creating a poor user experience. We propose in this paper an automated approach named CiD for systematically modelling the lifecycle of the Android APIs and analysing app bytecode to flag usages that can lead to potential compatibility issues. We demonstrate the usefulness of CiD by helping developers repair their apps, and we validate that our tool outperforms the state-of-the-art on benchmark apps that take into account several challenges for automatic detection.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–163},
numpages = {11},
keywords = {API-related Compatibility Issue, Android, Framework Base, CiD},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

