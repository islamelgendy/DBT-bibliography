@inproceedings{10.1145/2155620.2155645,
author = {Zhang, Jiaqi and Xiong, Weiwei and Liu, Yang and Park, Soyeon and Zhou, Yuanyuan and Ma, Zhiqiang},
title = {ATDetector: Improving the Accuracy of a Commercial Data Race Detector by Identifying Address Transfer},
year = {2011},
isbn = {9781450310536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2155620.2155645},
doi = {10.1145/2155620.2155645},
abstract = {In order to take advantage of multi-core hardware, more and more applications are becoming multi-threaded. Unfortunately concurrent programs are prone to bugs, such as data races. Recently much work has been devoted to detecting data races in multi-threaded programs. Most tools, however, require the accurate knowledge of synchronizations in the program, and may otherwise suffer from false positives in race detection, limiting their usability. To address this problem, some tools such as Intel® Inspector provide mechanisms for suppressing false positives and/or annotating synchronizations not automatically recognized by the tools. However, they require users' input or even changes of the source code.We took a different approach to address this problem. More specifically, we first used a state-of-the-art commercial data race detector, namely Intel® Inspector on 17 applications of various types including 5 servers, 5 client/desktop applications, and 7 scientific ones, without utilizing any suppression or annotation mechanisms provided by the product that need users' input. We examined a total of 1420 false data races and identified two major root causes including address transfer, where one thread passes memory address to another thread. We found more than 62% false data races were caused by address transfer. Based on this observation, we designed and implemented an algorithm that automatically identify address transfer and use the information to prune the false data races. Our evaluation with 8 real-world applications shows that it can effectively prune all false data races caused by unrecognized address transfers, without eliminating any true data race that was originally reported.},
booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {206–215},
numpages = {10},
keywords = {data race, false positive, concurrency bug},
location = {Porto Alegre, Brazil},
series = {MICRO-44}
}

@inproceedings{10.5555/339771.339839,
author = {Perkins, J. A. and Gorzela, R. S.},
title = {Programming Paradigms Involving Exceptions: A Software Quality Approach},
year = {1987},
publisher = {George Washington University},
address = {USA},
booktitle = {Proceedings of the Joint Ada Conference Fifth National Conference on Ada Technology and Fourth Washington Ada Symposium},
pages = {142–150},
numpages = {9},
location = {Arlington, Virginia, USA},
series = {WADAS '87}
}

@article{10.1145/1824760.1824761,
author = {Ko, Amy J. and Myers, Brad A.},
title = {Extracting and Answering Why and Why Not Questions about Java Program Output},
year = {2010},
issue_date = {August 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1824760.1824761},
doi = {10.1145/1824760.1824761},
abstract = {When software developers want to understand the reason for a program's behavior, they must translate their questions about the behavior into a series of questions about code, speculating about the causes in the process. The Whyline is a new kind of debugging tool that avoids such speculation by instead enabling developers to select a question about program output from a set of “why did and why didn't” questions extracted from the program's code and execution. The tool then finds one or more possible explanations for the output in question. These explanations are derived using a static and dynamic slicing, precise call graphs, reachability analyses, and new algorithms for determining potential sources of values. Evaluations of the tool on two debugging tasks showed that developers with the Whyline were three times more successful and twice as fast at debugging, compared to developers with traditional breakpoint debuggers. The tool has the potential to simplify debugging and program understanding in many software development contexts.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {4},
numpages = {36},
keywords = {Whyline, debugging, questions}
}

@inproceedings{10.1145/2950290.2950357,
author = {Xu, Zhaogui and Liu, Peng and Zhang, Xiangyu and Xu, Baowen},
title = {Python Predictive Analysis for Bug Detection},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950357},
doi = {10.1145/2950290.2950357},
abstract = { Python is a popular dynamic language that allows quick software development. However, Python program analysis engines are largely lacking. In this paper, we present a Python predictive analysis. It first collects the trace of an execution, and then encodes the trace and unexecuted branches to symbolic constraints. Symbolic variables are introduced to denote input values, their dynamic types, and attribute sets, to reason about their variations. Solving the constraints identifies bugs and their triggering inputs. Our evaluation shows that the technique is highly effective in analyzing real-world complex programs with a lot of dynamic features and external library calls, due to its sophisticated encoding design based on traces. It identifies 46 bugs from 11 real-world projects, with 16 new bugs. All reported bugs are true positives. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {121–132},
numpages = {12},
keywords = {Python, Debugging, Dynamic Language, Predictive Analysis},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inbook{10.1145/3453483.3454084,
author = {Takashima, Yoshiki and Martins, Ruben and Jia, Limin and P\u{a}s\u{a}reanu, Corina S.},
title = {SyRust: Automatic Testing of Rust Libraries with Semantic-Aware Program Synthesis},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454084},
abstract = {Rust’s type system ensures the safety of Rust programs; however, programmers can side-step some of the strict typing rules by using the unsafe keyword. A common use of unsafe Rust is by libraries. Bugs in these libraries undermine the safety of the entire Rust program. Therefore, it is crucial to thoroughly test library APIs to rule out bugs. Unfortunately, such testing relies on programmers to manually construct test cases, which is an inefficient and ineffective process. The goal of this paper is to develop a methodology for automatically generating Rust programs to effectively test Rust library APIs. The main challenge is to synthesize well-typed Rust programs to account for proper chaining of API calls and Rust’s ownership type system and polymorphic types. We develop a program synthesis technique for Rust library API testing, which relies on a novel logical encoding of typing constraints from Rust’s ownership type system. We implement SyRust, a testing framework for Rust libraries that automatically synthesizes semantically valid test cases. Our experiments on 30 popular open-source Rust libraries found 4 new bugs.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {899–913},
numpages = {15}
}

@inproceedings{10.1145/2393596.2393666,
author = {Anand, Saswat and Naik, Mayur and Harrold, Mary Jean and Yang, Hongseok},
title = {Automated Concolic Testing of Smartphone Apps},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393666},
doi = {10.1145/2393596.2393666},
abstract = {We present an algorithm and a system for generating input events to exercise smartphone apps. Our approach is based on concolic testing and generates sequences of events automatically and systematically. It alleviates the path-explosion problem by checking a condition on program executions that identifies subsumption between different event sequences. We also describe our implementation of the approach for Android, the most popular smartphone app platform, and the results of an evaluation that demonstrates its effectiveness on five Android apps.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {59},
numpages = {11},
keywords = {testing event-driven programs, GUI testing, Android},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1145/3377814.3381711,
author = {Pang, Candy and Hindle, Abram and Barbosa, Denilson},
title = {Understanding Devops Education with Grounded Theory},
year = {2020},
isbn = {9781450371247},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377814.3381711},
doi = {10.1145/3377814.3381711},
abstract = {DevOps stands for Development-Operations. It arises from the IT industry as a movement aligning development and operations teams. DevOps is broadly recognized as an IT standard, and there is high demand for DevOps practitioners in industry. Since ACM &amp; IEEE suggest that undergraduate computer science curricula "must adequately prepare [students] for the workforce", we studied whether undergraduates acquired adequate DevOps skills to fulfill the demand for DevOps practitioners in industry. We employed Grounded Theory (GT), a social science qualitative research methodology, to study DevOps education from academic and industrial perspectives. In academia, academics were not motivated to learn or adopt DevOps, and we did not find strong evidence of academics teaching DevOps. Academics need incentives to adopt DevOps, in order to stimulate interest in teaching DevOps. In industry, DevOps practitioners lack clearly defined roles and responsibilities, for the DevOps topic is diverse and growing too fast. Therefore, practitioners can only learn DevOps through hands-on working experience. As a result, academic institutions should provide fundamental DevOps education (in culture, procedure, and technology) to prepare students for their future DevOps advancement in industry. Based on our findings, we proposed five groups of future studies to advance DevOps education in academia.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Software Engineering Education and Training},
pages = {107–118},
numpages = {12},
keywords = {continuous delivery, continuous integration, grounded theory, software engineering, devops, education},
location = {Seoul, South Korea},
series = {ICSE-SEET '20}
}

@inproceedings{10.1145/3183440.3190334,
author = {Gulzar, Muhammad Ali},
title = {Interactive and Automated Debugging for Big Data Analytics},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3190334},
doi = {10.1145/3183440.3190334},
abstract = {An abundance of data in many disciplines of science, engineering, national security, health care, and business has led to the emerging field of Big Data Analytics that run in a cloud computing environment. To process massive quantities of data in the cloud, developers leverage Data-Intensive Scalable Computing (DISC) systems such as Google's MapReduce, Hadoop, and Spark.Currently, developers do not have easy means to debug DISC applications. The use of cloud computing makes application development feel more like batch jobs and the nature of debugging is therefore post-mortem. Developers of big data applications write code that implements a data processing pipeline and test it on their local workstation with a small sample data, downloaded from a TB-scale data warehouse. They cross fingers and hope that the program works in the expensive production cloud. When a job fails or they get a suspicious result, data scientists spend hours guessing at the source of the error, digging through post-mortem logs. In such cases, the data scientists may want to pinpoint the root cause of errors by investigating a subset of corresponding input records.The vision of my work is to provide interactive, real-time and automated debugging services for big data processing programs in modern DISC systems with minimum performance impact. My work investigates the following research questions in the context of big data analytics: (1) What are the necessary debugging primitives for interactive big data processing? (2) What scalable fault localization algorithms are needed to help the user to localize and characterize the root causes of errors? (3) How can we improve testing efficiency during iterative development of DISC applications by reasoning the semantics of dataflow operators and user-defined functions used inside dataflow operators in tandem?To answer these questions, we synthesize and innovate ideas from software engineering, big data systems, and program analysis, and coordinate innovations across the software stack from the user-facing API all the way down to the systems infrastructure.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {509–511},
numpages = {3},
keywords = {fault localization, data provenance, data-intensive scalable computing (DISC), automated debugging, big data, and data cleaning, test minimization, debugging and testing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3464970.3468413,
author = {Mattis, Toni and Beckmann, Tom and Rein, Patrick and Hirschfeld, Robert},
title = {First-Class Concepts: Reifying Architectural Knowledge beyond the Dominant Decomposition},
year = {2021},
isbn = {9781450385428},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3464970.3468413},
doi = {10.1145/3464970.3468413},
abstract = {In software engineering, programs are ideally partitioned into independently maintainable and understandable modules. As a system grows, its architecture gradually loses the capability to modularly accommodate new concepts. While refactoring is expensive and the language might lack appropriate primary language constructs to express certain cross-cutting concerns, programmers are still able to explain and delineate convoluted concepts through secondary means: code comments, use of whitespace and arrangement of code, documentation, or communicating tacit knowledge. Secondary constructs are easy to change and provide high flexibility in communicating cross-cutting concerns and other concepts among programmers. However, they have no reified representation that can be explored and maintained through tools. In this exploratory work, we discuss novel ways to express a wide range of concepts, including cross-cutting concerns, patterns, and lifecycle artifacts independently of the dominant decomposition imposed by an existing architecture. Our concepts are first-class objects inside the programming environment that retain the capability to change as easily as code comments. We explore new tools that allow programmers to view and change programs from conceptual perspectives rather than scattering their attention across existing modules. Our designs are geared towards facilitating multiple secondary perspectives on a system to co-exist alongside the original architecture, hence making it easier to explore, understand, and explain complex contexts and narratives not expressible in traditional modularity constructs.},
booktitle = {Proceedings of the 13th ACM International Workshop on Context-Oriented Programming and Advanced Modularity},
pages = {9–15},
numpages = {7},
keywords = {remodularization, exploratory programming, program comprehension, modularity, software engineering},
location = {Virtual, Denmark},
series = {COP 2021}
}

@inproceedings{10.1145/2642937.2642972,
author = {Wang, Wenwen and Wang, Zhenjiang and Wu, Chenggang and Yew, Pen-Chung and Shen, Xipeng and Yuan, Xiang and Li, Jianjun and Feng, Xiaobing and Guan, Yong},
title = {Localization of Concurrency Bugs Using Shared Memory Access Pairs},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642972},
doi = {10.1145/2642937.2642972},
abstract = {We propose an effective approach to automatically localize buggy shared memory accesses that trigger concurrency bugs. Compared to existing approaches, our approach has two advantages. First, as long as enough successful runs of a concurrent program are collected, our approach can localize buggy shared memory accesses even with only one single failed run captured, as opposed to the requirement of capturing multiple failed runs in existing approaches. This is a significant advantage because it is more difficult to capture the elusive failed runs than the successful runs in practice. Second, our approach exhibits more precise bug localization results because it also captures buggy shared memory accesses in those failed runs that terminate prematurely, which are often neglected in existing approaches. Based on this proposed approach, we also implement a prototype, named LOCON. Evaluation results on 16 common concurrency bugs show that all buggy shared memory accesses that trigger these bugs can be precisely localized by LOCON with only one failed run captured.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {611–622},
numpages = {12},
keywords = {localization, shared memory access pair, concurrency bug},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/2664243.2664282,
author = {Pomonis, Marios and Petsios, Theofilos and Jee, Kangkook and Polychronakis, Michalis and Keromytis, Angelos D.},
title = {IntFlow: Improving the Accuracy of Arithmetic Error Detection Using Information Flow Tracking},
year = {2014},
isbn = {9781450330053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664243.2664282},
doi = {10.1145/2664243.2664282},
abstract = {Integer overflow and underflow, signedness conversion, and other types of arithmetic errors in C/C++ programs are among the most common software flaws that result in exploitable vulnerabilities. Despite significant advances in automating the detection of arithmetic errors, existing tools have not seen widespread adoption mainly due to their increased number of false positives. Developers rely on wrap-around counters, bit shifts, and other language constructs for performance optimizations and code compactness, but those same constructs, along with incorrect assumptions and conditions of undefined behavior, are often the main cause of severe vulnerabilities. Accurate differentiation between legitimate and erroneous uses of arithmetic language intricacies thus remains an open problem.As a step towards addressing this issue, we present IntFlow, an accurate arithmetic error detection tool that combines static information flow tracking and dynamic program analysis. By associating sources of untrusted input with the identified arithmetic errors, IntFlow differentiates between non-critical, possibly developer-intended undefined arithmetic operations, and potentially exploitable arithmetic bugs. IntFlow examines a broad set of integer errors, covering almost all cases of C/C++ undefined behaviors, and achieves high error detection coverage. We evaluated IntFlow using the SPEC benchmarks and a series of real-world applications, and measured its effectiveness in detecting arithmetic error vulnerabilities and reducing false positives. IntFlow successfully detected all real-world vulnerabilities for the tested applications and achieved a reduction of 89% in false positives over standalone static code instrumentation.},
booktitle = {Proceedings of the 30th Annual Computer Security Applications Conference},
pages = {416–425},
numpages = {10},
keywords = {information flow tracking, static analysis, arithmetic errors},
location = {New Orleans, Louisiana, USA},
series = {ACSAC '14}
}

@inproceedings{10.1145/3324884.3416641,
author = {Zhang, Qian and Wang, Jiyuan and Gulzar, Muhammad Ali and Padhye, Rohan and Kim, Miryung},
title = {BigFuzz: Efficient Fuzz Testing for Data Analytics Using Framework Abstraction},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416641},
doi = {10.1145/3324884.3416641},
abstract = {As big data analytics become increasingly popular, data-intensive scalable computing (DISC) systems help address the scalability issue of handling large data. However, automated testing for such data-centric applications is challenging, because data is often incomplete, continuously evolving, and hard to know a priori. Fuzz testing has been proven to be highly effective in other domains such as security; however, it is nontrivial to apply such traditional fuzzing to big data analytics directly for three reasons: (1) the long latency of DISC systems prohibits the applicability of fuzzing: na\"{\i}ve fuzzing would spend 98% of the time in setting up a test environment; (2) conventional branch coverage is unlikely to scale to DISC applications because most binary code comes from the framework implementation such as Apache Spark; and (3) random bit or byte level mutations can hardly generate meaningful data, which fails to reveal real-world application bugs.We propose a novel coverage-guided fuzz testing tool for big data analytics, called BigFuzz. The key essence of our approach is that: (a) we focus on exercising application logic as opposed to increasing framework code coverage by abstracting the DISC framework using specifications. BigFuzz performs automated source to source transformations to construct an equivalent DISC application suitable for fast test generation, and (b) we design schema-aware data mutation operators based on our in-depth study of DISC application error types. BigFuzz speeds up the fuzzing time by 78 to 1477X compared to random fuzzing, improves application code coverage by 20% to 271%, and achieves 33% to 157% improvement in detecting application errors. When compared to the state of the art that uses symbolic execution to test big data analytics, BigFuzz is applicable to twice more programs and can find 81% more bugs.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {722–733},
numpages = {12},
keywords = {test generation, big data analytics, fuzz testing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3106237.3106286,
author = {Garcia, Joshua and Hammad, Mahmoud and Ghorbani, Negar and Malek, Sam},
title = {Automatic Generation of Inter-Component Communication Exploits for Android Applications},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106286},
doi = {10.1145/3106237.3106286},
abstract = {Although a wide variety of approaches identify vulnerabilities in Android apps, none attempt to determine exploitability of those vulnerabilities. Exploitability can aid in reducing false positives of vulnerability analysis, and can help engineers triage bugs. Specifically, one of the main attack vectors of Android apps is their inter-component communication interface, where apps may receive messages called Intents. In this paper, we provide the first approach for automatically generating exploits for Android apps, called LetterBomb, relying on a combined path-sensitive symbolic execution-based static analysis, and the use of software instrumentation and test oracles. We run LetterBomb on 10,000 Android apps from Google Play, where we identify 181 exploits from 835 vulnerable apps. Compared to a state-of-the-art detection approach for three ICC-based vulnerabilities, LetterBomb obtains 33%-60% more vulnerabilities at a 6.66 to 7 times faster speed.},
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {661–671},
numpages = {11},
keywords = {exploit, Android, test oracle, vulnerability, test generation},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/169627.169851,
author = {Ries, B. and Anderson, R. and Auld, W. and Breazeal, D. and Callaghan, K. and Richards, E. and Smith, W.},
title = {The Paragon Performance Monitoring Environment},
year = {1993},
isbn = {0818643404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/169627.169851},
doi = {10.1145/169627.169851},
booktitle = {Proceedings of the 1993 ACM/IEEE Conference on Supercomputing},
pages = {850–859},
numpages = {10},
location = {Portland, Oregon, USA},
series = {Supercomputing '93}
}

@inproceedings{10.1145/3377811.3380423,
author = {Tao, Guanhong and Ma, Shiqing and Liu, Yingqi and Xu, Qiuling and Zhang, Xiangyu},
title = {TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380423},
doi = {10.1145/3377811.3380423},
abstract = {Recurrent Neural Networks (RNN) can deal with (textual) input with various length and hence have a lot of applications in software systems and software engineering applications. RNNs depend on word embeddings that are usually pre-trained by third parties to encode textual inputs to numerical values. It is well known that problematic word embeddings can lead to low model accuracy. In this paper, we propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples. We then leverage the diagnosis results as guidance to harden/repair the embeddings. Our experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {986–998},
numpages = {13},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/2700094,
author = {Watkins, Lanier and Robinson, William H. and Beyah, Raheem},
title = {Using Network Traffic to Infer Hardware State: A Kernel-Level Investigation},
year = {2015},
issue_date = {May 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2700094},
doi = {10.1145/2700094},
abstract = {In this article, we illustrate that the boundary of a general-purpose node can be extended into the network by extracting information from network traffic generated by that general-purpose node to infer the state of its hardware components. This information is represented in a delay signature latent within the network traffic. In contrast, the traditional approach to determine the internal state of a node’s resources meant that a software application with internal processes had to be resident on the node. The aforementioned delay signature is the keystone that provides a correlation between network traffic and the internal state of the source node. We characterize this delay signature by (1) identifying the different types of assembly language instructions that source this delay and (2) describing how architectural techniques, such as instruction pipelining and caching, give rise to this delay signature. In theory, highly utilized nodes (due to multiple threads) will contain excessive context switching and contention for shared resources. One important shared resource is main memory, and excessive use of this resource by applications and internal processes eventually leads to a decrease in cache efficiency that eventually stalls the instruction pipeline. Our results support this theory; specifically, we have observed that excessive context switching in active applications increases the effective memory access time and wastes precious CPU cycles, thus adding additional delay to the execution of load, store, and other instructions. Because the operating system (OS) kernel accesses memory to send network packets, the delay signature is induced into network traffic in situations where user-level utilization is high. We demonstrate this theory in two case studies: (1) resource discovery in cluster grids and (2) network-based detection of bitcoin mining on compromised nodes.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {apr},
articleno = {55},
numpages = {22},
keywords = {grid computing, LEON4 processor, passive resource discovery, clusters assembly language instructions}
}

@inproceedings{10.1145/3236024.3236085,
author = {Henkel, Jordan and Lahiri, Shuvendu K. and Liblit, Ben and Reps, Thomas},
title = {Code Vectors: Understanding Programs through Embedded Abstracted Symbolic Traces},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236085},
doi = {10.1145/3236024.3236085},
abstract = {With the rise of machine learning, there is a great deal of interest in treating programs as data to be fed to learning algorithms. However, programs do not start off in a form that is immediately amenable to most off-the-shelf learning techniques. Instead, it is necessary to transform the program to a suitable representation before a learning technique can be applied.  In this paper, we use abstractions of traces obtained from symbolic execution of a program as a representation for learning word embeddings. We trained a variety of word embeddings under hundreds of parameterizations, and evaluated each learned embedding on a suite of different tasks. In our evaluation, we obtain 93% top-1 accuracy on a benchmark consisting of over 19,000 API-usage analogies extracted from the Linux kernel. In addition, we show that embeddings learned from (mainly) semantic abstractions provide nearly triple the accuracy of those learned from (mainly) syntactic abstractions.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {163–174},
numpages = {12},
keywords = {Program Understanding, Linux, Word Embeddings, Analogical Reasoning},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2038916.2038925,
author = {Patil, Swapnil and Polte, Milo and Ren, Kai and Tantisiriroj, Wittawat and Xiao, Lin and L\'{o}pez, Julio and Gibson, Garth and Fuchs, Adam and Rinaldi, Billie},
title = {YCSB++: Benchmarking and Performance Debugging Advanced Features in Scalable Table Stores},
year = {2011},
isbn = {9781450309769},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2038916.2038925},
doi = {10.1145/2038916.2038925},
abstract = {Inspired by Google's BigTable, a variety of scalable, semi-structured, weak-semantic table stores have been developed and optimized for different priorities such as query speed, ingest speed, availability, and interactivity. As these systems mature, performance benchmarking will advance from measuring the rate of simple workloads to understanding and debugging the performance of advanced features such as ingest speed-up techniques and function shipping filters from client to servers. This paper describes YCSB++, a set of extensions to the Yahoo! Cloud Serving Benchmark (YCSB) to improve performance understanding and debugging of these advanced features. YCSB++ includes multi-tester coordination for increased load and eventual consistency measurement, multi-phase workloads to quantify the consequences of work deferment and the benefits of anticipatory configuration optimization such as B-tree pre-splitting or bulk loading, and abstract APIs for explicit incorporation of advanced features in benchmark tests. To enhance performance debugging, we customized an existing cluster monitoring tool to gather the internal statistics of YCSB++, table stores, system services like HDFS, and operating systems, and to offer easy post-test correlation and reporting of performance behaviors. YCSB++ features are illustrated in case studies of two BigTable-like table stores, Apache HBase and Accumulo, developed to emphasize high ingest rates and finegrained security.},
booktitle = {Proceedings of the 2nd ACM Symposium on Cloud Computing},
articleno = {9},
numpages = {14},
keywords = {YCSB, scalable table stores, NoSQL, benchmarking},
location = {Cascais, Portugal},
series = {SOCC '11}
}

@book{10.1145/3368274,
author = {Halvorson, Michael J.},
title = {Code Nation: Personal Computing and the Learn to Program Movement in America},
year = {2020},
isbn = {9781450377584},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {Code Nation explores the rise of software development as a social, cultural, and technical phenomenon in American history. The movement germinated in government and university labs during the 1950s, gained momentum through corporate and counterculture experiments in the 1960s and 1970s, and became a broad-based computer literacy movement in the 1980s. As personal computing came to the fore, learning to program was transformed by a groundswell of popular enthusiasm, exciting new platforms, and an array of commercial practices that have been further amplified by distributed computing and the Internet. The resulting society can be depicted as a “Code Nation”—a globally-connected world that is saturated with computer technology and enchanted by software and its creation.Code Nation is a new history of personal computing that emphasizes the technical and business challenges that software developers faced when building applications for CP/M, MS-DOS, UNIX, Microsoft Windows, the Apple Macintosh, and other emerging platforms. It is a popular history of computing that explores the experiences of novice computer users, tinkerers, hackers, and power users, as well as the ideals and aspirations of leading computer scientists, engineers, educators, and entrepreneurs. Computer book and magazine publishers also played important, if overlooked, roles in the diffusion of new technical skills, and this book highlights their creative work and influence.Code Nation offers a “behind-the-scenes” look at application and operating-system programming practices, the diversity of historic computer languages, the rise of user communities, early attempts to market PC software, and the origins of “enterprise” computing systems. Code samples and over 80 historic photographs support the text. The book concludes with an assessment of contemporary efforts to teach computational thinking to young people.}
}

@inproceedings{10.1145/3129790.3129822,
author = {Nakagawa, Elisa Yumi and Allian, Ana and Oliveira, Brauner and Sena, Bruno and Paes, Carlos and Lana, Cristiane and Feitosa, Daniel and Santos, Daniel and Zaniro, D\^{e}nis and Dias, Di\'{o}genes and Horita, Fl\'{a}vio and Affonso, Frank Jos\'{e} and Abdalla, Gabriel and Vicente, Isabella and Duarte, Leonardo and Felizardo, Katia and Garc\'{e}s, Lina and Oliveira, Lucas and Gon\c{c}alves, Marcelo and Morais, Maria Gabriela and Guessi, Milena and Silva, Nilson and Bianchi, Thiago and Volpato, Tiago and Neto, Valdemar V. Graciano and Zani, Vinicius and Manzano, Wallace},
title = {Software Architecture and Reference Architecture of Software-Intensive Systems and Systems-of-Systems: Contributions to the State of the Art},
year = {2017},
isbn = {9781450352178},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3129790.3129822},
doi = {10.1145/3129790.3129822},
abstract = {Complex software-intensive systems are more and more required as a solution for diverse critical application domains; at the same time, software architecture and also reference architecture have attracted attention as means to more adequately produce and evolve such systems. The main goal of this paper is to summarize our principal contributions in software architecture and reference architecture of software-intensive systems, including Systems-of-Systems. We intend this work can also inspire the opening of other related research lines towards founding the sustainability of such software-intensive systems.},
booktitle = {Proceedings of the 11th European Conference on Software Architecture: Companion Proceedings},
pages = {4–11},
numpages = {8},
keywords = {software architecture, system-of-systems, reference architecture},
location = {Canterbury, United Kingdom},
series = {ECSA '17}
}

