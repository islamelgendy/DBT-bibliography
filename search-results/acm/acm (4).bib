@inproceedings{10.1145/2491411.2491415,
author = {Nagappan, Meiyappan and Zimmermann, Thomas and Bird, Christian},
title = {Diversity in Software Engineering Research},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491415},
doi = {10.1145/2491411.2491415},
abstract = { One of the goals of software engineering research is to achieve generality: Are the phenomena found in a few projects reflective of others? Will a technique perform as well on projects other than the projects it is evaluated on? While it is common sense to select a sample that is representative of a population, the importance of diversity is often overlooked, yet as important. In this paper, we combine ideas from representativeness and diversity and introduce a measure called sample coverage, defined as the percentage of projects in a population that are similar to the given sample. We introduce algorithms to compute the sample coverage for a given set of projects and to select the projects that increase the coverage the most. We demonstrate our technique on research presented over the span of two years at ICSE and FSE with respect to a population of 20,000 active open source projects monitored by Ohloh.net. Knowing the coverage of a sample enhances our ability to reason about the findings of a study. Furthermore, we propose reporting guidelines for research: in addition to coverage scores, papers should discuss the target population of the research (universe) and dimensions that potentially can influence the outcomes of a research (space). },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {466–476},
numpages = {11},
keywords = {Representativeness, Sampling, Coverage, Diversity},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inbook{10.1145/3460319.3464834,
author = {Elsner, Daniel and Hauer, Florian and Pretschner, Alexander and Reimer, Silke},
title = {Empirically Evaluating Readily Available Information for Regression Test Optimization in Continuous Integration},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464834},
abstract = {Regression test selection (RTS) and prioritization (RTP) techniques aim to reduce testing efforts and developer feedback time after a change to the code base. Using various information sources, including test traces, build dependencies, version control data, and test histories, they have been shown to be effective. However, not all of these sources are guaranteed to be available and accessible for arbitrary continuous integration (CI) environments. In contrast, metadata from version control systems (VCSs) and CI systems are readily available and inexpensive. Yet, corresponding RTP and RTS techniques are scattered across research and often only evaluated on synthetic faults or in a specific industrial context. It is cumbersome for practitioners to identify insights that apply to their context, let alone to calibrate associated parameters for maximum cost-effectiveness. This paper consolidates existing work on RTP and unsafe RTS into an actionable methodology to build and evaluate such approaches that exclusively rely on CI and VCS metadata. To investigate how these approaches from prior research compare in heterogeneous settings, we apply the methodology in a large-scale empirical study on a set of 23 projects covering 37,000 CI logs and 76,000 VCS commits. We find that these approaches significantly outperform established RTP baselines and, while still triggering 90% of the failures, we show that practitioners can expect to save on average 84% of test execution time for unsafe RTS. We also find that it can be beneficial to limit training data, features from test history work better than change-based features, and, somewhat surprisingly, simple and well-known heuristics often outperform complex machine-learned models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {491–504},
numpages = {14}
}

@inproceedings{10.1145/3395363.3397370,
author = {Liu, Zhibo and Wang, Shuai},
title = {How Far We Have Come: Testing Decompilation Correctness of C Decompilers},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397370},
doi = {10.1145/3395363.3397370},
abstract = {A C decompiler converts an executable (the output from a C compiler) into source code. The recovered C source code, once recompiled, will produce an executable with the same functionality as the original executable. With over twenty years of development, C decompilers have been widely used in production to support reverse engineering applications, including legacy software migration, security retrofitting, software comprehension, and to act as the first step in launching adversarial software exploitations. As the paramount component and the trust base in numerous cybersecurity tasks, C decompilers have enabled the analysis of malware, ransomware, and promoted cybersecurity professionals’ understanding of vulnerabilities in real-world systems.  In contrast to this flourishing market, our observation is that in academia, outputs of C decompilers (i.e., recovered C source code) are still not extensively used. Instead, the intermediate representations are often more desired for usage when developing applications such as binary security retrofitting. We acknowledge that such conservative approaches in academia are a result of widespread and pessimistic views on the decompilation correctness. However, in conventional software engineering and security research, how much of a problem is, for instance, reusing a piece of simple legacy code by taking the output of modern C decompilers?  In this work, we test decompilation correctness to present an up-to-date understanding regarding modern C decompilers. We detected a total of 1,423 inputs that can trigger decompilation errors from four popular decompilers, and with extensive manual effort, we identified 13 bugs in two open-source decompilers. Our findings show that the overly pessimistic view of decompilation correctness leads researchers to underestimate the potential of modern decompilers; the state-of-the-art decompilers certainly care about the functional correctness, and they are making promising progress. However, some tasks that have been studied for years in academia, such as type inference and optimization, still impede C decompilers from generating quality outputs more than is reflected in the literature. These issues rarely receive enough attention and can lead to great confusion that misleads users.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {475–487},
numpages = {13},
keywords = {Reverse Engineering, Decompiler, Software Testing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3092703.3098229,
author = {Hall\'{e}, Sylvain and Khoury, Rapha\"{e}l},
title = {SealTest: A Simple Library for Test Sequence Generation},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098229},
doi = {10.1145/3092703.3098229},
abstract = { SealTest is a Java library for generating test sequences based on a formal specification. It allows a user to easily define a wide range of coverage metrics using multiple specification languages. Its simple and generic architecture makes it a useful testing tool for dynamic software systems, as well as an appropriate research testbed for implementing and experimentally comparing test sequence generation algorithms. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {392–395},
numpages = {4},
keywords = {software testing, test sequence generation, UML statecharts, temporal logic},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/3425174.3425175,
author = {Sales, Camila Pereira and de Santiago J\'{u}nior, Valdivino Alexandre},
title = {Investigating Multi and Many-Objective Metaheuristics to Support Software Integration Testing},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425175},
doi = {10.1145/3425174.3425175},
abstract = {In spite of the fact that Search-Based Software Testing (SBST) is a very appealing field today, there are few studies that deal with software integration testing and, even so, most of these works are not truly related to the generation of test cases to this testing level. In this paper, we present a method, InMeHy, which aims at investigating the use of metaheuristics to derive integration test cases based on C++ source code. A graph is created based on the code which represents the integration of several classes of the application. Multi and Many-Objective metaheuristics (Evolutionary Algorithms) were considered to generate integration test cases and were assessed via three quality indicators. Results show that the traditional Indicator-Based Evolutionary Algorithm (IBEA) turned out to be the best out of four algorithms evaluated, including newer Many-Objective strategies such as Nondominated Sorting Genetic Algorithm-III (NSGA-III).},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {1–10},
numpages = {10},
keywords = {Metaheuristics, C++ Applications, Test Automation, Search-Based Software Testing, Integration Testing},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{10.1145/3395363.3402644,
author = {Xue, Feng},
title = {Automated Mobile Apps Testing from Visual Perspective},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3402644},
doi = {10.1145/3395363.3402644},
abstract = {The current implementation of automated mobile apps testing generally relies on internal program information, such as reading code or GUI layout files, capturing event streams. This paper proposes an approach of automated mobile apps testing from a completely visual perspective. It uses computer vision technology to enable computer to judge the internal functions from the external GUI information of mobile apps as we humans do and generates test strategy for execution, which improves the interactivity, flexibility, and authenticity of testing. We believe that this vision-based testing approach will further help alleviate the contradiction between the current huge test requirements of mobile apps and the relatively lack of testers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {577–581},
numpages = {5},
keywords = {Mobile applications, Test automation, Computer vision, Software testing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2993288.2993295,
author = {Ouriques, Jo\~{a}o F. S. and Cartaxo, Emanuela G. and Machado, Patr\'{\i}cia D. L. and Neto, Francisco G. O. and Coutinho, Ana E. V. B.},
title = {On the Use of Fault Abstractions for Assessing System Test Case Prioritization Techniques},
year = {2016},
isbn = {9781450347662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993288.2993295},
doi = {10.1145/2993288.2993295},
abstract = {Empirical studies often evaluate Test Case Prioritization (TCP) techniques by measuring their ability to uncover faults as early as possible. In this context, techniques reorder test cases using different sources of information. These prioritizations are evaluated considering faults detected by failure occurrences during testing execution. While this accounts for validity, results can be biased. For instance, conclusions regarding a TCP technique's behavior may be affected by the number of failures, faults and, in turn, the failure-fault relationship. We propose the use of fault abstractions, which relate faults to the related test cases that fail, to exercise techniques on preliminary empirical studies of system level TCP techniques. Rather than modeling specific classes of faults, our strategy is based on identifying cliques of test cases that are related according to a distance function as a fault abstraction, assuming that the test cases can fail due to a number of faults in common. Our strategy calculate these fault abstractions based on the set of maximal cliques obtained from test case distance graphs along with a set of essential test cases. Therefore, one can sample fault abstractions respecting different failure-fault relationships and apply on empirical studies. In our studies, even though our strategy is not predictive, a significant amount of fault abstractions calculated by the proposed technique corresponded to actual reported faults. Therefore Fault abstractions may represent the relationship between faults and failures to assess TCP techniques when none or a small amount of artifacts is available to perform a preliminary empirical evaluation.},
booktitle = {Proceedings of the 1st Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {7},
numpages = {10},
keywords = {Empirical Software Engineering, Fault Abstractions, Software Testing, System Testing, Test Case Prioritization},
location = {Maringa, Parana, Brazil},
series = {SAST}
}

@inproceedings{10.1145/3092703.3092731,
author = {Zhang, Mengshi and Li, Xia and Zhang, Lingming and Khurshid, Sarfraz},
title = {Boosting Spectrum-Based Fault Localization Using PageRank},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3092731},
doi = {10.1145/3092703.3092731},
abstract = { Manual debugging is notoriously tedious and time consuming. Therefore, various automated fault localization techniques have been proposed to help with manual debugging. Among the existing fault localization techniques, spectrum-based fault localization (SBFL) is one of the most widely studied techniques due to being lightweight. A focus of existing SBFL techniques is to consider how to differentiate program source code entities (i.e., one dimension in program spectra); indeed, this focus is aligned with the ultimate goal of finding the faulty lines of code. Our key insight is to enhance existing SBFL techniques by additionally considering how to differentiate tests (i.e., the other dimension in program spectra), which, to the best of our knowledge, has not been studied in prior work.  We present PRFL, a lightweight technique that boosts spectrum-based fault localization by differentiating tests using PageRank algorithm. Given the original program spectrum information, PRFL uses PageRank to recompute the spectrum information by considering the contributions of different tests. Then, traditional SBFL techniques can be applied on the recomputed spectrum information to achieve more effective fault localization. Although simple and lightweight, PRFL has been demonstrated to outperform state-of-the-art SBFL techniques significantly (e.g., ranking 42% more real faults within Top-1 compared with the most effective traditional SBFL technique) with low overhead (e.g., around 2 minute average extra overhead on real faults) on 357 real faults from 5 Defects4J projects and 30692 artificial (i.e., mutation) faults from 87 GitHub projects, demonstrating a promising future for considering the contributions of different tests during fault localization. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {261–272},
numpages = {12},
keywords = {Spectrum-based fault localization, Software testing, PageRank},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/1095242.1095262,
author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
title = {Locating Where Faults Will Be},
year = {2005},
isbn = {1595932577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1095242.1095262},
doi = {10.1145/1095242.1095262},
booktitle = {Proceedings of the 2005 Conference on Diversity in Computing},
pages = {48–50},
numpages = {3},
keywords = {fault-prone, empirical study, software testing, regression model, software faults, prediction},
location = {Albuquerque, New Mexico, USA},
series = {TAPIA '05}
}

@inproceedings{10.1145/3460319.3464810,
author = {Cheng, Runxiang and Zhang, Lingming and Marinov, Darko and Xu, Tianyin},
title = {Test-Case Prioritization for Configuration Testing},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464810},
doi = {10.1145/3460319.3464810},
abstract = {Configuration changes are among the dominant causes of failures of large-scale software system deployment. Given the velocity of configuration changes, typically at the scale of hundreds to thousands of times daily in modern cloud systems, checking these configuration changes is critical to prevent failures due to misconfigurations. Recent work has proposed configuration testing, Ctest, a technique that tests configuration changes together with the code that uses the changed configurations. Ctest can automatically generate a large number of ctests that can effectively detect misconfigurations, including those that are hard to detect by traditional techniques. However, running ctests can take a long time to detect misconfigurations. Inspired by traditional test-case prioritization (TCP) that aims to reorder test executions to speed up detection of regression code faults, we propose to apply TCP to reorder ctests to speed up detection of misconfigurations. We extensively evaluate a total of 84 traditional and novel ctest-specific TCP techniques. The experimental results on five widely used cloud projects demonstrate that TCP can substantially speed up misconfiguration detection. Our study provides guidelines for applying TCP to configuration testing in practice.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {452–465},
numpages = {14},
keywords = {Test prioritization, reliability, configuration, software testing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3460319.3464811,
author = {Zohdinasab, Tahereh and Riccio, Vincenzo and Gambi, Alessio and Tonella, Paolo},
title = {DeepHyperion: Exploring the Feature Space of Deep Learning-Based Systems through Illumination Search},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464811},
doi = {10.1145/3460319.3464811},
abstract = {Deep Learning (DL) has been successfully applied to a wide range of application domains, including safety-critical ones. Several DL testing approaches have been recently proposed in the literature but none of them aims to assess how different interpretable features of the generated inputs affect the system's behaviour.  In this paper, we resort to Illumination Search to find the highest-performing test cases (i.e., misbehaving and closest to misbehaving), spread across the cells of a map representing the feature space of the system. We introduce a methodology that guides the users of our approach in the tasks of identifying and quantifying the dimensions of the feature space for a given domain. We developed DeepHyperion, a search-based tool for DL systems that illuminates, i.e., explores at large, the feature space, by providing developers with an interpretable feature map where automatically generated inputs are placed along with information about the exposed behaviours.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {79–90},
numpages = {12},
keywords = {software testing, search based software engineering, self-driving cars, deep learning},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3092703.3098237,
author = {Katz, Deborah S.},
title = {Understanding Intended Behavior Using Models of Low-Level Signals},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098237},
doi = {10.1145/3092703.3098237},
abstract = { As software systems increase in complexity and operate with less human supervision, it becomes more difficult to use traditional techniques to detect when software is not behaving as intended. Furthermore, many systems operating today are nondeterministic and operate in unpredictable environments, making it difficult to even define what constitutes correct behavior. I propose a family of novel techniques to model the behavior of executing programs using low-level signals collected during executions. The models provide a basis for predicting whether an execution of the program or program unit under test represents intended behavior. I have demonstrated success with these techniques for detecting faulty and unexpected behavior on small programs. I propose to extend the work to smaller units of large, complex programs. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {424–427},
numpages = {4},
keywords = {Oracle problem, Software testing, Software quality},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/2771783.2771805,
author = {Yatoh, Kohsuke and Sakamoto, Kazunori and Ishikawa, Fuyuki and Honiden, Shinichi},
title = {Feedback-Controlled Random Test Generation},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771805},
doi = {10.1145/2771783.2771805},
abstract = { Feedback-directed random test generation is a widely used technique to generate random method sequences. It leverages feedback to guide generation. However, the validity of feedback guidance has not been challenged yet. In this paper, we investigate the characteristics of feedback-directed random test generation and propose a method that exploits the obtained knowledge that excessive feedback limits the diversity of tests. First, we show that the feedback loop of feedback-directed generation algorithm is a positive feedback loop and amplifies the bias that emerges in the candidate value pool. This over-directs the generation and limits the diversity of generated tests. Thus, limiting the amount of feedback can improve diversity and effectiveness of generated tests. Second, we propose a method named feedback-controlled random test generation, which aggressively controls the feedback in order to promote diversity of generated tests. Experiments on eight different, real-world application libraries indicate that our method increases branch coverage by 78% to 204% over the original feedback-directed algorithm on large-scale utility libraries. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {316–326},
numpages = {11},
keywords = {Test generation, Random testing, Diversity},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/1007512.1007539,
author = {Bowring, James F. and Rehg, James M. and Harrold, Mary Jean},
title = {Active Learning for Automatic Classification of Software Behavior},
year = {2004},
isbn = {1581138202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007512.1007539},
doi = {10.1145/1007512.1007539},
abstract = {A program's behavior is ultimately the collection of all its executions. This collection is diverse, unpredictable, and generally unbounded. Thus it is especially suited to statistical analysis and machine learning techniques. The primary focus of this paper is on the automatic classification of program behavior using execution data. Prior work on classifiers for software engineering adopts a classical batch-learning approach. In contrast, we explore an active-learning paradigm for behavior classification. In active learning, the classifier is trained incrementally on a series of labeled data elements. Secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the Markov property, and that the resultant Markov models of individual program executions can be automatically clustered into effective predictors of program behavior. We present a technique that models program executions as Markov models, and a clustering method for Markov models that aggregates multiple program executions into effective behavior classifiers. We evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.},
booktitle = {Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {195–205},
numpages = {11},
keywords = {Markov models, machine learning, software behavior, software testing},
location = {Boston, Massachusetts, USA},
series = {ISSTA '04}
}

@article{10.1145/1013886.1007539,
author = {Bowring, James F. and Rehg, James M. and Harrold, Mary Jean},
title = {Active Learning for Automatic Classification of Software Behavior},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1013886.1007539},
doi = {10.1145/1013886.1007539},
abstract = {A program's behavior is ultimately the collection of all its executions. This collection is diverse, unpredictable, and generally unbounded. Thus it is especially suited to statistical analysis and machine learning techniques. The primary focus of this paper is on the automatic classification of program behavior using execution data. Prior work on classifiers for software engineering adopts a classical batch-learning approach. In contrast, we explore an active-learning paradigm for behavior classification. In active learning, the classifier is trained incrementally on a series of labeled data elements. Secondly, we explore the thesis that certain features of program behavior are stochastic processes that exhibit the Markov property, and that the resultant Markov models of individual program executions can be automatically clustered into effective predictors of program behavior. We present a technique that models program executions as Markov models, and a clustering method for Markov models that aggregates multiple program executions into effective behavior classifiers. We evaluate an application of active learning to the efficient refinement of our classifiers by conducting three empirical studies that explore a scenario illustrating automated test plan augmentation.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {195–205},
numpages = {11},
keywords = {software behavior, Markov models, software testing, machine learning}
}

@inproceedings{10.1145/2351676.2351682,
author = {Gong, Liang and Lo, David and Jiang, Lingxiao and Zhang, Hongyu},
title = {Diversity Maximization Speedup for Fault Localization},
year = {2012},
isbn = {9781450312042},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2351676.2351682},
doi = {10.1145/2351676.2351682},
abstract = { Fault localization is useful for reducing debugging effort. However, many fault localization techniques require non-trivial number of test cases with oracles, which can determine whether a program behaves correctly for every test input. Test oracle creation is expensive because it can take much manual labeling effort. Given a number of test cases to be executed, it is challenging to minimize the number of test cases requiring manual labeling and in the meantime achieve good fault localization accuracy. To address this challenge, this paper presents a novel test case selection strategy based on Diversity Maximization Speedup (DMS). DMS orders a set of unlabeled test cases in a way that maximizes the effectiveness of a fault localization technique. Developers are only expected to label a much smaller number of test cases along this ordering to achieve good fault localization results. Our experiments with more than 250 bugs from the Software-artifact Infrastructure Repository show (1) that DMS can help existing fault localization techniques to achieve comparable accuracy with on average 67% fewer labeled test cases than previously best test case prioritization techniques, and (2) that given a labeling budget (i.e., a fixed number of labeled test cases), DMS can help existing fault localization techniques reduce their debugging cost (in terms of the amount of code needed to be inspected to locate faults). We conduct hypothesis test and show that the saving of the debugging cost we achieve for the real C programs are statistically significant. },
booktitle = {Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering},
pages = {30–39},
numpages = {10},
keywords = {Fault Localization, Test Case Prioritization},
location = {Essen, Germany},
series = {ASE 2012}
}

@inproceedings{10.1145/2931037.2931054,
author = {Mao, Ke and Harman, Mark and Jia, Yue},
title = {Sapienz: Multi-Objective Automated Testing for Android Applications},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931054},
doi = {10.1145/2931037.2931054},
abstract = { We introduce Sapienz, an approach to Android testing that uses multi-objective search-based testing to automatically explore and optimise test sequences, minimising length, while simultaneously maximising coverage and fault revelation. Sapienz combines random fuzzing, systematic and search-based exploration, exploiting seeding and multi-level instrumentation. Sapienz significantly outperforms (with large effect size) both the state-of-the-art technique Dynodroid and the widely-used tool, Android Monkey, in 7/10 experiments for coverage, 7/10 for fault detection and 10/10 for fault-revealing sequence length. When applied to the top 1,000 Google Play apps, Sapienz found 558 unique, previously unknown crashes. So far we have managed to make contact with the developers of 27 crashing apps. Of these, 14 have confirmed that the crashes are caused by real faults. Of those 14, six already have developer-confirmed fixes. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {94–105},
numpages = {12},
keywords = {Test generation, Search-based software testing, Android},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3395363.3397380,
author = {Vanover, Jackson and Deng, Xuan and Rubio-Gonz\'{a}lez, Cindy},
title = {Discovering Discrepancies in Numerical Libraries},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397380},
doi = {10.1145/3395363.3397380},
abstract = {Numerical libraries constitute the building blocks for software applications that perform numerical calculations. Thus, it is paramount that such libraries provide accurate and consistent results. To that end, this paper addresses the problem of finding discrepancies between synonymous functions in different numerical libraries as a means of identifying incorrect behavior. Our approach automatically finds such synonymous functions, synthesizes testing drivers, and executes differential tests to discover meaningful discrepancies across numerical libraries. We implement our approach in a tool named FPDiff, and provide an evaluation on four popular numerical libraries: GNU Scientific Library (GSL), SciPy, mpmath, and jmat. FPDiff finds a total of 126 equivalence classes with a 95.8% precision and 79% recall, and discovers 655 instances in which an input produces a set of disagreeing outputs between function synonyms, 150 of which we found to represent 125 unique bugs. We have reported all bugs to library maintainers; so far, 30 bugs have been fixed, 9 have been found to be previously known, and 25 more have been acknowledged by developers.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {488–501},
numpages = {14},
keywords = {floating point, differential testing, software testing, numerical libraries},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@article{10.1145/2807593,
author = {Baudry, Benoit and Monperrus, Martin},
title = {The Multiple Facets of Software Diversity: Recent Developments in Year 2000 and Beyond},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/2807593},
doi = {10.1145/2807593},
abstract = {Early experiments with software diversity in the mid 1970s investigated N-version programming and recovery blocks to increase the reliability of embedded systems. Four decades later, the literature about software diversity has expanded in multiple directions: goals (fault tolerance, security, software engineering), means (managed or automated diversity), and analytical studies (quantification of diversity and its impact). Our article contributes to the field of software diversity as the first work that adopts an inclusive vision of the area, with an emphasis on the most recent advances in the field. This survey includes classical work about design and data diversity for fault tolerance, as well as the cybersecurity literature that investigates randomization at different system levels. It broadens this standard scope of diversity to include the study and exploitation of natural diversity and the management of diverse software products. Our survey includes the most recent works, with an emphasis from 2000 to the present. The targeted audience is researchers and practitioners in one of the surveyed fields who miss the big picture of software diversity. Assembling the multiple facets of this fascinating topic sheds a new light on the field.},
journal = {ACM Comput. Surv.},
month = {sep},
articleno = {16},
numpages = {26},
keywords = {design principles, program transformation, Software diversity}
}

@inproceedings{10.1145/2489295.2489299,
author = {Lynch, Michael and Cerqueus, Thomas and Thorpe, Christina},
title = {Testing a Cloud Application: IBM SmartCloud Inotes: Methodologies and Tools},
year = {2013},
isbn = {9781450321624},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2489295.2489299},
doi = {10.1145/2489295.2489299},
abstract = { IBM SmartCloud is a branded collection of Cloud products and solutions from IBM. It includes Infrastructure as a Service (IaaS), Software as a Service (SaaS), and Platform as a Service (PaaS) offered through public, private and hybrid cloud delivery models. This paper focuses on the software testing process employed for the SmartCloud iNotes SaaS application, providing details of the methodologies and tools developed to streamline testing. The new tools have enabled the testing team to meet the pace of the highly agile development team, enabling a more efficient software development lifecycle. Results indicate that the methodologies and tools used have increased the performance of the testing team: there was a decrease in the number of bugs present in the code (prior to release), and an overall increase in customer satisfaction. },
booktitle = {Proceedings of the 2013 International Workshop on Testing the Cloud},
pages = {13–17},
numpages = {5},
keywords = {Software testing, tool, methodology},
location = {Lugano, Switzerland},
series = {TTC 2013}
}

@inproceedings{10.1145/3356317.3356325,
author = {da Silva, R\^{o}mulo Martins and Cruz, Cafer and de S. Campos, Heleno and Murta, Leonardo G. P. and de Oliveira Neves, V\^{a}nia},
title = {What is the Adoption Level of Automated Support for Testing in Open-Source Ecosystems?},
year = {2019},
isbn = {9781450376488},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3356317.3356325},
doi = {10.1145/3356317.3356325},
abstract = {In the last decades, different kinds of automated support for testing have emerged in the open-source community. However, we still have limited evidence about the adoption level of such automated support in practice, considering different programming language ecosystems. In this paper, we investigate the adoption of automated support for testing among 184 popular open-source projects. Besides, we also investigate test coverage and metrics correlations on 571 open-source projects. As results, we found that projects written in Go, PHP, and JavaScript are the ones that most adopt automated support and that JavaScript and Python projects have the largest test coverage, with, on average, 84% and 81%, respectively. Moreover, we also found overall negligible correlations between projects' amount of stars, commits and source lines of code and coverage. Knowing that an open-source project has a high test coverage may enhance users' confidence in using this project. Besides that, we also listed the testing tools, libraries or frameworks that are most adopted for each programming language ecosystem. It may help developers in choosing appropriate automated support. Finally, we established a research agenda on this topic that motivates deeper studies as future work.},
booktitle = {Proceedings of the IV Brazilian Symposium on Systematic and Automated Software Testing},
pages = {80–89},
numpages = {10},
keywords = {Testing Tools, Open Source, White-Box Coverage, Software Testing, Repository Mining},
location = {Salvador, Brazil},
series = {SAST 2019}
}

