@inproceedings{10.1145/1879211.1879223,
author = {Myers, Colin and Duke, David},
title = {A Map of the Heap: Revealing Design Abstractions in Runtime Structures},
year = {2010},
isbn = {9781450300285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879211.1879223},
doi = {10.1145/1879211.1879223},
abstract = {Visual representations of runtime software structures such as heap memory graphs can aid in debugging and help to develop program understanding. However, such structures may contain thousands of objects and have no obvious spatial organisation. If the program contains flaws the appearance of objects may well differ from the user's expectations. Navigating these graphs can be challenging to the user as the space is abstract and potentially unfamiliar.To alleviate this problem we employ a systematic approach grounded in the principles of navigational landmarks. We identify subgraphs within the heap that correspond to significant design abstractions and apply various visualization techniques to highlight and organise these structures. The aim is to provide the user with recognisable features that are linked to more familiar representations of the software. We claim that the enhanced representation can support existing memory debugging tools by providing the user with a usable 'map' of an otherwise abstract data space.The results are demonstrated using data extracted from an instrumented version of the Visualization Tool Kit (VTK), a complex and widely-used architecture for data visualization.},
booktitle = {Proceedings of the 5th International Symposium on Software Visualization},
pages = {63–72},
numpages = {10},
keywords = {graph visualization, debugging, landmarks, heap memory},
location = {Salt Lake City, Utah, USA},
series = {SOFTVIS '10}
}

@inproceedings{10.1145/1141277.1141707,
author = {Capron, Guillaume},
title = {Static Analysis of Time Bounded Reactive Properties of Boolean Symbols},
year = {2006},
isbn = {1595931082},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1141277.1141707},
doi = {10.1145/1141277.1141707},
abstract = {We present a method for checking if macro-definitions written in C respect their specification. We are interested in simple time-bounded reactive properties. We use the abstract interpretation framework and a compact representation of sets of traces to provide a formalization of the specification, the semantics and the algorithms allowing us to build a representation of the set of traces.},
booktitle = {Proceedings of the 2006 ACM Symposium on Applied Computing},
pages = {1827–1834},
numpages = {8},
keywords = {abstract interpretation, static analysis},
location = {Dijon, France},
series = {SAC '06}
}

@inproceedings{10.1145/1710035.1710043,
author = {Lalanne, Felipe and Maag, Stephane},
title = {From the IMS PoC Service Monitoring to Its Formal Conformance Testing},
year = {2009},
isbn = {9781605585369},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1710035.1710043},
doi = {10.1145/1710035.1710043},
abstract = {Because of the increased industrial and research communities interest in the IP Multimedia Subsystem (IMS), the needs of formal testing for IMS applications are becoming critical. In this work we take as a case study the IMS Push over Cellular (PoC) service, an OMA standard, and propose a formal testing approach to check that its implementation respects the standard requirements. The approach considers a formal model of the service as well a set of formal invariants representing the most relevant properties expected from the implementation. The invariants are verified on the PoC formal specification and then automatically checked on the execution traces of the implementation. Experimental results are then processed to provide testing verdicts on the implementation under test. Besides, we discuss the results analysis that show false positives essentially raised by inter-working SIP services.},
booktitle = {Proceedings of the 6th International Conference on Mobile Technology, Application &amp; Systems},
articleno = {8},
numpages = {8},
keywords = {IMS, testing, service implementation, formal model},
location = {Nice, France},
series = {Mobility '09}
}

@inproceedings{10.1145/3342195.3387554,
author = {Rogora, Daniele and Carzaniga, Antonio and Diwan, Amer and Hauswirth, Matthias and Soul\'{e}, Robert},
title = {Analyzing System Performance with Probabilistic Performance Annotations},
year = {2020},
isbn = {9781450368827},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3342195.3387554},
doi = {10.1145/3342195.3387554},
abstract = {To understand, debug, and predict the performance of complex software systems, we develop the concept of probabilistic performance annotations. In essence, we annotate components (e.g., methods) with a relation between a measurable performance metric, such as running time, and one or more features of the input or the state of that component. We use two forms of regression analysis: regression trees and mixture models. Such relations can capture non-trivial behaviors beyond the more classic algorithmic complexity of a component. We present a method to derive such annotations automatically by generalizing observed measurements. We illustrate the use of our approach on three complex systems---the ownCloud distributed storage service; the MySQL database system; and the x264 video encoder library and application---producing non-trivial characterizations of the performance. Notably, we isolate a performance regression and identify the root cause of a second performance bug in MySQL.},
booktitle = {Proceedings of the Fifteenth European Conference on Computer Systems},
articleno = {43},
numpages = {14},
keywords = {performance analysis, performance, instrumentation, dynamic analysis},
location = {Heraklion, Greece},
series = {EuroSys '20}
}

@inproceedings{10.1145/28395.28420,
author = {Goldreich, O. and Micali, S. and Wigderson, A.},
title = {How to Play ANY Mental Game},
year = {1987},
isbn = {0897912217},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/28395.28420},
doi = {10.1145/28395.28420},
abstract = {We present a polynomial-time algorithm that, given as a input the description of a game with incomplete information and any number of players, produces a protocol for playing the game that leaks no partial information, provided the majority of the players is honest.Our algorithm automatically solves all the multi-party protocol problems addressed in complexity-based cryptography during the last 10 years. It actually is a completeness theorem for the class of distributed protocols with honest majority. Such completeness theorem is optimal in the sense that, if the majority of the players is not honest, some protocol problems have no efficient solution [C].},
booktitle = {Proceedings of the Nineteenth Annual ACM Symposium on Theory of Computing},
pages = {218–229},
numpages = {12},
location = {New York, New York, USA},
series = {STOC '87}
}

@inproceedings{10.1145/1460412.1460423,
author = {Khan, Mohammad Maifi Hasan and Le, Hieu Khac and Ahmadi, Hossein and Abdelzaher, Tarek F. and Han, Jiawei},
title = {Dustminer: Troubleshooting Interactive Complexity Bugs in Sensor Networks},
year = {2008},
isbn = {9781595939906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1460412.1460423},
doi = {10.1145/1460412.1460423},
abstract = {This paper presents a tool for uncovering bugs due to interactive complexity in networked sensing applications. Such bugs are not localized to one component that is faulty, but rather result from complex and unexpected interactions between multiple often individually non-faulty components. Moreover, the manifestations of these bugs are often not repeatable, making them particularly hard to find, as the particular sequence of events that invokes the bug may not be easy to reconstruct. Because of the distributed nature of failure scenarios, our tool looks for sequences of events that may be responsible for faulty behavior, as opposed to localized bugs such as a bad pointer in a module. An extensible framework is developed where a front-end collects runtime data logs of the system being debugged and an offline back-end uses frequent discriminative pattern mining to uncover likely causes of failure. We provide a case study of debugging a recent multichannel MAC protocol that was found to exhibit corner cases of poor performance (worse than single channel MAC). The tool helped uncover event sequences that lead to a highly degraded mode of operation. Fixing the problem significantly improved the performance of the protocol.We also provide a detailed analysis of tool overhead in terms of memory requirements and impact on the running application.},
booktitle = {Proceedings of the 6th ACM Conference on Embedded Network Sensor Systems},
pages = {99–112},
numpages = {14},
keywords = {distributed automated debugging, wireless sensor networks, protocol debugging},
location = {Raleigh, NC, USA},
series = {SenSys '08}
}

@article{10.1145/1290520.1290523,
author = {Binkley, David and Harman, Mark and Krinke, Jens},
title = {Empirical Study of Optimization Techniques for Massive Slicing},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {0164-0925},
url = {https://doi.org/10.1145/1290520.1290523},
doi = {10.1145/1290520.1290523},
abstract = {This article presents results from a study of techniques that improve the performance of graph-based interprocedural slicing of the System Dependence Graph (SDG). This is useful in “massive slicing” where slices are required for many or all of the possible set of slicing criteria. Several different techniques are considered, including forming strongly connected components, topological sorting, and removing transitive edges.Data collected from a test bed of just over 1,000,000 lines of code are presented. This data illustrates the impact on computation time of the techniques. Together, the best combination produces a 71% reduction in run-time (and a 64% reduction in memory usage). The complete set of techniques also illustrates the point at which faster computation is not viable due to prohibitive preprocessing costs.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {nov},
pages = {3–es},
numpages = {33},
keywords = {empirical study, Slicing, internal representation, performance enhancement}
}

@article{10.1145/2629422,
author = {Iwanicki, Konrad and Horban, Przemyslaw and Glazar, Piotr and Strzelecki, Karol},
title = {Bringing Modern Unit Testing Techniques to Sensornets},
year = {2014},
issue_date = {February 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {2},
issn = {1550-4859},
url = {https://doi.org/10.1145/2629422},
doi = {10.1145/2629422},
abstract = {Unit testing, an important facet of software quality assurance, is underappreciated by wireless sensor network (sensornet) developers. This is likely because our tools lag behind the rest of the computing field. As a remedy, we present a new framework that enables modern unit testing techniques in sensornets. Although the framework takes a holistic approach to unit testing, its novelty lies mainly in two aspects. First, to boost test development, it introduces embedded mock modules that automatically abstract out dependencies of tested code. Second, to automate test assessment, it provides embedded code coverage tools that identify untested control flow paths in the code. We demonstrate that in sensornets these features pose unique problems, solving which requires dedicated support from the compiler and operating system. However, the solutions have the potential to offer substantial benefits. In particular, they reduce the unit test development effort by a few factors compared to existing solutions. At the same time, they facilitate obtaining full code coverage, compared to merely 57--72% that can be achieved with integration tests. They also allow for intercepting and reporting many classes of runtime failures, thereby simplifying the diagnosis of software flaws. Finally, they enable fine-grained management of the quality of sensornet software.},
journal = {ACM Trans. Sen. Netw.},
month = {aug},
articleno = {25},
numpages = {41},
keywords = {embedded systems, code coverage, Unit testing, wireless sensor networks, mock objects, software quality assurance}
}

@article{10.5555/3007225.3007250,
author = {Chung, Sam and Bang, Soon},
title = {Identifying Knowledge, Skills, and Abilities (KSA) for Devops-Aware Server Side Web Application with the Grounded Theory},
year = {2016},
issue_date = {October 2016},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {32},
number = {1},
issn = {1937-4771},
abstract = {The purpose of this research is to propose what fundamental Knowledge (K), Skills (S), and Abilities (A) should be taught in web development courses, which can support not only development but also operations, namely DevOps. Instead of focusing on client side web application development using standard web technologies, we focus on server side web application development which holds many challenges due to a diversity of languages and emerging technologies. The advent of cloud computing reduced the gap between web application development and deployment/operation. Also, it brought an emerging concept, DevOps, to the web application development community. We identify what KSAs are needed to support DevOps and attempt to include them in web development courses. When the KSAs are included in web application development courses, students are well prepared for the emerging practice and computing paradigm of DevOps.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {110–116},
numpages = {7}
}

@inproceedings{10.1145/2491411.2491426,
author = {Schur, Matthias and Roth, Andreas and Zeller, Andreas},
title = {Mining Behavior Models from Enterprise Web Applications},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491426},
doi = {10.1145/2491411.2491426},
abstract = { Today's enterprise web applications demand very high release cycles---and consequently, frequent tests. Automating these tests typically requires a behavior model: A description of the states the application can be in, the transitions between these states, and the expected results. Furthermore one needs scripts to make the abstract actions (transitions) in the model executable. As specifying such behavior models and writing the necessary scripts manually is a hard task, a possible alternative could be to extract them from existing applications. However, mining such models can be a challenge, in particular because one needs to know when two states are equivalent, as well as how to reach that state. We present ProCrawl (PROcess CRAWLer), a generic approach to mine behavior models from (multi-user) enterprise web applications. ProCrawl observes the behavior of the application through its user interface, generates and executes tests to explore unobserved behavior. In our evaluation of three non-trivial web applications (an open-source shop system, an SAP product compliance application, and an open-source conference manager), ProCrawl produces models that precisely abstract application behavior and which can be directly used for effective model-based regression testing. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {422–432},
numpages = {11},
keywords = {dynamic analysis, model-based testing, Specification mining},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inproceedings{10.1145/306363.306366,
author = {Giguette, Ray and Hassell, Johnette},
title = {Toward a Resourceful Method of Software Fault Tolerance},
year = {1999},
isbn = {1581131283},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/306363.306366},
doi = {10.1145/306363.306366},
booktitle = {Proceedings of the 37th Annual Southeast Regional Conference (CD-ROM)},
pages = {1–es},
series = {ACM-SE 37}
}

@inproceedings{10.1145/3368089.3409758,
author = {Terragni, Valerio and Jahangirova, Gunel and Tonella, Paolo and Pezz\`{e}, Mauro},
title = {Evolutionary Improvement of Assertion Oracles},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409758},
doi = {10.1145/3368089.3409758},
abstract = {Assertion oracles are executable boolean expressions placed inside the program that should pass (return true) for all correct executions and fail (return false) for all incorrect executions. Because designing perfect assertion oracles is difficult, assertions often fail to distinguish between correct and incorrect executions. In other words, they are prone to false positives and false negatives. In this paper, we propose GAssert (Genetic ASSERTion improvement), the first technique to automatically improve assertion oracles. Given an assertion oracle and evidence of false positives and false negatives, GAssert implements a novel co-evolutionary algorithm that explores the space of possible assertions to identify one with fewer false positives and false negatives. Our empirical evaluation on 34 Java methods from 7 different Java code bases shows that GAssert effectively improves assertion oracles. GAssert outperforms two baselines (random and invariant-based oracle improvement), and is comparable with and in some cases even outperformed human-improved assertions.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1178–1189},
numpages = {12},
keywords = {program assertions, evolutionary algorithm, oracle improvement},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/3382025.3414725,
author = {Ferreira, Fischer and Viggiato, Markos and Souza, Maur\'{\i}cio and Figueiredo, Eduardo},
title = {Testing Configurable Software Systems: The Failure Observation Challenge},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414725},
doi = {10.1145/3382025.3414725},
abstract = {Configurable software systems can be adapted or configured according to a set of features to increase reuse and productivity. The testing process is essential because configurations that fail may potentially hurt user experience and degrade the reputation of a project. However, testing configurable systems is very challenging due to the number of configurations to run with each test, leading to a combinatorial explosion in the number of configurations and tests. Currently, several testing techniques and tools have been proposed to deal with this challenge, but their potential practical application remains mostly unexplored. To encourage the research area on testing configurable systems, researchers and practitioners should be able to try out their solutions in common datasets. In this paper, we propose a dataset with 22 configurable software systems and an extensive test suite. Moreover, we report failures found in these systems and source code metrics to allow evaluating candidate solutions. We hope to engage the community and stimulate new and existing approaches to the problem of testing configurable systems.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {28},
numpages = {6},
keywords = {software product line, testing configurable systems},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/3368089.3409709,
author = {Vassallo, Carmine and Proksch, Sebastian and Jancso, Anna and Gall, Harald C. and Di Penta, Massimiliano},
title = {Configuration Smells in Continuous Delivery Pipelines: A Linter and a Six-Month Study on GitLab},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409709},
doi = {10.1145/3368089.3409709},
abstract = {An effective and efficient application of Continuous Integration (CI) and Delivery (CD) requires software projects to follow certain principles and good practices. Configuring such a CI/CD pipeline is challenging and error-prone. Therefore, automated linters have been proposed to detect errors in the pipeline. While existing linters identify syntactic errors, detect security vulnerabilities or misuse of the features provided by build servers, they do not support developers that want to prevent common misconfigurations of a CD pipeline that potentially violate CD principles (“CD smells”). To this end, we propose CD-Linter, a semantic linter that can automatically identify four different smells in pipeline configuration files. We have evaluated our approach through a large-scale and long-term study that consists of (i) monitoring 145 issues (opened in as many open-source projects) over a period of 6 months, (ii) manually validating the detection precision and recall on a representative sample of issues, and (iii) assessing the magnitude of the observed smells on 5,312 open-source projects on GitLab. Our results show that CD smells are accepted and fixed by most of the developers and our linter achieves a precision of 87% and a recall of 94%. Those smells can be frequently observed in the wild, as 31% of projects with long configurations are affected by at least one smell.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {327–337},
numpages = {11},
keywords = {Continuous Delivery, Continuous Integration, DevOps, Linter, Anti-pattern, Configuration},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2804371.2804372,
author = {Stillwell, Mark and Coutinho, Jose G. F.},
title = {A DevOps Approach to Integration of Software Components in an EU Research Project},
year = {2015},
isbn = {9781450338172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2804371.2804372},
doi = {10.1145/2804371.2804372},
abstract = { We present a description of the development and deployment infrastructure being created to support the integration effort of HARNESS, an EU FP7 project. HARNESS is a multi-partner research project intended to bring the power of heterogeneous resources to the cloud. It consists of a number of different services and technologies that interact with the OpenStack cloud computing platform at various levels. Many of these components are being developed independently by different teams at different locations across Europe, and keeping the work fully integrated is a challenge. We use a combination of Vagrant based virtual machines, Docker containers, and Ansible playbooks to provide a consistent and up-to-date environment to each developer. The same playbooks used to configure local virtual machines are also used to manage a static testbed with heterogeneous compute and storage devices, and to automate ephemeral larger-scale deployments to Grid'5000. Access to internal projects is managed by GitLab, and automated testing of services within Docker-based environments and integrated deployments within virtual-machines is provided by Buildbot. },
booktitle = {Proceedings of the 1st International Workshop on Quality-Aware DevOps},
pages = {1–6},
numpages = {6},
keywords = {Docker, Automated Testing, GitLab, Configuration Management, Ansible, BuildBot, OpenStack, Vagrant, DevOps},
location = {Bergamo, Italy},
series = {QUDOS 2015}
}

@inproceedings{10.5555/319568.319624,
author = {Redwine, Samuel T. and Riddle, William E.},
title = {Software Technology Maturation},
year = {1985},
isbn = {0818606207},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {We have reviewed the growth and propagation of a variety of software technologies in an attempt to discover natural characteristics of the process as well as principles and techniques useful in transitioning modern software technology into widespread use. What we have looked at is the technology maturation process, the process by which a piece of technology is first conceived, then shaped into something usable, and finally “marketed” to the point that it is found in the repertoire of a majority of professionals.A major interest is the time required for technology maturation — and our conclusion is that technology maturation generally takes much longer than popularly thought, especially for major technology areas. But our prime interest is in determining what actions, if any, can accelerate the maturation of technology, in particular that part of maturation that has to do with transitioning the technology into widespread use. Our observations concerning maturation facilitators and inhibitors are the major subject of this paper.},
booktitle = {Proceedings of the 8th International Conference on Software Engineering},
pages = {189–200},
numpages = {12},
location = {London, England},
series = {ICSE '85}
}

@inproceedings{10.5555/800241.807239,
author = {Aygun, Birol O.},
title = {Environments for Monitoring and Dynamic Analysis of Execution},
year = {1973},
publisher = {IEEE Press},
abstract = {The research underlying this report is concerned with improving the lot of the programmer in debugging, analyzing and modelling his, or others', programs. It is my belief that the area of “execution analysis tools” has been neglected in the formative stages of system planning in contemporary systems. This has led to the design of many software and hardware “probes”, “performance monitors”, debugging aids, etc. which have been forced to work with uncooperative hardware and operating systems. The state of the art is especially inadequate in on-line debugging and analysis tools. Witness, for example, the difficulties of backtracking, or tracing the values of a set of core locations, or determining all the successors of a particular node in a program (a node being defined in some intuitive sense for the time being), except in several machine-simulator based systems (see (9) for some good examples).In a survey (1) of on-line debugging techniques in 1966, Evans and Darley remark that computer use is becoming “debugging-limited” rather than limited by memory size or processor speed. They further predict that this will be the state of affairs until methods for proving that programs have certain properties are successfully developed and come into wide use. I quite agree with that evaluation and further envision that “high-level execution analysis” tools will become a staple in future software engineering laboratories.},
booktitle = {Proceedings of the 1st Symposium on Simulation of Computer Systems},
pages = {178–197},
numpages = {20},
location = {Gaithersburg, Maryland, USA},
series = {ANSS '73}
}

@inproceedings{10.1145/3447786.3456252,
author = {Choi, Brian and Burns, Randal and Huang, Peng},
title = {Understanding and Dealing with Hard Faults in Persistent Memory Systems},
year = {2021},
isbn = {9781450383349},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3447786.3456252},
doi = {10.1145/3447786.3456252},
abstract = {The advent of Persistent Memory (PM) devices enables systems to actively persist information at low costs, including program state traditionally in volatile memory. However, this trend poses a reliability challenge in which multiple classes of soft faults that go away after restart in traditional systems turn into hard (recurring) faults in PM systems. In this paper, we first characterize this rising problem with an empirical study of 28 real-world bugs. We analyze how they cause hard faults in PM systems. We then propose Arthas, a tool to effectively recover PM systems from hard faults. Arthas checkpoints PM states via fine-grained versioning and uses program slicing of fault instructions to revert problematic PM states to good versions. We evaluate Arthas on 12 real-world hard faults from five large PM systems. Arthas successfully recovers the systems for all cases while discarding 10\texttimes{} less data on average compared to state-of-the-art checkpoint-rollback solutions.},
booktitle = {Proceedings of the Sixteenth European Conference on Computer Systems},
pages = {441–457},
numpages = {17},
location = {Online Event, United Kingdom},
series = {EuroSys '21}
}

@inproceedings{10.5555/2819261.2819271,
author = {Sabor, Korosh Koochekian and Thiel, Stuart},
title = {Adaptive Random Testing by Static Partitioning},
year = {2015},
publisher = {IEEE Press},
abstract = {Despite the importance of the random testing approach, random testing is not used in isolation, but plays a core role in many testing methods. On the basis that evenly distributed test cases are more likely to reveal non-point pattern failure regions, various Adaptive Random Testing (ART) methods have been proposed. Many of these methods use a variety of distance calculations, with corresponding computational overhead; newly proposed methods like ART by bisection, random partitioning or dynamic iterative partitioning try to decrease computational overhead while maintaining the performance. In this article we have proposed a new ART method that has similar performance to existing ART methods while having less computational overhead.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {28–32},
numpages = {5},
keywords = {random testing, static partitioning, static, adaptive random testing},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.1145/2667190.2667194,
author = {Dantas, Henrique and Erkin, Zekeriya and Doerr, Christian and Hallie, Raymond and Bij, Gerrit van der},
title = {EFuzz: A Fuzzer for DLMS/COSEM Electricity Meters},
year = {2014},
isbn = {9781450331548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2667190.2667194},
doi = {10.1145/2667190.2667194},
abstract = {Smart grids enable new functionalities like remote and micro management and consequently, provide increased efficiency, easy management and effectiveness of the entire power grid infrastructure. In order to achieve this, smart meters are attached to the communication network, collecting fine granular data. Unfortunately, as the smart meters are limited devices connected to the network and running software, they also make the whole smart grid more vulnerable than the traditional grids in term of software problems and even possible cyber attacks. In this paper, we work towards an increased software security of smart metering devices and propose a fuzzing framework, eFuzz, built on the generic fuzzing framework Peach to detect software problems. eFuzz tests smart metering devices based on the communication protocol DLMS/COSEM, the standard protocol used in Europe, for possible faults. Our experiments prove the effectiveness of using an automated fuzzing framework compared to resource demanding, human made software protocol inspections. As an example, eFuzz detected between 10 and 40 bugs in different configurations in less than 3 hours while a manual inspection takes weeks. We also investigate the quality of the eFuzz results by comparing with the traditional non-automated evaluation of the same device with respect to scope and efficiency. Our analysis shows that eFuzz is a powerful tool for security inspections for smart meters, and embedded systems in general.},
booktitle = {Proceedings of the 2nd Workshop on Smart Energy Grid Security},
pages = {31–38},
numpages = {8},
keywords = {automated testing, software vulnerabilities, fuzz testing},
location = {Scottsdale, Arizona, USA},
series = {SEGS '14}
}

