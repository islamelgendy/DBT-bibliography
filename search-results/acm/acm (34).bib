@inproceedings{10.1145/2212776.2212421,
author = {Scaffidi, Christopher and Brandt, Joel and Burnett, Margaret and Dove, Andrew and Myers, Brad},
title = {SIG: End-User Programming},
year = {2012},
isbn = {9781450310161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2212776.2212421},
doi = {10.1145/2212776.2212421},
abstract = {As users continue to grow in number and diversity, end-user programming is playing an increasingly central role in shaping software to meet the broad, varied, rapidly changing needs of the world. Numerous companies have therefore begun to sell tools enabling end users to create programs. In parallel, ongoing academic research is aimed at helping end-user programmers create and adapt new kinds of programs in new ways. This special interest group meeting will bring together the community of researchers and companies focused on creating end-user programming tools, thereby facilitating technology transfer and future collaboration.},
booktitle = {CHI '12 Extended Abstracts on Human Factors in Computing Systems},
pages = {1193–1996},
numpages = {804},
keywords = {end-user software engineering (euse), end-user development (eud), end-user programming (eup)},
location = {Austin, Texas, USA},
series = {CHI EA '12}
}

@inproceedings{10.1145/2647908.2655981,
author = {Acher, Mathieu and Alf\'{e}rez, Mauricio and Galindo, Jos\'{e} A. and Romenteau, Pierre and Baudry, Benoit},
title = {ViViD: A Variability-Based Tool for Synthesizing Video Sequences},
year = {2014},
isbn = {9781450327398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2647908.2655981},
doi = {10.1145/2647908.2655981},
abstract = {We present ViViD, a variability-based tool to synthesize variants of video sequences. ViViD is developed and used in the context of an industrial project involving consumers and providers of video processing algorithms. The goal is to synthesize synthetic video variants with a wide range of characteristics to then test the algorithms. We describe the key components of ViViD (1) a variability language and an environment to model what can vary within a video sequence; (2) a reasoning back-end to generate relevant testing configurations; (3) a video synthesizer in charge of producing variants of video sequences corresponding to configurations. We show how ViViD can synthesize realistic videos with different characteristics such as luminances, vehicles and persons that cover a diversity of testing scenarios.},
booktitle = {Proceedings of the 18th International Software Product Line Conference: Companion Volume for Workshops, Demonstrations and Tools - Volume 2},
pages = {143–147},
numpages = {5},
keywords = {combinatorial interaction testing, variability modeling, prioritization, T-wise, video generation, multimedia},
location = {Florence, Italy},
series = {SPLC '14}
}

@article{10.1145/3149119,
author = {Abal, Iago and Melo, Jean and St\u{a}nciulescu, \c{S}tefan and Brabrand, Claus and Ribeiro, M\'{a}rcio and W\k{a}sowski, Andrzej},
title = {Variability Bugs in Highly Configurable Systems: A Qualitative Analysis},
year = {2018},
issue_date = {July 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {26},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3149119},
doi = {10.1145/3149119},
abstract = {Variability-sensitive verification pursues effective analysis of the exponentially many variants of a program family. Several variability-aware techniques have been proposed, but researchers still lack examples of concrete bugs induced by variability, occurring in real large-scale systems. A collection of real world bugs is needed to evaluate tool implementations of variability-sensitive analyses by testing them on real bugs. We present a qualitative study of 98 diverse variability bugs (i.e., bugs that occur in some variants and not in others) collected from bug-fixing commits in the Linux, Apache, BusyBox, and Marlin repositories. We analyze each of the bugs, and record the results in a database. For each bug, we create a self-contained simplified version and a simplified patch, in order to help researchers who are not experts on these subject studies to understand them, so that they can use these bugs for evaluation of their tools. In addition, we provide single-function versions of the bugs, which are useful for evaluating intra-procedural analyses. A web-based user interface for the database allows to conveniently browse and visualize the collection of bugs. Our study provides insights into the nature and occurrence of variability bugs in four highly-configurable systems implemented in C/C++, and shows in what ways variability hinders comprehension and the uncovering of software bugs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
articleno = {10},
numpages = {34},
keywords = {Bugs, feature interactions, Linux, software variability}
}

@inbook{10.1145/3238147.3240465,
author = {Wang, Wenyu and Li, Dengfeng and Yang, Wei and Cao, Yurui and Zhang, Zhenwen and Deng, Yuetang and Xie, Tao},
title = {An Empirical Study of Android Test Generation Tools in Industrial Cases},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3240465},
abstract = {User Interface (UI) testing is a popular approach to ensure the quality of mobile apps. Numerous test generation tools have been developed to support UI testing on mobile apps, especially for Android apps. Previous work evaluates and compares different test generation tools using only relatively simple open-source apps, while real-world industrial apps tend to have more complex functionalities and implementations. There is no direct comparison among test generation tools with regard to effectiveness and ease-of-use on these industrial apps. To address such limitation, we study existing state-of-the-art or state-of-the-practice test generation tools on 68 widely-used industrial apps. We directly compare the tools with regard to code coverage and fault-detection ability. According to our results, Monkey, a state-of-the-practice tool from Google, achieves the highest method coverage on 22 of 41 apps whose method coverage data can be obtained. Of all 68 apps under study, Monkey also achieves the highest activity coverage on 35 apps, while Stoat, a state-of-the-art tool, is able to trigger the highest number of unique crashes on 23 apps. By analyzing the experimental results, we provide suggestions for combining different test generation tools to achieve better performance. We also report our experience in applying these tools to industrial apps under study. Our study results give insights on how Android UI test generation tools could be improved to better handle complex industrial apps.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {738–748},
numpages = {11}
}

@inproceedings{10.1145/2568225.2568324,
author = {Monperrus, Martin},
title = {A Critical Review of "Automatic Patch Generation Learned from Human-Written Patches": Essay on the Problem Statement and the Evaluation of Automatic Software Repair},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568324},
doi = {10.1145/2568225.2568324},
abstract = { At ICSE'2013, there was the first session ever dedicated to automatic program repair. In this session, Kim et al. presented PAR, a novel template-based approach for fixing Java bugs. We strongly disagree with key points of this paper. Our critical review has two goals. First, we aim at explaining why we disagree with Kim and colleagues and why the reasons behind this disagreement are important for research on automatic software repair in general. Second, we aim at contributing to the field with a clarification of the essential ideas behind automatic software repair. In particular we discuss the main evaluation criteria of automatic software repair: understandability, correctness and completeness. We show that depending on how one sets up the repair scenario, the evaluation goals may be contradictory. Eventually, we discuss the nature of fix acceptability and its relation to the notion of software correctness. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {234–242},
numpages = {9},
keywords = {Bugs, automatic patch generation, error recovery, faults, automatic program fixing, automatic software repair},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/3107091.3107093,
author = {Truong, Hong-Linh and Berardinelli, Luca},
title = {Testing Uncertainty of Cyber-Physical Systems in IoT Cloud Infrastructures: Combining Model-Driven Engineering and Elastic Execution},
year = {2017},
isbn = {9781450351126},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3107091.3107093},
doi = {10.1145/3107091.3107093},
abstract = { Today's cyber-physical systems (CPS) span IoT and cloud-based datacenter infrastructures, which are highly heterogeneous with various types of uncertainty. Thus, testing uncertainties in these CPS is a challenging and multidisciplinary activity. We need several tools for modeling, deployment, control, and analytics to test and evaluate uncertainties for different configurations of the same CPS. In this paper, we explain why using state-of-the art model-driven engineering (MDE) and model-based testing (MBT) tools is not adequate for testing uncertainties of CPS in IoT Cloud infrastructures. We discus how to combine them with techniques for elastic execution to dynamically provision both CPS under test and testing utilities to perform tests in various IoT Cloud infrastructures. },
booktitle = {Proceedings of the 1st ACM SIGSOFT International Workshop on Testing Embedded and Cyber-Physical Systems},
pages = {5–8},
numpages = {4},
keywords = {testing, uncertainty, Cloud, IoT, MDE, MBT, elasticity},
location = {Santa Barbara, CA, USA},
series = {TECPS 2017}
}

@article{10.1145/2786984.2786997,
author = {Chandra, Ranveer and Karlsson, B\"{o}rje F. and Lane, Nicholas D. and Liang, Chieh-Jan Mike and Nath, Suman and Padhye, Jitu and Ravindranath, Lenin and Zhao, Feng},
title = {How to the Smash Next Billion Mobile App Bugs?},
year = {2015},
issue_date = {January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {19},
number = {1},
issn = {2375-0529},
url = {https://doi.org/10.1145/2786984.2786997},
doi = {10.1145/2786984.2786997},
abstract = {With users increasingly dependent on their phones, tablets, and wearables, the mobile app ecosystem is more important today than ever before. Creating and distributing apps has never been more accessible. Even single developers can now reach global audiences. But mobile apps must cope with extremely varied and dynamic operating conditions due to factors like diverse device characteristics, wireless network heterogeneity, and varied user behavior. App developers and operators of app marketplaces both lack testing tools that can effectively account for such diversity and, as a result, app failures and performance bugs (like excessive energy consumption) are commonly found today. To address this challenge to mobile app development, we have developed key techniques for scalable automated mobile app testing within two prototype services --- VanarSena and Caiipa. In this paper, we describe our vision for SMASH, a unified cloud-based mobile app testing service that combines the strengths of both previous systems to tackle the complexities presently faced by testers of mobile apps.},
journal = {GetMobile: Mobile Comp. and Comm.},
month = {jun},
pages = {34–38},
numpages = {5}
}

@article{10.1145/2492248.2492262,
author = {Bhasin, Harsh and Singla, Neha and Sharma, Shruti},
title = {Cellular Automata Based Test Data Generation},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/2492248.2492262},
doi = {10.1145/2492248.2492262},
abstract = {Manual Test Data Generation is an expensive, error prone and tedious task. Therefore, there is an immediate need to make the automation of this process as efficient and effective as possible. The work presented intends to automate the process of Test Data Generation with a goal of attaining maximum coverage. A Cellular Automata system is discrete in space and time. Cellular Automata have been applied to things like designing water distribution systems and studying the patterns of migration. This fascinating technique has been amalgamated with standard test data generation techniques to give rise to a technique which generates better test cases than the existing techniques. The approach has been verified on programs selected in accordance with their Lines of Code and utility. The results obtained have been verified. The proposed work is a part of a larger system being developed, which takes into account both black box and white box testing.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {1–7},
numpages = {7},
keywords = {test data generation, cellular automata, testing, autocorrelation, path coverage}
}

@inproceedings{10.1145/3167132.3167424,
author = {Eniser, Hasan Ferit and Sen, Alper and Polat, Suleyman Olcay},
title = {Fancymock: Creating Virtual Services from Transactions},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167424},
doi = {10.1145/3167132.3167424},
abstract = {Heterogeneous component-based applications such as API dependent applications, cloud-based systems, and Service Oriented Architectures (SOA) are commonly used in enterprise software systems. Testing such complicated systems can be challenging due to multiple reasons including unavailability of components, high cost of using services, or high overhead of transactions. Service virtualization (emulation) is an approach to mimic the behavior of a given component. Virtual services can be created by analyzing service interface specifications (such as WSDL), by recording and replaying transactions, or by defining the behavior manually. There is currently a lack of studies that propose automated and intelligent methods for service virtualization. In this paper, we develop FancyMock which is a smart service virtualization tool. We make use of machine learning and bioinformatics algorithms in FancyMock. Our virtual services can synthesize valid and logical responses in an acceptable amount of time. Furthermore, our approach does not assume any message format. We demonstrate the validity of our approach on three different data sets collected from real life services and obtain promising results.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1576–1578},
numpages = {3},
keywords = {service virtualization, service oriented architectures, mocking},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2508075.2508096,
author = {An, Kyoungho and Kuroda, Takayuki and Gokhale, Aniroddha and Tambe, Sumant and Sorbini, Andrea},
title = {Model-Driven Generative Framework for Automated OMG DDS Performance Testing in the Cloud},
year = {2013},
isbn = {9781450319959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508075.2508096},
doi = {10.1145/2508075.2508096},
abstract = {The Object Management Group's (OMG) Data Distribution Service (DDS) provides many configurable policies which determine end-to-end quality of service (QoS) delivered to the applications. It is challenging, however, to predict the application's performance in terms of latencies, throughput, and resource usage because diverse combinations of QoS configurations influence QoS of applications in different ways. To overcome this problem, design-time formal methods have been applied with mixed success, but a lack of sufficient accuracy in prediction, tool support, and understanding of formalism has prevented wider adoption of the formal techniques. A promising approach to address this challenge is to emulate application behavior and gather data on the QoS parameters of interest by experimentation. To realize this approach, we have developed a middleware framework that uses model-driven generative mechanisms to automate performance testing of a large number of DDS QoS configuration combinations that can be deployed and tested on a cloud platform.},
booktitle = {Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, &amp; Applications: Software for Humanity},
pages = {93–94},
numpages = {2},
keywords = {performance testing, generative programming, publish/subscribe, model-driven engineering},
location = {Indianapolis, Indiana, USA},
series = {SPLASH '13}
}

@proceedings{10.1145/1982595,
title = {AST '11: Proceedings of the 6th International Workshop on Automation of Software Test},
year = {2011},
isbn = {9781450305921},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
abstract = {As proud co-chairs of the 6th workshop on Automation of Software Test, we would like to take this opportunity to welcome you to AST2011 and to thank all participants for taking the time out from their busy lives and the work in their home countries to make the effort to attend this workshop.The theme of this year's workshop, aligned with the ICSE 2011 conference theme, is Relating Software Design to Test Automation. The theme covers several prominent perspectives both in Software Design and Automation of Software Test practice. The broad scope of AST is reflected in the wide range of topics covered by the 36 submissions that we received. Of these, with the 32 members of the AST Program Committee from both academic and industrial research labs, we selected 15 regular research papers that are grouped into six sessions, representing a diversity of test automation related topics. The six sessions are: Model-Based TestingTesting for Web and Service-Based SystemsTesting Strategies I and IIApplication of Testing I and IIThe schedule of the two workshop days will be intensive yet productive. Besides the six regular paper sessions, we will have two poster sessions covering promising ongoing research work in test automation. The program of each day will start from a prominent keynote speech, one from academy and the other from industry. A multi-day charette on the key near-term challenges in relating software architecture and automated software test will be the highlight of this workshop. On the first day, the participants will identify challenges related to software architecture support for testing and divide into teams of a half dozen people. Each team will take up one of the related questions and create a feasible near-term research pathway to effectively address it. On the second day, each team will finalize a presentation on their challenge and pathway, and will then present their ideas to the workshop as a whole.},
location = {Waikiki, Honolulu, HI, USA}
}

@inbook{10.1145/3461002.3473070,
author = {Acher, Mathieu and Perrouin, Gilles and Cordy, Maxime},
title = {BURST: A Benchmarking Platform for Uniform Random Sampling Techniques},
year = {2021},
isbn = {9781450384704},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461002.3473070},
abstract = {We present BURST, a benchmarking platform for uniform random sampling techniques. With BURST, researchers have a flexible, controlled environment in which they can evaluate the scalability and uniformity of their sampling. BURST comes with an extensive --- and extensible --- benchmark dataset comprising 128 feature models, including challenging, real-world models of the Linux kernel. BURST takes as inputs a sampling tool, a set of feature models and a sampling budget. It automatically translates any feature model of the set in DIMACS and invokes the sampling tool to generate the budgeted number of samples. To evaluate the scalability of the sampling tool, BURST measures the time the tool needs to produce the requested sample. To evaluate the uniformity of the produced sample, BURST integrates the state-of-the-art and proven statistical test Barbarik. We envision BURST to become the starting point of a standardisation initiative of sampling tool evaluation. Given the huge interest of research for sampling algorithms and tools, this initiative would have the potential to reach and crosscut multiple research communities including AI, ML, SAT and SPL.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume B},
pages = {36–40},
numpages = {5}
}

@inproceedings{10.1145/3236024.3264835,
author = {Guo, Jianmin and Jiang, Yu and Zhao, Yue and Chen, Quan and Sun, Jiaguang},
title = {DLFuzz: Differential Fuzzing Testing of Deep Learning Systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264835},
doi = {10.1145/3236024.3264835},
abstract = {Deep learning (DL) systems are increasingly applied to safety-critical domains such as autonomous driving cars. It is of significant importance to ensure the reliability and robustness of DL systems. Existing testing methodologies always fail to include rare inputs in the testing dataset and exhibit low neuron coverage. In this paper, we propose DLFuzz, the first differential fuzzing testing framework to guide DL systems exposing incorrect behaviors. DLFuzz keeps minutely mutating the input to maximize the neuron coverage and the prediction difference between the original input and the mutated input, without manual labeling effort or cross-referencing oracles from other DL systems with the same functionality. We present empirical evaluations on two well-known datasets to demonstrate its efficiency. Compared with DeepXplore, the state-of-the-art DL whitebox testing framework, DLFuzz does not require extra efforts to find similar functional DL systems for cross-referencing check, but could generate 338.59% more adversarial inputs with 89.82% smaller perturbations, averagely obtain 2.86% higher neuron coverage, and save 20.11% time consumption.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {739–743},
numpages = {5},
keywords = {Fuzzing Testing, Neuron Coverage, Deep Learning},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3106237.3106298,
author = {Su, Ting and Meng, Guozhu and Chen, Yuting and Wu, Ke and Yang, Weiming and Yao, Yao and Pu, Geguang and Liu, Yang and Su, Zhendong},
title = {Guided, Stochastic Model-Based GUI Testing of Android Apps},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106298},
doi = {10.1145/3106237.3106298},
abstract = { Mobile apps are ubiquitous, operate in complex environments and are developed under the time-to-market pressure. Ensuring their correctness and reliability thus becomes an important challenge. This paper introduces Stoat, a novel guided approach to perform stochastic model-based testing on Android apps. Stoat operates in two phases: (1) Given an app as input, it uses dynamic analysis enhanced by a weighted UI exploration strategy and static analysis to reverse engineer a stochastic model of the app's GUI interactions; and (2) it adapts Gibbs sampling to iteratively mutate/refine the stochastic model and guides test generation from the mutated models toward achieving high code and model coverage and exhibiting diverse sequences. During testing, system-level events are randomly injected to further enhance the testing effectiveness.  Stoat was evaluated on 93 open-source apps. The results show (1) the models produced by Stoat cover 17~31% more code than those by existing modeling tools; (2) Stoat detects 3X more unique crashes than two state-of-the-art testing tools, Monkey and Sapienz. Furthermore, Stoat tested 1661 most popular Google Play apps, and detected 2110 previously unknown and unique crashes. So far, 43 developers have responded that they are investigating our reports. 20 of reported crashes have been confirmed, and 8 already fixed. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {245–256},
numpages = {12},
keywords = {GUI Testing, Mobile Apps, Model-based Testing},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.5555/257734.257767,
author = {Rothermel, Gregg and Harrold, Mary Jean},
title = {A Framework for Evaluating Regression Test Selection Techniques},
year = {1994},
isbn = {081865855X},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of the 16th International Conference on Software Engineering},
pages = {201–210},
numpages = {10},
location = {Sorrento, Italy},
series = {ICSE '94}
}

@inproceedings{10.1145/3131151.3131160,
author = {de Cleva Farto, Guilherme and Endo, Andre Takeshi},
title = {Reuse of Model-Based Tests in Mobile Apps},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131160},
doi = {10.1145/3131151.3131160},
abstract = {Mobile apps have been introduced in our lives and as a specific class of software, developers and testers have to deal with new challenges. For instance, testing all configurations and characteristics of apps might be an expensive activity. It would be desirable to maximize the return of investment for systematic and automated test cases. This paper proposes an approach for mobile apps in which test models are reused to (i) reduce the effort for concretization and (ii) test other characteristics of apps, such as device-specific events, unpredictable users' interaction, telephony events for GSM/SMS, and sensors and hardware events. We assume that a model-based testing approach was applied to GUI testing of Android apps. Specifically, test models are designed using Event Sequence Graphs (ESGs) and concrete adapters developed using Java and automated testing frameworks. We evaluated the approach and its supporting tool with three Android apps in an industrial setting.},
booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
pages = {184–193},
numpages = {10},
keywords = {Event Sequence Graph, Mobile Applications, Android, Automated Tests, Mobile Testing, Reuse, Model-Based Testing},
location = {Fortaleza, CE, Brazil},
series = {SBES'17}
}

@article{10.1145/596992.597001,
author = {Jeon, Taewoong and Seung, Hyon Woo and Lee, Sungyoung},
title = {Embedding Built-in Tests in Hot Spots of an Object-Oriented Framework},
year = {2002},
issue_date = {August 2002},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/596992.597001},
doi = {10.1145/596992.597001},
abstract = {This paper describes a scheme of encapsulating test support code as built-in test (BIT) components and embedding them into the hot spots of an object-oriented framework so that defects caused by the modification and extension of the framework can be detected effectively and efficiently through testing. The test components embedded into a framework in this way increase the testability of the framework by making it easy to control and observe the process of framework testing. The proposed technique is illustrated using the facilities of C++. Our testing scheme, however, is equally applicable to other object-oriented languages. Using our scheme, test components can be designed and embedded into the hot spots of a framework without incurring changes or intervention to the framework code, and also can be attached and detached dynamically to/from the framework as needed at run-time.},
journal = {SIGPLAN Not.},
month = {aug},
pages = {25–34},
numpages = {10},
keywords = {object-oriented framework, testability, built-in test (BIT), hook classes}
}

@inproceedings{10.1145/1529282.1529405,
author = {Silva, Fernando R. C. and Almeida, Eduardo S. and Meira, Silvio R. L.},
title = {An Approach for Component Testing and Its Empirical Validation},
year = {2009},
isbn = {9781605581668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1529282.1529405},
doi = {10.1145/1529282.1529405},
abstract = {The lack of information limits component consumers to understand candidate components sufficiently in a way they can check if a given component fulfills its goal. Thus, this paper presents an approach to support component testing aiming to reduce the lack of information between component producers and component consumers. Additionally, the approach is covered by a CASE tool integrated in the development environment. An experimental study was performed in order to evaluate its efficiency and difficulties of its use. The experimental study indicates that the approach is viable and the tool support provides effort reduction to component producers and component consumers.},
booktitle = {Proceedings of the 2009 ACM Symposium on Applied Computing},
pages = {574–581},
numpages = {8},
keywords = {component-based development, component testing, software reuse, software quality},
location = {Honolulu, Hawaii},
series = {SAC '09}
}

@article{10.1145/1050849.1050862,
author = {Gill, Nasib S.},
title = {Factors Affecting Effective Software Quality Management Revisited},
year = {2005},
issue_date = {March 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1050849.1050862},
doi = {10.1145/1050849.1050862},
abstract = {Developing a good software system is a very complex task. In order to produce a good software product, several measures for software quality attributes need to be taken into account. System complexity measurement plays a vital role in controlling and managing software quality because it generally affects the software quality attributes like software reliability, software testability and software maintainability. Thus, software quality assurance (SQA) needs to be addressed keeping in view the new strategies, tool, methodologies and techniques applicable to software development life cycle.This paper is primarily aimed at revisiting and examining peculiar aspects of software development process that affect software quality management process. These aspects of software development process include software reliability measurement, ISO approach applicable to software quality and some aspects related to software testing improvement. Software testing and evaluation methods/tools/techniques do not guarantee effective testing and ensure high software quality. The way to improve the effectiveness of testing is to improve the attitude of software developers towards testing.In this paper, all these factors affecting software quality management have been discussed as well as all the possible improvements have been suggested. The results of this paper may be quite helpful to the researchers in quantifying the specific measuring tools for these software quality attributes.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–4},
numpages = {4},
keywords = {software quality management, software reliability, software quality assurance, software quality, ISO}
}

@inproceedings{10.1109/ICSE.2017.61,
author = {Chekam, Thierry Titcheu and Papadakis, Mike and Traon, Yves Le and Harman, Mark},
title = {An Empirical Study on Mutation, Statement and Branch Coverage Fault Revelation That Avoids the Unreliable Clean Program Assumption},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.61},
doi = {10.1109/ICSE.2017.61},
abstract = {Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed (`clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings also revealed that fault revelation starts to increase significantly only once relatively high levels of coverage are attained.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {597–608},
numpages = {12},
keywords = {code coverage, mutation testing, test effectiveness, test adequacy, real faults},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

