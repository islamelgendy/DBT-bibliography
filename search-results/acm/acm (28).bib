@article{10.1145/3508362,
author = {Chen, Junjie and Suo, Chenyao},
title = {Boosting Compiler Testing via Compiler Optimization Exploration},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3508362},
doi = {10.1145/3508362},
abstract = {Compilers are a kind of important software, and similar to the quality assurance of other software, compiler testing is one of the most widely-used ways of guaranteeing their quality. Compiler bugs tend to occur in compiler optimizations. Detecting optimization bugs needs to consider two main factors: 1) the optimization flags controlling the accessability of the compiler buggy code should be turned on; and 2) the test program should be able to trigger the buggy code. However, existing compiler testing approaches only consider the latter to generate effective test programs, but just run them under several pre-defined optimization levels (e.g., -O0, -O1, -O2, -O3, -Os in GCC). To better understand the influence of compiler optimizations on compiler testing, we conduct the first empirical study, and find that 1) all the bugs detected under the widely-used optimization levels are also detected under the explored optimization settings (we call a combination of optimization flags turned on for compilation an optimization setting), while 83.54% of bugs are only detected under the latter; 2) there exist both inhibition effect and promotion effect among optimization flags for compiler testing, indicating the necessity and challenges of considering the factor of compiler optimizations in compiler testing. We then propose the first approach, called COTest, by considering both factors to test compilers. Specifically, COTest first adopts machine learning (the XGBoost algorithm) to model the relationship between test programs and optimization settings, to predict the bug-triggering probability of a test program under an optimization setting. Then, it designs a diversity augmentation strategy to select a set of diverse candidate optimization settings for prediction for a test program. Finally, Top-K optimization settings are selected for compiler testing according to the predicted bug-triggering probabilities. The experiments on GCC and LLVM demonstrate its effectiveness, especially COTest detects 17 previously unknown bugs, 11 of which have been fixed or confirmed by developers.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
keywords = {Compiler Testing, Compiler Optimization, Machine Learning}
}

@inproceedings{10.1145/2338966.2336799,
author = {Alipour, Mohammad Amin and Groce, Alex},
title = {Extended Program Invariants: Applications in Testing and Fault Localization},
year = {2012},
isbn = {9781450314558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338966.2336799},
doi = {10.1145/2338966.2336799},
abstract = { Invariants are powerful tools for program analysis and reasoning.Several tools and techniques have been developed to infer invariants of a program. Given a test suite for a program, an invariant detection tool (IDT) extracts (potential) invariants from the program execution on test cases of the test suite. The resultant invariants contain relations only over variables and constants that are visible to the IDT. IDTs are usually unable to extract invariants about execution features like taken branches, since programs usually do not have state variables for such features. Thus, the IDT has no information about such features in order to infer relations between them. We speculate that invariants about execution features are useful for understanding test suites; we call these invariants, extended invariants.  In this paper, we discuss potential applications of extended invariants in understanding of test suites, and fault localization. We illustrate the usefulness of extended invariants with some small examples that use basic block count as the execution feature in extended invariants. We believe extended invariants provide useful information about execution of programs that can be utilized in program analysis and testing. },
booktitle = {Proceedings of the Ninth International Workshop on Dynamic Analysis},
pages = {7–11},
numpages = {5},
location = {Minneapolis, MN, USA},
series = {WODA 2012}
}

@inproceedings{10.1145/2304510.2304517,
author = {Letarte, Dominic and Gauthier, Francois and Merlo, Ettore and Sutyanyong, Nattavut and Zuzarte, Calisto},
title = {Targeted Genetic Test SQL Generation for the DB2 Database},
year = {2012},
isbn = {9781450314299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304510.2304517},
doi = {10.1145/2304510.2304517},
abstract = {Automatic Query generators have been shown to be effective tools for software testing. For the most part, they have been used in system testing for the database as a whole or to generate specific queries to test specific features with not much randomness. In this work we explore the problems encountered when using a genetic algorithm to generate SQL for testing a large database system. General random SQL generation that tests the database system as a whole using genetic algorithms is relatively simple. One would need to generate millions of test cases to have a reasonable chance of hitting specific combinations of features. In order to optimize the testing, one needs to generate targeted SQL queries that narrow the testing to specific feature areas and feature combinations but yet preserve a certain amount of randomness and exploit the strength of a genetic algorithm. To do this effectively, the test generator needs to be guided so that it does not stray too much from the goals of the more targeted test requirement. In this work we explore a genetic algorithm approach to generate test queries that exercise target sub-sequences of features. Genetic algorithm parameters such as genome representation, reproduction, fitness evaluation, and selection are described. Preliminary results obtained comparing the presented approach with a random query generator are presented and discussed. We further present the DB2 SQL Query Optimizer, the application which we are using as a case study and target queries that go through certain optimization rule sequences. This application is larger and more complex in terms of code size and data input complexity then software previously used for studying test data generation.},
booktitle = {Proceedings of the Fifth International Workshop on Testing Database Systems},
articleno = {5},
numpages = {6},
keywords = {genetic algorithms, automatic query generators},
location = {Scottsdale, Arizona},
series = {DBTest '12}
}

@inproceedings{10.1145/2591062.2591077,
author = {Mattavelli, Andrea},
title = {Understanding the Redundancy of Software Systems},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591077},
doi = {10.1145/2591062.2591077},
abstract = { Our research aims to study and characterize the redundancy of software systems. Intuitively, a software is redundant when it can perform the same functionality in different ways. Researches have successfully defined several techniques that exploit various form of redundancy, for example for tolerating failures at runtime and for testing purposes.  We aim to formalize and study the redundancy of software systems in general. In particular, we are interested in the intrinsic redundancy of software systems, that is a form of undocumented redundancy present in software systems as consequence of various design and implementation decisions. In this thesis we will formalize the intuitive notion of redundancy. On the basis of such formalization, we will investigate the pervasiveness and the fundamental characteristics of the intrinsic redundancy of software systems. We will study the nature, the origin, and various forms of such redundancy. We will also develop techniques to automatically identify the intrinsic redundancy of software systems. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {698–701},
numpages = {4},
keywords = {Redundancy, equivalence, execution diversity},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2338967.2336812,
author = {Bradbury, Jeremy S. and Segall, Itai and Farchi, Eitan and Jalbert, Kevin and Kelk, David},
title = {Using Combinatorial Benchmark Construction to Improve the Assessment of Concurrency Bug Detection Tools},
year = {2012},
isbn = {9781450314565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338967.2336812},
doi = {10.1145/2338967.2336812},
abstract = { Many different techniques for testing and analyzing concurrency programs have been proposed in the literature. Currently, it is difficult to assess the fitness of a particular concurrency bug detection method and to compare it to other bug detection methods due to a lack of unbiased data that is representative of the kinds of concurrency programs that are used in practice. To address this problem we propose a new benchmark of concurrent Java programs that is constructed using combinatorial test design. In this paper we present our combinatorial model for creating a benchmark, we propose a new concurrency benchmark and we discuses the relationship between our new benchmarks and existing benchmarks. Specific combinations of the model parameters define different interleaving spaces, thus differentiating between different test tools. },
booktitle = {Proceedings of the 2012 Workshop on Parallel and Distributed Systems: Testing, Analysis, and Debugging},
pages = {25–35},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {PADTAD 2012}
}

@inproceedings{10.1145/3442391.3442411,
author = {Fischer, Stefan and Ramler, Rudolf and Klammer, Claus and Rabiser, Rick},
title = {Testing of Highly Configurable Cyber-Physical Systems – A Multiple Case Study},
year = {2021},
isbn = {9781450388245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3442391.3442411},
doi = {10.1145/3442391.3442411},
abstract = {Cyber-physical systems, i.e., systems that seamlessly integrate computation and physical components, are typically highly-configurable systems. Testing such systems is particularly challenging because they comprise a large number of heterogeneous components that can be configured and combined in different ways. Despite a plethora of work investigating software testing in general and software product line testing in particular, variability in tests and how industry does actually manage testing highly configurable cyber-physical systems is not well understood. In this paper, we report the results of a multiple case study we conducted with three companies developing and maintaining highly-configurable cyber-physical systems focusing on their testing practices, with a particular focus on how they manage variability in tests. We conclude that experienced-based selection of configurations for testing is currently predominant. Variability modeling techniques are not utilized and the dependencies between configuration options are only partially modeled at best. However, the companies are aware of the situation and have the need and desire to cover more configuration combinations by automated tests. This in turn raises many questions, which might also be of interest to the scientific community and motivate future research. },
booktitle = {15th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {19},
numpages = {10},
keywords = {interview, variability testing, industry case, configuration testing},
location = {Krems, Austria},
series = {VaMoS'21}
}

@article{10.1145/2716276.2723708,
author = {Binder, Robert V. and Legeard, Bruno and Kramer, Anne},
title = {Model-Based Testing: Where Does It Stand? MBT Has Positive Effects on Efficiency and Effectiveness, Even If It Only Partially Fulfills High Expectations.},
year = {2015},
issue_date = {December 2014 / January 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/2716276.2723708},
doi = {10.1145/2716276.2723708},
abstract = {You have probably heard about MBT (model-based testing), but like many software-engineering professionals who have not used MBT, you might be curious about others’ experience with this test-design method. From mid-June 2014 to early August 2014, we conducted a survey to learn how MBT users view its efficiency and effectiveness. The 2014 MBT User Survey, a follow-up to a similar 2012 survey, was open to all those who have evaluated or used any MBT approach. Its 32 questions included some from a survey distributed at the 2013 User Conference on Advanced Automated Testing. Some questions focused on the efficiency and effectiveness of MBT, providing the figures that managers are most interested in. Other questions were more technical and sought to validate a common MBT classification scheme. A common classification scheme could help users understand both the general diversity and specific approaches. The 2014 survey provides a realistic picture of the current state of MBT practice. This article presents some highlights of the survey findings.},
journal = {Queue},
month = {dec},
pages = {40–48},
numpages = {9}
}

@inproceedings{10.1145/1145735.1145741,
author = {Andrews, James H. and Haldar, Susmita and Lei, Yong and Li, Felix Chun Hang},
title = {Tool Support for Randomized Unit Testing},
year = {2006},
isbn = {159593457X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1145735.1145741},
doi = {10.1145/1145735.1145741},
abstract = {There are several problem areas that must be addressed when applying randomization to unit testing. As yet no general, fully automated solution that works for all units has been proposed. We therefore have developed RUTE-J, a Java package intended to help programmers do randomized unit testing in Java. In this paper, we describe RUTE-J and illustrate how it supports the development of per-unit solutions for the problems of randomized unit testing. We report on an experiment in which we applied RUTE-J to the standard Java TreeMap class, measuring the efficiency and effectiveness of the technique. We also illustrate the use of randomized testing in experimentation, by adapting RUTE-J so that it generates randomized minimal covering test suites, and measuring the effectiveness of the test suites generated.},
booktitle = {Proceedings of the 1st International Workshop on Random Testing},
pages = {36–45},
numpages = {10},
keywords = {unit testing, randomized testing},
location = {Portland, Maine},
series = {RT '06}
}

@article{10.1145/3105906,
author = {Monperrus, Martin},
title = {Automatic Software Repair: A Bibliography},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3105906},
doi = {10.1145/3105906},
abstract = {This article presents a survey on automatic software repair. Automatic software repair consists of automatically finding a solution to software bugs without human intervention. This article considers all kinds of repairs. First, it discusses behavioral repair where test suites, contracts, models, and crashing inputs are taken as oracle. Second, it discusses state repair, also known as runtime repair or runtime recovery, with techniques such as checkpoint and restart, reconfiguration, and invariant restoration. The uniqueness of this article is that it spans the research communities that contribute to this body of knowledge: software engineering, dependability, operating systems, programming languages, and security. It provides a novel and structured overview of the diversity of bug oracles and repair operators used in the literature.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {17},
numpages = {24},
keywords = {Program repair, self-healing software}
}

@inproceedings{10.1145/1370175.1370223,
author = {Arcuri, Andrea},
title = {On the Automation of Fixing Software Bugs},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370175.1370223},
doi = {10.1145/1370175.1370223},
abstract = {Software Testing can take up to half of the resources of the development of new software. Although there has been a lot of work on automating the testing phase, fixing a bug after its presence has been discovered is still a duty of the programmers. Techniques to help the software developers for locating bugs exist though, and they take name of Automated Debugging. However, to our best knowledge, there has been only little attempt in the past to completely automate the actual changing of the software for fixing the bugs. Therefore, in this paper we propose an evolutionary approach to automate the task of fixing bugs. The basic idea is to evolve the programs (e.g., by using Genetic Programming) with a fitness function that is based on how many unit tests they are able to pass. If a formal specification of the buggy software is given, more sophisticated fitness functions can be designed. Moreover, by using the formal specification as an oracle, we can generate as many unit tests as we want. Hence, a co-evolution between programs and unit tests might take place to give even better results. It is important to know that, to fix the bugs in a program with this novel approach, a user needs only to provide either a formal specification or a set of unit tests. No other information is required.},
booktitle = {Companion of the 30th International Conference on Software Engineering},
pages = {1003–1006},
numpages = {4},
keywords = {repair, automated debugging, co-evolution, automatic bug fixing, genetic programming},
location = {Leipzig, Germany},
series = {ICSE Companion '08}
}

@inproceedings{10.1145/1967677.1967693,
author = {Navabpour, Samaneh and Bonakdarpour, Borzoo and Fischmeister, Sebastian},
title = {Software Debugging and Testing Using the Abstract Diagnosis Theory},
year = {2011},
isbn = {9781450305556},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1967677.1967693},
doi = {10.1145/1967677.1967693},
abstract = {In this paper, we present a notion of observability and controllability in the context of software testing and debugging. Our view of observability is based on the ability of developers, testers, and debuggers to trace back a data dependency chain and observe the value of a variable by starting from a set of variables that are naturally observable (e.g., input/output variables). Likewise, our view of controllability enables one to modify and control the value of a variable through a data dependency chain by starting from a set of variables that can be modified (e.g., input variables). Consequently, the problem that we study in this paper is to identify the minimum number of variables that have to be made observable/controllable in order for a tester or debugger to observe/control the value of another set of variables of interest, given the source code. We show that our problem is an instance of the well-known abstract diagnosis problem, where the objective is to find the minimum number of faulty components in a digital circuit, given the system description and value of input/output variables. We show that our problem is NP-complete even if the length of data dependencies is at most 2. In order to cope with the inevitable exponential complexity, we propose a mapping from the general problem, where the length of data dependency chains is unknown a priori, to integer linear programming. Our method is fully implemented in a tool chain for MISRA-C compliant source codes. Our experiments with several real-world applications show that in average, a significant number of debugging points can be reduced using our methods. This result is our motivation to apply our approach in debugging and instrumentation of embedded software, where changes must be minimal as they can perturb the timing constraints and resource consumption. Another interesting application of our results is in data logging of non-terminating embedded systems, where axillary data storage devices are slow and have limited size.},
booktitle = {Proceedings of the 2011 SIGPLAN/SIGBED Conference on Languages, Compilers and Tools for Embedded Systems},
pages = {111–120},
numpages = {10},
keywords = {testing, diagnosis, logging, software debugging},
location = {Chicago, IL, USA},
series = {LCTES '11}
}

@article{10.1145/2016603.1967693,
author = {Navabpour, Samaneh and Bonakdarpour, Borzoo and Fischmeister, Sebastian},
title = {Software Debugging and Testing Using the Abstract Diagnosis Theory},
year = {2011},
issue_date = {May 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/2016603.1967693},
doi = {10.1145/2016603.1967693},
abstract = {In this paper, we present a notion of observability and controllability in the context of software testing and debugging. Our view of observability is based on the ability of developers, testers, and debuggers to trace back a data dependency chain and observe the value of a variable by starting from a set of variables that are naturally observable (e.g., input/output variables). Likewise, our view of controllability enables one to modify and control the value of a variable through a data dependency chain by starting from a set of variables that can be modified (e.g., input variables). Consequently, the problem that we study in this paper is to identify the minimum number of variables that have to be made observable/controllable in order for a tester or debugger to observe/control the value of another set of variables of interest, given the source code. We show that our problem is an instance of the well-known abstract diagnosis problem, where the objective is to find the minimum number of faulty components in a digital circuit, given the system description and value of input/output variables. We show that our problem is NP-complete even if the length of data dependencies is at most 2. In order to cope with the inevitable exponential complexity, we propose a mapping from the general problem, where the length of data dependency chains is unknown a priori, to integer linear programming. Our method is fully implemented in a tool chain for MISRA-C compliant source codes. Our experiments with several real-world applications show that in average, a significant number of debugging points can be reduced using our methods. This result is our motivation to apply our approach in debugging and instrumentation of embedded software, where changes must be minimal as they can perturb the timing constraints and resource consumption. Another interesting application of our results is in data logging of non-terminating embedded systems, where axillary data storage devices are slow and have limited size.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {111–120},
numpages = {10},
keywords = {logging, software debugging, diagnosis, testing}
}

@inproceedings{10.1109/ASE.2019.00138,
author = {Tokumoto, Susumu and Takayama, Kuniharu},
title = {PHANTA: Diversified Test Code Quality Measurement for Modern Software Development},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00138},
doi = {10.1109/ASE.2019.00138},
abstract = {Test code is becoming more essential to the modern software development process. However, practitioners often pay inadequate attention to key aspects of test code quality, such as bug detectability, maintainability and speed. Existing tools also typically report a single test code quality measure, such as code coverage, rather than a diversified set of metrics. To measure and visualize quality of test code in a comprehensive fashion, we developed an integrated test code analysis tool called Phanta. In this show case, we posit that the enhancement of test code quality is key to modernizing software development, and show how Phanta's techniques measure the quality using mutation analysis, test code clone detection, and so on. Further, we present an industrial case study where Phanta was applied to analyze test code in a real Fujitsu project, and share lessons learned from the case study.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1206–1207},
numpages = {2},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.5555/3200334.3200347,
author = {Duretec, Kresimir and Rauber, Andreas and Becker, Christoph},
title = {A Text Extraction Software Benchmark Based on a Synthesized Dataset},
year = {2017},
isbn = {9781538638613},
publisher = {IEEE Press},
abstract = {Text extraction plays an important function for data processing workflows in digital libraries. For example, it is a crucial prerequisite for evaluating the quality of migrated textual documents. Complex file formats make the extraction process error-prone and have made it very challenging to verify the correctness of extraction components. Based on digital preservation and information retrieval scenarios, three quality requirements in terms of effectiveness of text extraction tools are identified: 1) is a certain text snippet correctly extracted from a document, 2) does the extracted text appear in the right order relative to other elements and, 3) is the structure of the text preserved. A number of text extraction tools is available fulfilling these three quality requirements to various degrees. However, systematic benchmarks to evaluate those tools are still missing, mainly due to the lack of datasets with accompanying ground truth. The contribution of this paper is two-fold. First we describe a dataset generation method based on model driven engineering principles and use it to synthesize a dataset and its ground truth directly from a model. Second, we define a benchmark for text extraction tools and complete an experiment to calculate performance measures for several tools that cover the three quality requirements. The results demonstrate the benefits of the approach in terms of scalability and effectiveness in generating ground truth for content and structure of text elements.},
booktitle = {Proceedings of the 17th ACM/IEEE Joint Conference on Digital Libraries},
pages = {109–118},
numpages = {10},
keywords = {model driven engineering, dataset, performance measures, software benchmark, software testing, digital preservation, text extraction, ground truth},
location = {Toronto, Ontario, Canada},
series = {JCDL '17}
}

@inproceedings{10.1145/3377024.3377042,
author = {Krieter, Sebastian and Th\"{u}m, Thomas and Schulze, Sandro and Saake, Gunter and Leich, Thomas},
title = {YASA: Yet Another Sampling Algorithm},
year = {2020},
isbn = {9781450375016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377024.3377042},
doi = {10.1145/3377024.3377042},
abstract = {Configurable systems allow users to derive customized software variants with behavior and functionalities tailored to individual needs. Developers of these configurable systems need to ensure that each configured software variant works as intended. Thus, software testing becomes highly relevant, but also highly expensive due to large configuration spaces that grow exponentially in the number of features. To this end, sampling techniques, such as t-wise interaction sampling, are used to generate a small yet representative subset of configurations, which can be tested even with a limited amount of resources. However, even state-of-the-art t-wise interaction sampling techniques do not scale well for systems with large configuration spaces. In this paper, we introduce the configurable technique YASA that aims to be more efficient than other existing techniques and enables control over trading-off sampling time and sample size. The general algorithm of YASA is based on the existing technique IPOG, but introduces several improvements and options to adapt the sampling procedure to a given configurable system. We evaluate our approach in terms of sampling time and sample size by comparing it to existing t-wise interaction sampling techniques. We find that YASA performs well even for large-scale system and is also able to produce smaller samples than existing techniques.},
booktitle = {Proceedings of the 14th International Working Conference on Variability Modelling of Software-Intensive Systems},
articleno = {4},
numpages = {10},
keywords = {configurable system, T-wise sampling, software product lines, product-based testing},
location = {Magdeburg, Germany},
series = {VAMOS '20}
}

@inproceedings{10.1145/1368088.1368139,
author = {Mei, Lijun and Chan, W.K. and Tse, T.H.},
title = {Data Flow Testing of Service-Oriented Workflow Applications},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368139},
doi = {10.1145/1368088.1368139},
abstract = {WS-BPEL applications are a kind of service-oriented application. They use XPath extensively to integrate loosely-coupled workflow steps. However, XPath may extract wrong data from the XML messages received, resulting in erroneous results in the integrated process. Surprisingly, although XPath plays a key role in workflow integration, inadequate researches have been conducted to address the important issues in software testing. This paper tackles the problem. It also demonstrates a novel transformation strategy to construct artifacts. We use the mathematical definitions of XPath constructs as rewriting rules, and propose a data structure called XPath Rewriting Graph (XRG), which not only models how an XPath is conceptually rewritten but also tracks individual rewritings progressively. We treat the mathematical variables in the applied rewriting rules as if they were program variables, and use them to analyze how information may be rewritten in an XPath conceptually. We thus develop an algorithm to construct XRGs and a novel family of data flow testing criteria to test WS-BPEL applications. Experiment results show that our testing approach is promising.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {371–380},
numpages = {10},
keywords = {rewriting rules, xpath, workflow testing, service-orientation, ws-bpel, xml document model, testing, soa, xml},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/3338906.3338908,
author = {Ahmadi, Reza and Dingel, Juergen},
title = {Concolic Testing for Models of State-Based Systems},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338908},
doi = {10.1145/3338906.3338908},
abstract = {Testing models of modern cyber-physical systems is not straightforward due to timing constraints, numerous if not infinite possible behaviors, and complex communications between components. Software testing tools and approaches that can generate test cases to test these systems are therefore important. Many of the existing automatic approaches support testing at the implementation level only. The existing model-level testing tools either treat the model as a black box (e.g., random testing approaches) or have limitations when it comes to generating complex test sequences (e.g., symbolic execution). This paper presents a novel approach and tool support for automatic unit testing of models of real-time embedded systems by conducting concolic testing, a hybrid testing technique based on concrete and symbolic execution. Our technique conducts automatic concolic testing in two phases. In the first phase, model is isolated from its environment, is transformed to a testable model and is integrated with a test harness. In the second phase, the harness tests the model concolically and reports the test execution results. We describe an implementation of our approach in the context of Papyrus-RT, an open source Model Driven Engineering (MDE) tool based on the modeling language UML-RT, and report the results of applying our concolic testing approach to a set of standard benchmark models to validate our approach.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {4–15},
numpages = {12},
keywords = {Concolic Testing, Model-driven Engineering, State Machine},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@article{10.1145/3517036,
author = {Yin, Zijing and Xu, Yiwen and Ma, Fuchen and Gao, Haohao and Qiao, Lei and Jiang, Yu},
title = {Scanner++: Enhanced Vulnerability Detection of Web Applications with Attack Intent Synchronization},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3517036},
doi = {10.1145/3517036},
abstract = {Scanners are commonly applied for detecting vulnerabilities in web applications. Various scanners with different strategies are widely in use, but their performance is challenged by the increasing diversity of target applications that have more complex attack surfaces (i.e., website paths) and covert vulnerabilities that can only be exploited by more sophisticated attack vectors (i.e., payloads). In this paper, we propose Scanner++, a framework that improves web vulnerability detection of existing scanners through combining their capabilities with attack intent synchronization. We design Scanner++ as a proxy-based architecture while using a package-based intent synchronization approach. Scanner++ first uses a purification mechanism to aggregate and refine attack intents, consisting of attack surfaces and attack vectors extracted from the base scanners’ request packets. Then, Scanner++ uses a runtime intent synchronization mechanism to select relevant attack intents according to the scanners’ detection spots to guide their scanning process. Consequently, base scanners can expand their attack surfaces, generate more diverse attack vectors and achieve better vulnerability detection performance. For evaluation, we implemented and integrated Scanner++ together with four widely used scanners, BurpSuite, AWVS, Arachni, and ZAP, testing it on ten benchmark web applications and three well-tested real-world web applications of a critical financial platform from our industry partner. Working under the Scanner++ framework helps BurpSuite, AWVS, Arachni, and ZAP cover 15.26%, 37.14%, 59.21%, 68.54% more pages, construct 12.95x, 1.13x, 15.03x, 52.66x more attack packets, and discover 77, 55, 77, 176 more bugs respectively. Furthermore, Scanner++ detected 8 serious previously unknown vulnerabilities on real-world applications, while the base scanners only found 3 of them.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
keywords = {Attack Intent, Scanner, Web Security, Synchronization}
}

@inproceedings{10.1145/2393596.2393636,
author = {Park, Sangmin and Hossain, B. M. Mainul and Hussain, Ishtiaque and Csallner, Christoph and Grechanik, Mark and Taneja, Kunal and Fu, Chen and Xie, Qing},
title = {CarFast: Achieving Higher Statement Coverage Faster},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393636},
doi = {10.1145/2393596.2393636},
abstract = {Test coverage is an important metric of software quality, since it indicates thoroughness of testing. In industry, test coverage is often measured as statement coverage. A fundamental problem of software testing is how to achieve higher statement coverage faster, and it is a difficult problem since it requires testers to cleverly find input data that can steer execution sooner toward sections of application code that contain more statements.We created a novel fully automatic approach for aChieving higher stAtement coveRage FASTer (CarFast), which we implemented and evaluated on twelve generated Java applications whose sizes range from 300 LOC to one million LOC. We compared CarFast with several popular test case generation techniques, including pure random, adaptive random, and Directed Automated Random Testing (DART). Our results indicate with strong statistical significance that when execution time is measured in terms of the number of runs of the application on different input test data, CarFast outperforms the evaluated competitive approaches on most subject applications.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {35},
numpages = {11},
keywords = {experimentation, testing, statement coverage},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1145/3368089.3409737,
author = {Gaaloul, Khouloud and Menghi, Claudio and Nejati, Shiva and Briand, Lionel C. and Wolfe, David},
title = {Mining Assumptions for Software Components Using Machine Learning},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409737},
doi = {10.1145/3368089.3409737},
abstract = {Software verification approaches aim to check a software component under analysis for all possible environments. In reality, however, components are expected to operate within a larger system and are required to satisfy their requirements only when their inputs are constrained by environment assumptions. In this paper, we propose EPIcuRus, an approach to automatically synthesize environment assumptions for a component under analysis (i.e., conditions on the component inputs under which the component is guaranteed to satisfy its requirements). EPIcuRus combines search-based testing, machine learning and model checking. The core of EPIcuRus is a decision tree algorithm that infers environment assumptions from a set of test results including test cases and their verdicts. The test cases are generated using search-based testing, and the assumptions inferred by decision trees are validated through model checking. In order to improve the efficiency and effectiveness of the assumption generation process, we propose a novel test case generation technique, namely Important Features Boundary Test (IFBT), that guides the test generation based on the feedback produced by machine learning. We evaluated EPIcuRus by assessing its effectiveness in computing assumptions on a set of study subjects that include 18 requirements of four industrial models. We show that, for each of the 18 requirements, EPIcuRus was able to compute an assumption to ensure the satisfaction of that requirement, and further, ≈78% of these assumptions were computed in one hour.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {159–171},
numpages = {13},
keywords = {Search-based software testing, Machine learning, Model checking, Decision trees, Environment assumptions},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/1370042.1370061,
author = {Ribeiro, Jos\'{e} Carlos Bregieiro and Rela, M\'{a}rio Zenha and de Vega, Francisco Fernand\'{e}z},
title = {A Strategy for Evaluating Feasible and Unfeasible Test Cases for the Evolutionary Testing of Object-Oriented Software},
year = {2008},
isbn = {9781605580302},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370042.1370061},
doi = {10.1145/1370042.1370061},
abstract = {Evolutionary Testing is an emerging methodology for automatically producing high quality test data. The focus of our on-going work is precisely on generating test data for the structural unit-testing of object-oriented Java programs. The primary objective is that of efficiently guiding the search process towards the definition of a test set that achieves full structural coverage of the test object.However, the state problem of object-oriented programs requires specifying carefully fine-tuned methodologies that promote the traversal of problematic structures and difficult control-flow paths - which often involves the generation of complex and intricate test cases, that define elaborate state scenarios.This paper proposes a methodology for evaluating the quality of both feasible and unfeasible test cases - i.e., those that are effectively completed and terminate with a call to the method under test, and those that abort prematurely because a runtime exception is thrown during test case execution. With our approach, unfeasible test cases are considered at certain stages of the evolutionary search, promoting diversity and enhancing the possibility of achieving full coverage.},
booktitle = {Proceedings of the 3rd International Workshop on Automation of Software Test},
pages = {85–92},
numpages = {8},
keywords = {search-based test case generation, evolutionary testing, strongly-typed genetic programming, object-orientation},
location = {Leipzig, Germany},
series = {AST '08}
}

