@article{10.1145/2492248.2492263,
author = {Dalal, Sandeep and Chhillar, Rajender Singh},
title = {Empirical Study of Root Cause Analysis of Software Failure},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/2492248.2492263},
doi = {10.1145/2492248.2492263},
abstract = {Root Cause Analysis (RCA) is the process of identifying project issues, correcting them and taking preventive actions to avoid occurrences of such issues in the future. Issues could be variance in schedule, effort, cost, productivity, expected results of software, performance parameters and customer satisfaction. RCA also involves collecting valid data, analyzing it, deriving metrics and finding root causes using RCA methods. In this paper we will do Root cause analysis of some severe software failures that happened in the past and of some failures in ongoing projects in the software Industry. We will also describe various RCA methods and processes used in the software Industry to reduce the chances of software failure.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {1–7},
numpages = {7},
keywords = {process and methods, RCA, root cause analysis, root cause, software failure causes}
}

@article{10.1145/367008.367015,
author = {Bible, John and Rothermel, Gregg and Rosenblum, David S.},
title = {A Comparative Study of Coarse- and Fine-Grained Safe Regression Test-Selection Techniques},
year = {2001},
issue_date = {April 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {10},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/367008.367015},
doi = {10.1145/367008.367015},
abstract = {Regression test-selection techniques reduce the cost of regression testing by selecting a subset of an existing test suite to use in retesting a modified program. Over the past two decades, numerous regression test-selection techniques have been described in the literature. Initial empirical studies of some of these techniques have suggested that they can indeed benefit testers, but so far, few studies have empirically compared different techniques. In this paper, we present the results of a comparative empirical study of two safe regression test-selection techniques. The techniques we studied have been implemented as the tools DejaVu and TestTube; we compared these tools in terms of a cost model incorporating precision (ability to eliminate  unnecessary test cases), analysis cost, and test execution cost. Our results indicate, that in many instances, despite its relative lack of precision, TestTube can reduce the time required for regression testing as much as the more precise DejaVu. In other instances, particularly where the time required to execute test cases is long, DejaVu's superior precision gives it a clear advantage over TestTube. Such variations in relative performance can complicate a tester's choice of which tool to use. Our experimental results suggest that a hybrid regression test-selection tool that combines features of TestTube and DejaVu may be an answer to these complications; we present an initial case study that demonstrates the potential benefit of such a tool.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
pages = {149–183},
numpages = {35},
keywords = {regression testing, regression test selection}
}

@article{10.1145/2000791.2000796,
author = {Binkley, David W. and Harman, Mark and Lakhotia, Kiran},
title = {FlagRemover: A Testability Transformation for Transforming Loop-Assigned Flags},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2000791.2000796},
doi = {10.1145/2000791.2000796},
abstract = {Search-Based Testing is a widely studied technique for automatically generating test inputs, with the aim of reducing the cost of software engineering activities that rely upon testing. However, search-based approaches degenerate to random testing in the presence of flag variables, because flags create spikes and plateaux in the fitness landscape. Both these features are known to denote hard optimization problems for all search-based optimization techniques. Several authors have studied flag removal transformations and fitness function refinements to address the issue of flags, but the problem of loop-assigned flags remains unsolved. This article introduces a testability transformation along with a tool that transforms programs with loop-assigned flags into flag-free equivalents, so that existing search-based test data generation approaches can successfully be applied. The article presents the results of an empirical study that demonstrates the effectiveness and efficiency of the testability transformation on programs including those made up of open source and industrial production code, as well as test data generation problems specifically created to denote hard optimization problems.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {aug},
articleno = {12},
numpages = {33},
keywords = {testability transformation, flags, empirical evaluation, Evolutionary testing}
}

@inproceedings{10.1145/1390817.1390821,
author = {Illes-Seifert, Timea and Paech, Barbara},
title = {Exploring the Relationship of History Characteristics and Defect Count: An Empirical Study},
year = {2008},
isbn = {9781605580517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390817.1390821},
doi = {10.1145/1390817.1390821},
abstract = {During the lifetime of a project, a huge amount of information is generated, e.g. in versioning systems or bug data bases. When analysed appropriately, the knowledge about the previous project characteristics allows estimating the project's future evolution. For example, it is very valuable to know particular history characteristics of a file indicating its fault proneness because it helps testers to focus their testing effort on these specific files. In this paper, we present the results of an empirical study, exploring the relationship between history characteristics of software entities and their defects. For this purpose, we analyze 9 open source java projects. The results show that there are some history characteristics that highly correlate with defects in software, e.g. the number of changes and the number of distinct authors performing changes to a file. The number of co-changed files does not correlate with the defect count.},
booktitle = {Proceedings of the 2008 Workshop on Defects in Large Software Systems},
pages = {11–15},
numpages = {5},
keywords = {defect database, empirical study, versioning systems},
location = {Seattle, Washington},
series = {DEFECTS '08}
}

@article{10.1145/3408896,
author = {Holmes, Josie and Ahmed, Iftekhar and Brindescu, Caius and Gopinath, Rahul and Zhang, He and Groce, Alex},
title = {Using Relative Lines of Code to Guide Automated Test Generation for Python},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3408896},
doi = {10.1145/3408896},
abstract = {Raw lines of code (LOC) is a metric that does not, at first glance, seem extremely useful for automated test generation. It is both highly language-dependent and not extremely meaningful, semantically, within a language: one coder can produce the same effect with many fewer lines than another. However, relative LOC, between components of the same project, turns out to be a highly useful metric for automated testing. In this article, we make use of a heuristic based on LOC counts for tested functions to dramatically improve the effectiveness of automated test generation. This approach is particularly valuable in languages where collecting code coverage data to guide testing has a very high overhead. We apply the heuristic to property-based Python testing using the TSTL (Template Scripting Testing Language) tool. In our experiments, the simple LOC heuristic can improve branch and statement coverage by large margins (often more than 20%, up to 40% or more) and improve fault detection by an even larger margin (usually more than 75% and up to 400% or more). The LOC heuristic is also easy to combine with other approaches and is comparable to, and possibly more effective than, two well-established approaches for guiding random testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {28},
numpages = {38},
keywords = {static code metrics, Automated test generation, testing heuristics}
}

@inproceedings{10.1145/3022099.3022101,
author = {Araiza-Illan, Dejanira and Pipe, Anthony G. and Eder, Kerstin},
title = {Intelligent Agent-Based Stimulation for Testing Robotic Software in Human-Robot Interactions},
year = {2016},
isbn = {9781450342599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3022099.3022101},
doi = {10.1145/3022099.3022101},
abstract = {The challenges of robotic software testing extend beyond conventional software testing. Valid, realistic and interesting tests need to be generated for multiple programs and hardware running concurrently, deployed into dynamic environments with people. We investigate the use of Belief-Desire-Intention (BDI) agents as models for test generation, in the domain of human-robot interaction (HRI) in simulations. These models provide rational agency, causality, and a reasoning mechanism for planning, which emulate both intelligent and adaptive robots, as well as smart testing environments directed by humans. We introduce reinforcement learning (RL) to automate the exploration of the BDI models using a reward function based on coverage feedback. Our approach is evaluated using a collaborative manufacture example, where the robotic software under test is stimulated indirectly via a simulated human co-worker. We conclude that BDI agents provide intuitive models for test generation in the HRI domain. Our results demonstrate that RL can fully automate BDI model exploration, leading to very effective coverage-directed test generation.},
booktitle = {Proceedings of the 3rd Workshop on Model-Driven Robot Software Engineering},
pages = {9–16},
numpages = {8},
keywords = {Coverage-directed test generation, Simulation-based testing, Model-based test generation, Reinforcement learning, Verification agents, Belief-Desire-Intention agents, Human-robot interaction},
location = {Leipzig, Germany},
series = {MORSE '16}
}

@article{10.1145/2659118.2659126,
author = {Babu, P. Arun and Kumar, C. Senthil and Murali, N. and Jayakumar, T.},
title = {Towards Assessment of Software Reliability and Its Characteristics in Safety Systems of Nuclear Reactors},
year = {2014},
issue_date = {September 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2659118.2659126},
doi = {10.1145/2659118.2659126},
abstract = {As software failures in critical systems could be life threatening and catastrophic, the increase in software-based controls for safety operations demand a systematic evaluation of software reliability. To understand the dynamics behind building reliable software, not only is it important to estimate software reliability with high confidence, it is necessary to study the factors that are likely to affect software reliability. This is especially important for systems that are yet to be certified to be used in the field, i.e. have no operational history. Such systems are often difficult to analyze as no or little operational information is available.This paper attempts to generalize the following relationships in safety-critical software: (i) software reliability versus number of faults in software, (ii) software reliability versus results of static and dynamic analysis, and (iii) software reliability versus safety. The present study reports and discusses results observed in safety- critical software in nuclear reactors using mutation-based testing.The results observed in the case studies indicate that (i) reliability estimates based on number of bugs present in software are likely to be inaccurate for safety-critical software, (ii) the relationship between reliability and errors observed during dynamic analysis indicates that the average warnings and errors decrease exponentially as the reliability increases. No conclusive relationship was found between software reliability and warnings observed during static analysis; and (iii) for safety-critical software, the required safety can be achieved by improving the reliability, however the converse is not always true.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {1–17},
numpages = {17},
keywords = {mutation testing, software safety, safety critical software, test adequacy, software reliability}
}

@inproceedings{10.1145/2642937.2642983,
author = {Lucia and Lo, David and Xia, Xin},
title = {Fusion Fault Localizers},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642983},
doi = {10.1145/2642937.2642983},
abstract = {Many spectrum-based fault localization techniques have been proposed to measure how likely each program element is the root cause of a program failure. For various bugs, the best technique to localize the bugs may differ due to the characteristics of the buggy programs and their program spectra. In this paper, we leverage the diversity of existing spectrum-based fault localization techniques to better localize bugs using data fusion methods. Our proposed approach consists of three steps: score normalization, technique selection, and data fusion. We investigate two score normalization methods, two technique selection methods, and five data fusion methods resulting in twenty variants of Fusion Localizer. Our approach is bug specific in which the set of techniques to be fused are adaptively selected for each buggy program based on its spectra. Also, it requires no training data, i.e., execution traces of the past buggy programs.We evaluate our approach on a common benchmark dataset and a dataset consisting of real bugs from three medium to large programs. Our evaluation demonstrates that our approach can significantly improve the effectiveness of existing state-of-the-art fault localization techniques. Compared to these state-of-the-art techniques, the best variants of Fusion Localizer can statistically significantly reduce the amount of code to be inspected to find all bugs. Our best variants can increase the proportion of bugs localized when developers only inspect the top 10% most suspicious program elements by more than 10% and increase the number of bugs that can be successfully localized when developers only inspect up to 10 program blocks by more than 20%.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {127–138},
numpages = {12},
keywords = {fault localization, data fusion},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1145/3134720,
author = {Paine, Drew and Lee, Charlotte P.},
title = {"Who Has Plots?": Contextualizing Scientific Software, Practice, and Visualizations},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {CSCW},
url = {https://doi.org/10.1145/3134720},
doi = {10.1145/3134720},
abstract = {Software is an integral element of the work of science yet it is not commonly an object of inquiry in studies of scientific infrastructures. This paper presents findings from an ethnographic study of a cosmology group's collaborative scientific software production. We demonstrate how these cosmologists use plots to simultaneously test their software and analyze data while interrogating multiple layers of infrastructural components. We broaden perspectives on scientific software development using a sociotechnical, software studies lens to examine this work of scientific discovery as a creative and embodied, yet exacting and methodical, activity that requires a 'human in the loop'. We offer a new reading of scientific software practices to convey how creating scientific software is often really the act of doing science itself--an intervention we believe is necessary to more successfully support scientific software sharing and infrastructure production.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {dec},
articleno = {85},
numpages = {21},
keywords = {cosmology, plots, software studies, research infrastructures, scientific software development, ethnographic study., epoch of reionization, scientific software testing, scientific visualizations}
}

@article{10.1145/3302542.3302544,
author = {Danglot, Benjamin and An, Gabin},
title = {Genetic Improvement Events in 2018},
year = {2019},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {4},
url = {https://doi.org/10.1145/3302542.3302544},
doi = {10.1145/3302542.3302544},
abstract = {Genetic improvement uses optimisation and machine learning techniques, particularly heuristic search and evolutionary algorithms, to improve existing software. The main application is automatic bug fixing, where reducing or eliminating buggy behaviour improves a program. Other applications involve automatically producing a better program that runs faster, uses less memory, uses less energy, or runs on a different type of computer.This article summarise two international events exclusively devoted to this topic in 2018.},
journal = {SIGEVOlution},
month = {jan},
pages = {9–13},
numpages = {5}
}

@article{10.1145/2632434.2632446,
author = {Suresh, Yeresime and Rath, Santanu Ku.},
title = {Evolutionary Algorithms for Object-Oriented Test Data Generation},
year = {2014},
issue_date = {July 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/2632434.2632446},
doi = {10.1145/2632434.2632446},
abstract = {Identification of effective test data for testing a software application is a difficult task. The presence of a large number of decision nodes in a program makes it difficult to test all modules, and as a result consumes a lot of testers' time. The effort required for testing can be reduced by automatic generation of test data for particular modules. Out of numerous optimization algorithms, evolutionary algorithms can help in this scenario by generating relevant test data. The ability of evolutionary algorithms to obtain effective solutions from a very large search space of candidate solutions can be used for automatic test data generation. This paper explores the automatic generation of test data for object-oriented programs based on the concept of the extended control flow graph by utilizing the binary particle swarm optimization and artificial bee colony optimization algorithms. The proposed approach is applied to a bank ATM case study. The experimental results obtained, when compared with the clonal selection algorithm, reveal that the artificial bee colony optimization algorithm is more efficient for generating effective test data than the binary particle swarm optimization and clonal selection algorithms.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {aug},
pages = {1–6},
numpages = {6},
keywords = {evolutionary algorithm, extended control flow graph, test data, fitness function, bank ATM}
}

@article{10.1145/3511804,
author = {Pan, Minxue and Lu, Yifei and Pei, Yu and Zhang, Tian and Li, Xuandong},
title = {Preference-Wise Testing of Android Apps via Test Amplification},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511804},
doi = {10.1145/3511804},
abstract = {Preferences, the setting options provided by Android, are an essential part of Android apps. Preferences allow users to change app features and behaviors dynamically, and therefore their impacts need to be considered when testing the apps. Unfortunately, few test cases explicitly specify the assignments of valid values to the preferences, or configurations, under which they should be executed, and few existing mobile testing tools take the impact of preferences into account or provide help to testers in identifying and setting up the configurations for running the tests. This paper presents the Prefest approach to effective testing of Android apps with preferences. Given an Android app and a set of test cases for the app, Prefest amplifies the test cases with a small number of configurations to exercise more behaviors and detect more bugs that are related to preferences. In an experimental evaluation conducted on real-world Android apps, amplified test cases produced by Prefest from automatically generated test cases covered significantly more code of the apps and detected 7 real bugs, and the tool’s test amplification time was at the same order of magnitude as the running time of the input test cases. Prefest’s effectiveness and efficiency in amplifying programmer-written test cases was comparable with that in amplifying automatically generated test cases.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
keywords = {Android testing, preference-wise testing, Android apps}
}

@article{10.1145/2789209,
author = {Emam, Seyedeh Sepideh and Miller, James},
title = {Test Case Prioritization Using Extended Digraphs},
year = {2015},
issue_date = {December 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/2789209},
doi = {10.1145/2789209},
abstract = {Although many test case prioritization techniques exist, their performance is far from perfect. Hence, we propose a new fault-based test case prioritization technique to promote fault-revealing test cases in model-based testing (MBT) procedures. We seek to improve the fault detection rate—a measure of how fast a test suite is able to detect faults during testing—in scenarios such as regression testing. We propose an extended digraph model as the basis of this new technique. The model is realized using a novel reinforcement-learning (RL)- and hidden-Markov-model (HMM)-based technique which is able to prioritize test cases for regression testing objectives. We present a method to initialize and train an HMM based upon RL concepts applied to an application's digraph model. The model prioritizes test cases based upon forward probabilities, a new test case prioritization approach. In addition, we also propose an alternative approach to prioritizing test cases according to the amount of change they cause in applications. To evaluate the effectiveness of the proposed techniques, we perform experiments on graphical user interface (GUI)-based applications and compare the results with state-of-the-art test case prioritization approaches. The experimental results show that the proposed technique is able to detect faults early within test runs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {6},
numpages = {41},
keywords = {Fault-based test case prioritization, additional statement coverage, random prioritization, reinforcement learning, model-based testing (MBT), HMM, GUI testing}
}

@article{10.1145/3375572.3375578,
author = {Vos, Tanja E. J. and Prasetya, I. S. W. B. and Eldh, Sigrid and Getir, Sinem and Parsai, Ali and Aho, Pekka},
title = {Automating TEST Case Design, Selection and Evaluation Report on 10 Editions of A-TESTWorkshop},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3375572.3375578},
doi = {10.1145/3375572.3375578},
abstract = {Trends such as globalisation, standardisation and shorter life-cycles place great demands on the flexibility of the software industry. In order to compete and cooperate on an international scale, a constantly decreasing time to market and an increasing level of quality are essential. Testing is at the moment the most important and mostly used quality assurance technique applied in industry. However, the complexity of software and hence of their development amount is increasing. Modern systems get larger and more complex, as they connect large amounts of components that interact in many different ways and have constantly changing and different types of requirements (functionality, dependability, usability, performance etc.). Data processing that impacts all aspects of our life is increasingly distributed over clouds and devices. This leads to new concerns, such as availability, security, and privacy, which are aspects that also needs to be tested. Consequently, the development of cost-effective and high-quality systems opens new challenges that cannot be faced only with traditional testing approaches, and specifically manual testing is simply insufficient and unreliable to manage the speed needed, and ensure the coverage of ever-changing systems. New techniques for systematization and automation of testing throughout the software and system life-cycle are required.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {21–24},
numpages = {4}
}

@article{10.1145/2557833.2557843,
author = {Bhasin, Harsh},
title = {Artificial Life and Cellular Automata Based Automated Test Case Generator},
year = {2014},
issue_date = {January 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {39},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/2557833.2557843},
doi = {10.1145/2557833.2557843},
abstract = {Manual test data generation is carried out by using the ability of neurons to recognize patterns. The nervous system and the brain coordinate to generate test cases, which are capable of finding potential faults. Automated test data generators lack the ability to produce efficient test cases because they do not imitate natural processes. This paper proposes using Artificial Life based systems for generating test cases. Cellular Automata and Langton's loop have been used to accomplish the above task. Cellular Automata are parallel distributed systems capable of reproducing using self generated patterns. These fascinating techniques have been amalgamated with standard test data generation techniques to give rise to a methodology, which generates test cases for white box testing. Langton's Loops have been used to generate test cases for Black Box Testing. The approach has been verified on a set of 20 programs. The programs have been selected on the basis of their Lines of Code and utility. The results obtained have been verified using Average Probability of Fault Detection. This paper also proposes a new framework capable of crafting test cases taking into account the oracle cost.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {feb},
pages = {1–5},
numpages = {5},
keywords = {cellular automata, testing, artificial life, automated test data generation}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00031,
author = {Najafi, Armin and Shang, Weiyi and Rigby, Peter C.},
title = {Improving Test Effectiveness Using Test Executions History: An Industrial Experience Report},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00031},
doi = {10.1109/ICSE-SEIP.2019.00031},
abstract = {The cost of software testing has become a burden for software companies in the era of rapid release and continuous integration. Our industrial collaborator Ericsson also faces the challenges of expensive testing processes which are typically part of a complex and specialized testing environment. In order to assist Ericsson with improving the test effectiveness of one of its large subsystems, we adopt test selection and prioritization approaches based on test execution history from prior research. By adopting and simulating those approaches on six months of testing data from our subject system, we confirm the existence of valuable information in the test execution history. In particular, the association between test failures provide the most value to the test selection and prioritization processes. More importantly, during this exercise, we encountered various challenges that are unseen or undiscussed in prior research. We document the challenges, our solutions and the lessons learned as an experience report. Our experiences can be valuable for other software testing practitioners and researchers who would like to adopt existing test effectiveness improvement approaches into their work environment.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {213–222},
numpages = {10},
keywords = {test effectiveness, industrial experience report, test prioritization, test selection},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@article{10.1145/3394112,
author = {Chen, Junjie and Wu, Zhuo and Wang, Zan and You, Hanmo and Zhang, Lingming and Yan, Ming},
title = {Practical Accuracy Estimation for Efficient Deep Neural Network Testing},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/3394112},
doi = {10.1145/3394112},
abstract = {Deep neural network (DNN) has become increasingly popular and DNN testing is very critical to guarantee the correctness of DNN, i.e., the accuracy of DNN in this work. However, DNN testing suffers from a serious efficiency problem, i.e., it is costly to label each test input to know the DNN accuracy for the testing set, since labeling each test input involves multiple persons (even with domain-specific knowledge) in a manual way and the testing set is large-scale. To relieve this problem, we propose a novel and practical approach, called PACE (which is short for Practical ACcuracy Estimation), which selects a small set of test inputs that can precisely estimate the accuracy of the whole testing set. In this way, the labeling costs can be largely reduced by just labeling this small set of selected test inputs. Besides achieving a precise accuracy estimation, to make PACE more practical it is also required that it is interpretable, deterministic, and as efficient as possible. Therefore, PACE first incorporates clustering to interpretably divide test inputs with different testing capabilities (i.e., testing different functionalities of a DNN model) into different groups. Then, PACE utilizes the MMD-critic algorithm, a state-of-the-art example-based explanation algorithm, to select prototypes (i.e., the most representative test inputs) from each group, according to the group sizes, which can reduce the impact of noise due to clustering. Meanwhile, PACE also borrows the idea of adaptive random testing to select test inputs from the minority space (i.e., the test inputs that are not clustered into any group) to achieve great diversity under the required number of test inputs. The two parallel selection processes (i.e., selection from both groups and the minority space) compose the final small set of selected test inputs. We conducted an extensive study to evaluate the performance of PACE based on a comprehensive benchmark (i.e., 24 pairs of DNN models and testing sets) by considering different types of models (i.e., classification and regression models, high-accuracy and low-accuracy models, and CNN and RNN models) and different types of test inputs (i.e., original, mutated, and automatically generated test inputs). The results demonstrate that PACE is able to precisely estimate the accuracy of the whole testing set with only 1.181%∼2.302% deviations, on average, significantly outperforming the state-of-the-art approaches.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {oct},
articleno = {30},
numpages = {35},
keywords = {Deep neural network testing, labeling, test optimization, test input selection}
}

@inproceedings{10.1145/2462307.2462321,
author = {Zhou, Junzan and Li, Shanping and Zhang, Zhen and Ye, Zhen},
title = {Position Paper: Cloud-Based Performance Testing: Issues and Challenges},
year = {2013},
isbn = {9781450320511},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2462307.2462321},
doi = {10.1145/2462307.2462321},
abstract = {Conducting performance testing is essential to evaluate system performance. With the emergence of cloud computing, applying cloud resources for large-scale performance testing become very attractive. Many organizations have applied cloud-based performance testing in realistic projects. Cloud computing brings many benefits for performance testing, while we also have to face many new problems such as performance variation of cloud platform and security problems. In this overview, we discuss the differences between traditional and cloud-based performance testing. We investigate the state-of-art of cloud-based performance testing. We address the key issues with relevant challenges. For some of the issues, we formalize the problems and give our initial idea. We focus on the quality of workload generation and present our experimental results to validate the existence and degree of the challenges. We think that it is beneficial to apply cloud-based performance testing in many cases.},
booktitle = {Proceedings of the 2013 International Workshop on Hot Topics in Cloud Services},
pages = {55–62},
numpages = {8},
keywords = {challenge, performance testing, load testing, cloud, overview},
location = {Prague, Czech Republic},
series = {HotTopiCS '13}
}

@inproceedings{10.1145/3324884.3416552,
author = {Quan, Lili and Guo, Qianyu and Chen, Hongxu and Xie, Xiaofei and Li, Xiaohong and Liu, Yang and Hu, Jing},
title = {SADT: Syntax-Aware Differential Testing of Certificate Validation in SSL/TLS Implementations},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416552},
doi = {10.1145/3324884.3416552},
abstract = {The security assurance of SSL/TLS critically depends on the correct validation of X.509 certificates. Therefore, it is important to check whether a certificate is correctly validated by the SSL/TLS implementations. Although differential testing has been proven to be effective in finding semantic bugs, it still suffers from the following limitations: (1) The syntax of test cases cannot be correctly guaranteed. (2) Current test cases are not diverse enough to cover more implementation behaviours. This paper tackles these problems by introducing SADT, a novel syntax-aware differential testing framework for evaluating the certificate validation process in SSL/TLS implementations. We first propose a tree-based mutation strategy to ensure that the generated certificates are syntactically correct, and then diversify the certificates by sharing interesting test cases among all target SSL/TLS implementations. Such generated certificates are more likely to trigger discrepancies among SSL/TLS implementations, which may indicate some potential bugs.To evaluate the effectiveness of our approach, we applied SADT on testing 6 widely used SSL/TLS implementations, compared with the state-of-the-art fuzzing technique (i.e., AFL) and two differential testing techniques (i.e., NEZHA and RFCcert). The results show that SADT outperforms other techniques in generating discrepancies. In total, 64 unique discrepancies were discovered by SADT, and 13 of them have been confirmed as bugs or fixed by the developers.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {524–535},
numpages = {12},
keywords = {differential testing, certificate validation, SSL/TLS implementation},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inbook{10.1109/ICSE-SEET52601.2021.00011,
author = {Chong, Chun Yong and Thongtanunam, Patanamon and Tantithamthavorn, Chakkrit},
title = {Assessing the Students' Understanding and Their Mistakes in Code Review Checklists: An Experience Report of 1,791 Code Review Checklist Questions from 394 Students},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00011},
abstract = {Code review is a widely-used practice in software development companies to identify defects. Hence, code review has been included in many software engineering curricula at universities worldwide. However, teaching code review is still a challenging task because the code review effectiveness depends on the code reading and analytical skills of a reviewer. While several studies have investigated the code reading techniques that students should use to find defects during code review, little has focused on a learning activity that involves analytical skills. Indeed, developing a code review checklist should stimulate students to develop their analytical skills to anticipate potential issues (i.e., software defects). Yet, it is unclear whether students can anticipate potential issues given their limited experience in software development (programming, testing, etc.). We perform a qualitative analysis to investigate whether students are capable of creating code review checklists, and if the checklists can be used to guide reviewers to find defects. In addition, we identify common mistakes that students make when developing a code review checklist. Our results show that while there are some misconceptions among students about the purpose of code review, students are able to anticipate potential defects and create a relatively good code review checklist. Hence, our results lead us to conclude that developing a code review checklist can be a part of the learning activities for code review in order to scaffold students' skills.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {20–29},
numpages = {10}
}

