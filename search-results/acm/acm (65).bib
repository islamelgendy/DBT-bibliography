@inproceedings{10.1145/3185089.3185152,
author = {Li, Boshu and Wu, Wenjun and Hu, Zhenhui},
title = {Evaluation of Software Quality for Competition-Based Software Crowdsourcing Projects},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185152},
doi = {10.1145/3185089.3185152},
abstract = {Crowdsourcing-based Software Development (CSSD) performs as: many software practitioners use their own experience and technology to participate software development related tasks, through the open platform such as TopCoder. Crowdsourcing software quality issue has caught some researchers' attention, but it is still far from enough, and no work has been done on evaluating crowdsourcing software projects from a macro point of view. In the paper, we apply traditional quality evaluation practice and theory into the evaluation of crowdsourcing-based software quality by proper modification. The main contributions of this paper are: evaluate TopCoder software quality from the perspective of Project Rating and Project Effort respectively, and explore their aggregation strategies. In order to explore the relationship between them, we introduce the definition of quality assurance effort. We believe the final project rating indicator and quality assurance effort can help a project manager to make reasonable decisions on crowdsourcing-based software development tasks.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {102–109},
numpages = {8},
keywords = {Software Quality, Software Competition, TopCoder, Project Effort, Crowdsourcing-based Software Development, Project Rating, Quality Assurance Effort},
location = {Kuantan, Malaysia},
series = {ICSCA 2018}
}

@inproceedings{10.1145/800230.806990,
author = {Seyfer, Harlan K.},
title = {Tailoring Testing to a Specific Compiler—Experiences},
year = {1982},
isbn = {0897910745},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800230.806990},
doi = {10.1145/800230.806990},
abstract = {The testing of the Univac UCS-Pascal compiler is described. Tests were acquired from various sources, converted from existing tests, and developed in house. Test development and execution using the Univac Test Controller System is illustrated with examples. The experiences gained from this and other compiler testing efforts are described.},
booktitle = {Proceedings of the 1982 SIGPLAN Symposium on Compiler Construction},
pages = {140–152},
numpages = {13},
location = {Boston, Massachusetts, USA},
series = {SIGPLAN '82}
}

@article{10.1145/872726.806990,
author = {Seyfer, Harlan K.},
title = {Tailoring Testing to a Specific Compiler—Experiences},
year = {1982},
issue_date = {June 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/872726.806990},
doi = {10.1145/872726.806990},
abstract = {The testing of the Univac UCS-Pascal compiler is described. Tests were acquired from various sources, converted from existing tests, and developed in house. Test development and execution using the Univac Test Controller System is illustrated with examples. The experiences gained from this and other compiler testing efforts are described.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {140–152},
numpages = {13}
}

@inproceedings{10.1145/1137702.1137716,
author = {Wagner, Stefan},
title = {Modelling the Quality Economics of Defect-Detection Techniques},
year = {2006},
isbn = {1595933999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1137702.1137716},
doi = {10.1145/1137702.1137716},
abstract = {There are various ways to evaluate defect-detection techniques. However, for a comprehensive evaluation the only possibility is to reduce all influencing factors to costs. There are already some models and metrics for the cost of quality that can be used in that context. These models allow the structuring of the costs but do not show all influencing factors and their relationships. This paper proposes an analytical model for the economics of defect-detection techniques that can be used for analysis and optimisation of the usage of such techniques. In particular we analyse the sensitivity of the model and how the model can be applied in practice.},
booktitle = {Proceedings of the 2006 International Workshop on Software Quality},
pages = {69–74},
numpages = {6},
keywords = {defect-detection techniques, sensitivity analysis, quality model, software quality economics},
location = {Shanghai, China},
series = {WoSQ '06}
}

@inproceedings{10.1145/174675.175935,
author = {Agrawal, Hiralal},
title = {Dominators, Super Blocks, and Program Coverage},
year = {1994},
isbn = {0897916360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/174675.175935},
doi = {10.1145/174675.175935},
abstract = {In this paper we present techniques to find subsets of nodes of a flowgraph that satisfy the following property: A test set that exercises all nodes in a subset exercises all nodes in the flowgraph. Analogous techniques to find subsets of edges are also proposed. These techniques may be used to significantly reduce the cost of coverage testing of programs. A notion of a super block consisting of one or more basic blocks in that super block must be exercised by the same input. Dominator relationships among super blocks are used to identify a subset of the super blocks whose coverage implies that of all super blocks and, in turn, that of all basic blocks. Experiments with eight systems in the range of 1-75K lines of code show that, on the average, test cases targeted to cover just 29% of the basic blocks and 32% of the branches ensure 100% block and branch coverage, respectively.},
booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {25–34},
numpages = {10},
location = {Portland, Oregon, USA},
series = {POPL '94}
}

@inproceedings{10.1145/3236024.3236037,
author = {Lehmann, Daniel and Pradel, Michael},
title = {Feedback-Directed Differential Testing of Interactive Debuggers},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236037},
doi = {10.1145/3236024.3236037},
abstract = {To understand, localize, and fix programming errors, developers often rely on interactive debuggers. However, as debuggers are software, they may themselves have bugs, which can make debugging unnecessarily hard or even cause developers to reason about bugs that do not actually exist in their code. This paper presents the first automated testing technique for interactive debuggers. The problem of testing debuggers is fundamentally different from the well-studied problem of testing compilers because debuggers are interactive and because they lack a specification of expected behavior. Our approach, called DBDB, generates debugger actions to exercise the debugger and records traces that summarize the debugger's behavior. By comparing traces of multiple debuggers with each other, we find diverging behavior that points to bugs and other noteworthy differences. We evaluate DBDB on the JavaScript debuggers of Firefox and Chromium, finding 19 previously unreported bugs, eight of which are already fixed by the developers.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {610–620},
numpages = {11},
keywords = {Interactive debuggers, JavaScript, Differential testing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3368089.3417942,
author = {Escobar-Vel\'{a}squez, Camilo and Riveros, Diego and Linares-V\'{a}squez, Mario},
title = {MutAPK 2.0: A Tool for Reducing Mutation Testing Effort of Android Apps},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417942},
doi = {10.1145/3368089.3417942},
abstract = {Mutation testing is a time consuming process because large sets of fault-injected-versions of an original app are generated and executed with the purpose of evaluating the quality of a given test suite. In the case of Android apps, recent studies even suggest that mutant generation and mutation testing effort could be greater when the mutants are generated at the APK level. To reduce that effort, useless (e.g., equivalent) mutants should be avoided and mutant selection techniques could be used to reduce the set of mutants used with mutation testing. However, despite the existence of mutation testing tools, none of those tools provides features for removing useless mutants and sampling mutant sets. In this paper, we present MutAPK 2.0, an improved version of our open source mutant generation tool (MutAPK) for Android apps at APK level. To the best of our knowledge, MutAPK 2.0 is the first tool that enables the removal of dead-code mutants, provides a set of mutant selection strategies, and removes automatically equivalent and duplicate mutants. MutAPK 2.0 is publicly available at GitHub: https://thesoftwaredesignlab.github.io/MutAPK/ VIDEO: https://thesoftwaredesignlab.github.io/MutAPK/video.html},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1611–1615},
numpages = {5},
keywords = {Mutant Selection, Mutation Testing, Duplicate, Equivalent, Dead code},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/2806777.2806937,
author = {Huang, Jian and Zhang, Xuechen and Schwan, Karsten},
title = {Understanding Issue Correlations: A Case Study of the Hadoop System},
year = {2015},
isbn = {9781450336512},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2806777.2806937},
doi = {10.1145/2806777.2806937},
abstract = {Over the last decade, Hadoop has evolved into a widely used platform for Big Data applications. Acknowledging its wide-spread use, we present a comprehensive analysis of the solved issues with applied patches in the Hadoop ecosystem. The analysis is conducted with a focus on Hadoop's two essential components: HDFS (storage) and MapReduce (computation), it involves a total of 4218 solved issues over the last six years, covering 2180 issues from HDFS and 2038 issues from MapReduce. Insights derived from the study concern system design and development, particularly with respect to correlated issues and correlations between root causes of issues and characteristics of the Hadoop subsystems. These findings shed light on the future development of Big Data systems, on their testing, and on bug-finding tools.},
booktitle = {Proceedings of the Sixth ACM Symposium on Cloud Computing},
pages = {2–15},
numpages = {14},
keywords = {Hadoop, bug study, issue correlation, big data},
location = {Kohala Coast, Hawaii},
series = {SoCC '15}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search Based Software Engineering for Software Product Line Engineering: A Survey and Directions for Future Work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, SPL, genetic programming, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1109/ICSE.2017.42,
author = {Ma, Wanwangying and Chen, Lin and Zhang, Xiangyu and Zhou, Yuming and Xu, Baowen},
title = {How Do Developers Fix Cross-Project Correlated Bugs? A Case Study on the GitHub Scientific Python Ecosystem},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.42},
doi = {10.1109/ICSE.2017.42},
abstract = {GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects; and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of ecosystem, as well as shed light on the requirements of issue trackers for such bugs.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {381–392},
numpages = {12},
keywords = {cross-project correlated bugs, coordinate, GitHub ecosystems, root causes tracking},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/3468264.3468586,
author = {Chen, Ke and Li, Yufei and Chen, Yingfeng and Fan, Changjie and Hu, Zhipeng and Yang, Wei},
title = {GLIB: Towards Automated Test Oracle for Graphically-Rich Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468586},
doi = {10.1145/3468264.3468586},
abstract = {Graphically-rich applications such as games are ubiquitous with attractive visual effects of Graphical User Interface (GUI) that offers a bridge between software applications and end-users. However, various types of graphical glitches may arise from such GUI complexity and have become one of the main component of software compatibility issues. Our study on bug reports from game development teams in NetEase Inc. indicates that graphical glitches frequently occur during the GUI rendering and severely degrade the quality of graphically-rich applications such as video games. Existing automated testing techniques for such applications focus mainly on generating various GUI test sequences and check whether the test sequences can cause crashes. These techniques require constant human attention to captures non-crashing bugs such as bugs causing graphical glitches. In this paper, we present the first step in automating the test oracle for detecting non-crashing bugs in graphically-rich applications. Specifically, we propose GLIB based on a code-based data augmentation technique to detect game GUI glitches. We perform an evaluation of GLIB on 20 real-world game apps (with bug reports available) and the result shows that GLIB can achieve 100% precision and 99.5% recall in detecting non-crashing bugs such as game GUI glitches. Practical application of GLIB on another 14 real-world games (without bug reports) further demonstrates that GLIB can effectively uncover GUI glitches, with 48 of 53 bugs reported by GLIB having been confirmed and fixed so far.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1093–1104},
numpages = {12},
keywords = {Game Testing, Automated Test Oracle, Deep Learning, GUI Testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3167132.3167247,
author = {Yoon, Hyunmin and Majeed, Shakaiba and Ryu, Minsoo},
title = {Exploring OS-Based Full-System Deterministic Replay},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167247},
doi = {10.1145/3167132.3167247},
abstract = {Modern computer systems have various sources of nondeterminism such as external inputs, concurrency in software and hardware, asynchronous interrupts and timing variations. With these sources of nondeterminism, many errors and bugs can remain undetected during development, manifesting in the form of corrupt data, hangs, crashes or other catastrophic results. Deterministic full-system replay helps in identifying the cause of such failures by reproducing a previously happened execution. Existing full-system deterministic replay schemes are based on either a special hardware implementation or a virtualization platform. Though beneficial, either they require non-trivial modifications to hardware or suffer from lack of reproducibility. This paper presents an innovative operating system (OS) based replay framework called Software Black Box (SBB) which is the first attempt to provide full-system replay without requiring any special hardware implementation or virtualization. It can reproduce the entire execution of a computer system including, user-level processes, OS functions and device drivers with instruction level accuracy. We implemented a prototype of SBB for Linux operating system ported to the ARM uniprocessor environment and evaluated its performance using Phoronix benchmark suites and some networking workloads. The results are promising, making it suitable for many purposes including debugging, testing, security and performance analysis.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1077–1086},
numpages = {10},
keywords = {embedded systems, debugging, record and replay, full-system replay},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2884781.2884834,
author = {Xie, Xiaoyuan and Liu, Zicong and Song, Shuo and Chen, Zhenyu and Xuan, Jifeng and Xu, Baowen},
title = {Revisit of Automatic Debugging via Human Focus-Tracking Analysis},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884834},
doi = {10.1145/2884781.2884834},
abstract = {In many fields of software engineering, studies on human behavior have attracted a lot of attention; however, few such studies exist in automated debugging. Parnin and Orso conducted a pioneering study comparing the performance of programmers in debugging with and without a ranking-based fault localization technique, namely Spectrum-Based Fault Localization (SBFL). In this paper, we revisit the actual helpfulness of SBFL, by addressing some major problems that were not resolved in Parnin and Orso's study. Our investigation involved 207 participants and 17 debugging tasks. A user-friendly SBFL tool was adopted. It was found that SBFL tended not to be helpful in improving the efficiency of debugging. By tracking and analyzing programmers' focus of attention, we characterized their source code navigation patterns and provided in-depth explanations to the observations. Results indicated that (1) a short "first scan" on the source code tended to result in inefficient debugging; and (2) inspections on the pinpointed statements during the "follow-up browsing" were normally just quick skimming. Moreover, we found that the SBFL assistance may even slightly weaken programmers' abilities in fault detection. Our observations imply interference between the mechanism of automated fault localization and the actual assistance needed by programmers in debugging. To resolve this interference, we provide several insights and suggestions.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {808–819},
numpages = {12},
keywords = {spectrum-based fault localization, fault comprehension, automated debugging, navigation pattern, attention tracking, user studies},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2362536.2362567,
author = {Savolainen, Juha and Mannion, Mike and Kuusela, Juha},
title = {Developing Platforms for Multiple Software Product Lines},
year = {2012},
isbn = {9781450310949},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2362536.2362567},
doi = {10.1145/2362536.2362567},
abstract = {Many approaches to software product line engineering have been founded on the development of a single product line platform. However as customer requirements change and new products are added to the product line, software producers recognize that the platform cannot be "stretched" indefinitely and a significant problem is striking a balance between development efficiency by increasing platform commonality and customer dissatisfaction from products with additional undesirable features and properties.One alternative is to develop multiple product lines (MPLs). However the challenge remains about what to include in a multiple product line platform. Drawing upon industrial experience of working with 4 companies, this paper explores the characteristics of the contexts in which MPLs are a viable alternative development strategy and then proposes a framework of approaches to platform development.},
booktitle = {Proceedings of the 16th International Software Product Line Conference - Volume 1},
pages = {220–228},
numpages = {9},
keywords = {software reuse, industrial experience, multiple product lines},
location = {Salvador, Brazil},
series = {SPLC '12}
}

@inproceedings{10.5555/2819009.2819017,
author = {Park, Yongbae and Hong, Shin and Kim, Moonzoo and Lee, Dongju and Cho, Junhee},
title = {Systematic Testing of Reactive Software with Non-Deterministic Events: A Case Study on LG Electric Oven},
year = {2015},
publisher = {IEEE Press},
abstract = {Most home appliance devices such as electric ovens are reactive systems which repeat receiving a user input/event through an event handler, updating their internal state based on the input, and generating outputs. A challenge to test a reactive program is to check if the program correctly reacts to various non-deterministic sequence of events because an unexpected sequence of events may make the system fail due to the race conditions between the main loop and asynchronous event handlers. Thus, it is important to systematically generate/test various sequences of events by controlling the order of events and relative timing of event occurrences with respect to the main loop execution. In this paper, we report our industrial experience to solve the aforementioned problem by developing a systematic event generation framework based on concolic testing technique. We have applied the framework to a LG electric oven and detected several critical bugs including one that makes the oven ignore user inputs due to the illegal state transition.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 2},
pages = {29–38},
numpages = {10},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1109/ICSE.2009.5070516,
author = {Buse, Raymond P. L. and Weimer, Westley},
title = {The Road Not Taken: Estimating Path Execution Frequency Statically},
year = {2009},
isbn = {9781424434534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2009.5070516},
doi = {10.1109/ICSE.2009.5070516},
abstract = {A variety of compilers, static analyses, and testing frameworks rely heavily on path frequency information. Uses for such information range from optimizing transformations to bug finding. Path frequencies are typically obtained through profiling, but that approach is severely restricted: it requires running programs in an indicative environment, and on indicative test inputs. We present a descriptive statistical model of path frequency based on features that can be readily obtained from a program's source code. Our model is over 90% accurate with respect to several benchmarks, and is sufficient for selecting the 5% of paths that account for over half of a program's total runtime. We demonstrate our technique's robustness by measuring its performance as a static branch predictor, finding it to be more accurate than previous approaches on average. Finally, our qualitative analysis of the model provides insight into which source-level features indicate “hot paths.”},
booktitle = {Proceedings of the 31st International Conference on Software Engineering},
pages = {144–154},
numpages = {11},
series = {ICSE '09}
}

@inproceedings{10.1145/3324884.3416590,
author = {Wang, Shangwen and Wen, Ming and Lin, Bo and Wu, Hongjun and Qin, Yihao and Zou, Deqing and Mao, Xiaoguang and Jin, Hai},
title = {Automated Patch Correctness Assessment: How Far Are We?},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416590},
doi = {10.1145/3324884.3416590},
abstract = {Test-based automated program repair (APR) has attracted huge attention from both industry and academia. Despite the significant progress made in recent studies, the overfitting problem (i.e., the generated patch is plausible but overfitting) is still a major and long-standing challenge. Therefore, plenty of techniques have been proposed to assess the correctness of patches either in the patch generation phase or in the evaluation of APR techniques. However, the effectiveness of existing techniques has not been systematically compared and little is known to their advantages and disadvantages. To fill this gap, we performed a large-scale empirical study in this paper. Specifically, we systematically investigated the effectiveness of existing automated patch correctness assessment techniques, including both static and dynamic ones, based on 902 patches automatically generated by 21 APR tools from 4 different categories. Our empirical study revealed the following major findings: (1) static code features with respect to patch syntax and semantics are generally effective in differentiating overfitting patches over correct ones; (2) dynamic techniques can generally achieve high precision while heuristics based on static code features are more effective towards recall; (3) existing techniques are more effective towards certain projects and types of APR techniques while less effective to the others; (4) existing techniques are highly complementary to each other. For instance, a single technique can only detect at most 53.5% of the overfitting patches while 93.3% of them can be detected by at least one technique when the oracle information is available. Based on our findings, we designed an integration strategy to first integrate static code features via learning, and then combine with others by the majority voting strategy. Our experiments show that the strategy can enhance the performance of existing patch correctness assessment techniques significantly.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {968–980},
numpages = {13},
keywords = {program repair, patch correctness, empirical assessment},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1109/ICSE43902.2021.00073,
author = {Truelove, Andrew and de Almeida, Eduardo Santana and Ahmed, Iftekhar},
title = {We'll Fix It in Post: What Do Bug Fixes in Video Game Update Notes Tell Us?},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00073},
doi = {10.1109/ICSE43902.2021.00073},
abstract = {Bugs that persist into releases of video games can have negative impacts on both developers and users, but particular aspects of testing in game development can lead to difficulties in effectively catching these missed bugs. It has become common practice for developers to apply updates to games in order to fix missed bugs. These updates are often accompanied by notes that describe the changes to the game included in the update. However, some bugs reappear even after an update attempts to fix them. In this paper, we develop a taxonomy for bug types in games that is based on prior work. We examine 12,122 bug fixes from 723 updates for 30 popular games on the Steam platform. We label the bug fixes included in these updates to identify the frequency of these different bug types, the rate at which bug types recur over multiple updates, and which bug types are treated as more severe. Additionally, we survey game developers regarding their experience with different bug types and what aspects of game development they most strongly associate with bug appearance. We find that Information bugs appear the most frequently in updates, while Crash bugs recur the most frequently and are often treated as more severe than other bug types. Finally, we find that challenges in testing, code quality, and bug reproduction have a close association with bug persistence. These findings should help developers identify which aspects of game development could benefit from greater attention in order to prevent bugs. Researchers can use our results in devising tools and methods to better identify and address certain bug types.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {736–747},
numpages = {12},
location = {Madrid, Spain},
series = {ICSE '21}
}

@inbook{10.1109/ICSE-SEET52601.2021.00026,
author = {Henley, Austin Z. and Ball, Julian and Klein, Benjamin and Rutter, Aiden and Lee, Dylan},
title = {An Inquisitive Code Editor for Addressing Novice Programmers' Misconceptions of Program Behavior},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00026},
abstract = {Novice programmers face numerous barriers while attempting to learn how to code that may deter them from pursuing a computer science degree or career in software development. In this work, we propose a tool concept to address the particularly challenging barrier of novice programmers holding misconceptions about how their code behaves. Specifically, the concept involves an inquisitive code editor that: (1) identifies misconceptions by periodically prompting the novice programmer with questions about their program's behavior, (2) corrects the misconceptions by generating explanations based on the program's actual behavior, and (3) prevents further misconceptions by inserting test code and utilizing other educational resources. We have implemented portions of the concept as plugins for the Atom code editor and conducted informal surveys with students and instructors. Next steps include deploying the tool prototype to students enrolled in introductory programming courses.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {165–170},
numpages = {6}
}

@inproceedings{10.1145/3338502.3359760,
author = {Chen, Yongheng and Song, Linhai and Xing, Xinyu and Xu, Fengyuan and Wu, Wenfei},
title = {Automated Finite State Machine Extraction},
year = {2019},
isbn = {9781450368346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338502.3359760},
doi = {10.1145/3338502.3359760},
abstract = {Finite state machine (FSM) is a type of computation models widely used in various software programs. Extracting implemented FSMs has many important applications in the networking, software engineering and security domains. In this paper, we first conduct an empirical study to understand how FSMs are implemented in real-world software. Under the guidance of our study results, we then design a static analysis tool, FSMExtractor, to automatically identify and synthesize implemented FSMs. Evaluation using 160 software programs from three sources shows that FSMExtractor can extract all implemented FSMs and report very few false positives.},
booktitle = {Proceedings of the 3rd ACM Workshop on Forming an Ecosystem Around Software Transformation},
pages = {9–15},
numpages = {7},
keywords = {static analysis, finite state machine},
location = {London, United Kingdom},
series = {FEAST'19}
}

@inproceedings{10.1145/2884781.2884796,
author = {Cheung, Shing-Chi and Chen, Wanjun and Liu, Yepang and Xu, Chang},
title = {CUSTODES: Automatic Spreadsheet Cell Clustering and Smell Detection Using Strong and Weak Features},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884796},
doi = {10.1145/2884781.2884796},
abstract = {Various techniques have been proposed to detect smells in spreadsheets, which are susceptible to errors. These techniques typically detect spreadsheet smells through a mechanism based on a fixed set of patterns or metric thresholds. Unlike conventional programs, tabulation styles vary greatly across spreadsheets. Smell detection based on fixed patterns or metric thresholds, which are insensitive to the varying tabulation styles, can miss many smells in one spreadsheet while reporting many spurious smells in another. In this paper, we propose CUSTODES to effectively cluster spreadsheet cells and detect smells in these clusters. The clustering mechanism can automatically adapt to the tabulation styles of each spreadsheet using strong and weak features. These strong and weak features capture the invariant and variant parts of tabulation styles, respectively. As smelly cells in a spreadsheet normally occur in minority, they can be mechanically detected as clusters' outliers in feature spaces. We implemented and applied CUSTODES to 70 spreadsheets files randomly sampled from the EUSES corpus. These spreadsheets contain 1,610 formula cell clusters. Experimental results confirmed that CUSTODES is effective. It successfully detected harmful smells that can induce computation anomalies in spreadsheets with an F-measure of 0.72, outperforming state-of-the-art techniques.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {464–475},
numpages = {12},
keywords = {feature modeling, end-user programming, spreadsheets, smell detection, cell clustering},
location = {Austin, Texas},
series = {ICSE '16}
}

