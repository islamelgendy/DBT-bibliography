@article{10.1145/3450356,
author = {Abrecht, Stephanie and Gauerhof, Lydia and Gladisch, Christoph and Groh, Konrad and Heinzemann, Christian and Woehrle, Matthias},
title = {Testing Deep Learning-Based Visual Perception for Automated Driving},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {4},
issn = {2378-962X},
url = {https://doi.org/10.1145/3450356},
doi = {10.1145/3450356},
abstract = {Due to the impressive performance of deep neural networks (DNNs) for visual perception, there is an increased demand for their use in automated systems. However, to use deep neural networks in practice, novel approaches are needed, e.g., for testing. In this work, we focus on the question of how to test deep learning-based visual perception functions for automated driving. Classical approaches for testing are not sufficient: A purely statistical approach based on a dataset split is not enough, as testing needs to address various purposes and not only average case performance. Additionally, a complete specification is elusive due to the complexity of the perception task in the open context of automated driving. In this article, we review and discuss existing work on testing DNNs for visual perception with a special focus on automated driving for test input and test oracle generation as well as test adequacy. We conclude that testing of DNNs in this domain requires several diverse test sets. We show how such tests sets can be constructed based on the presented approaches addressing different purposes based on the presented methods and identify open research questions.},
journal = {ACM Trans. Cyber-Phys. Syst.},
month = {sep},
articleno = {37},
numpages = {28},
keywords = {autonomous driving, Software testing, perception, deep learning, automated driving, computer vision}
}

@inbook{10.1145/3474624.3476014,
author = {Niemiec, William and Silva, Keslley and Cota, Erika},
title = {ExecutionFlow: A Tool to Compute Test Paths of Java Methods and Constructors},
year = {2021},
isbn = {9781450390613},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474624.3476014},
abstract = { Calculating the code coverage of a test suite depends on the identification of the executed test paths. Available code coverage tools are limited to a few basic tracing strategies and do not provide this information as proper test paths, precluding its use in further analyses. We present ExecutionFlow, a solution to trace executed test paths in Java code. The tool is based on the use of a debugger, aspect-oriented programming, and a testing framework. Through empirical validation we show that it is possible to obtain the test path from several methods and constructors, including high complexity ones. Video demonstration:&nbsp;https://youtu.be/2W5BjVuc6fw },
booktitle = {Brazilian Symposium on Software Engineering},
pages = {221–226},
numpages = {6}
}

@inproceedings{10.1145/2994291.2994303,
author = {Enr\'{\i}quez, J. G. and Blanco, Raquel and Dom\'{\i}nguez-Mayo, F. J. and Tuya, Javier and Escalona, M. J.},
title = {Towards an MDE-Based Approach to Test Entity Reconciliation Applications},
year = {2016},
isbn = {9781450344012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2994291.2994303},
doi = {10.1145/2994291.2994303},
abstract = { The management of large volumes of data has given rise to significant challenges to the entity reconciliation problem (which refers to combining data from different sources for a unified vision) due to the fact that the data are becoming more unstructured, unclean and incomplete, need to be more linked, etc. Testing the applications that implement the entity reconciliation problem is crucial to ensure both the correctness of the reconciliation process and the quality of the reconciled data. In this paper, we present a first approach, based on MDE, which allows the creation of test models for the integration testing of entity reconciliation applications. },
booktitle = {Proceedings of the 7th International Workshop on Automating Test Case Design, Selection, and Evaluation},
pages = {74–77},
numpages = {4},
keywords = {Software Testing, Entity Reconciliation, MDE, Big Data},
location = {Seattle, WA, USA},
series = {A-TEST 2016}
}

@inproceedings{10.1145/3266237.3266273,
author = {Braga, Rony\'{e}rison and Neto, Pedro Santos and Rab\^{e}lo, Ricardo and Santiago, Jos\'{e} and Souza, Matheus},
title = {A Machine Learning Approach to Generate Test Oracles},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266273},
doi = {10.1145/3266237.3266273},
abstract = {One of the essential activities for quality assurance in software development is the software testing. Studies report that Software Testing is one of the most costly activities in the development process, can reach up to 50 percent of its total cost. One of the great challenges of conducting software testing is related to the automation of a mechanism known as "test oracle". This work presents an approach based on machine learning (ML) for automation of the test oracle mechanism in software. The approach uses historical usage data from an application captured by inserting a capture component into the application under test. These data go through a Knowledge Discovery in Database step and are then used for training to generate an oracle suitable for the application under test. Four experiments were executed with web applications to evaluate the proposed approach. The first and second experiments were performed with a fictitious application, with faults inserted randomly in the first experiment, inserted by a developer in the second one and inserted by mutation tests in third one. The fourth experiment was carried out with a large real application in order to assure the results of the preliminary experiments. The experiments presented indications of the suitability of the approach to the solution of the problem.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {142–151},
numpages = {10},
keywords = {machine learning, testing automation, test oracle},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/1808266.1808276,
author = {de la Riva, Claudio and Su\'{a}rez-Cabal, Mar\'{\i}a Jos\'{e} and Tuya, Javier},
title = {Constraint-Based Test Database Generation for SQL Queries},
year = {2010},
isbn = {9781605589701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808266.1808276},
doi = {10.1145/1808266.1808276},
abstract = {Populating test databases with meaningful test data is a difficult task as it involves generating data for many joined tables that must be diverse enough to be able to reveal faults and small enough to make the testing process efficient. This paper proposes an approach for the automatic generation of a test database for a set of SQL queries using a test criterion specifically tailored for the SQL language (SQLFpc). Given as input a schema database and a set of test requirements derived from the application of the test criterion to the target queries, the approach returns a database instance which satisfies the test requirements. Both the schema and the test requirements are modeled in the Alloy language, after which the analyzer generates the test database. The approach is evaluated on a real case study and the results show its feasibility, generating a test database of reduced size with an elevated coverage and mutation score.},
booktitle = {Proceedings of the 5th Workshop on Automation of Software Test},
pages = {67–74},
numpages = {8},
keywords = {alloy toolset, MCDC, database testing, SQL coverage, software testing, SQLFpc, test database generation},
location = {Cape Town, South Africa},
series = {AST '10}
}

@inproceedings{10.1145/3468264.3473136,
author = {Chen, Tsong Yueh and Tse, T. H.},
title = {New Visions on Metamorphic Testing after a Quarter of a Century of Inception},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473136},
doi = {10.1145/3468264.3473136},
abstract = {Metamorphic testing (MT) was introduced about a quarter of a century ago. It is increasingly being accepted by researchers and the industry as a useful testing technique. The studies, research results, applications, and extensions of MT have given us many insights and visions for its future. Our visions include: MRs will be a practical means to top up test case generation techniques, beyond the alleviation of the test oracle problem; MT will not only be a standalone technique, but conveniently integrated with other methods; MT and MRs will evolve beyond software testing, or even beyond verification; MRs may be anything that you can imagine, beyond the necessary properties of algorithms; MT research will be beyond empirical studies and move toward a theoretical foundation; MT will not only bring new concepts to software testing but also new concepts to other disciplines; MRs will alleviate the reliable test set problem beyond traditional approaches. These visions may help researchers explore the challenges and opportunities for MT in the next decade.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1487–1490},
numpages = {4},
keywords = {Testing, Test oracle, Metamorphic relation, Debugging, Proving, Metamorphic testing, Reliable test set},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/1085130.1085142,
author = {Nikolik, Borislav},
title = {Convergence Debugging},
year = {2005},
isbn = {1595930507},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1085130.1085142},
doi = {10.1145/1085130.1085142},
abstract = {This paper proposes a new practical automatic debugging method, called Convergence Debugging, which isolates a set of test cases that converge on the internal root cause of a failure. This method involves evaluating the debugging effectiveness of a set of test case by utilizing a new measure of code-level distance between a set of debug test cases and the test case that caused the failure. The same distance measure could also be used to select a set of debug test cases that maximize debugging effectiveness. In order to gain insights into the root cause of a failure, the debug test cases could be used to analyze the differences between the failed test case of interest and the debug test cases.An industrial-strength tool, called Diversity Analyzer, for programs written in C, C++, C#, Java and VB in the Microsoft .NET environment, is used to experiment with the effectiveness of Convergence Debugging in locating faults in industrial software.},
booktitle = {Proceedings of the Sixth International Symposium on Automated Analysis-Driven Debugging},
pages = {89–98},
numpages = {10},
keywords = {convergence hypothesis, test diversity, testing, debugging, convergence, test dispersion},
location = {Monterey, California, USA},
series = {AADEBUG'05}
}

@article{10.1145/3389126,
author = {Miranda, Breno and Bertolino, Antonia},
title = {Testing Relative to Usage Scope: Revisiting Software Coverage Criteria},
year = {2020},
issue_date = {July 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3389126},
doi = {10.1145/3389126},
abstract = {Coverage criteria provide a useful and widely used means to guide software testing; however, indiscriminately pursuing full coverage may not always be convenient or meaningful, as not all entities are of interest in any usage context. We aim at introducing a more meaningful notion of coverage that takes into account how the software is going to be used. Entities that are not going to be exercised by the user should not contribute to the coverage ratio. We revisit the definition of coverage measures, introducing a notion of relative coverage. According to this notion, we provide a definition and a theoretical framework of relative coverage, within which we discuss implications on testing theory and practice. Through the evaluation of three different instances of relative coverage, we could observe that relative coverage measures provide a more effective strategy than traditional ones: we could reach higher coverage measures, and test cases selected by relative coverage could achieve higher reliability. We hint at several other useful implications of relative coverage notion on different aspects of software testing.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jun},
articleno = {18},
numpages = {24},
keywords = {relative coverage, Coverage testing}
}

@inproceedings{10.1145/3477132.3483549,
author = {Gong, Sishuai and Altinb\"{u}ken, Deniz and Fonseca, Pedro and Maniatis, Petros},
title = {Snowboard: Finding Kernel Concurrency Bugs through Systematic Inter-Thread Communication Analysis},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483549},
doi = {10.1145/3477132.3483549},
abstract = {Kernel concurrency bugs are challenging to find because they depend on very specific thread interleavings and test inputs. While separately exploring kernel thread interleavings or test inputs has been closely examined, jointly exploring interleavings and test inputs has received little attention, in part due to the resulting vast search space. Using precious, limited testing resources to explore this search space and execute just the right concurrent tests in the proper order is critical.This paper proposes Snowboard a testing framework that generates and executes concurrent tests by intelligently exploring thread interleavings and test inputs jointly. The design of Snowboard is based on a concept called potential memory communication (PMC), a guess about pairs of tests that, when executed concurrently, are likely to perform memory accesses to shared addresses, which in turn may trigger concurrency bugs. To identify PMCs, Snowboard runs tests sequentially from a fixed initial kernel state, collecting their memory accesses. It then pairs up tests that write and read the same region into candidate concurrent tests. It executes those tests using the associated PMC as a scheduling hint to focus interleaving search only on those schedules that directly affect the relevant memory accesses. By clustering candidate tests on various features of their PMCs, Snowboard avoids testing similar behaviors, which would be inefficient. Finally, by executing tests from small clusters first, it prioritizes uncommon suspicious behaviors that may have received less scrutiny.Snowboard discovered 14 new concurrency bugs in Linux kernels 5.3.10 and 5.12-rc3, of which 12 have been confirmed by developers. Six of these bugs cause kernel panics and filesystem errors, and at least two have existed in the kernel for many years, showing that this approach can uncover hard-to-find, critical bugs. Furthermore, we show that covering as many distinct pairs of uncommon read/write instructions as possible is the test-prioritization strategy with the highest bug yield for a given test-time budget.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {66–83},
numpages = {18},
keywords = {Software testing and debugging, Kernel concurrency bug, Operating systems security, Concurrency programming},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{10.1145/3468264.3468581,
author = {Chiou, Paul T. and Alotaibi, Ali S. and Halfond, William G. J.},
title = {Detecting and Localizing Keyboard Accessibility Failures in Web Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468581},
doi = {10.1145/3468264.3468581},
abstract = {The keyboard is the most universally supported input method operable by people with disabilities. Yet, many popular websites lack keyboard accessible mechanism, which could cause failures that make the website unusable. In this paper, we present a novel approach for automatically detecting and localizing keyboard accessibility failures in web applications. Our extensive evaluation of our technique on real world web pages showed that our technique was able to detect keyboard failures in web applications with high precision and recall and was able to accurately identify the underlying elements in the web pages that led to the observed problems.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {855–867},
numpages = {13},
keywords = {Software Testing, Keyboard Navigation, WCAG, Web Accessibility},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/2491411.2491434,
author = {Nguyen, Cu D. and Marchetto, Alessandro and Tonella, Paolo},
title = {Automated Oracles: An Empirical Study on Cost and Effectiveness},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491434},
doi = {10.1145/2491411.2491434},
abstract = { Software testing is an effective, yet expensive, method to improve software quality. Test automation, a potential way to reduce testing cost, has received enormous research attention recently, but the so-called “oracle problem” (how to decide the PASS/FAIL outcome of a test execution) is still a major obstacle to such cost reduction. We have extensively investigated state-of-the-art works that contribute to address this problem, from areas such as specification mining and model inference. In this paper, we compare three types of automated oracles: Data invariants, Temporal invariants, and Finite State Automata. More specifically, we study the training cost and the false positive rate; we evaluate also their fault detection capability. Seven medium to large, industrial application subjects and real faults have been used in our empirical investigation. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {136–146},
numpages = {11},
keywords = {Specification Mining, Empirical Study, Automated Testing Oracles},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inbook{10.1145/3474198.3478214,
author = {Du, Xiaozhi and Liu, Jinlan and He, Hongmei},
title = {Generating Feasible Paths under C-ZOT Coverage from EFSM},
year = {2021},
isbn = {9781450390149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3474198.3478214},
abstract = {Model-based software testing technology automatically generates test cases to save time. However, test case generation based on the EFSM model focus on generating simple transition paths under transition coverage which leads to inadequate testing. To address this problem, we propose a complex test case generation algorithm based on C-ZOT coverage from EFSM. Our method adds length to measure complex of a transition path instead of only considering dataflow dependency. C-ZOT coverage that we proposed considers cyclic transitions and add the idea of boundary value testing to form a more complete coverage than transition coverage. The results of experiments show that the number of transition paths under C-ZOT coverage is 22.89% higher than the number under transition coverage, the rate of simple transition paths to generate test data is 45.06% higher than the rate of complex transition paths to generate test data.},
booktitle = {International Conference on Frontiers of Electronics, Information and Computation Technologies},
articleno = {92},
numpages = {8}
}

@inproceedings{10.1145/2591062.2591109,
author = {Liu, Huai and Yusuf, Iman I. and Schmidt, Heinz W. and Chen, Tsong Yueh},
title = {Metamorphic Fault Tolerance: An Automated and Systematic Methodology for Fault Tolerance in the Absence of Test Oracle},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591109},
doi = {10.1145/2591062.2591109},
abstract = { A system may fail due to an internal bug or a fault in its execution environment. Incorporating fault tolerance strategies enables such system to complete its function despite the failure of some of its parts. Prior to the execution of some fault tolerance strategies, failure detection is needed. Detecting incorrect output, for instance, assumes the existence of an oracle to check the correctness of program outputs given an input. However, in many practical situations, oracle does not exist or is extremely difficult to apply. Such an oracle problem is a major challenge in the context of software testing. In this paper, we propose to apply metamorphic testing, a software testing method that alleviates the oracle problem, into fault tolerance. The proposed technique supports failure detection without the need of oracles. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {420–423},
numpages = {4},
keywords = {metamorphic testing, metamorphic relation, oracle problem, fault tolerance},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2593702.2593725,
author = {L., Anoop T. and Venkataramanappa, Kiran and Gopadi, Manjunath and A., Srividya and V., Madhumathi K.},
title = {Leverage Human Aspects in Test Engineering},
year = {2014},
isbn = {9781450328609},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593702.2593725},
doi = {10.1145/2593702.2593725},
abstract = { Since the Industrial Revolution, mankind has been striving to find simpler and easier solutions for human problems. With the evolution of computers and software, testing also came to the forefront. This whitepaper derives the context from a test organization within SAP Labs India Pvt. Ltd. Over the years, research and innovations in testing have brought in various test methodologies and tools to improve testing. Most of these innovations require development and maintenance of highly skilled test engineers. One aspect which is not probably explored enough is the utilization of diverse human intelligence is (like university students, business users, differently abled people etc.) across the ecosystem. We focus on the following 3 areas of testing where we have made positive experiences using diverse workforce  1. Regression testing 2. Automation 3. research },
booktitle = {Proceedings of the 7th International Workshop on Cooperative and Human Aspects of Software Engineering},
pages = {129–130},
numpages = {2},
keywords = {Automation, Regression testing, Research},
location = {Hyderabad, India},
series = {CHASE 2014}
}

@inproceedings{10.1145/3377811.3380395,
author = {Humbatova, Nargiz and Jahangirova, Gunel and Bavota, Gabriele and Riccio, Vincenzo and Stocco, Andrea and Tonella, Paolo},
title = {Taxonomy of Real Faults in Deep Learning Systems},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380395},
doi = {10.1145/3377811.3380395},
abstract = {The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1110–1121},
numpages = {12},
keywords = {real faults, deep learning, taxonomy, software testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2676743.2676744,
author = {Grace, Paul and Barbosa, Justan and Pickering, Brian and Surridge, Mike},
title = {Taming the Interoperability Challenges of Complex IoT Systems},
year = {2014},
isbn = {9781450332347},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2676743.2676744},
doi = {10.1145/2676743.2676744},
abstract = {The Internet of Things is characterised by extreme heterogeneity of communication protocols and data formats; hence ensuring diverse devices can interoperate with one another remains a significant challenge. Model-driven development and testing solutions have been proposed as methods to aid software developers achieve interoperability compliance in the face of this increasing complexity. However, current approaches often involve complicated and domain specific models (e.g. web services described by WSDL). In this paper, we explore a lightweight, middleware independent, model-driven development framework to help developers tame the challenges of composing IoT services that interoperate with one another. The framework is based upon two key contributions: i) patterns of interoperability behaviour, and ii) a software framework to monitor and reason about interoperability success or failure. We show using a case-study from the FI-WARE Future Internet Service domain that this interoperability framework can support non-expert developers address interoperability challenges. We also deployed tools built atop the framework and made them available in the XIFI large-scale FI-PPP test environment.},
booktitle = {Proceedings of the 1st ACM Workshop on Middleware for Context-Aware Applications in the IoT},
pages = {1–6},
numpages = {6},
keywords = {internet of things, software testing, architectural patterns, model-driven software engineering, interoperability},
location = {Bordeaux, France},
series = {M4IOT '14}
}

@inproceedings{10.1145/3131151.3131157,
author = {Villanes, Isabel K. and Ascate, Silvia M. and Gomes, Josias and Dias-Neto, Arilo Claudio},
title = {What Are Software Engineers Asking about Android Testing on Stack Overflow?},
year = {2017},
isbn = {9781450353267},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3131151.3131157},
doi = {10.1145/3131151.3131157},
abstract = {Software testing represents an important activity to achieve quality during mobile application development. The constant evolution of mobile applications in previous years relating to size and complexity entails the need to improve testing techniques and tools. In this context, developers/testers often resort to specialized communities or Question &amp; Answer repositories to clarity doubts regarding testing in a practical and efficient way. Thus, these repositories become a popular source of data to understand the current context of software testing practices. In this paper, we present a study using the Stack Overflow repository for analyzing and clustering the main topics on Android testing. We employed the LDA algorithm to summarize the mobile testing related questions. Our findings show that topics such as testing tools, functional testing, and unit testing are often discussed when compared to other topics. We also analyzed the evolution of the interest of Android testing tools. Results show that developers are more interested in Appium, Espresso, Monkey, and Robotium tools.},
booktitle = {Proceedings of the 31st Brazilian Symposium on Software Engineering},
pages = {104–113},
numpages = {10},
keywords = {Stack Overflow, Mobile Testing, Model Topics},
location = {Fortaleza, CE, Brazil},
series = {SBES'17}
}

@inproceedings{10.1145/3242887.3242889,
author = {Zaman, Tarannum Shaila and Yu, Tingting},
title = {Extracting Implicit Programming Rules: Comparing Static and Dynamic Approaches},
year = {2018},
isbn = {9781450359757},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242887.3242889},
doi = {10.1145/3242887.3242889},
abstract = {Programs often follow implicit programming rules, such as, function call A must be followed by function call B. Rules of such kinds are rarely documented by developers. Nevertheless, programming rules play an important role in software testing and maintenance. For example, the rules can be used as test oracles to detect violations. If a programmer can be notified of these rules before updating the source code, the chances of generating defects due to rule violations might be minimized. Prior works have used static and dynamic analysis techniques to extract implicit programming rules, but none compares the effectiveness of the two techniques. In this paper, we have undertaken an empirical study to compare the two techniques when they are being used for extracting programming rules. Our results indicate that the performance of the dynamic analysis technique depends on the number and the diversity of the traces. Moreover, the dynamic analysis technique generates more precise rules than the static analysis technique if a diverse and sufficient number of test cases are provided.},
booktitle = {Proceedings of the 7th International Workshop on Software Mining},
pages = {1–7},
numpages = {7},
keywords = {Static Analysis, Dynamic analysis, Implicit Programming Rules, Empirical study},
location = {Montpellier, France},
series = {SoftwareMining 2018}
}

@inproceedings{10.1145/2791060.2791073,
author = {Lachmann, Remo and Lity, Sascha and Lischke, Sabrina and Beddig, Simon and Schulze, Sandro and Schaefer, Ina},
title = {Delta-Oriented Test Case Prioritization for Integration Testing of Software Product Lines},
year = {2015},
isbn = {9781450336130},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2791060.2791073},
doi = {10.1145/2791060.2791073},
abstract = {Software product lines have potential to allow for mass customization of products. Unfortunately, the resulting, vast amount of possible product variants with commonalities and differences leads to new challenges in software testing. Ideally, every product variant should be tested, especially in safety-critical systems. However, due to the exponentially increasing number of product variants, testing every product variant is not feasible. Thus, new concepts and techniques are required to provide efficient SPL testing strategies exploiting the commonalities of software artifacts between product variants to reduce redundancy in testing. In this paper, we present an efficient integration testing approach for SPLs based on delta modeling. We focus on test case prioritization. As a result, only the most important test cases for every product variant are tested, reducing the number of executed test cases significantly, as testing can stop at any given point because of resource constraints while ensuring that the most important test cases have been covered. We present the general concept and our evaluation results. The results show a measurable reduction of executed test cases compared to single-software testing approaches.},
booktitle = {Proceedings of the 19th International Conference on Software Product Line},
pages = {81–90},
numpages = {10},
keywords = {regression testing, architecture-based testing, delta-oriented software product lines, test case prioritization},
location = {Nashville, Tennessee},
series = {SPLC '15}
}

@inproceedings{10.1145/3236454.3236490,
author = {Riccio, Vincenzo and Amalfitano, Domenico and Fasolino, Anna Rita},
title = {Is This the Lifecycle We Really Want? An Automated Black-Box Testing Approach for Android Activities},
year = {2018},
isbn = {9781450359399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236454.3236490},
doi = {10.1145/3236454.3236490},
abstract = {Android is today the world's most popular mobile operating system and the demand for quality to Android mobile apps has grown together with their spread. Testing is a well-known approach for assuring the quality of software applications but Android apps have several peculiarities compared to traditional software applications that have to be taken into account by testers. Several studies have pointed out that mobile apps suffer from issues that can be attributed to Activity lifecycle mishandling, e.g. crashes, hangs, waste of system resources. Therefore the lifecycle of the Activities composing an app should be properly considered by testing approaches. In this paper we propose ALARic, a fully automated Black-Box Event-based testing technique that explores an application under test for detecting issues tied to the Android Activity lifecycle. ALARic has been implemented in a tool. We conducted an experiment involving 15 real Android apps that showed the effectiveness of ALARic in finding GUI failures and crashes tied to the Activity lifecycle. In the study, ALARic proved to be more effective in detecting crashes than Monkey, the state-of-the practice automated Android testing tool.},
booktitle = {Companion Proceedings for the ISSTA/ECOOP 2018 Workshops},
pages = {68–77},
numpages = {10},
location = {Amsterdam, Netherlands},
series = {ISSTA '18}
}

