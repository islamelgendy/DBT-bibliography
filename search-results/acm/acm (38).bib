@inproceedings{10.5555/257572.257668,
author = {Dalal, S. R. and Horgan, J. R. and Kettenring, J. R.},
title = {Reliable Software and Communication: Software Quality, Reliability, and Safety},
year = {1993},
isbn = {0897915887},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
booktitle = {Proceedings of the 15th International Conference on Software Engineering},
pages = {425–435},
numpages = {11},
location = {Baltimore, Maryland, USA},
series = {ICSE '93}
}

@inproceedings{10.1145/3243218.3243219,
author = {Kowalczyk, Emily and Cohen, Myra B. and Memon, Atif M.},
title = {Configurations in Android Testing: They Matter},
year = {2018},
isbn = {9781450359733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3243218.3243219},
doi = {10.1145/3243218.3243219},
abstract = {Android has rocketed to the top of the mobile market thanks in large part to its open source model. Vendors use Android for their devices for free, and companies make customizations to suit their needs. This has resulted in a myriad of configurations that are extant in the user space today. In this paper, we show that differences in configurations, if ignored, can lead to differences in test outputs and code coverage. Consequently, researchers who develop new testing techniques and evaluate them on only one or two configurations are missing a necessary dimension in their experiments and developers who ignore this may release buggy software. In a large study on 18 apps across 88 configurations, we show that only one of the 18 apps studied showed no variation at all. The rest showed variation in either, or both, code coverage and test results. 15% of the 2,000 plus test cases across all of the apps vary, and some of the variation is subtle, i.e. not just a test crash. Our results suggest that configurations in Android testing do matter and that developers need to test using configuration-aware techniques.},
booktitle = {Proceedings of the 1st International Workshop on Advances in Mobile App Analysis},
pages = {1–6},
numpages = {6},
keywords = {Mobile Testing, Android},
location = {Montpellier, France},
series = {A-Mobile 2018}
}

@inproceedings{10.1145/1868328.1868357,
author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
title = {Programmer-Based Fault Prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868357},
doi = {10.1145/1868328.1868357},
abstract = {Background: Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions.Aims: We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve defect predictions. We also continue an earlier study to evaluate the use of counts of the number of developers who modified a file as predictors of the file's future faultiness.Method: We analyzed change reports filed by 107 programmers for 16 releases of a system with 1,400,000 LOC and 3100 files. A "bug ratio" was defined for programmers, measuring the proportion of faulty files in release R out of all files modified by the programmer in release R-1. The study compares the bug ratios of individual programmers to the average bug ratio, and also assesses the consistency of the bug ratio across releases for individual programmers.Results: Bug ratios varied widely among all the programmers, as well as for many individual programmers across all the releases that they participated in. We found a statistically significant correlation between the bug ratios for programmers for the first half of changed files versus the ratios for the second half, indicating a measurable degree of persistence in the bug ratio. However, when the computation was repeated with the bug ratio controlled not only by release, but also by file size, the correlation disappeared. In addition to the bug ratios, we confirmed that counts of the cumulative number of different developers changing a file over its lifetime can help to improve predictions, while other developer counts are not helpful.Conclusions: The results from this preliminary study indicate that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. The study is limited to a single large system, and its results may not hold more widely. The bug ratio is only one way of measuring the "fault-proneness" of an individual programmer's coding, and we intend to investigate other ways of evaluating bug introduction by individuals.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {19},
numpages = {10},
keywords = {fault-prone, empirical study, regression model, bug ratio, software faults, prediction},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/2632362.2632370,
author = {Emmi, Michael and Ozkan, Burcu Kulahcioglu and Tasiran, Serdar},
title = {Exploiting Synchronization in the Analysis of Shared-Memory Asynchronous Programs},
year = {2014},
isbn = {9781450324526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632362.2632370},
doi = {10.1145/2632362.2632370},
abstract = { As asynchronous programming becomes more mainstream, program analyses capable of automatically uncovering programming errors are increasingly in demand. Since asynchronous program analysis is computationally costly, current approaches sacrifice completeness and focus on limited sets of asynchronous task schedules that are likely to expose programming errors. These approaches are based on parameterized task schedulers, each of which admits schedules which are variations of a default deterministic schedule. By increasing the parameter value, a larger variety of schedules is explored, at a higher cost. The efficacy of these approaches depends largely on the default deterministic scheduler on which varying schedules are fashioned.  We find that the limited exploration of asynchronous program behaviors can be made more efficient by designing parameterized schedulers which better match the inherent ordering of program events, e.g., arising from waiting for an asynchronous task to complete. We follow a reduction-based "sequentialization" approach to analyzing asynchronous programs, which leverages existing (sequential) program analysis tools by encoding asynchronous program executions, according to a particular scheduler, as the executions of a sequential program. Analysis based on our new scheduler comes at no greater computational cost, and provides strictly greater behavioral coverage than analysis based on existing parameterized schedulers; we validate these claims both conceptually, with complexity and behavioral-inclusion arguments, and empirically, by discovering actual reported bugs faster with smaller parameter values. },
booktitle = {Proceedings of the 2014 International SPIN Symposium on Model Checking of Software},
pages = {20–29},
numpages = {10},
keywords = {Sequentialization, Concurrency, Asynchronous programs},
location = {San Jose, CA, USA},
series = {SPIN 2014}
}

@inproceedings{10.1145/2908080.2908095,
author = {Chen, Yuting and Su, Ting and Sun, Chengnian and Su, Zhendong and Zhao, Jianjun},
title = {Coverage-Directed Differential Testing of JVM Implementations},
year = {2016},
isbn = {9781450342612},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2908080.2908095},
doi = {10.1145/2908080.2908095},
abstract = { Java virtual machine (JVM) is a core technology, whose reliability is critical. Testing JVM implementations requires painstaking effort in designing test classfiles (*.class) along with their test oracles. An alternative is to employ binary fuzzing to differentially test JVMs by blindly mutating seeding classfiles and then executing the resulting mutants on different JVM binaries for revealing inconsistent behaviors. However, this blind approach is not cost effective in practice because most of the mutants are invalid and redundant. This paper tackles this challenge by introducing classfuzz, a coverage-directed fuzzing approach that focuses on representative classfiles for differential testing of JVMs’ startup processes. Our core insight is to (1) mutate seeding classfiles using a set of predefined mutation operators (mutators) and employ Markov Chain Monte Carlo (MCMC) sampling to guide mutator selection, and (2) execute the mutants on a reference JVM implementation and use coverage uniqueness as a discipline for accepting representative ones. The accepted classfiles are used as inputs to differentially test different JVM implementations and find defects. We have implemented classfuzz and conducted an extensive evaluation of it against existing fuzz testing algorithms. Our evaluation results show that classfuzz can enhance the ratio of discrepancy-triggering classfiles from 1.7% to 11.9%. We have also reported 62 JVM discrepancies, along with the test classfiles, to JVM developers. Many of our reported issues have already been confirmed as JVM defects, and some even match recent clarifications and changes to the Java SE 8 edition of the JVM specification. },
booktitle = {Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {85–99},
numpages = {15},
keywords = {MCMC sampling, fuzz testing, Differential testing, Java virtual machine},
location = {Santa Barbara, CA, USA},
series = {PLDI '16}
}

@article{10.1145/2980983.2908095,
author = {Chen, Yuting and Su, Ting and Sun, Chengnian and Su, Zhendong and Zhao, Jianjun},
title = {Coverage-Directed Differential Testing of JVM Implementations},
year = {2016},
issue_date = {June 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2980983.2908095},
doi = {10.1145/2980983.2908095},
abstract = { Java virtual machine (JVM) is a core technology, whose reliability is critical. Testing JVM implementations requires painstaking effort in designing test classfiles (*.class) along with their test oracles. An alternative is to employ binary fuzzing to differentially test JVMs by blindly mutating seeding classfiles and then executing the resulting mutants on different JVM binaries for revealing inconsistent behaviors. However, this blind approach is not cost effective in practice because most of the mutants are invalid and redundant. This paper tackles this challenge by introducing classfuzz, a coverage-directed fuzzing approach that focuses on representative classfiles for differential testing of JVMs’ startup processes. Our core insight is to (1) mutate seeding classfiles using a set of predefined mutation operators (mutators) and employ Markov Chain Monte Carlo (MCMC) sampling to guide mutator selection, and (2) execute the mutants on a reference JVM implementation and use coverage uniqueness as a discipline for accepting representative ones. The accepted classfiles are used as inputs to differentially test different JVM implementations and find defects. We have implemented classfuzz and conducted an extensive evaluation of it against existing fuzz testing algorithms. Our evaluation results show that classfuzz can enhance the ratio of discrepancy-triggering classfiles from 1.7% to 11.9%. We have also reported 62 JVM discrepancies, along with the test classfiles, to JVM developers. Many of our reported issues have already been confirmed as JVM defects, and some even match recent clarifications and changes to the Java SE 8 edition of the JVM specification. },
journal = {SIGPLAN Not.},
month = {jun},
pages = {85–99},
numpages = {15},
keywords = {fuzz testing, Differential testing, MCMC sampling, Java virtual machine}
}

@inproceedings{10.1145/1083292.1083295,
author = {Huang, LiGuo and Boehm, Barry},
title = {Using IDAVE to Determine Availability Requirements},
year = {2005},
isbn = {1595931228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083292.1083295},
doi = {10.1145/1083292.1083295},
abstract = {Different systems have different success-critical stakeholders. Even for the same system, these stakeholders may depend on it in different ways for different scenarios. Therefore a one-size-fits-all dependability metric is unachievable in practice. In order to cost-effectively achieve the stakeholders' desired dependability attribute requirements for a given project, we have to solve such problems as how to define an appropriate level for a particular dependability attribute and how much dependability investment is enough for a particular software/scenario class. However, the answers to those questions are traditionally difficult to obtain. This paper uses a hypothetical Lunar Biological Laboratory (LBL) as an example to illustrate how to use the iDAVE model to determine the appropriate levels of availability requirements for different software/scenario classes based on their different ROI profiles.},
booktitle = {Proceedings of the Third Workshop on Software Quality},
pages = {1–4},
numpages = {4},
keywords = {dependability, value, cost, reliability, quality, availability},
location = {St. Louis, Missouri},
series = {3-WoSQ}
}

@article{10.1145/1082983.1083295,
author = {Huang, LiGuo and Boehm, Barry},
title = {Using IDAVE to Determine Availability Requirements},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1083295},
doi = {10.1145/1082983.1083295},
abstract = {Different systems have different success-critical stakeholders. Even for the same system, these stakeholders may depend on it in different ways for different scenarios. Therefore a one-size-fits-all dependability metric is unachievable in practice. In order to cost-effectively achieve the stakeholders' desired dependability attribute requirements for a given project, we have to solve such problems as how to define an appropriate level for a particular dependability attribute and how much dependability investment is enough for a particular software/scenario class. However, the answers to those questions are traditionally difficult to obtain. This paper uses a hypothetical Lunar Biological Laboratory (LBL) as an example to illustrate how to use the iDAVE model to determine the appropriate levels of availability requirements for different software/scenario classes based on their different ROI profiles.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–4},
numpages = {4},
keywords = {availability, quality, reliability, cost, dependability, value}
}

@inproceedings{10.1145/3338906.3341175,
author = {Fu, Ying and Ren, Meng and Ma, Fuchen and Shi, Heyuan and Yang, Xin and Jiang, Yu and Li, Huizhong and Shi, Xiang},
title = {EVMFuzzer: Detect EVM Vulnerabilities via Fuzz Testing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3341175},
doi = {10.1145/3338906.3341175},
abstract = {Ethereum Virtual Machine (EVM) is the run-time environment for smart contracts and its vulnerabilities may lead to serious problems to the Ethereum ecology. With lots of techniques being continuously developed for the validation of smart contracts, the testing of EVM remains challenging because of the special test input format and the absence of oracles. In this paper, we propose EVMFuzzer, the first tool that uses differential fuzzing technique to detect vulnerabilities of EVM. The core idea is to continuously generate seed contracts and feed them to the target EVM and the benchmark EVMs, so as to find as many inconsistencies among execution results as possible, eventually discover vulnerabilities with output cross-referencing. Given a target EVM and its APIs, EVMFuzzer generates seed contracts via a set of predefined mutators, and then employs dynamic priority scheduling algorithm to guide seed contracts selection and maximize the inconsistency. Finally, EVMFuzzer leverages benchmark EVMs as cross-referencing oracles to avoid manual checking. With EVMFuzzer, we have found several previously unknown security bugs in four widely used EVMs, and 5 of which had been included in Common Vulnerabilities and Exposures (CVE) IDs in U.S. National Vulnerability Database. The video is presented at https://youtu.be/9Lejgf2GSOk.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1110–1114},
numpages = {5},
keywords = {domain-specific mutation, fuzzing, Differential testing, EVM},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2491627.2491646,
author = {Marijan, Dusica and Gotlieb, Arnaud and Sen, Sagar and Hervieu, Aymeric},
title = {Practical Pairwise Testing for Software Product Lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491646},
doi = {10.1145/2491627.2491646},
abstract = {One key challenge for software product lines is efficiently managing variability throughout their lifecycle. In this paper, we address the problem of variability in software product lines testing. We (1) identify a set of issues that must be addressed to make software product line testing work in practice and (2) provide a framework that combines a set of techniques to solve these issues. The framework integrates feature modelling, combinatorial interaction testing and constraint programming techniques. First, we extract variability in a software product line as a feature model with specified feature interdependencies. We then employ an algorithm that generates a minimal set of valid test cases covering all 2-way feature interactions for a given time interval. Furthermore, we evaluate the framework on an industrial SPL and show that using the framework saves time and provides better test coverage. In particular, our experiments show that the framework improves industrial testing practice in terms of (i) 17% smaller set of test cases that are (a) valid and (b) guarantee all 2-way feature coverage (as opposite to 19.2% 2-way feature coverage in the hand made test set), and (ii) full flexibility and adjustment of test generation to available testing time.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {227–235},
numpages = {9},
keywords = {software product lines, feature modelling, variability management},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inproceedings{10.5555/2662413.2662421,
author = {Jehan, Seema and Pill, Ingo and Wotawa, Franz},
title = {Functional SOA Testing Based on Constraints},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {In the fierce competition on today's software market, Service-Oriented Architectures (SOAs) are an established design paradigm. Essential concepts like modularization, reuse, and the corresponding IP core business are inherently supported in the development and operation of SOAs that offer flexibility in many aspects and thus optimal conditions also for heterogeneous system developments. The intrinsics of large and complex SOA enterprises, however, require us to adopt and evolve our verification technology, in order to achieve expected software quality levels. In this paper, we contribute to this challenge by proposing a constraint based testing approach for SOAs. In our work, we augment a SOA's BPEL business model with pre- and postcondition contracts defining essential component traits, and derive a suite of feasible test cases to be executed after assessing its quality via corresponding coverage criteria. We illustrate our approach's viability via a running example as well as experimental results, and discuss current and envisioned automation levels in the context of a test and diagnosis workflow.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {33–39},
numpages = {7},
location = {San Francisco, California},
series = {AST '13}
}

@inproceedings{10.1145/3345629.3345636,
author = {Nguyen, An and Le, Bach and Nguyen, Vu},
title = {Prioritizing Automated User Interface Tests Using Reinforcement Learning},
year = {2019},
isbn = {9781450372336},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3345629.3345636},
doi = {10.1145/3345629.3345636},
abstract = {User interface testing validates the correctness of an application through visual cues and interactive events emitted in real world usages. Performing user interface tests is a time-consuming process, and thus, many studies have focused on prioritizing test cases to help maintain the effectiveness of testing while reducing the need for a full execution. This paper describes a novel prioritization method that combines Reinforcement Learning and interaction coverage testing concepts. While Reinforcement Learning has been found to be suitable for rapid changing projects with abundant historical data, interaction coverage considers in depth the event-based aspects of user interface testing and provides a granular level at which the Reinforcement Learning system can gain more insights into individual test cases. We experiment and assess the proposed method using five data sets, finding that the method outperforms related methods and has the potential to be used in practice.},
booktitle = {Proceedings of the Fifteenth International Conference on Predictive Models and Data Analytics in Software Engineering},
pages = {56–65},
numpages = {10},
keywords = {test prioritization, automation testing, reinforcement learning},
location = {Recife, Brazil},
series = {PROMISE'19}
}

@inproceedings{10.1109/ASE.2015.102,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki and Gmeiner, Johannes and Ramler, Rudolf},
title = {GRT: An Automated Test Generator Using Orchestrated Program Analysis},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.102},
doi = {10.1109/ASE.2015.102},
abstract = {While being highly automated and easy to use, existing techniques of random testing suffer from low code coverage and defect detection ability for practical software applications. Most tools use a pure black-box approach, which does not use knowledge specific to the software under test. Mining and leveraging the information of the software under test can be promising to guide random testing to overcome such limitations.Guided Random Testing (GRT) implements this idea. GRT performs static analysis on software under test to extract relevant knowledge and further combines the information extracted at run-time to guide the whole test generation procedure. GRT is highly configurable, with each of its six program analysis components implemented as a pluggable module whose parameters can be adjusted. Besides generating test cases, GRT also automatically creates a test coverage report. We show our experience in GRT tool development and demonstrate its practical usage using two concrete application scenarios.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {842–847},
numpages = {6},
keywords = {dynamic analysis, automatic test generation, static analysis, bug detection, random testing},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@article{10.1145/2934672,
author = {Gay, Gregory and Rajan, Ajitha and Staats, Matt and Whalen, Michael and Heimdahl, Mats P. E.},
title = {The Effect of Program and Model Structure on the Effectiveness of MC/DC Test Adequacy Coverage},
year = {2016},
issue_date = {August 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/2934672},
doi = {10.1145/2934672},
abstract = {Test adequacy metrics defined over the structure of a program, such as Modified Condition and Decision Coverage (MC/DC), are used to assess testing efforts. However, MC/DC can be “cheated” by restructuring a program to make it easier to achieve the desired coverage. This is concerning, given the importance of MC/DC in assessing the adequacy of test suites for critical systems domains. In this work, we have explored the impact of implementation structure on the efficacy of test suites satisfying the MC/DC criterion using four real-world avionics systems.Our results demonstrate that test suites achieving MC/DC over implementations with structurally complex Boolean expressions are generally larger and more effective than test suites achieving MC/DC over functionally equivalent, but structurally simpler, implementations. Additionally, we found that test suites generated over simpler implementations achieve significantly lower MC/DC and fault-finding effectiveness when applied to complex implementations, whereas test suites generated over the complex implementation still achieve high MC/DC and attain high fault finding over the simpler implementation. By measuring MC/DC over simple implementations, we can significantly reduce the cost of testing, but in doing so, we also reduce the effectiveness of the testing process. Thus, developers have an economic incentive to “cheat” the MC/DC criterion, but this cheating leads to negative consequences. Accordingly, we recommend that organizations require MC/DC over a structurally complex implementation for testing purposes to avoid these consequences.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {25},
numpages = {34},
keywords = {fault finding, Coverage}
}

@inproceedings{10.1109/ASE.2015.87,
author = {Lin, Ziyi and Marinov, Darko and Zhong, Hao and Chen, Yuting and Zhao, Jianjun},
title = {JaConTeBe: A Benchmark Suite of Real-World Java Concurrency Bugs},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.87},
doi = {10.1109/ASE.2015.87},
abstract = {Researchers have proposed various approaches to detect concurrency bugs and improve multi-threaded programs, but performing evaluations of these approaches still remains a substantial challenge. We survey the existing evaluations and find out that they often use code or bugs not representative of real world. To improve representativeness, we have prepared JaConTeBe, a benchmark suite of 47 confirmed concurrency bugs from 8 popular open-source projects, supplemented with test cases for reproducing buggy behaviors. Running three approaches on JaConTeBe shows that our benchmark suite confirms some limitations of the three approaches. We submitted JaConTeBe to the SIR repository (a software-artifact repository for rigorous controlled experiments), and it was included as a part of SIR.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {178–189},
numpages = {12},
keywords = {benchmark suite, evaluations, JaConTeBe, SIR, Java concurrency bugs},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3338906.3338937,
author = {Aggarwal, Aniya and Lohia, Pranay and Nagar, Seema and Dey, Kuntal and Saha, Diptikalyan},
title = {Black Box Fairness Testing of Machine Learning Models},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338937},
doi = {10.1145/3338906.3338937},
abstract = {Any given AI system cannot be accepted unless its trustworthiness is proven. An important characteristic of a trustworthy AI system is the absence of algorithmic bias. 'Individual discrimination' exists when a given individual different from another only in 'protected attributes' (e.g., age, gender, race, etc.) receives a different decision outcome from a given machine learning (ML) model as compared to the other individual. The current work addresses the problem of detecting the presence of individual discrimination in given ML models. Detection of individual discrimination is test-intensive in a black-box setting, which is not feasible for non-trivial systems. We propose a methodology for auto-generation of test inputs, for the task of detecting individual discrimination. Our approach combines two well-established techniques - symbolic execution and local explainability for effective test case generation. We empirically show that our approach to generate test cases is very effective as compared to the best-known benchmark systems that we examine.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {625–635},
numpages = {11},
keywords = {Individual Discrimination, Fairness Testing, Symbolic Execution, Local Explainability},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2568225.2568278,
author = {Gopinath, Rahul and Jensen, Carlos and Groce, Alex},
title = {Code Coverage for Suite Evaluation by Developers},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568278},
doi = {10.1145/2568225.2568278},
abstract = { One of the key challenges of developers testing code is determining a test suite's quality -- its ability to find faults. The most common approach is to use code coverage as a measure for test suite quality, and diminishing returns in coverage or high absolute coverage as a stopping rule. In testing research, suite quality is often evaluated by a suite's ability to kill mutants (artificially seeded potential faults). Determining which criteria best predict mutation kills is critical to practical estimation of test suite quality. Previous work has only used small sets of programs, and usually compares multiple suites for a single program. Practitioners, however, seldom compare suites --- they evaluate one suite. Using suites (both manual and automatically generated) from a large set of real-world open-source projects shows that evaluation results differ from those for suite-comparison: statement (not block, branch, or path) coverage predicts mutation kills best. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {72–82},
numpages = {11},
keywords = {statistical analysis, evaluation of coverage criteria, test frameworks},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.5555/602902.602910,
author = {Costa, Jos\'{e} C. and Devadas, Srinivas and Monteiro, Jos\'{e} C.},
title = {Observability Analysis of Embedded Software for Coverage-Directed Validation},
year = {2000},
isbn = {0780364481},
publisher = {IEEE Press},
abstract = {The most common approach to checking correctness of a hardware or software design is to verify that a description of the design has the proper behavior as elicited by a series of input stimuli. In the case of software, the program is simply run with the appropriate inputs, and in the case of hardware, its description written in a hardware description language (HDL) is simulated with the appropriate input vectors. In coverage-directed validation, coverage metrics are defined that quantitatively measure the degree of verification coverage of the design.Motivated by recent work on observability-based coverage metrics for models described in a hardware description language, we develop a method that computes an observability-based code coverage metric for embedded software written in a high-level programming language. Given a set of input vectors, our metric indicates the instructions that had no effect on the output. An assignment that was not relevant to generate the output value cannot be considered as being covered. Results show that our method offers a significantly more accurate assessment of design verification coverage than statement coverage. Existing coverage methods for hardware can be used with our method to build a verification methodology for mixed hardware/software or embedded systems.},
booktitle = {Proceedings of the 2000 IEEE/ACM International Conference on Computer-Aided Design},
pages = {27–32},
numpages = {6},
location = {San Jose, California},
series = {ICCAD '00}
}

@inproceedings{10.1145/1595696.1595748,
author = {Vangala, Vipindeep and Czerwonka, Jacek and Talluri, Phani},
title = {Test Case Comparison and Clustering Using Program Profiles and Static Execution},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595748},
doi = {10.1145/1595696.1595748},
abstract = {Selection of diverse test cases and elimination of duplicates are two major problems in product testing life cycle, especially in sustained engineering environment. In order to solve these, we introduce a framework of test case comparison metrics which will quantitatively describe the distance between any arbitrary test case pair of an existing test suite, allowing various test case analysis applications. We combine program profiles from test execution, static analysis and statistical techniques to capture various aspects of test execution and compute a specialized test case distance measurement. Using these distance metrics, we drive a customized hierarchical test suite clustering algorithm that groups similar test cases together. We present an industrial strength framework called SPIRiT that works at binary level, implementing different metrics in the form of coverage, control, data, def-use, temporal variances and does test case clustering. This is step towards integrating runtime analysis, static analysis, statistical techniques and machine learning to drive new generation of test suite analysis algorithms.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {293–294},
numpages = {2},
keywords = {testing, machine learning, sustained engineering, static analysis},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/761849.761856,
author = {Domino, Madeline Ann and Collins, Rosann Webb and Hevner, Alan R. and Cohen, Cynthia F.},
title = {Conflict in Collaborative Software Development},
year = {2003},
isbn = {1581136668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/761849.761856},
doi = {10.1145/761849.761856},
abstract = {Pair Programming is an innovative collaborative software development methodology. Anecdotal and empirical evidence suggests that this agile development method produces better quality software in reduced time with higher levels of developer satisfaction. To date, little explanation has been offered as to why these improved performance outcomes occur. In this qualitative study, we focus on how individual differences, and specifically task conflict, impact results of the collaborative software development process and related outcomes. We illustrate that low to moderate levels of task conflict actually enhance performance, while high levels mitigate otherwise anticipated positive results.},
booktitle = {Proceedings of the 2003 SIGMIS Conference on Computer Personnel Research: Freedom in Philadelphia--Leveraging Differences and Diversity in the IT Workforce},
pages = {44–51},
numpages = {8},
keywords = {pair programming, conflict, agile methods, collaborative software development},
location = {Philadelphia, Pennsylvania},
series = {SIGMIS CPR '03}
}

@inbook{10.1109/ICSE43902.2021.00019,
author = {Kim, Dong Jae and Tsantalis, Nikolaos and Chen, Tse-Hsun Peter and Yang, Jinqiu},
title = {Studying Test Annotation Maintenance in the Wild},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00019},
abstract = {Since the introduction of annotations in Java 5, the majority of testing frameworks, such as JUnit, TestNG, and Mockito, have adopted annotations in their core design. This adoption affected the testing practices in every step of the test life-cycle, from fixture setup and test execution to fixture teardown. Despite the importance of test annotations, most research on test maintenance has mainly focused on test code quality and test assertions. As a result, there is little empirical evidence on the evolution and maintenance of test annotations. To fill this gap, we perform the first fine-grained empirical study on annotation changes. We developed a tool to mine 82,810 commits and detect 23,936 instances of test annotation changes from 12 open-source Java projects. Our main findings are: (1) Test annotation changes are more frequent than rename and type change refactorings. (2) We recover various migration efforts within the same testing framework or between different frameworks by analyzing common annotation replacement patterns. (3) We create a taxonomy by manually inspecting and classifying a sample of 368 test annotation changes and documenting the motivations driving these changes. Finally, we present a list of actionable implications for developers, researchers, and framework designers.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {62–73},
numpages = {12}
}

@article{10.1145/3529318,
author = {Braiek, Houssem Ben and Khomh, Foutse},
title = {Testing Feedforward Neural Networks Training Programs},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3529318},
doi = {10.1145/3529318},
abstract = {Nowadays, we are witnessing an increasing effort to improve the performance and trustworthiness of Deep Neural Networks (DNNs), with the aim to enable their adoption in safety critical systems such as self-driving cars or aircraft collision-avoidance systems. Multiple testing techniques are proposed to generate test cases that can expose inconsistencies in the behavior of DNN models. These techniques assume implicitly that the training program is bug-free and appropriately configured. However, satisfying this assumption for a novel problem requires significant engineering work to prepare the data, design the DNN, implement the training program, and tune the hyperparameters in order to produce the model for which current automated test data generators search for corner-case behaviors. All these model training steps can be error-prone. Therefore, it is crucial to detect and correct errors throughout all the engineering steps of DNN-based software systems and not only on the resulting DNN model. In this paper, we gather a catalog of training issues and based on their symptoms and their effects on the behavior of the training program, we propose practical verification routines to detect the aforementioned issues, automatically, by continuously validating that some important properties of the learning dynamics hold during the training. Then, we design, TheDeepChecker, an end-to-end property-based debugging approach for DNN training programs and implement it as a TensorFlow-based library. As an empirical evaluation, we conduct a case study to assess the effectiveness of TheDeepChecker on synthetic and real-world buggy DL programs and compare its performance to that of the Amazon SageMaker Debugger (SMD). Results show that TheDeepChecker’s on-execution validation of DNN-based program’s properties through three sequential phases (pre-, on-, and post-fitting), succeeds in revealing several coding bugs and system misconfigurations errors, early on and at a low cost. Moreover, our property-based approach outperforms the SMD’s offline rules verification on training logs in terms of detection accuracy for unstable learning issues and coverage of additional DL bugs.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {mar},
keywords = {neural networks, property-based debugging, training programs}
}

