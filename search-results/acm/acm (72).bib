@inproceedings{10.1145/581339.581414,
author = {Maccari, Alessandro},
title = {Experiences in Assessing Product Family Software Architecture for Evolution},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581414},
doi = {10.1145/581339.581414},
abstract = {Software architecture assessments are a means to detect architectural problems before the bulk of development work is done. They facilitate planning of improvement activities early in the lifecycle and allow limiting the changes on any existing software. This is particularly beneficial when the architecture has been planned to (or already does) support a whole product family, or a set of products that share common requirements, architecture, components or code. As the family requirements evolve and new products are added, the need to assess the evolvability of the existing architecture is vital. I illustrate two assessment case studies I have recently worked on in the mobile telephone software domain: the Symbian operating system platform and the network resource access control software system. The former assessment has been carried out as a task within the European project ESAPS, while the latter has been performed solely by Nokia. By means of simple experimental data, I show evidence of the usefulness of architectural assessment as rated by the participating stakeholders. Both assessments have led to the identification of previously unknown architectural defects, and to the consequent planning of improvement initiatives. In both cases, stakeholders noted that a number of side benefits, including improvement of communication and architectural documentation, were also of considerable importance. I illustrate the lessons we have learned, and outline suggestions for future research and experimentation.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {585–592},
numpages = {8},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/3437992.3439934,
author = {Annenkov, Danil and Milo, Mikkel and Nielsen, Jakob Botsch and Spitters, Bas},
title = {Extracting Smart Contracts Tested and Verified in Coq},
year = {2021},
isbn = {9781450382991},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3437992.3439934},
doi = {10.1145/3437992.3439934},
abstract = {We implement extraction of Coq programs to functional languages based on MetaCoq's certified erasure. As part of this, we implement an optimisation pass removing unused arguments. We prove the pass correct wrt. a conventional call-by-value operational semantics of functional languages. We apply this to two functional smart contract languages, Liquidity and Midlang, and to the functional language Elm.  Our development is done in the context of the ConCert framework that enables smart contract verification. We contribute a verified boardroom voting smart contract featuring maximum voter privacy such that each vote is kept private except under collusion of all other parties.  We also integrate property-based testing into ConCert using QuickChick and our development is the first to support testing properties of interacting smart contracts. We test several complex contracts such as a DAO-like contract, an escrow contract, an implementation of a Decentralized Finance (DeFi) contract which includes a custom token standard (Tezos FA2), and more.  In total, this gives us a way to write dependent programs in Coq, test them semi-automatically, verify, and then extract to functional smart contract languages, while retaining a small trusted computing base of only MetaCoq and the pretty-printers into these languages.},
booktitle = {Proceedings of the 10th ACM SIGPLAN International Conference on Certified Programs and Proofs},
pages = {105–121},
numpages = {17},
keywords = {smart contracts, proof assistants, formal verification, software correctness, blockchain, Coq, code extraction, certified programming, property-based testing},
location = {Virtual, Denmark},
series = {CPP 2021}
}

@inproceedings{10.1145/3412452.3423573,
author = {Cernat, Marina and Staicu, Adelina Nicoleta and Stefanescu, Alin},
title = {Towards Automated Testing of RPA Implementations},
year = {2020},
isbn = {9781450381017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412452.3423573},
doi = {10.1145/3412452.3423573},
abstract = {Robotic Process Automation (RPA) is a technology that has grown tremendously in the last years, due to its usability in the area of process automation. An essential part of any software development process is quality assurance, so testing will be very important for RPA processes. However, the classical software techniques are not always suitable for the RPA software robots due to the mix of the graphical description of the robots and their implementations. In this short paper, we describe the state of the practice for testing of software robots and propose some ideas of test automation using model-based testing.},
booktitle = {Proceedings of the 11th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {21–24},
numpages = {4},
keywords = {RPA testing, Model-based testing, Test automation, Robotic Process Automation (RPA)},
location = {Virtual, USA},
series = {A-TEST 2020}
}

@inproceedings{10.1145/2591062.2591114,
author = {Martinez, Matias and Weimer, Westley and Monperrus, Martin},
title = {Do the Fix Ingredients Already Exist? An Empirical Inquiry into the Redundancy Assumptions of Program Repair Approaches},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591114},
doi = {10.1145/2591062.2591114},
abstract = { Much initial research on automatic program repair has focused on experimental results to probe their potential to find patches and reduce development effort. Relatively less effort has been put into understanding the hows and whys of such approaches. For example, a critical assumption of the GenProg technique is that certain bugs can be fixed by copying and re-arranging existing code. In other words, GenProg assumes that the fix ingredients already exist elsewhere in the code. In this paper, we formalize these assumptions around the concept of ``temporal redundancy''. A temporally redundant commit is only composed of what has already existed in previous commits. Our experiments show that a large proportion of commits that add existing code are temporally redundant. This validates the fundamental redundancy assumption of GenProg. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {492–495},
numpages = {4},
keywords = {mining software repositories, automatic software repair},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2617548.2617550,
author = {Seaton, Chris and Van De Vanter, Michael L. and Haupt, Michael},
title = {Debugging at Full Speed},
year = {2014},
isbn = {9781450329163},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2617548.2617550},
doi = {10.1145/2617548.2617550},
abstract = {Debugging support for highly optimized execution environments is notoriously difficult to implement. The Truffle/Graal platform for implementing dynamic languages offers an opportunity to resolve the apparent trade-off between debugging and high performance.Truffle/Graal-implemented languages are expressed as abstract syntax tree (AST) interpreters. They enjoy competitive performance through platform support for type specialization, partial evaluation, and dynamic optimization/deoptimization. A prototype debugger for Ruby, implemented on this platform, demonstrates that basic debugging services can be implemented with modest effort and without significant impact on program performance. Prototyped functionality includes breakpoints, both simple and conditional, at lines and at local variable assignments.The debugger interacts with running programs by inserting additional nodes at strategic AST locations; these are semantically transparent by default, but when activated can observe and interrupt execution. By becoming in effect part of the executing program, these "wrapper" nodes are subject to full runtime optimization, and they incur zero runtime overhead when debugging actions are not activated. Conditions carry no overhead beyond evaluation of the expression, which is optimized in the same way as user code, greatly improving the prospects for capturing rarely manifested bugs. When a breakpoint interrupts program execution, the platform automatically restores the full execution state of the program (expressed as Java data structures), as if running in the unoptimized AST interpreter. This then allows full introspection of the execution data structures such as the AST and method activation frames when in the interactive debugger console.Our initial evaluation indicates that such support could be permanently enabled in production environments.},
booktitle = {Proceedings of the Workshop on Dynamic Languages and Applications},
pages = {1–13},
numpages = {13},
keywords = {deoptimization, Truffle, virtual machines},
location = {Edinburgh, United Kingdom},
series = {Dyla'14}
}

@inproceedings{10.1145/3168829,
author = {Fu, Xinwei and Lee, Dongyoon and Jung, Changhee},
title = {NAdroid: Statically Detecting Ordering Violations in Android Applications},
year = {2018},
isbn = {9781450356176},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3168829},
doi = {10.1145/3168829},
abstract = {Modern mobile applications use a hybrid concurrency model. In this model, events are handled sequentially by event loop(s), and long-running tasks are offloaded to other threads. Concurrency errors in this hybrid concurrency model can take multiple forms: traditional atomicity and ordering violations between threads, as well as ordering violations between event callbacks on a single event loop.  This paper presents nAdroid, a static ordering violation detector for Android applications. Using our threadification technique, nAdroid statically models event callbacks as threads. Threadification converts ordering violations between event callbacks into ordering violations between threads, after which state-of-the-art thread-based race detection tools can be applied. nAdroid then applies a combination of sound and unsound filters, based on the Android concurrency model and its happens-before relation, to prune out false and benign warnings.  We evaluated nAdroid with 27 open source Android applications. Experimental results show that nAdroid detects 88 (at least 58 new) harmful ordering violations, and outperforms the state-of-the-art static technique with fewer false negatives and false positives.},
booktitle = {Proceedings of the 2018 International Symposium on Code Generation and Optimization},
pages = {62–74},
numpages = {13},
keywords = {Ordering violation, Android, Threadification, Debugging, Data race, Static analysis, Use-after-free},
location = {Vienna, Austria},
series = {CGO 2018}
}

@inproceedings{10.5555/2820518.2820552,
author = {Choetkiertikul, Morakot and Dam, Hoa Khanh and Tran, Truyen and Ghose, Aditya},
title = {Characterization and Prediction of Issue-Related Risks in Software Projects},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Identifying risks relevant to a software project and planning measures to deal with them are critical to the success of the project. Current practices in risk assessment mostly rely on high-level, generic guidance or the subjective judgements of experts. In this paper, we propose a novel approach to risk assessment using historical data associated with a software project. Specifically, our approach identifies patterns of past events that caused project delays, and uses this knowledge to identify risks in the current state of the project. A set of risk factors characterizing "risky" software tasks (in the form of issues) were extracted from five open source projects: Apache, Duraspace, JBoss, Moodle, and Spring. In addition, we performed feature selection using a sparse logistic regression model to select risk factors with good discriminative power. Based on these risk factors, we built predictive models to predict if an issue will cause a project delay. Our predictive models are able to predict both the risk impact (i.e. the extend of the delay) and the likelihood of a risk occurring. The evaluation results demonstrate the effectiveness of our predictive models, achieving on average 48%--81% precision, 23%--90% recall, 29%--71% F-measure, and 70%--92% Area Under the ROC Curve. Our predictive models also have low error rates: 0.39--0.75 for Macro-averaged Mean Cost-Error and 0.7--1.2 for Macro-averaged Mean Absolute Error.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {280–291},
numpages = {12},
location = {Florence, Italy},
series = {MSR '15}
}

@inproceedings{10.5555/800244.807312,
author = {Osterweil, Leon J. and Fosdick, Lloyd D.},
title = {Program Testing Techniques Using Simulated Execution},
year = {1976},
publisher = {IEEE Press},
abstract = {Simulation is proving to be a valuable technique in testing computer programs. By simulating different aspects of a program's execution and structure it is possible to detect errors and sometimes demonstrate the absence of certain errors in the program. This presentation will explore three popular testing methodologies which employ simulation techniques. Each methodology is based upon a different type of simulation of the program. The differences in error detection capability resulting from these different choices of simulated execution will be examined. Finally a method for using the best characteristics of each technique in a general validation system will be presented.},
booktitle = {Proceedings of the 4th Symposium on Simulation of Computer Systems},
pages = {171–177},
numpages = {7},
location = {Boulder, Colorado, USA},
series = {ANSS '76}
}

@article{10.1145/1013610.807312,
author = {Osterweil, Leon J. and Fosdick, Lloyd D.},
title = {Program Testing Techniques Using Simulated Execution},
year = {1976},
issue_date = {July 1976},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {4},
issn = {0163-6103},
url = {https://doi.org/10.1145/1013610.807312},
doi = {10.1145/1013610.807312},
abstract = {Simulation is proving to be a valuable technique in testing computer programs. By simulating different aspects of a program's execution and structure it is possible to detect errors and sometimes demonstrate the absence of certain errors in the program. This presentation will explore three popular testing methodologies which employ simulation techniques. Each methodology is based upon a different type of simulation of the program. The differences in error detection capability resulting from these different choices of simulated execution will be examined. Finally a method for using the best characteristics of each technique in a general validation system will be presented.},
journal = {SIGSIM Simul. Dig.},
month = {jul},
pages = {171–177},
numpages = {7}
}

@article{10.1145/3241744,
author = {Troya, Javier and Segura, Sergio and Parejo, Jose Antonio and Ruiz-Cort\'{e}s, Antonio},
title = {Spectrum-Based Fault Localization in Model Transformations},
year = {2018},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {27},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3241744},
doi = {10.1145/3241744},
abstract = {Model transformations play a cornerstone role in Model-Driven Engineering (MDE), as they provide the essential mechanisms for manipulating and transforming models. The correctness of software built using MDE techniques greatly relies on the correctness of model transformations. However, it is challenging and error prone to debug them, and the situation gets more critical as the size and complexity of model transformations grow, where manual debugging is no longer possible.Spectrum-Based Fault Localization (SBFL) uses the results of test cases and their corresponding code coverage information to estimate the likelihood of each program component (e.g., statements) of being faulty. In this article we present an approach to apply SBFL for locating the faulty rules in model transformations. We evaluate the feasibility and accuracy of the approach by comparing the effectiveness of 18 different state-of-the-art SBFL techniques at locating faults in model transformations. Evaluation results revealed that the best techniques, namely Kulcynski2, Mountford, Ochiai, and Zoltar, lead the debugger to inspect a maximum of three rules to locate the bug in around 74% of the cases. Furthermore, we compare our approach with a static approach for fault localization in model transformations, observing a clear superiority of the proposed SBFL-based method.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {13},
numpages = {50},
keywords = {debugging, spectrum-based, Model transformation, testing, fault localization}
}

@inproceedings{10.5555/2818754.2818783,
author = {Lee, Yun Young and Marinov, Darko and Johnson, Ralph E.},
title = {Tempura: Temporal Dimension for IDEs},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Modern integrated development environments (IDEs) make many software engineering tasks easier by providing automated programming support such as code completion and navigation. However, such support -- and therefore IDEs as a whole -- operate on one revision of the code at a time, and leave handling of code history to external tools or plugins, such as EGit for Eclipse. For example, when a method is removed from a class, developers can no longer find the method through code completion. This forces developers to manually switch across different revisions or resort to using external tools when they need to learn about previous code revisions.We propose a novel approach of adding a temporal dimension to IDEs, enabling code completion and navigation to operate on multiple revisions of code at a time. We previously introduced the idea of temporal code completion and navigation, and presented a vision for how that idea may be realized. This paper realizes that vision by implementing and evaluating a prototype tool called Tempura. We describe our algorithm for processing and indexing historical code information from repositories for Tempura, and demonstrate Tempura's scalability with three large Eclipse projects. We also evaluate Tempura's usability through a controlled user study. The study participants learned about the code history with more accuracy when using Tempura compared to EGit. Although the sample size was not large enough to provide strong statistical significance, the results show a promising outlook for our approach.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {212–222},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3426425.3426935,
author = {Barraball, Chelsea and Raselimo, Moeketsi and Fischer, Bernd},
title = {An Interactive Feedback System for Grammar Development (Tool Paper)},
year = {2020},
isbn = {9781450381765},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426425.3426935},
doi = {10.1145/3426425.3426935},
abstract = {We describe gtutr, an interactive feedback system designed to assist students in developing context-free grammars and corresponding ANTLR parsers. It intelligently controls students' access to a large test suite for the target language. After each submission, gtutr analyzes any failing tests and uses the Needleman-Wunsch sequence alignment algorithm over the tests' rule traces to identify and eliminate similar failing tests. This reduces the redundancy in the feedback  given to the students and prevents them from being overloaded. gtutr uses simple gamification to encourage independent problem solving by students: it gives as little information as possible, and students need to prompt the system for further details such as failing tests similar to or different from already seen tests, or even for hints about rules that are the most likely to contain faults. It tracks the students' information requests and uses this to attenuate marks following an instructor-set penalty schema. The system also visualizes test outcomes over multiple submissions, helping students to keep track of the effects of their changes as their grammar development progresses.},
booktitle = {Proceedings of the 13th ACM SIGPLAN International Conference on Software Language Engineering},
pages = {101–107},
numpages = {7},
keywords = {fault localization, Compiler courses},
location = {Virtual, USA},
series = {SLE 2020}
}

@inproceedings{10.1145/1629575.1629587,
author = {Xu, Wei and Huang, Ling and Fox, Armando and Patterson, David and Jordan, Michael I.},
title = {Detecting Large-Scale System Problems by Mining Console Logs},
year = {2009},
isbn = {9781605587523},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1629575.1629587},
doi = {10.1145/1629575.1629587},
abstract = {Surprisingly, console logs rarely help operators detect problems in large-scale datacenter services, for they often consist of the voluminous intermixing of messages from many software components written by independent developers. We propose a general methodology to mine this rich source of information to automatically detect system runtime problems. We first parse console logs by combining source code analysis with information retrieval to create composite features. We then analyze these features using machine learning to detect operational problems. We show that our method enables analyses that are impossible with previous methods because of its superior ability to create sophisticated features. We also show how to distill the results of our analysis to an operator-friendly one-page decision tree showing the critical messages associated with the detected problems. We validate our approach using the Darkstar online game server and the Hadoop File System, where we detect numerous real problems with high accuracy and few false positives. In the Hadoop case, we are able to analyze 24 million lines of console logs in 3 minutes. Our methodology works on textual console logs of any size and requires no changes to the service software, no human input, and no knowledge of the software's internals.},
booktitle = {Proceedings of the ACM SIGOPS 22nd Symposium on Operating Systems Principles},
pages = {117–132},
numpages = {16},
keywords = {monitoring, statistical learning, console log analysis, source code analysis, pca, problem detection, tracing},
location = {Big Sky, Montana, USA},
series = {SOSP '09}
}

@inproceedings{10.1145/2837476.2837478,
author = {Ullah, Faheem and Gross, Thomas R.},
title = {Profiling for Detecting Performance Anomalies in Concurrent Software},
year = {2015},
isbn = {9781450339100},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2837476.2837478},
doi = {10.1145/2837476.2837478},
abstract = { Understanding and identifying performance problems is difficult for parallel applications, but is an essential part of software development for parallel systems. In addition to the same problems that exist when analysing sequential programs, software development tools for parallel systems must handle the large number of execution engines (cores) that result in different (possibly non-deterministic) schedules for different executions. Understanding where exactly a concurrent program spends its time (esp. if some aspects of the program paths depend on input data) is the first step towards improving program quality. State-of-the-art profilers, however, aid developers in performance diagnosis by providing hotness information at the level of a class or method (function) and usually report data for just a single program execution. This paper presents a profiling and analysis technique that consolidates execution information for multiple program executions. Currently, our tool's focus is on execution time (CPU cycles) but other metrics (stall cycles for functional units, cache miss rates, etc) are possible, provided such data can be obtained from the processor's monitoring unit. To detect the location of performance anomalies that are worth addressing, the average amount of time spent inside a code block, along with the statistical range of the minimum and maximum amount of time spent, is taken into account. The technique identifies performance bottlenecks at the fine-grained level of a basic block. It can indicate the probability of such a performance bottleneck appearing during actual program executions. The technique utilises profiling information across a range of inputs and tries to induce performance bottlenecks by delaying random memory accesses. The approach is evaluated by performing experiments on the data compression tool pbzip2, the multi-threaded download accelerator axel, the open source security scanner Nmap and Apache httpd web server. An experimental evaluation shows the tool to be effective in detecting performance bottlenecks at the level of a basic block. Modifications in the block that is identified by the tool result in performance improvement of over 2.6x in one case, compared to the original version of the program. The performance overhead incurred by the tool is a reasonable 2-7x in majority of the cases. },
booktitle = {Proceedings of the 2nd International Workshop on Software Engineering for Parallel Systems},
pages = {11–20},
numpages = {10},
keywords = {Software defects, Localisation, Profiling, Parallel programming, Dynamic binary instrumentation, Performance bugs, Measurement},
location = {Pittsburgh, PA, USA},
series = {SEPS 2015}
}

@inproceedings{10.1145/2001576.2001827,
author = {Assun\c{c}\~{a}o, Wesley Klewerton Guez and Colanzi, Thelma Elita and Pozo, Aurora Trinidad Ramirez and Vergilio, Silvia Regina},
title = {Establishing Integration Test Orders of Classes with Several Coupling Measures},
year = {2011},
isbn = {9781450305570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001576.2001827},
doi = {10.1145/2001576.2001827},
abstract = {During the inter-class test, a common problem, named Class Integration and Test Order (CITO) problem, involves the determination of a test class order that minimizes stub creation effort, and consequently test costs. The approach based on Multi-Objective Evolutionary Algorithms (MOEAs) has achieved promising results because it allows the use of different factors and measures that can affect the stubbing process. Many times these factors are in conflict and usually there is no a single solution for the problem. Existing works on MOEAs present some limitations. The approach was evaluated with only two coupling measures, based on the number of attributes and methods of the stubs to be created. Other MOEAs can be explored and also other coupling measures. Considering this fact, this paper investigates the performance of two evolutionary algorithms: NSGA-II and SPEA2, for the CITO problem with four coupling measures (objectives) related to: attributes, methods, number of distinct return types and distinct parameter types. An experimental study was performed with four real systems developed in Java. The obtained results point out that the MOEAs can be efficiently used to solve this problem with several objectives, achieving solutions with balanced compromise between the measures, and of minimal effort to test.},
booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
pages = {1867–1874},
numpages = {8},
keywords = {inter class testing, multi-objective evolutionary algorithms},
location = {Dublin, Ireland},
series = {GECCO '11}
}

@article{10.1145/200912.200913,
author = {Bates, Peter C.},
title = {Debugging Heterogeneous Distributed Systems Using Event-Based Models of Behavior},
year = {1995},
issue_date = {Feb. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {13},
number = {1},
issn = {0734-2071},
url = {https://doi.org/10.1145/200912.200913},
doi = {10.1145/200912.200913},
abstract = {We describe a high-level debugging approach, Event-Based Behavioral Abstraction (EBBA), in which debugging is treated as a process of creating models of expected program behaviors and comparing these to the actual behaviors exhibited by the program. The use of EBBA techniques can enhance debugging-tool transparency, reduce latency and uncertainty for fundamental debugging activities, and accommodate diverse, heterogeneous architectures. Using events and behavior models as a basic mechanism provides a uniform view of heterogeneous systems and enables analysis to be performed in well-defined ways. Their use also enables EBBA users to extend and reuse knowledge gained in solving previous problems to new situations. We describe our behavior-modeling algorithm that matches actual behavior to models and automates many behavior analysis steps. The algorithm matches behavior in as many ways as possible and resolves these to return the best match to the user. It deals readily with partial behavior matches and incomplete information. In particular, we describe a tool set we have built. The tool set has been used to investigate the behavior of a wide range of programs. The tools are modular and can be distributed readily throughout a system.},
journal = {ACM Trans. Comput. Syst.},
month = {feb},
pages = {1–31},
numpages = {31},
keywords = {debugging, events, behavior modeling}
}

@inproceedings{10.1145/2786805.2786838,
author = {Daka, Ermira and Campos, Jos\'{e} and Fraser, Gordon and Dorn, Jonathan and Weimer, Westley},
title = {Modeling Readability to Improve Unit Tests},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786838},
doi = {10.1145/2786805.2786838},
abstract = { Writing good unit tests can be tedious and error prone, but even once they are written, the job is not done: Developers need to reason about unit tests throughout software development and evolution, in order to diagnose test failures, maintain the tests, and to understand code written by other developers. Unreadable tests are more difficult to maintain and lose some of their value to developers. To overcome this problem, we propose a domain-specific model of unit test readability based on human judgements, and use this model to augment automated unit test generation. The resulting approach can automatically generate test suites with both high coverage and also improved readability. In human studies users prefer our improved tests and are able to answer maintenance questions about them 14% more quickly at the same level of accuracy. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {107–118},
numpages = {12},
keywords = {Readability, unit testing, automated test generation},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/2593902.2593910,
author = {Knych, Thomas W. and Baliga, Ashwin},
title = {Android Application Development and Testability},
year = {2014},
isbn = {9781450328784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593902.2593910},
doi = {10.1145/2593902.2593910},
abstract = { In this paper, we outline the challenges mobile development poses from a perspective of testability and application quality. We propose development and testing strategies to mitigate those challenges and discuss tools that can enable those strategies. },
booktitle = {Proceedings of the 1st International Conference on Mobile Software Engineering and Systems},
pages = {37–40},
numpages = {4},
keywords = {Test Sizes, Test Strategy, Testable Design, UI Testability, Android, Mobile App Development},
location = {Hyderabad, India},
series = {MOBILESoft 2014}
}

@inproceedings{10.1145/3338906.3338916,
author = {Cotroneo, Domenico and De Simone, Luigi and Liguori, Pietro and Natella, Roberto and Bidokhti, Nematollah},
title = {How Bad Can a Bug Get? An Empirical Analysis of Software Failures in the OpenStack Cloud Computing Platform},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338916},
doi = {10.1145/3338906.3338916},
abstract = {Cloud management systems provide abstractions and APIs for programmatically configuring cloud infrastructures. Unfortunately, residual software bugs in these systems can potentially lead to high-severity failures, such as prolonged outages and data losses. In this paper, we investigate the impact of failures in the context widespread OpenStack cloud management system, by performing fault injection and by analyzing the impact of the resulting failures in terms of fail-stop behavior, failure detection through logging, and failure propagation across components. The analysis points out that most of the failures are not timely detected and notified; moreover, many of these failures can silently propagate over time and through components of the cloud management system, which call for more thorough run-time checks and fault containment.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {200–211},
numpages = {12},
keywords = {Fault injection, OpenStack, Bug analysis},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3338906.3338921,
author = {Liew, Daniel and Cadar, Cristian and Donaldson, Alastair F. and Stinnett, J. Ryan},
title = {Just Fuzz It: Solving Floating-Point Constraints Using Coverage-Guided Fuzzing},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338921},
doi = {10.1145/3338906.3338921},
abstract = {We investigate the use of coverage-guided fuzzing as a means of proving satisfiability of SMT formulas over finite variable domains, with specific application to floating-point constraints. We show how an SMT formula can be encoded as a program containing a location that is reachable if and only if the program’s input corresponds to a satisfying assignment to the formula. A coverage-guided fuzzer can then be used to search for an input that reaches the location, yielding a satisfying assignment. We have implemented this idea in a tool, Just Fuzz-it Solver (JFS), and we present a large experimental evaluation showing that JFS is both competitive with and complementary to state-of-the-art SMT solvers with respect to solving floating-point constraints, and that the coverage-guided approach of JFS provides significant benefit over naive fuzzing in the floating-point domain. Applied in a portfolio manner, the JFS approach thus has the potential to complement traditional SMT solvers for program analysis tasks that involve reasoning about floating-point constraints.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {521–532},
numpages = {12},
keywords = {feedback-directed fuzzing, Constraint solving},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.5555/2820282.2820291,
author = {White, Martin and Linares-V\'{a}squez, Mario and Johnson, Peter and Bernal-C\'{a}rdenas, Carlos and Poshyvanyk, Denys},
title = {Generating Reproducible and Replayable Bug Reports from Android Application Crashes},
year = {2015},
publisher = {IEEE Press},
abstract = {Manually reproducing bugs is time-consuming and tedious. Software maintainers routinely try to reproduce unconfirmed issues using incomplete or noninformative bug reports. Consequently, while reproducing an issue, the maintainer must augment the report with information---such as a reliable sequence of descriptive steps to reproduce the bug---to aid developers with diagnosing the issue. This process encumbers issue resolution from the time the bug is entered in the issue tracking system until it is reproduced. This paper presents crashdroid, an approach for automating the process of reproducing a bug by translating the call stack from a crash report into expressive steps to reproduce the bug and a kernel event trace that can be replayed on-demand. crashdroid manages traceability links between scenarios' natural language descriptions, method call traces, and kernel event traces. We evaluated crashdroid on several open-source Android applications infected with errors. Given call stacks from crash reports, crashdroid was able to generate expressive steps to reproduce the bugs and automatically replay the crashes. Moreover, users were able to confirm the crashes faster with crashdroid than manually reproducing the bugs or using a stress-testing tool.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {48–59},
numpages = {12},
keywords = {crash and bug reports, reproducibility, Android},
location = {Florence, Italy},
series = {ICPC '15}
}

