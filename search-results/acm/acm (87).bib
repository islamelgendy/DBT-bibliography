@inproceedings{10.1145/3377811.3380384,
author = {Sotiropoulos, Thodoris and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {Practical Fault Detection in Puppet Programs},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380384},
doi = {10.1145/3377811.3380384},
abstract = {Puppet is a popular computer system configuration management tool. By providing abstractions that model system resources it allows administrators to set up computer systems in a reliable, predictable, and documented fashion. Its use suffers from two potential pitfalls. First, if ordering constraints are not correctly specified whenever a Puppet resource depends on another, the non-deterministic application of resources can lead to race conditions and consequent failures. Second, if a service is not tied to its resources (through the notification construct), the system may operate in a stale state whenever a resource gets modified. Such faults can degrade a computing infrastructure's availability and functionality.We have developed an approach that identifies these issues through the analysis of a Puppet program and its system call trace. Specifically, a formal model for traces allows us to capture the interactions of Puppet resources with the file system. By analyzing these interactions we identify (1) resources that are related to each other (e.g., operate on the same file), and (2) resources that should act as notifiers so that changes are correctly propagated. We then check the relationships from the trace's analysis against the program's dependency graph: a representation containing all the ordering constraints and notifications declared in the program. If a mismatch is detected, our system reports a potential fault.We have evaluated our method on a large set of popular Puppet modules, and discovered 92 previously unknown issues in 33 modules. Performance benchmarking shows that our approach can analyze in seconds real-world configurations with a magnitude measured in thousands of lines and millions of system calls.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {26–37},
numpages = {12},
keywords = {program analysis, ordering relationships, system calls, puppet, notifiers},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/2652483,
author = {Zhang, Pingyu and Elbaum, Sebastian},
title = {Amplifying Tests to Validate Exception Handling Code: An Extended Study in the Mobile Application Domain},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2652483},
doi = {10.1145/2652483},
abstract = {Validating code handling exceptional behavior is difficult, particularly when dealing with external resources that may be noisy and unreliable, as it requires (1) systematic exploration of the space of exceptions that may be thrown by the external resources, and (2) setup of the context to trigger specific patterns of exceptions. In this work, we first present a study quantifying the magnitude of the problem by inspecting the bug repositories of a set of popular applications in the increasingly relevant domain of Android mobile applications. The study revealed that 22% of the confirmed and fixed bugs have to do with poor exceptional handling code, and half of those correspond to interactions with external resources. We then present an approach that addresses this challenge by performing an systematic amplification of the program space explored by a test by manipulating the behavior of external resources. Each amplification attempts to expose a program's exception handling constructs to new behavior by mocking an external resource so that it returns normally or throws an exception following a predefined set of patterns. Our assessment of the approach indicates that it can be fully automated, is powerful enough to detect 67% of the faults reported in the bug reports of this kind, and is precise enough that 78% of the detected anomalies are fixed, and it has a great potential to assist developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {32},
numpages = {28},
keywords = {mobile applications, exception handling, Test transformation, test case generation, test amplification}
}

@inproceedings{10.1145/2632362.2632372,
author = {Jakobs, Marie-Christine and Wehrheim, Heike},
title = {Certification for Configurable Program Analysis},
year = {2014},
isbn = {9781450324526},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2632362.2632372},
doi = {10.1145/2632362.2632372},
abstract = { Configurable program analysis (CPA) is a generic concept for the formalization of different software analysis techniques in a single framework. With the tool CPAchecker, this framework allows for an easy configuration and subsequent automatic execution of analysis procedures ranging from data-flow analysis to model checking. The focus of the tool CPAchecker is thus on analysis. In this paper, we study configurability from the point of view of software certification. Certification aims at providing (via a prior analysis) a certificate of correctness for a program which is (a) tamper-proof and (b) more efficient to check for validity than a full analysis. Here, we will show how, given an analysis instance of a CPA, to construct a corresponding sound certification instance, thereby arriving at configurable program certification. We report on experiments with certification based on different analysis techniques, and in particular explain which characteristics of an underlying analysis allow us to design an efficient (in the above (b) sense) certification procedure. },
booktitle = {Proceedings of the 2014 International SPIN Symposium on Model Checking of Software},
pages = {30–39},
numpages = {10},
keywords = {Program Analysis, Configuration, Proof Carrying Code, Certification},
location = {San Jose, CA, USA},
series = {SPIN 2014}
}

@inproceedings{10.1145/3492321.3519558,
author = {Vinck, Jonas and Abrath, Bert and Coppens, Bart and Voulimeneas, Alexios and De Sutter, Bjorn and Volckaert, Stijn},
title = {Sharing is Caring: Secure and Efficient Shared Memory Support for MVEEs},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3519558},
doi = {10.1145/3492321.3519558},
abstract = {Multi-Variant Execution Environments (MVEEs) are a powerful tool for protecting legacy software against memory corruption attacks. MVEEs employ software diversity to run multiple variants of the same program in lockstep, whilst providing them with the same inputs and comparing their behavior. Well-constructed variants will behave equivalently under normal operating conditions but diverge when under attack. The MVEE detects these divergences and takes action before compromised variants can damage the host system.Existing MVEEs replicate inputs at the system call boundary, and therefore do not support programs that use shared-memory IPC with other processes, since shared memory pages can be read from and written to directly without system calls.We analyzed modern applications, ranging from web servers, over media players, to browsers, and observe that they rely heavily on shared memory, in some cases for their basic functioning and in other cases for enabling more advanced functionality. It follows that modern applications cannot enjoy the security provided by MVEEs unless those MVEEs support shared-memory IPC.This paper first identifies the requirements for supporting shared-memory IPC in an MVEE. We propose a design that involves techniques to identify and instrument accesses to shared memory pages, as well as techniques to replicate I/O through shared-memory IPC. We implemented these techniques in a prototype MVEE and report our findings through an evaluation of a range of benchmark programs. Our contributions enable the use of MVEEs on a far wider range of programs than previously supported. By overcoming one of the major remaining limitations of MVEEs, our contributions can help to bolster their real-world adoption.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {99–116},
numpages = {18},
keywords = {shared memory, security, OS},
location = {Rennes, France},
series = {EuroSys '22}
}

@inproceedings{10.1145/2945408.2945414,
author = {Gawali, Devidas and Apte, Varsha},
title = {The M3 (Measure-Measure-Model) Tool-Chain for Performance Prediction of Multi-Tier Applications},
year = {2016},
isbn = {9781450344111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945408.2945414},
doi = {10.1145/2945408.2945414},
abstract = { Performance prediction of multi-tier applications is a critical step in the life-cycle of an application. However, the target hardware platform on which performance prediction is re- quired is often different from the testbed one on which the application performance can be measured, and is usually un- available for deployment and load testing of the application. In this paper, we present M3 , our Measure-Measure-Model method, which uses a pipeline of three tools to solve this problem. The tool-chain starts with AutoPerf, which mea- sures the CPU service demands of the application on the testbed. CloneGen then takes this and the number and size of network calls as input and generates a clone, whose CPU service demand matches the application’s. This clone is then deployed on the target, instead of the original application, since its code is simple, does not need a full database, and is thus easier to install. AutoPerf is used again to measure CPU service demand of the clone on the target, under light load generation. Finally, this service demand is fed into PerfCenter which is a multi-tier application performance modeling tool, which can then predict the application per- formance on the target under any workload. We validated the predictions made using the M3 tool-chain against direct measurement made on two applications - DellDVD and RU- BiS, on various combinations of testbed and target platforms (Intel and AMD servers) and found that in almost all cases, prediction error was less than 20%. },
booktitle = {Proceedings of the 2nd International Workshop on Quality-Aware DevOps},
pages = {30–35},
numpages = {6},
keywords = {Benchmark, Modeling, Performance, Multi-tier, Prediction},
location = {Saarbr\"{u}cken, Germany},
series = {QUDOS 2016}
}

@inproceedings{10.5555/2818754.2818855,
author = {Kusano, Markus and Chattopadhyay, Arijit and Wang, Chao},
title = {Dynamic Generation of Likely Invariants for Multithreaded Programs},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {We propose a new method for dynamically generating likely invariants from multithreaded programs. While existing invariant generation tools work well on sequential programs, they are ineffective at reasoning about multithreaded programs both in terms of the number of real invariants generated and in terms of their usefulness in helping programmers. We address this issue by developing a new dynamic invariant generator consisting of an LLVM based code instrumentation front end, a systematic thread interleaving explorer, and a customized invariant inference engine. We show that efficient interleaving exploration strategies can be used to generate a diversified set of executions with little runtime overhead. Furthermore, we show that focusing on a small subset of thread-local transition invariants is often sufficient for reasoning about the concurrency behavior of programs. We have evaluated our new method on a set of open-source multithreaded C/C++ benchmarks. Our experiments show that our method can generate invariants that are significantly higher in quality than the previous state-of-the-art.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {835–846},
numpages = {12},
location = {Florence, Italy},
series = {ICSE '15}
}

@inbook{10.1145/3461001.3471146,
author = {Horcas, Jose-Miguel and Galindo, Jos\'{e} A. and Heradio, Ruben and Fernandez-Amoros, David and Benavides, David},
title = {Monte Carlo Tree Search for Feature Model Analyses: A General Framework for Decision-Making},
year = {2021},
isbn = {9781450384698},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3461001.3471146},
abstract = {The colossal solution spaces of most configurable systems make intractable their exhaustive exploration. Accordingly, relevant analyses remain open research problems. There exist analyses alternatives such as SAT solving or constraint programming. However, none of them have explored simulation-based methods. Monte Carlo-based decision making is a simulation-based method for dealing with colossal solution spaces using randomness. This paper proposes a conceptual framework that tackles various of those analyses using Monte Carlo methods, which have proven to succeed in vast search spaces (e.g., game theory). Our general framework is described formally, and its flexibility to cope with a diversity of analysis problems is discussed (e.g., finding defective configurations, feature model reverse engineering or getting optimal performance configurations). Additionally, we present a Python implementation of the framework that shows the feasibility of our proposal. With this contribution, we envision that different problems can be addressed using Monte Carlo simulations and that our framework can be used to advance the state of the art a step forward.},
booktitle = {Proceedings of the 25th ACM International Systems and Software Product Line Conference - Volume A},
pages = {190–201},
numpages = {12}
}

@inproceedings{10.1145/3324884.3416577,
author = {Wen, Chengyuan and Zhang, Yaxuan and He, Xiao and Meng, Na},
title = {Inferring and Applying Def-Use like Configuration Couplings in Deployment Descriptors},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416577},
doi = {10.1145/3324884.3416577},
abstract = {When building enterprise applications on Java frameworks (e.g., Spring), developers often specify components and configure operations with a special kind of XML files named "deployment descriptors (DD)". Maintaining such XML files is challenging and time-consuming; because (1) the correct configuration semantics is domain-specific but usually vaguely documented, and (2) existing compilers and program analysis tools rarely examine XML files. To help developers ensure the quality of DD, this paper presents a novel approach---Xeditor---that extracts configuration couplings (i.e., frequently co-occurring configurations) from DD, and adopts the coupling rules to validate new or updated files.Xeditor has two phases: coupling extraction and bug detection. To identify couplings, Xeditor first mines DD in open-source projects, and extracts XML entity pairs that (i) frequently coexist in the same files and (ii) hold the same data at least once. Xeditor then applies customized association rule mining to the extracted pairs. For bug detection, given a new XML file, Xeditor checks whether the file violates any coupling; if so, Xeditor reports the violation(s). For evaluation, we first created two data sets with the 4,248 DD mined from 1,137 GitHub projects. According to the experiments with these data sets, Xeditor extracted couplings with high precision (73%); it detected bugs with 92% precision, 96% recall, and 94% accuracy. Additionally, we applied Xeditor to the version history of another 478 GitHub projects. Xeditor identified 25 very suspicious XML updates, 15 of which were later fixed by developers.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {672–683},
numpages = {12},
keywords = {rule mining, configuration coupling, deployment descriptor},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/1669112.1669181,
author = {Lucia, Brandon and Ceze, Luis},
title = {Finding Concurrency Bugs with Context-Aware Communication Graphs},
year = {2009},
isbn = {9781605587981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1669112.1669181},
doi = {10.1145/1669112.1669181},
abstract = {Incorrect thread synchronization often leads to concurrency bugs that manifest nondeterministically and are difficult to detect and fix. Past work on detecting concurrency bugs has addressed the general problem in an ad-hoc fashion, focusing mostly on data races and atomicity violations.Using graphs to represent a multithreaded program execution is very natural, nodes represent static instructions and edges represent communication via shared memory. In this paper we make the fundamental observation that such basic context-oblivious graphs do not encode enough information to enable accurate bug detection. We propose context-aware communication graphs, a new kind of communication graph that encodes global ordering information by embedding communication contexts. We then build Bugaboo, a simple and generic framework that accurately detects complex concurrency bugs. Our framework collects communication graphs from multiple executions and uses invariant-based techniques to detect anomalies in the graphs.We built two versions of Bugaboo: BB-SW, which is fully implemented in software but suffers from significant slowdowns; and BB-HW, which relies on custom architecture support but has negligible performance degradation. BB-HW requires modest extensions to a commodity multicore processor and can be used in deployment settings. We evaluate both versions using applications such as MySQL, Apache, PARSEC, and several others. Our results show that Bugaboo identifies a wide variety of concurrency bugs, including challenging multivariable bugs, with few (often zero) unnecessary code inspections.},
booktitle = {Proceedings of the 42nd Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {553–563},
numpages = {11},
location = {New York, New York},
series = {MICRO 42}
}

@inproceedings{10.1145/2491627.2499880,
author = {Clarke, Dave and Schaefer, Ina and ter Beek, Maurice H. and Apel, Sven and Atlee, Joanne M.},
title = {Formal Methods and Analysis in Software Product Line Engineering: 4th Edition of FMSPLE Workshop Series},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2499880},
doi = {10.1145/2491627.2499880},
abstract = {FMSPLE 2013 is the fourth edition of the FMSPLE workshop series aimed at connecting researchers and practitioners interested in raising the efficiency and the effectiveness of software product line engineering through the application of innovative analysis approaches and formal methods.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {266–267},
numpages = {2},
keywords = {verification, evolution, testing, semantics, variability, software product lines, formal methods},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@article{10.1145/2345770.2345773,
author = {Izosimov, Viacheslav and Pop, Paul and Eles, Petru and Peng, Zebo},
title = {Scheduling and Optimization of Fault-Tolerant Embedded Systems with Transparency/Performance Trade-Offs},
year = {2012},
issue_date = {September 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {11},
number = {3},
issn = {1539-9087},
url = {https://doi.org/10.1145/2345770.2345773},
doi = {10.1145/2345770.2345773},
abstract = {In this article, we propose a strategy for the synthesis of fault-tolerant schedules and for the mapping of fault-tolerant applications. Our techniques handle transparency/performance trade-offs and use the fault-occurrence information to reduce the overhead due to fault tolerance. Processes and messages are statically scheduled, and we use process reexecution for recovering from multiple transient faults. We propose a fine-grained transparent recovery, where the property of transparency can be selectively applied to processes and messages. Transparency hides the recovery actions in a selected part of the application so that they do not affect the schedule of other processes and messages. While leading to longer schedules, transparent recovery has the advantage of both improved debuggability and less memory needed to store the fault-tolerant schedules.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {61},
numpages = {35},
keywords = {process mapping, transient faults, real-time scheduling, design optimization, intermittent faults, debuggability, Fault-tolerant embedded systems, conditional scheduling, safety-critical applications}
}

@inproceedings{10.1145/3314221.3314633,
author = {Fremont, Daniel J. and Dreossi, Tommaso and Ghosh, Shromona and Yue, Xiangyu and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A.},
title = {Scenic: A Language for Scenario Specification and Scene Generation},
year = {2019},
isbn = {9781450367127},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3314221.3314633},
doi = {10.1145/3314221.3314633},
abstract = {We propose a new probabilistic programming language for the design and analysis of perception systems, especially those based on machine learning. Specifically, we consider the problems of training a perception system to handle rare events, testing its performance under different conditions, and debugging failures. We show how a probabilistic programming language can help address these problems by specifying distributions encoding interesting types of inputs and sampling these to generate specialized training and test sets. More generally, such languages can be used for cyber-physical systems and robotics to write environment models, an essential prerequisite to any formal analysis. In this paper, we focus on systems like autonomous cars and robots, whose environment is a scene, a configuration of physical objects and agents. We design a domain-specific language, Scenic, for describing scenarios that are distributions over scenes. As a probabilistic programming language, Scenic allows assigning distributions to features of the scene, as well as declaratively imposing hard and soft constraints over the scene. We develop specialized techniques for sampling from the resulting distribution, taking advantage of the structure provided by Scenic's domain-specific syntax. Finally, we apply Scenic in a case study on a convolutional neural network designed to detect cars in road images, improving its performance beyond that achieved by state-of-the-art synthetic data generation methods.},
booktitle = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {63–78},
numpages = {16},
keywords = {fuzz testing, scenario description language, deep learning, probabilistic programming, automatic test generation, synthetic data},
location = {Phoenix, AZ, USA},
series = {PLDI 2019}
}

@inproceedings{10.1145/1655925.1655985,
author = {Mao, Chengying},
title = {Experiences in Security Testing for Web-Based Applications},
year = {2009},
isbn = {9781605587103},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1655925.1655985},
doi = {10.1145/1655925.1655985},
abstract = {Web-based application is the most prevalent pattern of software system, and has been widely used in the industry and society. However, its security problem brings great harassment to users, such as system crash and economic loss. So it has attracted lots of attention in both academic and industrial community. Although the existing researches have discussed such problem, they mainly focus on a specific security flaw but fail to provide an overall testing guidance. At first, an overall security testing framework for Web-based application is proposed in the paper. Subsequently, the security testing practice for a real-world Web application is carried out, and the corresponding experiences are reported. Test results show that the security testing framework can provide effectual direction for testing practice and reveal valuable security flaws.},
booktitle = {Proceedings of the 2nd International Conference on Interaction Sciences: Information Technology, Culture and Human},
pages = {326–330},
numpages = {5},
keywords = {management control, web application, experiences report, vulnerability, validation, security testing},
location = {Seoul, Korea},
series = {ICIS '09}
}

@inproceedings{10.1145/1944862.1944887,
author = {Mars, Jason and Tang, Lingjia and Soffa, Mary Lou},
title = {Directly Characterizing Cross Core Interference through Contention Synthesis},
year = {2011},
isbn = {9781450302418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1944862.1944887},
doi = {10.1145/1944862.1944887},
abstract = {In this paper, we present a direct methodology and framework for the measurement and characterization of an application's cross-core interference sensitivity on multicore microarchitectures. While prior works use indirect indicators, such as last level cache miss rate, to infer an application's cross-core interference sensitivity, our approach is direct, in that it characterizes the application's cross-core interference sensitivity using the performance impact due to actual contention. Our methodology and framework, the Cross-core interference Profiling Environment, or CiPE, is composed of a lightweight runtime environment on which a host application runs, along with a carefully designed contention synthesis engine that executes on a neighboring core. CiPE manipulates the co-running contention synthesis engine, while monitoring and analyzing the resulting dynamic impact on the host application.CiPE is able to characterize the cross-core interference sensitivity of the entire application, its individual phases, or source level code regions. To demonstrate the effectiveness of CiPE, we use CiPE characterizations to address two pressing problems. First, we use CiPE characterizations to perform contention conscious batch scheduling that minimizes cross-core interference, resulting in a 12% performance improvment on average when applied to the SPEC2006 benchmark suite, and beyond 20% in the case of mcf and omnetpp. Second, we use CiPE to design a performance analysis tool that is capable identifying contentious bottlenecks in application code.},
booktitle = {Proceedings of the 6th International Conference on High Performance and Embedded Architectures and Compilers},
pages = {167–176},
numpages = {10},
keywords = {cross-core interference, program understanding, multicore, dynamic analysis, contention, execution runtimes, profiling framework},
location = {Heraklion, Greece},
series = {HiPEAC '11}
}

@inproceedings{10.1145/2642937.2643014,
author = {Zuddas, Daniele and Jin, Wei and Pastore, Fabrizio and Mariani, Leonardo and Orso, Alessandro},
title = {MIMIC: Locating and Understanding Bugs by Analyzing Mimicked Executions},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643014},
doi = {10.1145/2642937.2643014},
abstract = {Automated debugging techniques aim to help developers locate and understand the cause of a failure, an extremely challenging yet fundamental task. Most state-of-the-art approaches suffer from two problems: they require a large number of passing and failing tests and report possible faulty code with no explanation. To mitigate these issues, we present MIMIC, a novel automated debugging technique that combines and extends our previous input generation and anomaly detection techniques. MIMIC (1) synthesizes multiple passing and failing executions similar to an observed failure and (2) uses these executions to detect anomalies in behavior that may explain the failure. We evaluated MIMIC on six failures of real-world programs with promising results: for five of these failures, MIMIC identified their root causes while producing a limited number of false positives. Most importantly, the anomalies identified by MIMIC provided information that may help developers understand (and ultimately eliminate) such root causes.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {815–826},
numpages = {12},
keywords = {anomaly detection, debugging, execution synthesis},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inproceedings{10.1145/2642937.2642963,
author = {Dewey, Kyle and Roesch, Jared and Hardekopf, Ben},
title = {Language Fuzzing Using Constraint Logic Programming},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2642963},
doi = {10.1145/2642937.2642963},
abstract = {Fuzz testing builds confidence in compilers and interpreters. It is desirable for fuzzers to allow targeted generation of programs that showcase specific language features and behaviors. However, the predominant program generation technique used by most language fuzzers, stochastic context-free grammars, does not have this property. We propose the use of constraint logic programming (CLP) for program generation. Using CLP, testers can write declarative predicates specifying interesting programs, including syntactic features and semantic behaviors. CLP subsumes and generalizes the stochastic grammar approach.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {725–730},
numpages = {6},
keywords = {automated testing, fuzzing, automated program generation},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@article{10.1145/2491522.2491524,
author = {Binkley, David and Gold, Nicolas and Harman, Mark and Islam, Syed and Krinke, Jens and Li, Zheng},
title = {Efficient Identification of Linchpin Vertices in Dependence Clusters},
year = {2013},
issue_date = {July 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/2491522.2491524},
doi = {10.1145/2491522.2491524},
abstract = {Several authors have found evidence of large dependence clusters in the source code of a diverse range of systems, domains, and programming languages. This raises the question of how we might efficiently locate the fragments of code that give rise to large dependence clusters. We introduce an algorithm for the identification of linchpin vertices, which hold together large dependence clusters, and prove correctness properties for the algorithm’s primary innovations. We also report the results of an empirical study concerning the reduction in analysis time that our algorithm yields over its predecessor using a collection of 38 programs containing almost half a million lines of code. Our empirical findings indicate improvements of almost two orders of magnitude, making it possible to process larger programs for which it would have previously been impractical.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
articleno = {7},
numpages = {35},
keywords = {empirical study, performance enhancement, Slicing, internal representation}
}

@inproceedings{10.1145/3494885.3494934,
author = {Kong, Jinlan and Zhou, Qinglei and Lin, Mengfei},
title = {Design And Implementation of Video Learning Platform Based on B / S Architecture},
year = {2021},
isbn = {9781450390675},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3494885.3494934},
doi = {10.1145/3494885.3494934},
abstract = {With the further popularization of computer technology, communication technology and network technology, students can't live or study without the help of network. Video learning platform is such a benchmark. It has a large number of data resource databases, breaks the boundaries of traditional teaching, reveals the development direction of modern education, and makes the learning environment more diversified and enriched. The system is deeply rooted in the background and significance of today's online education, integrates the practical problems existing in the immature video learning platform in the past, analyzes the specific needs of current teachers and students for the learning platform, grasps the development trend of the future education industry from offline to online, and realizes the effective management of a series of categories such as member courses in online learning. It makes use of the advantages of the computer industry It integrates with models, concepts and methods, adopts MVC three-tier pattern design, based on B / S architecture and Struts + Spring + Hibernate framework design, MySQL and eclipse in parallel, using mature Dao mode to access MySQL database, and comprehensively realizing a complete set of video learning system.},
booktitle = {2021 4th International Conference on Computer Science and Software Engineering (CSSE 2021)},
pages = {274–278},
numpages = {5},
keywords = {Online learning system, MySQL, B/S},
location = {Singapore, Singapore},
series = {CSSE 2021}
}

@article{10.1145/3533704,
author = {Jero, Samuel and Burow, Nathan and Ward, Bryan and Skowyra, Richard and Khazan, Roger and Shrobe, Howard and Okhravi, Hamed},
title = {TAG: Tagged Architecture Guide},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3533704},
doi = {10.1145/3533704},
abstract = {Software security defenses are routinely broken by the persistence of both security researchers and attackers. Hardware solutions based on tagging are emerging as a promising technique that provides strong security guarantees (e.g., memory safety) while incurring minimal runtime overheads and maintaining compatibility with existing codebases. Such schemes extend every word in memory with a tag and enforce security policies across them. This paper provides a survey of existing work on tagged architectures and describe the types of attacks such architectures aim to prevent as well as the guarantees they provide. It highlights the main distinguishing factors among tagged architectures and presents the diversity of designs and implementations that have been proposed. The survey reveals several real-world challenges have been neglected relating to both security and practical deployment. The challenges relate to the provisioning and enforcement phases of tagged architectures, and various overheads they incur. This work identifies these challenges as open research problems and provides suggestions for improving their security and practicality.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {apr}
}

@inproceedings{10.1145/2155620.2155645,
author = {Zhang, Jiaqi and Xiong, Weiwei and Liu, Yang and Park, Soyeon and Zhou, Yuanyuan and Ma, Zhiqiang},
title = {ATDetector: Improving the Accuracy of a Commercial Data Race Detector by Identifying Address Transfer},
year = {2011},
isbn = {9781450310536},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2155620.2155645},
doi = {10.1145/2155620.2155645},
abstract = {In order to take advantage of multi-core hardware, more and more applications are becoming multi-threaded. Unfortunately concurrent programs are prone to bugs, such as data races. Recently much work has been devoted to detecting data races in multi-threaded programs. Most tools, however, require the accurate knowledge of synchronizations in the program, and may otherwise suffer from false positives in race detection, limiting their usability. To address this problem, some tools such as Intel® Inspector provide mechanisms for suppressing false positives and/or annotating synchronizations not automatically recognized by the tools. However, they require users' input or even changes of the source code.We took a different approach to address this problem. More specifically, we first used a state-of-the-art commercial data race detector, namely Intel® Inspector on 17 applications of various types including 5 servers, 5 client/desktop applications, and 7 scientific ones, without utilizing any suppression or annotation mechanisms provided by the product that need users' input. We examined a total of 1420 false data races and identified two major root causes including address transfer, where one thread passes memory address to another thread. We found more than 62% false data races were caused by address transfer. Based on this observation, we designed and implemented an algorithm that automatically identify address transfer and use the information to prune the false data races. Our evaluation with 8 real-world applications shows that it can effectively prune all false data races caused by unrecognized address transfers, without eliminating any true data race that was originally reported.},
booktitle = {Proceedings of the 44th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {206–215},
numpages = {10},
keywords = {data race, false positive, concurrency bug},
location = {Porto Alegre, Brazil},
series = {MICRO-44}
}

