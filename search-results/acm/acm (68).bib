@article{10.1145/2531921,
author = {M\o{}ller, Anders and Schwarz, Mathias},
title = {Automated Detection of Client-State Manipulation Vulnerabilities},
year = {2014},
issue_date = {August 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {23},
number = {4},
issn = {1049-331X},
url = {https://doi.org/10.1145/2531921},
doi = {10.1145/2531921},
abstract = {Web application programmers must be aware of a wide range of potential security risks. Although the most common pitfalls are well described and categorized in the literature, it remains a challenging task to ensure that all guidelines are followed. For this reason, it is desirable to construct automated tools that can assist the programmers in the application development process by detecting weaknesses. Many vulnerabilities are related to Web application code that stores references to application state in the generated HTML documents to work around the statelessness of the HTTP protocol. In this article, we show that such client-state manipulation vulnerabilities are amenable to tool-supported detection.We present a static analysis for the widely used frameworks Java Servlets, JSP, and Struts. Given a Web application archive as input, the analysis identifies occurrences of client state and infers the information flow between the client state and the shared application state on the server. This makes it possible to check how client-state manipulation performed by malicious users may affect the shared application state and cause leakage or modifications of sensitive information. The warnings produced by the tool help the application programmer identify vulnerabilities before deployment. The inferred information can also be applied to configure a security filter that automatically guards against attacks at runtime. Experiments on a collection of open-source Web applications indicate that the static analysis is able to effectively help the programmer prevent client-state manipulation vulnerabilities. The analysis detects a total of 4,802 client-state parameters in ten applications, whereof 4,437 are classified as safe and 241 reveal exploitable vulnerabilities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {sep},
articleno = {29},
numpages = {30},
keywords = {information flow analysis, Web application security, static analysis}
}

@inproceedings{10.1145/2851613.2851864,
author = {Ahmad, Tanwir and Truscan, Dragos},
title = {Automatic Performance Space Exploration of Web Applications Using Genetic Algorithms},
year = {2016},
isbn = {9781450337397},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851613.2851864},
doi = {10.1145/2851613.2851864},
abstract = {We describe a tool-supported performance exploration approach in which we use genetic algorithms to find a potential user behavioural pattern that maximizes the resource utilization of the system under test. This work is built upon our previous work in which we generate load from workload models that describe the expected behaviour of the users. In this paper, we evolve a given probabilistic workload model (specified as a Markov Chain Model) by optimizing the probability distribution of the edges in the model and generating different solutions. During the evolution, the solutions are ranked according to their fitness values. The solutions with the highest fitness are chosen as parent solutions for generating offsprings. At the end of an experiment, we select the best solution among all the generations. We validate our approach by generating load from both the original and the best solution model, and by comparing the resource utilization they create on the system under test.},
booktitle = {Proceedings of the 31st Annual ACM Symposium on Applied Computing},
pages = {795–800},
numpages = {6},
keywords = {performance testing, performance exploration, markov chain model, genetic algorithms},
location = {Pisa, Italy},
series = {SAC '16}
}

@inproceedings{10.1145/2635868.2635871,
author = {Banerjee, Abhijeet and Chong, Lee Kee and Chattopadhyay, Sudipta and Roychoudhury, Abhik},
title = {Detecting Energy Bugs and Hotspots in Mobile Apps},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635871},
doi = {10.1145/2635868.2635871},
abstract = { Over the recent years, the popularity of smartphones has increased dramatically. This has lead to a widespread availability of smartphone applications. Since smartphones operate on a limited amount of battery power, it is important to develop tools and techniques that aid in energy-efficient application development. Energy inefficiencies in smartphone applications can broadly be categorized into energy hotspots and energy bugs. An energy hotspot can be described as a scenario where executing an application causes the smartphone to consume abnormally high amount of battery power, even though the utilization of its hardware resources is low. In contrast, an energy bug can be described as a scenario where a malfunctioning application prevents the smartphone from becoming idle, even after it has completed execution and there is no user activity. In this paper, we present an automated test generation framework that detects energy hotspots/bugs in Android applications. Our framework systematically generates test inputs that are likely to capture energy hotspots/bugs. Each test input captures a sequence of user interactions (e.g. touches or taps on the smartphone screen) that leads to an energy hotspot/bug in the application. Evaluation with 30 freely-available Android applications from Google Play/F-Droid shows the efficacy of our framework in finding hotspots/bugs. Manual validation of the experimental results shows that our framework reports reasonably low number of false positives. Finally, we show the usage of the generated results by improving the energy-efficiency of some Android applications. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {588–598},
numpages = {11},
keywords = {Energy consumption, Non-functional testing, Mobile apps},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/3503222.3507764,
author = {Theodoridis, Theodoros and Rigger, Manuel and Su, Zhendong},
title = {Finding Missed Optimizations through the Lens of Dead Code Elimination},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507764},
doi = {10.1145/3503222.3507764},
abstract = {Compilers are foundational software development tools and incorporate increasingly sophisticated optimizations. Due to their complexity, it is difficult to systematically identify opportunities for improving them. Indeed, the automatic discovery of missed optimizations has been an important and significant challenge. The few existing approaches either cannot accurately pinpoint missed optimizations or target only specific analyses. This paper tackles this challenge by introducing a novel, effective approach that --- in a simple and general manner --- automatically identifies a wide range of missed optimizations. Our core insight is to leverage dead code elimination (DCE) to both analyze how well compilers optimize code and identify missed optimizations: (1) insert "optimization markers" in the basic blocks of a given program, (2) compute the program's live/dead basic blocks using the "optimization markers", and (3) identify missed optimizations from how well compilers eliminate dead blocks. We essentially exploit that, since DCE heavily depends on the rest of the optimization pipeline, through the lens of DCE, one can systematically quantify how well compilers optimize code. We conduct an extensive analysis of GCC and LLVM using our approach, which (1) provides quantitative and qualitative insights regarding their optimization capabilities, and (2) uncovers a diverse set of missed optimizations. Our results also lead to 84 bug reports for GCC and LLVM, of which 62 have already been confirmed or fixed, demonstrating our work's strong practical utility. We expect that the simplicity and generality of our approach will make it widely applicable for understanding compiler performance and finding missed optimizations. This work opens and initiates this promising direction.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {697–709},
numpages = {13},
keywords = {compilers, testing, missed optimizations},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@article{10.1145/3313789,
author = {Reuling, Dennis and Kelter, Udo and B\"{u}rdek, Johannes and Lochau, Malte},
title = {Automated N-Way Program Merging for Facilitating Family-Based Analyses of Variant-Rich Software},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3313789},
doi = {10.1145/3313789},
abstract = {Nowadays software tends to come in many different, yet similar variants, often derived from a common code base via clone-and-own. Family-based-analysis strategies have recently shown very promising potential for improving efficiency in applying quality-assurance techniques to such variant-rich programs, as compared to variant-by-variant approaches. Unfortunately, these strategies require a single program representation superimposing all program variants in a syntactically well-formed, semantically sound, and variant-preserving manner, which is usually not available and manually hard to obtain in practice. In this article, we present a novel methodology, called SiMPOSE, for automatically generating superimpositions of existing program variants to facilitate family-based analyses of variant-rich software. To this end, we propose a novel N-way model-merging methodology to integrate the control-flow automaton (CFA) representations of N given variants of a C program into one unified CFA representation. CFA constitute a unified program abstraction used by many recent software-analysis tools for automated quality assurance. To cope with the inherent complexity of N-way model-merging, our approach (1) utilizes principles of similarity-propagation to reduce the number of potential N-way matches, and (2) enables us to decompose a set of N variants into arbitrary subsets and to incrementally derive an N-way superimposition from partial superimpositions. We apply our tool implementation of SiMPOSE to a selection of realistic C programs, frequently considered for experimental evaluation of program-analysis techniques. In particular, we investigate applicability and efficiency/effectiveness trade-offs of our approach by applying SiMPOSE in the context of family-based unit-test generation as well as model-checking as sample program-analysis techniques. Our experimental results reveal very impressive efficiency improvements by an average factor of up to 2.6 for test-generation and up to 2.4 for model-checking under stable effectiveness, as compared to variant-by-variant approaches, thus amortizing the additional effort required for merging. In addition, our results show that merging all N variants at once produces, in almost all cases, clearly more precise results than incremental step-wise 2-way merging. Finally, our comparison with major existing N-way merging techniques shows that SiMPOSE constitutes, in most cases, the best efficiency/effectiveness trade-off.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {13},
numpages = {59},
keywords = {control flow automata, quality assurance, Program merging, model matching, variability encoding}
}

@article{10.1145/3391202,
author = {Kim, Seulbae and Xu, Meng and Kashyap, Sanidhya and Yoon, Jungyeon and Xu, Wen and Kim, Taesoo},
title = {Finding Bugs in File Systems with an Extensible Fuzzing Framework},
year = {2020},
issue_date = {May 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1553-3077},
url = {https://doi.org/10.1145/3391202},
doi = {10.1145/3391202},
abstract = {File systems are too large to be bug free. Although handwritten test suites have been widely used to stress file systems, they can hardly keep up with the rapid increase in file system size and complexity, leading to new bugs being introduced. These bugs come in various flavors: buffer overflows to complicated semantic bugs. Although bug-specific checkers exist, they generally lack a way to explore file system states thoroughly. More importantly, no turnkey solution exists that unifies the checking effort of various aspects of a file system under one umbrella.In this article, to highlight the potential of applying fuzzing to find any type of file system bugs in a generic way, we propose Hydra, an extensible fuzzing framework. Hydra provides building blocks for file system fuzzing, including input mutators, feedback engines, test executors, and bug post-processors. As a result, developers only need to focus on building the core logic for finding bugs of their interests. We showcase the effectiveness of Hydra with four checkers that hunt crash inconsistency, POSIX violations, logic assertion failures, and memory errors. So far, Hydra has discovered 157 new bugs in Linux file systems, including three in verified file systems (FSCQ and Yxv6).},
journal = {ACM Trans. Storage},
month = {may},
articleno = {10},
numpages = {35},
keywords = {crash consistency, fuzzing, File systems, bug finding}
}

@inproceedings{10.1145/2642937.2643008,
author = {Harman, Mark and Jia, Yue and Reales Mateo, Pedro and Polo, Macario},
title = {Angels and Monsters: An Empirical Investigation of Potential Test Effectiveness and Efficiency Improvement from Strongly Subsuming Higher Order Mutation},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643008},
doi = {10.1145/2642937.2643008},
abstract = {We study the simultaneous test effectiveness and efficiency improvement achievable by Strongly Subsuming Higher Order Mutants (SSHOMs), constructed from 15,792 first order mutants in four Java programs. Using SSHOMs in place of the first order mutants they subsume yielded a 35%-45% reduction in the number of mutants required, while simultaneously improving test efficiency by 15% and effectiveness by between 5.6% and 12%. Trivial first order faults often combine to form exceptionally non-trivial higher order faults; apparently innocuous angels can combine to breed monsters. Nevertheless, these same monsters can be recruited to improve automated test effectiveness and efficiency.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {397–408},
numpages = {12},
keywords = {mutation testing, higher order mutants},
location = {Vasteras, Sweden},
series = {ASE '14}
}

@inbook{10.1145/3238147.3238213,
author = {Habib, Andrew and Pradel, Michael},
title = {How Many of All Bugs Do We Find? A Study of Static Bug Detectors},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238213},
abstract = {Static bug detectors are becoming increasingly popular and are widely used by professional software developers. While most work on bug detectors focuses on whether they find bugs at all, and on how many false positives they report in addition to legitimate warnings, the inverse question is often neglected: How many of all real-world bugs do static bug detectors find? This paper addresses this question by studying the results of applying three widely used static bug detectors to an extended version of the Defects4J dataset that consists of 15 Java projects with 594 known bugs. To decide which of these bugs the tools detect, we use a novel methodology that combines an automatic analysis of warnings and bugs with a manual validation of each candidate of a detected bug. The results of the study show that: (i) static bug detectors find a non-negligible amount of all bugs, (ii) different tools are mostly complementary to each other, and (iii) current bug detectors miss the large majority of the studied bugs. A detailed analysis of bugs missed by the static detectors shows that some bugs could have been found by variants of the existing detectors, while others are domain-specific problems that do not match any existing bug pattern. These findings help potential users of such tools to assess their utility, motivate and outline directions for future work on static bug detection, and provide a basis for future comparisons of static bug detection with other bug finding techniques, such as manual and automated testing.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {317–328},
numpages = {12}
}

@article{10.1145/1592434.1592439,
author = {Hoare, C.A.R. and Misra, Jayadev and Leavens, Gary T. and Shankar, Natarajan},
title = {The Verified Software Initiative: A Manifesto},
year = {2009},
issue_date = {October 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/1592434.1592439},
doi = {10.1145/1592434.1592439},
journal = {ACM Comput. Surv.},
month = {oct},
articleno = {22},
numpages = {8}
}

@inproceedings{10.1145/3377811.3380388,
author = {W\"{u}stholz, Valentin and Christakis, Maria},
title = {Targeted Greybox Fuzzing with Static Lookahead Analysis},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380388},
doi = {10.1145/3377811.3380388},
abstract = {Automatic test generation typically aims to generate inputs that explore new paths in the program under test in order to find bugs. Existing work has, therefore, focused on guiding the exploration toward program parts that are more likely to contain bugs by using an offline static analysis.In this paper, we introduce a novel technique for targeted greybox fuzzing using an online static analysis that guides the fuzzer toward a set of target locations, for instance, located in recently modified parts of the program. This is achieved by first semantically analyzing each program path that is explored by an input in the fuzzer's test suite. The results of this analysis are then used to control the fuzzer's specialized power schedule, which determines how often to fuzz inputs from the test suite. We implemented our technique by extending a state-of-the-art, industrial fuzzer for Ethereum smart contracts and evaluate its effectiveness on 27 real-world benchmarks. Using an online analysis is particularly suitable for the domain of smart contracts since it does not require any code instrumentation---adding instrumentation to contracts changes their semantics. Our experiments show that targeted fuzzing significantly outperforms standard greybox fuzzing for reaching 83% of the challenging target locations (up to 14x of median speed-up).},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {789–800},
numpages = {12},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/581339.581349,
author = {Dunsmore, Alastair and Roper, Marc and Wood, Murray},
title = {Further Investigations into the Development and Evaluation of Reading Techniques for Object-Oriented Code Inspection},
year = {2002},
isbn = {158113472X},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/581339.581349},
doi = {10.1145/581339.581349},
abstract = {This paper describes the development and experimental evaluation of a rigorous approach for effective object-oriented (OO) code inspection. Since their development, inspections have been shown to be powerful defect detection strategies but little research has been done to investigate their application to OO systems, which have very different structural and execution models compared to procedural systems. Previous investigations have demonstrated that the delocalised nature of OO software - the resolution of frequent non-local references, and the incongruous relationship between its static and dynamic representations, are primary inhibitors to its effective inspection. The experiment investigates a set of three complementary code reading techniques devised specifically to address these problems: one based on a checklist adapted to address the identified problems of OO inspections, one focused on the systematic construction of abstract specifications, and the last centered on the dynamic slice that a use-case takes through a system. The analysis shows that there is a significant difference in the number of defects found between the three reading techniques. The checklist-based technique emerges as the most effective approach but the other techniques also have noticeable strengths and so for the best results in a practical situation a combination of techniques is recommended.},
booktitle = {Proceedings of the 24th International Conference on Software Engineering},
pages = {47–57},
numpages = {11},
location = {Orlando, Florida},
series = {ICSE '02}
}

@inproceedings{10.1145/2664243.2664250,
author = {Jing, Yiming and Zhao, Ziming and Ahn, Gail-Joon and Hu, Hongxin},
title = {Morpheus: Automatically Generating Heuristics to Detect Android Emulators},
year = {2014},
isbn = {9781450330053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2664243.2664250},
doi = {10.1145/2664243.2664250},
abstract = {Emulator-based dynamic analysis has been widely deployed in Android application stores. While it has been proven effective in vetting applications on a large scale, it can be detected and evaded by recent Android malware strains that carry detection heuristics. Using such heuristics, an application can check the presence or contents of certain artifacts and infer the presence of emulators. However, there exists little work that systematically discovers those heuristics that would be eventually helpful to prevent malicious applications from bypassing emulator-based analysis. To cope with this challenge, we propose a framework called Morpheus that automatically generates such heuristics. Morpheus leverages our insight that an effective detection heuristic must exploit discrepancies observable by an application. To this end, Morpheus analyzes the application sandbox and retrieves observable artifacts from both Android emulators and real devices. Afterwards, Morpheus further analyzes the retrieved artifacts to extract and rank detection heuristics. The evaluation of our proof-of-concept implementation of Morpheus reveals more than 10,000 novel detection heuristics that can be utilized to detect existing emulator-based malware analysis tools. We also discuss the discrepancies in Android emulators and potential countermeasures.},
booktitle = {Proceedings of the 30th Annual Computer Security Applications Conference},
pages = {216–225},
numpages = {10},
keywords = {malware, emulator, Android},
location = {New Orleans, Louisiana, USA},
series = {ACSAC '14}
}

@inbook{10.1145/3372224.3380897,
author = {Li, Mingliang and Lin, Hao and Liu, Cai and Li, Zhenhua and Qian, Feng and Liu, Yunhao and Sun, Nian and Xu, Tianyin},
title = {Experience: Aging or Glitching? Why Does Android Stop Responding and What Can We Do about It?},
year = {2020},
isbn = {9781450370851},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3372224.3380897},
abstract = {Almost every Android user has unsatisfying experiences regarding responsiveness, in particular Application Not Responding (ANR) and System Not Responding (SNR) that directly disrupt user experience. Unfortunately, the community have limited understanding of the prevalence, characteristics, and root causes of unresponsiveness. In this paper, we make an in-depth study of ANR and SNR at scale based on fine-grained system-level traces crowdsourced from 30,000 Android systems. We find that ANR and SNR occur prevalently on all the studied 15 hardware models, and better hardware does not seem to relieve the problem. Moreover, as Android evolves from version 7.0 to 9.0, there are fewer ANR events but more SNR events. Most importantly, we uncover multifold root causes of ANR and SNR and pinpoint the largest inefficiency which roots in Android's flawed implementation of Write Amplification Mitigation (WAM). We design a practical approach to eliminating this largest root cause; after large-scale deployment, it reduces almost all (&gt;99%) ANR and SNR caused by WAM while only decreasing 3% of the data write speed. In addition, we document important lessons we have learned from this study, and have also released our measurement code/data to the research community.},
booktitle = {Proceedings of the 26th Annual International Conference on Mobile Computing and Networking},
articleno = {20},
numpages = {11}
}

@inproceedings{10.1145/170657.170752,
author = {Leif, Robert C. and Sara, Jason and Burgess, Ian and Kelly, Michael and Leif, Suzanne B. and Daly, Theresa},
title = {The Development of Software in the Ada Language for a Mid-Range Hematology Analyzer},
year = {1993},
isbn = {0897916212},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/170657.170752},
doi = {10.1145/170657.170752},
booktitle = {Proceedings of the Conference on TRI-Ada '93},
pages = {340–346},
numpages = {7},
location = {Seattle, Washington, USA},
series = {TRI-Ada '93}
}

@inproceedings{10.1145/2559636.2559841,
author = {Lier, Florian and L\"{u}tkebohle, Ingo and Wachsmuth, Sven},
title = {Towards Automated Execution and Evaluation of Simulated Prototype HRI Experiments},
year = {2014},
isbn = {9781450326582},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2559636.2559841},
doi = {10.1145/2559636.2559841},
abstract = {Autonomous robots are highly relevant targets for interaction studies, but can exhibit behavioral variability that confounds experimental validity. Currently, testing on real systems is the only means to prevent this, but remains very labour-intensive and often happens too late. To improve this situation, we are working towards early testing by means of partial simulation, with automated assessment, and based upon continuous software integration to prevent regressions. We will introduce the concept and describe a proof-of-concept that demonstrates fast feedback and coherent experiment results across repeated trials.},
booktitle = {Proceedings of the 2014 ACM/IEEE International Conference on Human-Robot Interaction},
pages = {230–231},
numpages = {2},
keywords = {human-robot interaction, simulation, continuous integration, system evaluation, testing},
location = {Bielefeld, Germany},
series = {HRI '14}
}

@inproceedings{10.1145/1370256.1370279,
author = {Roy, Chanchal K. and Cordy, James R.},
title = {Towards a Mutation-Based Automatic Framework for Evaluating Code Clone Detection Tools},
year = {2008},
isbn = {9781605581019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370256.1370279},
doi = {10.1145/1370256.1370279},
abstract = {In the last decade, a great many code clone detection tools have been proposed. Such a large number of tools calls for a quantitative comparison, and there have been several attempts to empirically evaluate and compare many of the state-of-the-art tools. However, a recent study shows that there are several factors that could influence the the validity of the results of such comparisons. In order to overcome the effects of such factors (at least in part), in this student poster paper we outline a mutation-based controlled frame-work for evaluating clone detection tools using edit-based mutation operators that model cloning actions. While the framework is not yet completely implemented and as yet we do not have experimental data, we anticipate that such a framework will provide a useful contribution to the community by providing a more solid objective foundation for tool evaluation.},
booktitle = {Proceedings of the 2008 C3S2E Conference},
pages = {137–140},
numpages = {4},
keywords = {framework, clone detection techniques, maintenance, evaluation, mutation analysis, software engineering},
location = {Montreal, Quebec, Canada},
series = {C3S2E '08}
}

@inproceedings{10.1145/2858930.2858936,
author = {Saeed, Ahmed and Ahmadinia, Ali and Just, Mike},
title = {Tag-Protector: An Effective and Dynamic Detection of Out-of-Bound Memory Accesses},
year = {2016},
isbn = {9781450340656},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858930.2858936},
doi = {10.1145/2858930.2858936},
abstract = {Programming languages permitting immediate memory accesses through pointers often result in applications having memory-related errors, which may lead to unpredictable failures and security vulnerabilities. A light-weight solution is presented in this paper to tackle such illegal memory accesses dynamically in C/C++ based applications. We propose a new and effective method of instrumenting an application's source code at compile time in order to detect out-of-bound memory accesses. It is based on creating tags, to be coupled with each memory allocation and then placing additional tag checking instructions for each access made to the memory. The proposed solution is evaluated by instrumenting applications from the BugBench benchmark suite and publicly available benchmark software, Runtime Intrusion Prevention Evaluator (RIPE), detecting all the bugs successfully. The performance and memory overhead is further analysed by instrumenting and executing real world applications.},
booktitle = {Proceedings of the Third Workshop on Cryptography and Security in Computing Systems},
pages = {31–36},
numpages = {6},
keywords = {Compile-time code instrumentation, illegal memory accesses, buffer overflows},
location = {Prague, Czech Republic},
series = {CS2 '16}
}

@inproceedings{10.1145/2950290.2950347,
author = {Kim, Chung Hwan and Rhee, Junghwan and Lee, Kyu Hyung and Zhang, Xiangyu and Xu, Dongyan},
title = {PerfGuard: Binary-Centric Application Performance Monitoring in Production Environments},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950347},
doi = {10.1145/2950290.2950347},
abstract = { Diagnosis of performance problems is an essential part of software development and maintenance. This is in particular a challenging problem to be solved in the production environment where only program binaries are available with limited or zero knowledge of the source code. This problem is compounded by the integration with a significant number of third-party software in most large-scale applications. Existing approaches either require source code to embed manually constructed logic to identify performance problems or support a limited scope of applications with prior manual analysis. This paper proposes an automated approach to analyze application binaries and instrument the binary code transparently to inject and apply performance assertions on application transactions. Our evaluation with a set of large-scale application binaries without access to source code discovered 10 publicly known real world performance bugs automatically and shows that PerfGuard introduces very low overhead (less than 3% on Apache and MySQL server) to production systems. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {595–606},
numpages = {12},
keywords = {Performance diagnosis, post-development testing},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3190508.3190552,
author = {Li, Jiaxin and Chen, Yuxi and Liu, Haopeng and Lu, Shan and Zhang, Yiming and Gunawi, Haryadi S. and Gu, Xiaohui and Lu, Xicheng and Li, Dongsheng},
title = {Pcatch: Automatically Detecting Performance Cascading Bugs in Cloud Systems},
year = {2018},
isbn = {9781450355841},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190508.3190552},
doi = {10.1145/3190508.3190552},
abstract = {Distributed systems have become the backbone of modern clouds. Users often expect high scalability and performance isolation from distributed systems. Unfortunately, a type of poor software design, which we refer to as performance cascading bugs (PCbugs), can often cause the slowdown of non-scalable code in one job to propagate, causing global performance degradation and even threatening system availability.This paper presents a tool, PCatch, that can automatically predict PCbugs by analyzing system execution under small-scale workloads. PCatch contains three key components in predicting PCbugs. It uses program analysis to identify code regions whose execution time can potentially increase dramatically with the workload size; it adapts the traditional happens-before model to reason about software resource contention and performance dependency relationship; it uses dynamic tracking to identify whether the slowdown propagation is contained in one job or not. Our evaluation using representative distributed systems, Cassandra, Hadoop MapReduce, HBase, and HDFS, shows that PCatch can accurately predict PCbugs based on small-scale workload execution.},
booktitle = {Proceedings of the Thirteenth EuroSys Conference},
articleno = {7},
numpages = {14},
keywords = {cloud computing, performance bugs, bug detection, distributed systems, cascading problems},
location = {Porto, Portugal},
series = {EuroSys '18}
}

@article{10.1145/2876441,
author = {Alimadadi, Saba and Sequeira, Sheldon and Mesbah, Ali and Pattabiraman, Karthik},
title = {Understanding JavaScript Event-Based Interactions with Clematis},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2876441},
doi = {10.1145/2876441},
abstract = {Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance tasks. The challenges mainly stem from the dynamic, event-driven, and asynchronous nature of the JavaScript language. We propose a generic technique for capturing low-level event-based interactions in a web application and mapping those to a higher-level behavioral model. This model is then transformed into an interactive visualization, representing episodes of triggered causal and temporal events, related JavaScript code executions, and their impact on the dynamic DOM state. Our approach, implemented in a tool called Clematis, allows developers to easily understand the complex dynamic behavior of their application at three different semantic levels of granularity. Furthermore, Clematis helps developers bridge the gap between test cases and program code by localizing the fault related to a test assertion. The results of our industrial controlled experiment show that Clematis is capable of improving the comprehension task accuracy by 157% while reducing the task completion time by 47%. A follow-up experiment reveals that Clematis improves the fault localization accuracy of developers by a factor of two.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {12},
numpages = {38},
keywords = {event-based interactions, web applications, Program comprehension, JavaScript, fault localization}
}

