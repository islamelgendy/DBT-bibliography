@inbook{10.1145/3238147.3238202,
author = {Ma, Lei and Juefei-Xu, Felix and Zhang, Fuyuan and Sun, Jiyuan and Xue, Minhui and Li, Bo and Chen, Chunyang and Su, Ting and Li, Li and Liu, Yang and Zhao, Jianjun and Wang, Yadong},
title = {DeepGauge: Multi-Granularity Testing Criteria for Deep Learning Systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238202},
abstract = {Deep learning (DL) defines a new data-driven programming paradigm that constructs the internal system logic of a crafted neuron network through a set of training data. We have seen wide adoption of DL in many safety-critical scenarios. However, a plethora of studies have shown that the state-of-the-art DL systems suffer from various vulnerabilities which can lead to severe consequences when applied to real-world applications. Currently, the testing adequacy of a DL system is usually measured by the accuracy of test data. Considering the limitation of accessible high quality test data, good accuracy performance on test data can hardly provide confidence to the testing adequacy and generality of DL systems. Unlike traditional software systems that have clear and controllable logic and functionality, the lack of interpretability in a DL system makes system analysis and defect detection difficult, which could potentially hinder its real-world deployment. In this paper, we propose DeepGauge, a set of multi-granularity testing criteria for DL systems, which aims at rendering a multi-faceted portrayal of the testbed. The in-depth evaluation of our proposed testing criteria is demonstrated on two well-known datasets, five DL systems, and with four state-of-the-art adversarial attack techniques against DL. The potential usefulness of DeepGauge sheds light on the construction of more generic and robust DL systems.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {120–131},
numpages = {12}
}

@inproceedings{10.1145/3092703.3098230,
author = {Casalnuovo, Casey and Suchak, Yagnik and Ray, Baishakhi and Rubio-Gonz\'{a}lez, Cindy},
title = {GitcProc: A Tool for Processing and Classifying GitHub Commits},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098230},
doi = {10.1145/3092703.3098230},
abstract = { Sites such as GitHub have created a vast collection of software artifacts that researchers interested in understanding and improving software systems can use. Current tools for processing such GitHub data tend to target project metadata and avoid source code processing, or process source code in a manner that requires significant effort for each language supported. This paper presents GitcProc, a lightweight tool based on regular expressions and source code blocks, which downloads projects and extracts their project history, including fine-grained source code information and development time bug fixes. GitcProc can track changes to both single-line and block source code structures and associate these changes to the surrounding function context with minimal set up required from users. We demonstrate GitcProc's ability to capture changes in multiple languages by evaluating it on C, C++, Java, and Python projects, and show it finds bug fixes and the context of source code changes effectively with few false positives. },
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {396–399},
numpages = {4},
keywords = {Information Extraction, Language Independence, Git Mining Tool},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@article{10.1145/3417330,
author = {Ma, Wei and Papadakis, Mike and Tsakmalis, Anestis and Cordy, Maxime and Traon, Yves Le},
title = {Test Selection for Deep Learning Systems},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3417330},
doi = {10.1145/3417330},
abstract = {Testing of deep learning models is challenging due to the excessive number and complexity of the computations involved. As a result, test data selection is performed manually and in an ad hoc way. This raises the question of how we can automatically select candidate data to test deep learning models. Recent research has focused on defining metrics to measure the thoroughness of a test suite and to rely on such metrics to guide the generation of new tests. However, the problem of selecting/prioritising test inputs (e.g., to be labelled manually by humans) remains open. In this article, we perform an in-depth empirical comparison of a set of test selection metrics based on the notion of model uncertainty (model confidence on specific inputs). Intuitively, the more uncertain we are about a candidate sample, the more likely it is that this sample triggers a misclassification. Similarly, we hypothesise that the samples for which we are the most uncertain are the most informative and should be used in priority to improve the model by retraining. We evaluate these metrics on five models and three widely used image classification problems involving real and artificial (adversarial) data produced by five generation algorithms. We show that uncertainty-based metrics have a strong ability to identify misclassified inputs, being three times stronger than surprise adequacy and outperforming coverage-related metrics. We also show that these metrics lead to faster improvement in classification accuracy during retraining: up to two times faster than random selection and other state-of-the-art metrics on all models we considered.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
articleno = {13},
numpages = {22},
keywords = {Deep learning testing, software testing, software engineering}
}

@inproceedings{10.1145/3425174.3425209,
author = {da Silva, Henrique Neves and Endo, Andre Takeshi and Eler, Marcelo Medeiros and Vergilio, Silvia Regina and Durelli, Vinicius H. S.},
title = {On the Relation between Code Elements and Accessibility Issues in Android Apps},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425209},
doi = {10.1145/3425174.3425209},
abstract = {Mobile apps have gone mainstream and become part of our daily lives. Currently, many efforts have been made to make apps more accessible to people with disabilities. However, little is still known on how to implement more accessible apps. In the Android API, there are (code) elements that may be employed to (in)directly improve the app's accessibility. This paper aims to investigate the prevalence of accessibility code elements and their relation to potential accessibility issues. First, we identified code elements of the native Android API that may be related to accessibility features, and mapped them to principles and success criteria of the Web Content Accessibility Guidelines (WCAG) 2.1. Using a sample of 111 open source mobile apps available in Google Play, we conducted a characterization study to examine the prevalence of accessibility code elements. We also analyzed how these code elements are related to issues detected by the static analyzer Android Lint and the accessibility testing tool MATE. Our results indicate that code elements are not widely used; the ones directly related to accessibility are present in only a few apps. Additionally, our results would seem to suggest that apps that adopt accessibility code elements, tend to have less accessibility issues. By analyzing our results from the standpoint of the WCAG principles, we conclude that there is room for improvement in terms of how both the Android API and automated testing tools deal with accessibility-related issues.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {40–49},
numpages = {10},
keywords = {Mobile apps. Accessibility},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{10.1007/978-3-642-30598-6_12,
author = {Poon, Pak-Lok and Chen, Tsong Yueh and Tse, T. H.},
title = {Choices, Choices: Comparing between CHOC'LATE and the Classification-Tree Methodology},
year = {2012},
isbn = {9783642305979},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-30598-6_12},
doi = {10.1007/978-3-642-30598-6_12},
abstract = {Two popular specification-based test case generation methods are the choice relation framework and the classification-tree methodology. Both of them come with associated tools and have been used in different applications with success. Since both methods are based on the idea of partition testing, they are similar in many aspects. Because of their similarities, software testers often find it difficult to decide which method to be used in a given testing scenario. This paper aims to provide a solution by first contrasting the strengths and weaknesses of both methods, followed by suggesting practical selection guidelines to cater for different testing scenarios.},
booktitle = {Proceedings of the 17th Ada-Europe International Conference on Reliable Software Technologies},
pages = {162–176},
numpages = {15},
keywords = {software testing, choice relation framework, classification-tree methodology},
location = {Stockholm, Sweden},
series = {Ada-Europe'12}
}

@inbook{10.1145/3293882.3330562,
author = {Lee, Sungho and Ryu, Sukyoung},
title = {Adlib: Analyzer for Mobile Ad Platform Libraries},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330562},
abstract = {Mobile advertising has become a popular advertising approach by taking advantage of various information from mobile devices and rich interaction with users. Mobile advertising platforms show advertisements of nearby restaurants to users using the geographic locations of their mobile devices, and also allow users to make reservations easily using their phone numbers. However, at the same time, they may open the doors for advertisements to steal device information or to perform malicious behaviors. When application developers integrate mobile advertising platform SDKs (AdSDKs) to their applications, they are informed of only the permissions required by the AdSDKs, and they may not be aware of the rich functionalities of the SDKs that are available to advertisements.  In this paper, we first report that various AdSDKs provide powerful functionalities to advertisements, which are seriously vulnerable to security threats. We present representative malicious behaviors by advertisements using APIs provided by AdSDKs. To mitigate the security vulnerability, we develop a static analyzer, Adlib, which analyzes Android Java libraries that use hybrid features to enable communication with JavaScript code and detects possible flows from the APIs that are accessible from third-party advertisements to device-specific features like geographic locations. Our evaluation shows that Adlib found genuine security vulnerabilities from real-world AdSDKs.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {262–272},
numpages = {11}
}

@inproceedings{10.1145/2338965.2336754,
author = {Thies, Andreas and Bodden, Eric},
title = {RefaFlex: Safer Refactorings for Reflective Java Programs},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336754},
doi = {10.1145/2338965.2336754},
abstract = { If programs access types and members through reflection, refactoring tools cannot guarantee that refactorings on those programs are behavior preserving. Refactoring approaches for highly reflective languages like Smalltalk therefore check behavior preservation using regression testing.  In this paper we propose RefaFlex, a novel and more defensive approach towards the refactoring of reflective (Java) programs. RefaFlex uses a dynamic program analysis to log reflective calls during test runs and then uses this information to proactively prevent the programmer from executing refactorings that could otherwise alter the program's behavior. This makes re-running test cases obsolete: when a refactoring is permitted, tests passing originally are guaranteed to pass for the refactored program as well. In some cases, we further re-write reflective calls, permitting refactorings that would otherwise have to be rejected.  We have implemented RefaFlex as an open source Eclipse plugin and offer extensions for six Eclipse refactoring tools addressing naming, typing, and accessibility issues. Our evaluation with 21,524 refactoring runs on three open source programs shows that our tool successfully prevents 1,358 non-behaviour-preserving refactorings which the plain Eclipse refactorings would have incorrectly permitted. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {1–11},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/3180155.3180192,
author = {Spadini, Davide and Aniche, Maur\'{\i}cio and Storey, Margaret-Anne and Bruntink, Magiel and Bacchelli, Alberto},
title = {When Testing Meets Code Review: Why and How Developers Review Tests},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180192},
doi = {10.1145/3180155.3180192},
abstract = {Automated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {677–687},
numpages = {11},
keywords = {gerrit, code review, software testing, automated testing},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3194718.3194727,
author = {Prasetya, I. S. W. B.},
title = {T3 @SBST2018 Benchmark, and How Much We Can Get from Asemantical Testing},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194727},
doi = {10.1145/3194718.3194727},
abstract = {This paper discusses the performance of the automated testing tool for Java called T3 and compares it with few other tools and human written tests in a benchmark set by the Java Unit Testing Tool Contest 2018. Since all the compared tools rely on randomization when generating their test data, albeit to different degrees and with different heuristics, this paper also tries to give some insight on just how far we can go without having to reconstruct the precise semantic of the programs under test in order to test them.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {30–33},
numpages = {4},
keywords = {automated unit testing Java, unit testing, fuzzing, random testing, automated testing},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@inproceedings{10.1145/2610384.2610394,
author = {Milea, Narcisa Andreea and Jiang, Lingxiao and Khoo, Siau-Cheng},
title = {Scalable Detection of Missed Cross-Function Refactorings},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610394},
doi = {10.1145/2610384.2610394},
abstract = { Refactoring is an important way to improve the design of existing code. Identifying refactoring opportunities (i.e., code fragments that can be refactored) in large code bases is a challenging task. In this paper, we propose a novel, automated and scalable technique for identifying cross-function refactoring opportunities that span more than one function (e.g., Extract Method and Inline Method). The key of our technique is the design of efficient vector inlining operations that emulate the effect of method inlining among code fragments, so that the problem of identifying cross-function refactoring can be reduced to the problem of finding similar vectors before and after inlining. We have implemented our technique in a prototype tool named ReDex which encodes Java programs to particular vectors. We have applied the tool to a large code base, 4.5 million lines of code, comprising of 200 bundle projects in the Eclipse ecosystem (e.g., Eclipse JDT, Eclipse PDE, Apache Commons, Hamcrest, etc.). Also, different from many other studies on detecting refactoring, ReDex only searches for code fragments that can be, but have not yet been, refactored in a way similar to some refactoring that happened in the code base. Our results show that ReDex can find 277 cross-function refactoring opportunities in 2 minutes, and 223 cases were labelled as true opportunities by users, and cover many categories of cross-function refactoring operations in classical refactoring books, such as Self Encapsulate Field, Decompose Conditional Expression, Hide Delegate, Preserve Whole Object, etc. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {138–148},
numpages = {11},
keywords = {Vector-based representation, Refactoring, Software Evolution},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@article{10.1145/3057269,
author = {Kazmi, Rafaqut and Jawawi, Dayang N. A. and Mohamad, Radziah and Ghani, Imran},
title = {Effective Regression Test Case Selection: A Systematic Literature Review},
year = {2017},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3057269},
doi = {10.1145/3057269},
abstract = {Regression test case selection techniques attempt to increase the testing effectiveness based on the measurement capabilities, such as cost, coverage, and fault detection. This systematic literature review presents state-of-the-art research in effective regression test case selection techniques. We examined 47 empirical studies published between 2007 and 2015. The selected studies are categorized according to the selection procedure, empirical study design, and adequacy criteria with respect to their effectiveness measurement capability and methods used to measure the validity of these results.The results showed that mining and learning-based regression test case selection was reported in 39% of the studies, unit level testing was reported in 18% of the studies, and object-oriented environment (Java) was used in 26% of the studies. Structural faults, the most common target, was used in 55% of the studies. Overall, only 39% of the studies conducted followed experimental guidelines and are reproducible.There are 7 different cost measures, 13 different coverage types, and 5 fault-detection metrics reported in these studies. It is also observed that 70% of the studies being analyzed used cost as the effectiveness measure compared to 31% that used fault-detection capability and 16% that used coverage.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {29},
numpages = {32},
keywords = {Software testing, coverage, cost effectiveness, SLR, fault detection ability}
}

@inproceedings{10.1145/1368088.1368099,
author = {Ciupa, Ilinca and Leitner, Andreas and Oriol, Manuel and Meyer, Bertrand},
title = {ARTOO: Adaptive Random Testing for Object-Oriented Software},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368099},
doi = {10.1145/1368088.1368099},
abstract = {Intuition is often not a good guide to know which testing strategies will work best. There is no substitute for experimental analysis based on objective criteria: how many faults a strategy finds, and how fast. "Random" testing is an example of an idea that intuitively seems simplistic or even dumb, but when assessed through such criteria can yield better results than seemingly smarter strategies. The efficiency of random testing is improved if the generated inputs are evenly spread across the input domain. This is the idea of Adaptive Random Testing (ART).ART was initially proposed for numerical inputs, on which a notion of distance is immediately available. To extend the ideas to the testing of object-oriented software, we have developed a notion of distance between objects and a new testing strategy called ARTOO, which selects as inputs objects that have the highest average distance to those already used as test inputs. ARTOO has been implemented as part of a tool for automated testing of object-oriented software.We present the ARTOO concepts, their implementation, and a set of experimental results of its application. Analysis of the results shows in particular that, compared to a directed random strategy, ARTOO reduces the number of tests generated until the first fault is found, in some cases by as much as two orders of magnitude. ARTOO also uncovers faults that the random strategy does not find in the time allotted, and its performance is more predictable.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {71–80},
numpages = {10},
keywords = {object distance, software testing, adaptive random testing},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/2771783.2784768,
author = {Bell, Jonathan and Kaiser, Gail},
title = {Dynamic Taint Tracking for Java with Phosphor (Demo)},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2784768},
doi = {10.1145/2771783.2784768},
abstract = { Dynamic taint tracking is an information flow analysis that can be applied to many areas of testing. Phosphor is the first portable, accurate and performant dynamic taint tracking system for Java. While previous systems for performing general-purpose taint tracking in the JVM required specialized research JVMs, Phosphor works with standard off-the-shelf JVMs (such as Oracle's HotSpot and OpenJDK's IcedTea). Phosphor also differs from previous portable JVM taint tracking systems that were not general purpose (e.g. tracked only tags on Strings and no other type), in that it tracks tags on all variables. We have also made several enhancements to Phosphor, to track taint tags through control flow (in addition to data flow), as well as to track an arbitrary number of relationships between taint tags (rather than be limited to only 32 tags). In this demonstration, we show how developers writing testing tools can benefit from Phosphor, and explain briefly how to interact with it. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {409–413},
numpages = {5},
keywords = {Taint Tracking, Dataflow Analysis},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inbook{10.1145/3468264.3468590,
author = {Wang, Xiao and Xiao, Lu and Yu, Tingting and Woepse, Anne and Wong, Sunny},
title = {An Automatic Refactoring Framework for Replacing Test-Production Inheritance by Mocking Mechanism},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468590},
abstract = {Unit testing focuses on verifying the functions of individual units of a software system. It is challenging due to the high inter-dependencies among software units. Developers address this by mocking-replacing the dependency by a "faked" object. Despite the existence of powerful, dedicated mocking frameworks, developers often turn to a "hand-rolled" approach-inheritance. That is, they create a subclass of the dependent class and mock its behavior through method overriding. However, this requires tedious implementation and compromises the design quality of unit tests. This work contributes a fully automated refactoring framework to identify and replace the usage of inheritance by using Mockito-a well received mocking framework. Our approach is built upon the empirical experience from five open source projects that use inheritance for mocking. We evaluate our approach on four other projects. Results show that our framework is efficient, generally applicable to new datasets, mostly preserves test case behaviors in detecting defects (in the form of mutants), and decouples test code from production code. The qualitative evaluation by experienced developers suggests that the auto-refactoring solutions generated by our framework improve the quality of the unit test cases in various aspects, such as making test conditions more explicit, as well as improved cohesion, readability, understandability, and maintainability with test cases.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {540–552},
numpages = {13}
}

@inproceedings{10.1145/2610384.2610392,
author = {Zhang, Chaoqiang and Groce, Alex and Alipour, Mohammad Amin},
title = {Using Test Case Reduction and Prioritization to Improve Symbolic Execution},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610392},
doi = {10.1145/2610384.2610392},
abstract = { Scaling symbolic execution to large programs or programs with complex inputs remains difficult due to path explosion and complex constraints, as well as external method calls. Additionally, creating an effective test structure with symbolic inputs can be difficult. A popular symbolic execution strategy in practice is to perform symbolic execution not “from scratch” but based on existing test cases. This paper proposes that the effectiveness of this approach to symbolic execution can be enhanced by (1) reducing the size of seed test cases and (2) prioritizing seed test cases to maximize exploration efficiency. The proposed test case reduction strategy is based on a recently introduced generalization of delta debugging, and our prioritization techniques include novel methods that, for this purpose, can outperform some traditional regression testing algorithms. We show that applying these methods can significantly improve the effectiveness of symbolic execution based on existing test cases. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {160–170},
numpages = {11},
keywords = {Test case reduction, Symbolic execution, Test prioritization},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/1572272.1572284,
author = {Polikarpova, Nadia and Ciupa, Ilinca and Meyer, Bertrand},
title = {A Comparative Study of Programmer-Written and Automatically Inferred Contracts},
year = {2009},
isbn = {9781605583389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1572272.1572284},
doi = {10.1145/1572272.1572284},
abstract = {Where do contracts - specification elements embedded in executable code - come from? To produce them, should we rely on the programmers, on automatic tools, or some combination?Recent work, in particular the Daikon system, has shown that it is possible to infer some contracts automatically from program executions. The main incentive has been an assumption that most programmers are reluctant to invent the contracts themselves. The experience of contract-supporting languages, notably Eiffel, disproves that assumption: programmers will include contracts if given the right tools. That experience also shows, however, that the resulting contracts are generally partial and occasionally incorrect.Contract inference tools provide the opportunity for studying objectively the quality of programmer-written contracts, and for assessing the respective roles of humans and tools. Working on 25 classes taken from different sources such as widely-used standard libraries and code written by students, we applied Daikon to infer contracts and compared the results (totaling more than 19500 inferred assertion clauses) with the already present contracts.We found that a contract inference tool can be used to strengthen programmer-written contracts, but cannot infer all contracts that humans write. The tool generates around five times as many relevant assertion clauses as written by programmers; but it only finds around 60% of those originally written by programmers. Around a third of the generated assertions clauses are either incorrect or irrelevant. The study also uncovered interesting correlations between the quality of inferred contracts and some code metrics.},
booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
pages = {93–104},
numpages = {12},
keywords = {dynamic contract inference, eiffel},
location = {Chicago, IL, USA},
series = {ISSTA '09}
}

@inproceedings{10.1145/2970276.2970333,
author = {Patrick, Matthew and Castle, Matthew D. and Stutt, Richard O. J. H. and Gilligan, Christopher A.},
title = {Automatic Test Image Generation Using Procedural Noise},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970333},
doi = {10.1145/2970276.2970333},
abstract = { It is difficult to test programs that input images, due to the large number of (pixel) values that must be chosen and the complex ways these values interact. Typically, such programs are tested manually, using images that have known results. However, this is a laborious process and limited in the range of tests that can be applied. We introduce a new approach for testing programs that input images automatically, using procedural noise and spatial statistics to create inputs that are both realistic and can easily be tuned to have specific properties. The effectiveness of our approach is illustrated on an epidemiological simulation of a recently introduced tree pest in Great Britain: Oriental Chestnut Gall Wasp. Our approach produces images that match the real landscapes more closely than other techniques and can be used (alongside metamorphic relations) to detect smaller (artificially introduced) errors with greater accuracy. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {654–659},
numpages = {6},
keywords = {software testing, test data generation, image processing},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3460319.3464820,
author = {Zhang, Lingfeng and Zhang, Yueling and Zhang, Min},
title = {Efficient White-Box Fairness Testing through Gradient Search},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464820},
doi = {10.1145/3460319.3464820},
abstract = {Deep learning (DL) systems are increasingly deployed for autonomous decision-making in a wide range of applications. Apart from the robustness and safety, fairness is also an important property that a well-designed DL system should have. To evaluate and improve individual fairness of a model, systematic test case generation for identifying individual discriminatory instances in the input space is essential. In this paper, we propose a framework EIDIG for efficiently discovering individual fairness violation. Our technique combines a global generation phase for rapidly generating a set of diverse discriminatory seeds with a local generation phase for generating as many individual discriminatory instances as possible around these seeds under the guidance of the gradient of the model output. In each phase, prior information at successive iterations is fully exploited to accelerate convergence of iterative optimization or reduce frequency of gradient calculation. Our experimental results show that, on average, our approach EIDIG generates 19.11% more individual discriminatory instances with a speedup of 121.49% when compared with the state-of-the-art method and mitigates individual discrimination by 80.03% with a limited accuracy loss after retraining.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {103–114},
numpages = {12},
keywords = {neural networks, fairness testing, test case generation, software bias},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/3377811.3380382,
author = {Wang, Jue and Jiang, Yanyan and Xu, Chang and Cao, Chun and Ma, Xiaoxing and Lu, Jian},
title = {ComboDroid: Generating High-Quality Test Inputs for Android Apps via Use Case Combinations},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380382},
doi = {10.1145/3377811.3380382},
abstract = {Android apps demand high-quality test inputs, whose generation remains an open challenge. Existing techniques fall short on exploring complex app functionalities reachable only by a long, meaningful, and effective test input. Observing that such test inputs can usually be decomposed into relatively independent short use cases, this paper presents ComboDroid, a fundamentally different Android app testing framework. ComboDroid obtains use cases for manifesting a specific app functionality (either manually provided or automatically extracted), and systematically enumerates the combinations of use cases, yielding high-quality test inputs.The evaluation results of ComboDroid on real-world apps are encouraging. Our fully automatic variant outperformed the best existing technique APE by covering 4.6% more code (APE only outperformed Monkey by 2.1%), and revealed four previously unknown bugs in extensively tested subjects. Our semi-automatic variant boosts the manual use cases obtained with little manual labor, achieving a comparable coverage (only 3.2% less) with a white-box human testing expert.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {469–480},
numpages = {12},
keywords = {mobile apps, software testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3338906.3338932,
author = {Zhang, Chengyu and Su, Ting and Yan, Yichen and Zhang, Fuyuan and Pu, Geguang and Su, Zhendong},
title = {Finding and Understanding Bugs in Software Model Checkers},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338932},
doi = {10.1145/3338906.3338932},
abstract = {Software Model Checking (SMC) is a well-known automatic program verification technique and frequently adopted for checking safety-critical software. Thus, the reliability of SMC tools themselves (i.e., software model checkers) is critical. However, little work exists on validating software model checkers, an important problem that this paper tackles by introducing a practical, automated fuzzing technique. For its simplicity and generality, we focus on control-flow reachability (e.g., whether or how many times a branch is reached) and address two specific challenges for effective fuzzing: oracle and scalability. Given a deterministic program, we (1) leverage its concrete executions to synthesize valid branch reachability properties (thus solving the oracle problem) and (2) fuse such individual properties into a single safety property (thus improving the scalability of fuzzing and reducing manual inspection). We have realized our approach as the MCFuzz tool and applied it to extensively test three state-of-the-art C software model checkers, CPAchecker, CBMC, and SeaHorn. MCFuzz has found 62 unique bugs in all three model checkers -- 58 have been confirmed, and 20 have been fixed. We have further analyzed and categorized these bugs (which are diverse), and summarized several lessons for building reliable and robust model checkers. Our testing effort has been well-appreciated by the model checker developers, and also led to improved tool usability and documentation.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {763–773},
numpages = {11},
keywords = {Fuzz Testing, Software Model Checking, Software Testing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

