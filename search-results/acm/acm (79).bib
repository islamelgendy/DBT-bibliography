@inproceedings{10.1145/1810295.1810351,
author = {Zhang, Cheng and Yan, Dacong and Zhao, Jianjun and Chen, Yuting and Yang, Shengqian},
title = {BPGen: An Automated Breakpoint Generator for Debugging},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810351},
doi = {10.1145/1810295.1810351},
abstract = {During debugging processes, breakpoints are frequently used to inspect and understand runtime behaviors of programs. Although most development environments offer convenient breakpoint facilities, the use of these environments usually requires considerable human efforts in order to generate useful breakpoints. Before setting breakpoints or typing breakpoint conditions, developers usually have to make some judgements and hypotheses on the basis of their observations and experience. To reduce this kind of efforts we present a tool, named BPGen, to automatically generate breakpoints for debugging. BPGen uses three well-known dynamic fault localization techniques in tandem to identify suspicious program statements and states, through which both conditional and unconditional breakpoints are generated. BPGen is implemented as an Eclipse plugin for supplementing the existing Eclipse JDT debugger.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {271–274},
numpages = {4},
keywords = {debugging, breakpoint},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@inproceedings{10.1145/2465449.2465469,
author = {Windm\"{u}ller, Stephan and Neubauer, Johannes and Steffen, Bernhard and Howar, Falk and Bauer, Oliver},
title = {Active Continuous Quality Control},
year = {2013},
isbn = {9781450321228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2465449.2465469},
doi = {10.1145/2465449.2465469},
abstract = {We present Active Continuous Quality Control (ACQC), a novel approach that employs incremental active automata learning technology periodically in order to infer evolving behavioral automata of complex applications accompanying the development process. This way we are able to closely monitor and steer the evolution of applications throughout their whole life-cycle with minimum manual effort. Key to this approach is to establish a stable level for comparison via an incrementally growing behavioral abstraction in terms of a user-centric communication alphabet: The letters of this alphabet, which may correspond to whole use cases, are intended to directly express the functionality from the user perspective. At the same time their choice allows one to focus on specific aspects, which establishes tailored abstraction levels on demand, which may be refined by adding new letters in the course of the systems evolution. This way ACQC does not only allow us to reveal serious bugs simply by inspecting difference views of the (tailored) models, but also to visually follow and control the effects of (intended) changes, which complements our model-checking-based quality control. All this will be illustrated along real-life scenarios that arose during the component-based development of a commercial editorial system.},
booktitle = {Proceedings of the 16th International ACM Sigsoft Symposium on Component-Based Software Engineering},
pages = {111–120},
numpages = {10},
keywords = {testing, active learning, validation, model checking},
location = {Vancouver, British Columbia, Canada},
series = {CBSE '13}
}

@inproceedings{10.1145/1460412.1460422,
author = {Cao, Qing and Abdelzaher, Tarek and Stankovic, John and Whitehouse, Kamin and Luo, Liqian},
title = {Declarative Tracepoints: A Programmable and Application Independent Debugging System for Wireless Sensor Networks},
year = {2008},
isbn = {9781595939906},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1460412.1460422},
doi = {10.1145/1460412.1460422},
abstract = {Effective debugging usually involves watching program state to diagnose bugs. When debugging sensor network applications, this approach is often time-consuming and errorprone, not only because of the lack of visibility into system state, but also because of the difficulty to watch the right variables at the right time. In this paper, we present declarative tracepoints, a debugging system that allows the user to insert a group of action-associated checkpoints, or tracepoints, to applications being debugged at runtime. Tracepoints do not require modifying application source code. Instead, they are written in a declarative, SQL-like language called TraceSQL independently. By triggering the associated actions when these checkpoints are reached, this system automates the debugging process by removing the human from the loop. We show that declarative tracepoints are able to express the core functionality of a range of previously isolated debugging techniques, such as EnviroLog, NodeMD, Sympathy, and StackGuard. We describe the design and implementation of the declarative tracepoints system, evaluate its overhead in terms of CPU slowdown, illustrate its expressiveness through the aforementioned debugging techniques, and finally demonstrate that it can be used to detect real bugs using case studies of three bugs based on the development of the LiteOS operating system.},
booktitle = {Proceedings of the 6th ACM Conference on Embedded Network Sensor Systems},
pages = {85–98},
numpages = {14},
keywords = {wireless sensor networks, declarative tracepoints, embedded debugging},
location = {Raleigh, NC, USA},
series = {SenSys '08}
}

@article{10.1145/29934.29937,
author = {Aoyama, M.},
title = {Concurrent Development of Software Systems},
year = {1987},
issue_date = {July 1987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/29934.29937},
doi = {10.1145/29934.29937},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {20–24},
numpages = {5}
}

@inproceedings{10.1145/2896921.2896928,
author = {Hughes, John and Norell, Ulf and Smallbone, Nicholas and Arts, Thomas},
title = {Find More Bugs with QuickCheck!},
year = {2016},
isbn = {9781450341516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896921.2896928},
doi = {10.1145/2896921.2896928},
abstract = {Random testing is increasingly popular and successful, but tends to spend most time rediscovering the "most probable bugs" again and again, reducing the value of long test runs on buggy software. We present a new automated method to adapt random test case generation so that already-discovered bugs are avoided, and further test effort can be devoted to searching for new bugs instead. We evaluate our method primarily against RANDOOP-style testing, in three different settings---our method avoids rediscovering bugs more successfully than RANDOOP and in some cases finds bugs that RANDOOP did not find at all.},
booktitle = {Proceedings of the 11th International Workshop on Automation of Software Test},
pages = {71–77},
numpages = {7},
keywords = {random testing, avoiding bugs, bug slippage},
location = {Austin, Texas},
series = {AST '16}
}

@inproceedings{10.1145/3184407.3184418,
author = {Ayala-Rivera, Vanessa and Kaczmarski, Maciej and Murphy, John and Darisa, Amarendra and Portillo-Dominguez, A. Omar},
title = {One Size Does Not Fit All: In-Test Workload Adaptation for Performance Testing of Enterprise Applications},
year = {2018},
isbn = {9781450350952},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3184407.3184418},
doi = {10.1145/3184407.3184418},
abstract = {The identification of workload-dependent performance issues, as well as their root causes, is a time-consuming and complex process which typically requires several iterations of tests (as this type of issues can depend on the input workloads), and heavily relies on human expert knowledge. To improve this process, this paper presents an automated approach to dynamically adapt the workload (used by a performance testing tool) during the test runs. As a result, the performance issues of the tested application can be revealed more quickly; hence, identifying them with less effort and expertise. Our experimental evaluation has assessed the accuracy of the proposed approach and the time savings that it brings to testers. The results have demonstrated the benefits of the approach by achieving a significant decrease in the time invested in performance testing (without compromising the accuracy of the test results), while introducing a low overhead in the testing environment.},
booktitle = {Proceedings of the 2018 ACM/SPEC International Conference on Performance Engineering},
pages = {211–222},
numpages = {12},
keywords = {testing, automation, workload, performance, analysis},
location = {Berlin, Germany},
series = {ICPE '18}
}

@inproceedings{10.1145/1810295.1810405,
author = {Nagappan, Meiyappan},
title = {Analysis of Execution Log Files},
year = {2010},
isbn = {9781605587196},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1810295.1810405},
doi = {10.1145/1810295.1810405},
abstract = {Log analysis can be used to find problems, define operational profiles, and even pro-actively prevent issues. The goal of my dissertation research is to investigate log management and analysis techniques suited for very large and very complex logs, such as those we might expect in a computational cloud system.},
booktitle = {Proceedings of the 32nd ACM/IEEE International Conference on Software Engineering - Volume 2},
pages = {409–412},
numpages = {4},
keywords = {analysis of textual data, fault isolation, diagnosis, log files},
location = {Cape Town, South Africa},
series = {ICSE '10}
}

@article{10.1145/2815021.2815036,
author = {Bjarnason, Elizabeth and Borg, Markus and Morandini, Mirko and Unterkalmsteiner, Michael and Felderer, Michael and Staats, Matthew},
title = {Summary of 2nd International Workshop on Requirements Engineering and Testing (RET 2015): Co-Located with ICSE 2015},
year = {2015},
issue_date = {September 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {5},
issn = {0163-5948},
url = {https://doi.org/10.1145/2815021.2815036},
doi = {10.1145/2815021.2815036},
abstract = {The RET (Requirements Engineering and Testing) workshop series provides a meeting point for researchers and practitioners from the two separate fields of Requirements Engineering (RE) and Testing. The goal is to improve the connection and alignment of these two areas through an exchange of ideas, challenges, practices, experiences and results. The long term aim is to build a community and a body of knowledge within the intersection of RE and Testing, i.e. RET. The 2nd workshop was held in co-location with ICSE 2015 in Florence, Italy. The workshop continued in the same interactive vein as the 1st one and included a keynote, paper presentations with ample time for discussions, and a group exercise. For true impact and relevance this cross-cutting area requires contribution from both RE and Testing, and from both researchers and practitioners. A range of papers were presented from short experience papers to full research papers that cover connections between the two fields. One of the main outputs of the 2nd workshop was a categorization of the presented workshop papers according to an initial definition of the area of RET which identifies the aspects RE, Testing and coordination effect.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {39–42},
numpages = {4},
keywords = {requirements engineering, testing, alignment}
}

@inproceedings{10.1145/2752489.2752497,
author = {Schroeder, Jan and Berger, Christian and Herpel, Thomas},
title = {Challenges from Integration Testing Using Interconnected Hardware-in-the-Loop Test Rigs at an Automotive OEM: An Industrial Experience Report},
year = {2015},
isbn = {9781450334440},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2752489.2752497},
doi = {10.1145/2752489.2752497},
abstract = {Developing automotive functions involves complex software to a growing extent while still following a consecutive waterfall-like development process: Integrating and testing software with other software and with hardware components is conducted towards the final phases during the development. For example, ambiguous requirements or unclear semantics in system interfaces show up very late and mostly not before integration testing. In this article, we are reporting about results from conducting interviews with integration and test engineers at a large automotive OEM about today's most resource-intense challenges when dealing with software integration testing tasks at interconnected hardware-in-the-loop test rig environments. Challenges in processes, communication, and implementation were mentioned most often as emerging topics to be tackled over and over again during this phase in a vehicle series development project.},
booktitle = {Proceedings of the First International Workshop on Automotive Software Architecture},
pages = {39–42},
numpages = {4},
keywords = {integration testing, software, technical debt, challenges, hardware-in-the-loop, automotive},
location = {Montr\'{e}al, QC, Canada},
series = {WASA '15}
}

@inproceedings{10.1109/CGO.2005.26,
author = {Nagpurkar, Priya and Krintz, Chandra and Sherwood, Timothy},
title = {Phase-Aware Remote Profiling},
year = {2005},
isbn = {076952298X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/CGO.2005.26},
doi = {10.1109/CGO.2005.26},
abstract = {Recent advances in networking and embedded device technology have made the vision of ubiquitous computing a reality; users can access the Internet's vast offerings any-time and anywhere. Moreover, battery-powered devices such as personal digital assistants and web-enabled mobile phones have successfully emerged as new access points to the world's digital infrastructure. This ubiquity offers a new opportunity for software developers: users can now participate in the software development, optimization, and evolution process while they use their software. Such participation requires effective techniques for gathering profile information from remote, resource-constrained devices. Further, these techniques must be unobtrusive and transparent to the user; profiles must be gathered using minimal computation, communication, and power. Toward this end, we present a flexible hardware-software scheme for efficient remote profiling. We rely on the extraction of meta information from executing programs in the form of phases, and then use this information to guide intelligent online sampling and to manage the communication of those samples. Our results indicate that phase-based remote profiling can reduce the communication, computation, and energy consumption overheads by 50-75% over random and periodic sampling.},
booktitle = {Proceedings of the International Symposium on Code Generation and Optimization},
pages = {191–202},
numpages = {12},
series = {CGO '05}
}

@article{10.1145/1127854.1127862,
author = {Neville-Neil, George},
title = {Kode Vicious Bugs Out: What Do You Do When Tools Fail?},
year = {2006},
issue_date = {April 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {3},
issn = {1542-7730},
url = {https://doi.org/10.1145/1127854.1127862},
doi = {10.1145/1127854.1127862},
abstract = {This month Kode Vicious serves up a mixed bag, including tackling the uncertainties of heisenbugs -- a nasty type of bug that’s been known to drive coders certifiably insane. He also gives us his list of must-reads. Are any of your favorites on the list? Read on to find out!},
journal = {Queue},
month = {apr},
pages = {10–12},
numpages = {3}
}

@inproceedings{10.1145/3302424.3303947,
author = {Michael, Ellis and Woos, Doug and Anderson, Thomas and Ernst, Michael D. and Tatlock, Zachary},
title = {Teaching Rigorous Distributed Systems With Efficient Model Checking},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303947},
doi = {10.1145/3302424.3303947},
abstract = {Writing correct distributed systems code is difficult, especially for novice programmers. The inherent asynchrony and need for fault-tolerance make errors almost inevitable. Industrial-strength testing and model checking have been shown to be effective at uncovering bugs, but they come at a cost --- in both time and effort --- that is far beyond what students can afford. To address this, we have developed an efficient model checking framework and visual debugger for distributed systems, with the goal of helping students find and fix bugs in near real-time. We identify two novel techniques for reducing the search state space to more efficiently find bugs in student implementations. We report our experiences using these tools to help over two hundred students build a correct, linearizable, fault-tolerant, dynamically-sharded key--value store.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {32},
numpages = {15},
keywords = {education, model checking, distributed systems},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{10.1109/ASE.2009.85,
author = {Wegrzynowicz, Patrycja and Stencel, Krzysztof},
title = {Towards a Comprehensive Test Suite for Detectors of Design Patterns},
year = {2009},
isbn = {9780769538914},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2009.85},
doi = {10.1109/ASE.2009.85},
abstract = {Detection of design patterns is an important part of reverse engineering. Availability of patterns provides for a better understanding of code and also makes analysis more efficient in terms of time and cost. In recent years, we have observed a continual improvement in the field of automatic detection of design patterns in source code. Existing approaches can detect a fairly broad range of design patterns, targeting both structural and behavioral aspects of patterns. However, it is not straightforward to assess and compare these approaches. There is no common ground on which to evaluate the accuracy of the detection approaches, given the existence of variants and specific code constructs used to implement a design pattern. We propose a systematic approach to constructing a comprehensive test suite for detectors of design patterns. This approach is applied to construct a test suite covering the Singleton pattern. The test suite contains many implementation variants of these patterns, along with such code constructs as method forwarding, access modifiers, and long inheritance paths. Furthermore, we use this test suite to compare three detection tools and to identify their strengths and weaknesses.},
booktitle = {Proceedings of the 2009 IEEE/ACM International Conference on Automated Software Engineering},
pages = {103–110},
numpages = {8},
keywords = {detection, test suite, design patterns},
series = {ASE '09}
}

@inbook{10.1109/ICSE43902.2021.00099,
author = {Weber, Max and Apel, Sven and Siegmund, Norbert},
title = {White-Box Performance-Influence Models: A Profiling and Learning Approach},
year = {2021},
isbn = {9781450390859},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE43902.2021.00099},
abstract = {Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods.We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration- and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering},
pages = {1059–1071},
numpages = {13}
}

@inproceedings{10.5555/2818754.2818787,
author = {Terragni, Valerio and Cheung, Shing-Chi and Zhang, Charles},
title = {RECONTEST: Effective Regression Testing of Concurrent Programs},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Concurrent programs proliferate as multi-core technologies advance. The regression testing of concurrent programs often requires running a failing test for weeks before catching a faulty interleaving, due to the myriad of possible interleavings of memory accesses arising from concurrent program executions. As a result, the conventional approach that selects a sub-set of test cases for regression testing without considering interleavings is insufficient In this paper we present ReConTest to address the problem by selecting the new interleavings that arise due to code changes. These interleavings must be explored in order to uncover regression bugs. ReConTest efficiently selects new interleavings by first identifying shared memory accesses that are affected by the changes, and then exploring only those problematic interleavings that contain at least one of these accesses. We have implemented ReConTest as an automated tool and evaluated it using 13 real-world concurrent program subjects. Our results show that ReConTest can significantly reduce the regression testing cost without missing any faulty interleavings induced by code changes.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {246–256},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/3477132.3483556,
author = {Fu, Xinwei and Kim, Wook-Hee and Shreepathi, Ajay Paddayuru and Ismail, Mohannad and Wadkar, Sunny and Lee, Dongyoon and Min, Changwoo},
title = {Witcher: Systematic Crash Consistency Testing for Non-Volatile Memory Key-Value Stores},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483556},
doi = {10.1145/3477132.3483556},
abstract = {The advent of non-volatile main memory (NVM) enables the development of crash-consistent software without paying storage stack overhead. However, building a correct crash-consistent program remains very challenging in the presence of a volatile cache. This paper presents Witcher, a systematic crash consistency testing framework, which detects both correctness and performance bugs in NVM-based persistent key-value stores and underlying NVM libraries, without test space explosion and without manual annotations or crash consistency checkers. To detect correctness bugs, Witcher automatically infers likely correctness conditions by analyzing data and control dependencies between NVM accesses. Then Witcher validates if any violation of them is a true crash consistency bug by checking output equivalence between executions with and without a crash. Moreover, Witcher detects performance bugs by analyzing the execution traces. Evaluation with 20 NVM key-value stores based on Intel's PMDK library shows that Witcher discovers 47 (36 new) correctness consistency bugs and 158 (113 new) performance bugs in both applications and PMDK.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {100–115},
numpages = {16},
keywords = {Debugging, Testing, Crash Consistency, Non-volatile Memory},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@article{10.1145/3428233,
author = {Zhang, Hengchu and Roth, Edo and Haeberlen, Andreas and Pierce, Benjamin C. and Roth, Aaron},
title = {Testing Differential Privacy with Dual Interpreters},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428233},
doi = {10.1145/3428233},
abstract = {Applying differential privacy at scale requires convenient ways to check that programs computing with sensitive data appropriately preserve privacy. We propose here a fully automated framework for testing differential privacy, adapting a well-known “pointwise” technique from informal proofs of differential privacy. Our framework, called DPCheck, requires no programmer annotations, handles all previously verified or tested algorithms, and is the first fully automated framework to distinguish correct and buggy implementations of PrivTree, a probabilistically terminating algorithm that has not previously been mechanically checked. We analyze the probability of DPCheck mistakenly accepting a non-private program and prove that, theoretically, the probability of false acceptance can be made exponentially small by suitable choice of test size. We demonstrate DPCheck’s utility empirically by implementing all benchmark algorithms from prior work on mechanical verification of differential privacy, plus several others and their incorrect variants, and show DPCheck accepts the correct implementations and rejects the incorrect variants. We also demonstrate how DPCheck can be deployed in a practical workflow to test differentially privacy for the 2020 US Census Disclosure Avoidance System (DAS).},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {165},
numpages = {26},
keywords = {symbolic execution, Differential privacy, testing}
}

@inproceedings{10.1145/3468264.3473920,
author = {Ayerdi, Jon and Terragni, Valerio and Arrieta, Aitor and Tonella, Paolo and Sagardui, Goiuria and Arratibel, Maite},
title = {Generating Metamorphic Relations for Cyber-Physical Systems with Genetic Programming: An Industrial Case Study},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473920},
doi = {10.1145/3468264.3473920},
abstract = {One of the major challenges in the verification of complex industrial Cyber-Physical Systems is the difficulty of determining whether a particular system output or behaviour is correct or not, the so-called test oracle problem. Metamorphic testing alleviates the oracle problem by reasoning on the relations that are expected to hold among multiple executions of the system under test, which are known as Metamorphic Relations (MRs). However, the development of effective MRs is often challenging and requires the involvement of domain experts. In this paper, we present a case study aiming at automating this process. To this end, we implemented GAssertMRs, a tool to automatically generate MRs with genetic programming. We assess the cost-effectiveness of this tool in the context of an industrial case study from the elevation domain. Our experimental results show that in most cases GAssertMRs outperforms the other baselines, including manually generated MRs developed with the help of domain experts. We then describe the lessons learned from our experiments and we outline the future work for the adoption of this technique by industrial practitioners.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1264–1274},
numpages = {11},
keywords = {oracle generation, metamorphic testing, quality of service, mutation testing, evolutionary algorithm, genetic programming, oracle improvement, cyber physical systems},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1145/3408056,
author = {Krishnamurthi, Shriram and Fisler, Kathi},
title = {Data-Centricity: A Challenge and Opportunity for Computing Education},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3408056},
doi = {10.1145/3408056},
abstract = {Rethinking the content of introductory computing around a data-centric approach to better engage and support a diversity of students.},
journal = {Commun. ACM},
month = {jul},
pages = {24–26},
numpages = {3}
}

@inproceedings{10.1145/3053600.3053614,
author = {Kaczmarski, Maciej and Perry, Philip and Murphy, John and Portillo-Dominguez, A. Omar},
title = {In-Test Adaptation of Workload in Enterprise Application Performance Testing},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053614},
doi = {10.1145/3053600.3053614},
abstract = {Performance testing is used to assess if an enterprise application can fulfil its expected Service Level Agreements. However, since some performance issues depend on the input workloads, it is common to use time-consuming and complex iterative test methods, which heavily rely on human expertise. This paper presents an automated approach to dynamically adapt the workload so that issues (e.g. bottlenecks) can be identified more quickly as well as with less effort and expertise. We present promising results from an initial validation prototype indicating an 18-fold decrease in the test time without compromising the accuracy of the test results, while only introducing a marginal overhead in the system.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {69–72},
numpages = {4},
keywords = {engineering, performance, testing, analysis},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

