@inproceedings{10.1145/2060329.2060349,
author = {Puolitaival, Olli-Pekka and Kanstr\'{e}n, Teemu},
title = {Towards Flexible and Efficient Model-Based Testing, Utilizing Domain-Specific Modelling},
year = {2010},
isbn = {9781450305495},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2060329.2060349},
doi = {10.1145/2060329.2060349},
abstract = {Model-Based Testing is a test automation technique that generates test cases based on a model of the system under test. Domain-specific modelling is a modelling approach where the developed system is modelled in terms of domain-specific concepts and these models are automatically transformed to other forms such as application code. In this paper, we will discuss the adoption and integration of domain-specific modelling with model-based testing tools. Since model-based testing tools utilise various modelling notations that typically diverge from a specific domain-model, we will discuss how domain specific models can be automatically transformed to become suitable models for a chosen model-based testing tool. Furthermore, by doing this in terms of a domain-specific meta-model, we will allow one to switch between various model-based testing tools.},
booktitle = {Proceedings of the 10th Workshop on Domain-Specific Modeling},
articleno = {8},
numpages = {6},
keywords = {domain-specific modelling, meta-model, model-based testing},
location = {Reno, Nevada},
series = {DSM '10}
}

@inproceedings{10.1145/3468264.3468625,
author = {Wang, Guancheng and Shen, Ruobing and Chen, Junjie and Xiong, Yingfei and Zhang, Lu},
title = {Probabilistic Delta Debugging},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468625},
doi = {10.1145/3468264.3468625},
abstract = {The delta debugging problem concerns how to reduce an object while preserving a certain property, and widely exists in many applications, such as compiler development, regression fault localization, and software debloating. Given the importance of delta debugging, multiple algorithms have been proposed to solve the delta debugging problem efficiently and effectively. However, the efficiency and effectiveness of the state-of-the-art algorithms are still not satisfactory. For example, the state-of-the-art delta debugging tool, CHISEL, may take up to 3 hours to reduce a single program with 14,092 lines of code, while the reduced program may be up to 2 times unnecessarily large. In this paper, we propose a probabilistic delta debugging algorithm (named ProbDD) to improve the efficiency and the effectiveness of delta debugging. Our key insight is, the ddmin algorithm, the basic algorithm upon which many existing approaches are built, follows a predefined sequence of attempts to remove elements from a sequence, and fails to utilize the information from existing test results. To address this problem, ProbDD builds a probabilistic model to estimate the probabilities of the elements to be kept in the produced result, selects a set of elements to maximize the gain of the next test based on the model, and improves the model based on the test results. We prove the correctness of ProbDD, and analyze the minimality of its result and the asymptotic number of tests under the worst case. The asymptotic number of tests in the worst case of ProbDD is O(n), which is smaller than that of ddmin, O(n2) worst-case asymptotic number of tests. Furthermore, we experimentally compared ProbDD with ddmin on 40 subjects in HDD and CHISEL, two approaches that wrap ddmin for reducing trees and C programs, respectively. The results show that, after replacing ddmin with ProbDD, HDD and CHISEL produce 59.48% and 11.51% smaller results and use 63.22% and 45.27% less time, respectively.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {881–892},
numpages = {12},
keywords = {Probabilistic Model, Delta Debugging},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/2695664.2695782,
author = {Vieira, Vaninha and Holl, Konstantin and Hassel, Michael},
title = {A Context Simulator as Testing Support for Mobile Apps},
year = {2015},
isbn = {9781450331968},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2695664.2695782},
doi = {10.1145/2695664.2695782},
abstract = {Context-aware mobile applications gain more and more influence on our daily life. Since mobile devices are equipped with various sensors to detect their environment, it is possible to receive and process information from beyond application and device borders. Within the development, these context-aware applications have to be verified to assure that they do not cause any failures. This contribution outlines challenges of testing context-aware mobile applications relating to their context factors and present our approach for a context simulator that provides support for modeling and simulation of context in different levels: physical and logical context, situations and scenarios. The simulator supports test case derivation and enables test case execution for several context sources as part of testing mobile applications.},
booktitle = {Proceedings of the 30th Annual ACM Symposium on Applied Computing},
pages = {535–541},
numpages = {7},
keywords = {mobile application testing, context-awareness, context modeling, context simulation, quality assurance},
location = {Salamanca, Spain},
series = {SAC '15}
}

@inproceedings{10.1145/1287624.1287634,
author = {Jiang, Lingxiao and Su, Zhendong and Chiu, Edwin},
title = {Context-Based Detection of Clone-Related Bugs},
year = {2007},
isbn = {9781595938114},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287624.1287634},
doi = {10.1145/1287624.1287634},
abstract = {Studies show that programs contain much similar code, commonly known as clones. One of the main reasons for introducing clones is programmers' tendency to copy and paste code to quickly duplicate functionality. We commonly believe that clones can make programs difficult to maintain and introduce subtle bugs. Although much research has proposed techniques for detecting and removing clones to improve software maintainability, little has considered how to detect latent bugs introduced by clones. In this paper, we introduce a general notion of context-based inconsistencies among clones and develop an efficient algorithm to detect such inconsistencies for locating bugs. We have implemented our algorithm and evaluated it on large open source projects including the latest versions of the Linux kernel and Eclipse. We have discovered many previously unknown bugs and programming style issues in both projects (with 57 for the Linux kernel and 38 for Eclipse). We have also categorized the bugs and style issues and noticed that they exhibit diverse characteristics and are difficult to detect with any single existing bug detection technique. We believe that our approach complements well these existing techniques.},
booktitle = {Proceedings of the the 6th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {55–64},
numpages = {10},
keywords = {code clone detection, context-based bug detection, code clone-related bugs, inconsistencies},
location = {Dubrovnik, Croatia},
series = {ESEC-FSE '07}
}

@inbook{10.1145/3468264.3468610,
author = {Zhang, Qian and Wang, Jiyuan and Kim, Miryung},
title = {HeteroFuzz: Fuzz Testing to Detect Platform Dependent Divergence for Heterogeneous Applications},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468610},
abstract = {As specialized hardware accelerators like FPGAs become a prominent part of the current computing landscape, software applications are increasingly constructed to leverage heterogeneous architectures. Such a trend is already happening in the domain of machine learning and Internet-of-Things (IoT) systems built on edge devices. Yet, debugging and testing methods for heterogeneous applications are currently lacking. These applications may look similar to regular C/C++ code but include hardware synthesis details in terms of preprocessor directives. Therefore, their behavior under heterogeneous architectures may diverge significantly from CPU due to hardware synthesis details. Further, the compilation and hardware simulation cycle takes an enormous amount of time, prohibiting frequent invocations required for fuzz testing.  We propose a novel fuzz testing technique, called HeteroFuzz, designed to specifically target heterogeneous applications and to detect platform-dependent divergence. The key essence of HeteroFuzz is that it uses a three-pronged approach to reduce the long latency of repetitively invoking a hardware simulator on a heterogeneous application. First, in addition to monitoring code coverage as a fuzzing guidance mechanism, we analyze synthesis pragmas in kernel code and monitor accelerator-relevant value spectra. Second, we design dynamic probabilistic mutations to increase the chance of hitting divergent behavior under different platforms. Third, we memorize the boundaries of seen kernel inputs and skip HLS simulator invocation if it can expose only redundant divergent behavior. We evaluate HeteroFuzz on seven real-world heterogeneous applications with FPGA kernels. HeteroFuzz is 754X faster in exposing the same set of distinct divergence symptoms than naive fuzzing. Probabilistic mutations contribute to 17.5X speed up than the one without. Selective invocation of HLS simulation contributes to 8.8X speed up than the one without.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {242–254},
numpages = {13}
}

@inproceedings{10.1145/2889160.2889243,
author = {Shimagaki, Junji and Kamei, Yasutaka and McIntosh, Shane and Hassan, Ahmed E. and Ubayashi, Naoyasu},
title = {A Study of the Quality-Impacting Practices of Modern Code Review at Sony Mobile},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889243},
doi = {10.1145/2889160.2889243},
abstract = {Nowadays, a flexible, lightweight variant of the code review process (i.e., the practice of having other team members critique software changes) is adopted by open source and proprietary software projects. While this flexibility is a blessing (e.g., enabling code reviews to span the globe), it does not mandate minimum review quality criteria like the formal code inspections of the past. Recent work shows that lax reviewing can impact the quality of open source systems. In this paper, we investigate the impact that code reviewing practices have on the quality of a proprietary system that is developed by Sony Mobile. We begin by replicating open source analyses of the relationship between software quality (as approximated by post-release defect-proneness) and: (1) code review coverage, i.e., the proportion of code changes that have been reviewed and (2) code review participation, i.e., the degree of reviewer involvement in the code review process. We also perform a qualitative analysis, with a survey of 93 stakeholders, semi-structured interviews with 15 stakeholders, and a follow-up survey of 25 senior engineers. Our results indicate that while past measures of review coverage and participation do not share a relationship with defect-proneness at Sony Mobile, reviewing measures that are aware of the Sony Mobile development context are associated with defect-proneness. Our results have lead to improvements of the Sony Mobile code review process.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {212–221},
numpages = {10},
keywords = {code review, software quality},
location = {Austin, Texas},
series = {ICSE '16}
}

@article{10.1145/3477040,
author = {Weiss, Kevin and Rottleuthner, Michel and Schmidt, Thomas C. and W\"{a}hlisch, Matthias},
title = {PHiLIP on the HiL: Automated Multi-Platform OS Testing With External Reference Devices},
year = {2021},
issue_date = {October 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {20},
number = {5s},
issn = {1539-9087},
url = {https://doi.org/10.1145/3477040},
doi = {10.1145/3477040},
abstract = {Developing an operating systems (OSs) for low-end embedded devices requires continuous adaptation to new hardware architectures and components, while serviceability of features needs to be assured for each individual platform under tight resource constraints. It is challenging to design a versatile and accurate heterogeneous test environment that is agile enough to cover a continuous evolution of the code base and platforms. This mission is even more challenging when organized in an agile open-source community process with many contributors such as for the RIOT OS. Hardware in the Loop (HiL) testing and Continuous Integration (CI) are automatable approaches to verify functionality, prevent regressions, and improve the overall quality at development speed in large community projects.In this paper, we present PHiLIP (Primitive Hardware in the Loop Integration Product), an open-source external reference device together with tools that validate the system software while it controls hardware and interprets physical signals. Instead of focusing on a specific test setting, PHiLIP takes the approach of a tool-assisted agile HiL test process, designed for continuous evolution and deployment cycles. We explain its design, describe how it supports HiL tests, evaluate performance metrics, and report on practical experiences of employing PHiLIP in an automated CI test infrastructure. Our initial deployment comprises 22 unique platforms, each of which executes 98 peripheral tests every night. PHiLIP allows for easy extension of low-cost, adaptive testing infrastructures but serves testing techniques and tools to a much wider range of applications.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {sep},
articleno = {91},
numpages = {26},
keywords = {operating system, IoT, hardware in the loop, constrained devices}
}

@inbook{10.1145/3341105.3374099,
author = {Dass, Shuvalaxmi and Namin, Akbar Siami},
title = {Vulnerability Coverage for Adequacy Security Testing},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3374099},
abstract = {Mainstream software applications and tools are the configurable platforms with an enormous number of parameters along with their values. Certain settings and possible interactions between these parameters may harden (or soften) the security and robustness of these applications against some known vulnerabilities. However, the large number of vulnerabilities reported and associated with these tools make the exhaustive testing of these tools infeasible against these vulnerabilities infeasible. As an instance of general software testing problem, the research question to address is whether the system under test is robust and secure against these vulnerabilities. This paper introduces the idea of "vulnerability coverage," a concept to adequately test a given application for a certain classes of vulnerabilities, as reported by the National Vulnerability Database (NVD). The deriving idea is to utilize the Common Vulnerability Scoring System (CVSS) as a means to measure the fitness of test inputs generated by evolutionary algorithms and then through pattern matching identify vulnerabilities that match the generated vulnerability vectors and then test the system under test for those identified vulnerabilities. We report the performance of two evolutionary algorithms (i.e., Genetic Algorithms and Particle Swarm Optimization) in generating the vulnerability pattern vectors.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {540–543},
numpages = {4}
}

@inproceedings{10.1145/3236024.3236079,
author = {Lee, Junhee and Hong, Seongjoon and Oh, Hakjoo},
title = {MemFix: Static Analysis-Based Repair of Memory Deallocation Errors for C},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236079},
doi = {10.1145/3236024.3236079},
abstract = {We present MemFix, an automated technique for fixing memory deallocation errors in C programs. MemFix aims to fix memory-leak, double-free, and use-after-free errors, which occur when developers fail to properly deallocate memory objects. MemFix attempts to fix these errors by finding a set of free-statements that correctly deallocate all allocated objects without causing double-frees and use-after-frees. The key insight behind MemFix is that finding such a set of deallocation statements corresponds to solving an exact cover problem derived from a variant of typestate static analysis. We formally present the technique and experimentally show that MemFix is able to fix real errors found in open-source programs. Because MemFix is based on a sound static analysis, the generated patches guarantee to fix the original errors without introducing new errors.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {95–106},
numpages = {12},
keywords = {Program Analysis, Debugging, Program Repair},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.5555/3049877.3049889,
author = {Parvez, Riyad and Ward, Paul A. S. and Ganesh, Vijay},
title = {Combining Static Analysis and Targeted Symbolic Execution for Scalable Bug-Finding in Application Binaries},
year = {2016},
publisher = {IBM Corp.},
address = {USA},
abstract = {Symbolic execution is an automated technique for program analysis that has recently become practical due to advances in constraint solvers. Symbolic execution eventually enumerates all feasible program executions, check assertions on all values of varaibles in a program path, and can prioritize executions of interest. However, path explosion, the fact that the number of program executions is typically at least exponential in the size of the program, hinders the adoption of symbolic execution in the real world.In this paper, we present a method for generating test-cases using symbolic execution which reach a given potentially buggy target statement. Such a potentially buggy program statement can be found by static program analysis or from crash-reports given by the users. The test-case generated by our technique serves as a proof of the bug. Generating crashes at the target statement have many applications including re-producing crashes, checking warnings generated by static program analysis tools, or analysis of source code patches in code review process.By constantly steering the symbolic execution along the branches that are most likely to lead to the target program statement and pruning the search space that are unlikely to reach the target, we were able to detect deep bugs in real programs. To tackle the memory requirement due to the exponential growth of program paths, we propose a new scheme to manage program execution paths without exhausting system memory. Experiments on real-life programs demonstrate that our tool WatSym, built on selective symbolic execution engine S2E, can generate crashing inputs in feasible time and order of magnitude better than symbolic approaches (as embodied by S2E) failed.},
booktitle = {Proceedings of the 26th Annual International Conference on Computer Science and Software Engineering},
pages = {116–127},
numpages = {12},
location = {Toronto, Ontario, Canada},
series = {CASCON '16}
}

@inproceedings{10.1145/2970276.2970356,
author = {Legunsen, Owolabi and Hassan, Wajih Ul and Xu, Xinyue and Ro\c{s}u, Grigore and Marinov, Darko},
title = {How Good Are the Specs? A Study of the Bug-Finding Effectiveness of Existing Java API Specifications},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970356},
doi = {10.1145/2970276.2970356},
abstract = { Runtime verification can be used to find bugs early, during software development, by monitoring test executions against formal specifications (specs). The quality of runtime verification depends on the quality of the specs. While previous research has produced many specs for the Java API, manually or through automatic mining, there has been no large-scale study of their bug-finding effectiveness.  We present the first in-depth study of the bug-finding effectiveness of previously proposed specs. We used JavaMOP to monitor 182 manually written and 17 automatically mined specs against more than 18K manually written and 2.1M automatically generated tests in 200 open-source projects. The average runtime overhead was under 4.3x. We inspected 652 violations of manually written specs and (randomly sampled) 200 violations of automatically mined specs. We reported 95 bugs, out of which developers already fixed 74. However, most violations, 82.81% of 652 and 97.89% of 200, were false alarms.  Our empirical results show that (1) runtime verification technology has matured enough to incur tolerable runtime overhead during testing, and (2) the existing API specifications can find many bugs that developers are willing to fix; however, (3) the false alarm rates are worrisome and suggest that substantial effort needs to be spent on engineering better specs and properly evaluating their effectiveness },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {602–613},
numpages = {12},
keywords = {empirical study, specification quality, runtime verification},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3377811.3380420,
author = {Sun, Zeyu and Zhang, Jie M. and Harman, Mark and Papadakis, Mike and Zhang, Lu},
title = {Automatic Testing and Improvement of Machine Translation},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380420},
doi = {10.1145/3377811.3380420},
abstract = {This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems. TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles). It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99%) on generating input pairs with consistent translations. With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36% and 40% inconsistency bugs. Black-box repair fixes 28% and 19% bugs on average for Google Translate and Transformer. Grey-box repair fixes 30% bugs on average for Transformer. Manual inspection indicates that the translations repaired by our approach improve consistency in 87% of cases (degrading it in 2%), and that our repairs have better translation acceptability in 27% of the cases (worse in 8%).},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {974–985},
numpages = {12},
keywords = {machine translation, translation consistency, testing and repair},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/3377644.3377650,
author = {Lai, Enmei and Luo, Wenjun},
title = {Static Analysis of Integer Overflow of Smart Contracts in Ethereum},
year = {2020},
isbn = {9781450377447},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377644.3377650},
doi = {10.1145/3377644.3377650},
abstract = {In recent years, vulnerabilities of smart contracts have frequently break out. In particular, integer overflow of smart contracts, a high-risk vulnerability, has caused huge financial losses. However, most tools currently fail to detect integer overflow in smart contracts. In this paper, we summarize 11 types of integer overflow features for Solidity smart contracts in Ethereum and abstractly define 83 corresponding XPath patterns. And we design an extensible static analysis tool to detect common integer overflow vulnerabilities of Solidity smart contracts in Ethereum through the defined XPath patterns. To evaluate our tool, we tested 7,000 verified Solidity smart contracts and found that there were 430 smart contracts with vulnerabilities of integer overflow. Experimental results show that there are still high-risk vulnerabilities of integer overflow in verified smart contracts.},
booktitle = {Proceedings of the 2020 4th International Conference on Cryptography, Security and Privacy},
pages = {110–115},
numpages = {6},
keywords = {XPath, smart contract, vulnerability detection, integer overflow},
location = {Nanjing, China},
series = {ICCSP 2020}
}

@article{10.1145/1217295.1217297,
author = {Binkley, David and Gold, Nicolas and Harman, Mark},
title = {An Empirical Study of Static Program Slice Size},
year = {2007},
issue_date = {April 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/1217295.1217297},
doi = {10.1145/1217295.1217297},
abstract = {This article presents results from a study of all slices from 43 programs, ranging up to 136,000 lines of code in size. The study investigates the effect of five aspects that affect slice size. Three slicing algorithms are used to study two algorithmic aspects: calling-context treatment and slice granularity. The remaining three aspects affect the upstream dependencies considered by the slicer. These include collapsing structure fields, removal of dead code, and the influence of points-to analysis.The results show that for the most precise slicer, the average slice contains just under one-third of the program. Furthermore, ignoring calling context causes a 50% increase in slice size, and while (coarse-grained) function-level slices are 33% larger than corresponding statement-level slices, they may be useful predictors of the (finer-grained) statement-level slice size. Finally, upstream analyses have an order of magnitude less influence on slice size.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
pages = {8–es},
numpages = {32},
keywords = {slice size, Program slicing}
}

@inproceedings{10.1145/1294921.1294927,
author = {Marchetti, Eda and Bertolino, Antonia},
title = {Profiling and Testing within Domains to Facilitate Document Exchangeability},
year = {2007},
isbn = {9781595937261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1294921.1294927},
doi = {10.1145/1294921.1294927},
abstract = {Increasingly today the trend emerges within several specific application domains of adopting standard metadata formats for codifying the documents and the files exchanged across applications and stakeholders of that domain. Given a standard format and some additional constraints that may apply to a specific community of user (localization of standard specifications), we then need ways for: i) deriving complying documents, and ii) testing the behaviour of services that process such documents. We have been investigating such issues in the e-learning and health domains. In this paper we mainly focus on the exchangeability of clinical documents and present an environment called Euclide currently under development, which is based on XML Schema specifications. Euclide will include the TAXI tool for the automated generation from the specified XML Schema of XML instances, to be used for black-box testing of the applications that process the clinical documents codified by the generated instances.},
booktitle = {Workshop on Domain Specific Approaches to Software Test Automation: In Conjunction with the 6th ESEC/FSE Joint Meeting},
pages = {34–40},
numpages = {7},
keywords = {XML schema, automatic conformance testing, clinical document},
location = {Dubrovnik, Croatia},
series = {DOSTA '07}
}

@inproceedings{10.1145/3203217.3203237,
author = {Pina, Lu\'{\i}s and Andronidis, Anastasios and Cadar, Cristian},
title = {<i>FreeDA</i>: Deploying Incompatible Stock Dynamic Analyses in Production via Multi-Version Execution},
year = {2018},
isbn = {9781450357616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3203217.3203237},
doi = {10.1145/3203217.3203237},
abstract = {Dynamic analyses such as those implemented by compiler sanitizers and Valgrind are effective at finding and diagnosing challenging bugs and security vulnerabilities. However, most analyses cannot be combined on the same program execution, and they incur a high overhead, which typically prevents them from being used in production.This paper addresses the ambitious goal of running concurrently multiple incompatible stock dynamic analysis tools in production, without requiring any modifications to the tools themselves or adding significant runtime overhead to the deployed system. This is accomplished using multi-version execution, in which the dynamic analyses are run concurrently with the native version, all on the same program execution.We implement our approach in a system called. FreeDA and show that it is applicable to several common scenarios, involving network servers and interactive applications. In particular, we show how incompatible stock dynamic analyses implemented by Clang's sanitizers and Valgrind can be used to check high-performance servers such as Memcached, Nginx and Redis, and interactive applications such as Git, HTop and OpenSSH.},
booktitle = {Proceedings of the 15th ACM International Conference on Computing Frontiers},
pages = {1–10},
numpages = {10},
keywords = {sanitizers, valgrind, multi-version execution},
location = {Ischia, Italy},
series = {CF '18}
}

@inproceedings{10.5555/2662413.2662435,
author = {Alves, Everton L. G. and Machado, Patricia D. L. and Massoni, Tiago and Santos, Samuel T. C.},
title = {A Refactoring-Based Approach for Test Case Selection and Prioritization},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {Refactoring edits, commonly applied during software development, may introduce faults in a previously-stable code. Therefore, regression testing is usually applied to check whether the code maintains its previous behavior. In order to avoid rerunning the whole regression suite, test case prioritization techniques have been developed to order test cases for earlier achievement of a given goal, for instance, improving the rate of fault detection during regression testing execution. However, as current techniques are usually general purpose, they may not be effective for early detection of refactoring faults. In this paper, we propose a refactoring-based approach for selecting and prioritizing regression test cases, which specializes selection/prioritization tasks according to the type of edit made. The approach has been evaluated through a case study that compares it to well-known prioritization techniques by using a real open-source Java system. This case study indicates that the approach can be more suitable for early detection of refactoring faults when comparing to the other prioritization techniques.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {93–99},
numpages = {7},
location = {San Francisco, California},
series = {AST '13}
}

@inproceedings{10.1145/2635868.2635912,
author = {Gulwani, Sumit and Radi\v{c}ek, Ivan and Zuleger, Florian},
title = {Feedback Generation for Performance Problems in Introductory Programming Assignments},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635912},
doi = {10.1145/2635868.2635912},
abstract = { Providing feedback on programming assignments manually is a tedious, error prone, and time-consuming task. In this paper, we motivate and address the problem of generating feedback on performance aspects in introductory programming assignments. We studied a large number of functionally correct student solutions to introductory programming assignments and observed: (1) There are different algorithmic strategies, with varying levels of efficiency, for solving a given problem. These different strategies merit different feedback. (2) The same algorithmic strategy can be implemented in countless different ways, which are not relevant for reporting feedback on the student program. We propose a light-weight programming language extension that allows a teacher to define an algorithmic strategy by specifying certain key values that should occur during the execution of an implementation. We describe a dynamic analysis based approach to test whether a student's program matches a teacher's specification. Our experimental results illustrate the effectiveness of both our specification language and our dynamic analysis. On one of our benchmarks consisting of 2316 functionally correct implementations to 3 programming problems, we identified 16 strategies that we were able to describe using our specification language (in 95 minutes after inspecting 66, i.e., around 3%, implementations). Our dynamic analysis correctly matched each implementation with its corresponding specification, thereby automatically producing the intended feedback. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {41–51},
numpages = {11},
keywords = {Education, performance analysis, dynamic analysis, trace specification, MOOCs},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/2486046.2486076,
author = {Laurent, Yoann and Bendraou, Reda and Gervais, Marie-Pierre},
title = {Generation of Process Using Multi-Objective Genetic Algorithm},
year = {2013},
isbn = {9781450320627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2486046.2486076},
doi = {10.1145/2486046.2486076},
abstract = { The growing complexity of processes whatever their kind (i.e. business, software, medical, military) stimulates the adoption of process execution, analysis and verification techniques. However, such techniques cannot be accurately validated as it is not possible to obtain numerous and realistic process models in order to stress test them. The small set of samples and ``toy'' models publically available in the literature is usually insufficient to conduct serious empirical studies and thus, to validate thoroughly work around process analysis and verification. In this paper, we face this problem by proposing a process model generator using a multi-objective genetic algorithm. The originality of our approach comes from the fact that process models are built through a sequence of high-level operations inspired by the way a process modeler could have actually performed to model a process. A working generator prototype has been implemented and shows that it is possible to quickly generate huge, syntactically sound and user-tailored process models. },
booktitle = {Proceedings of the 2013 International Conference on Software and System Process},
pages = {161–165},
numpages = {5},
keywords = {UML Activity, Generator, Process, Genetic Algorithm},
location = {San Francisco, CA, USA},
series = {ICSSP 2013}
}

@inproceedings{10.1145/2909437.2909439,
author = {Pflanzer, Moritz and Donaldson, Alastair F. and Lascu, Andrei},
title = {Automatic Test Case Reduction for OpenCL},
year = {2016},
isbn = {9781450343381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2909437.2909439},
doi = {10.1145/2909437.2909439},
abstract = {We report on an extension to the C-Reduce tool, for automatic reduction of C test cases, to handle OpenCL kernels. This enables an automated method for detecting bugs in OpenCL compilers, by generating large random kernels using the CLsmith generator, identifying kernels that yield result differences across OpenCL platforms and optimisation levels, and using our novel extension to C-Reduce to automatically reduce such kernels to minimal forms that can be filed as bug reports. A major part of our effort involved the design of ShadowKeeper, a new plugin for the Oclgrind simulator that provides accurate detection of accesses to uninitialised data. We present experimental results showing the effectiveness of our method for finding bugs in a number of OpenCL compilers.},
booktitle = {Proceedings of the 4th International Workshop on OpenCL},
articleno = {1},
numpages = {12},
location = {Vienna, Austria},
series = {IWOCL '16}
}

