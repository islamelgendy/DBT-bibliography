@inbook{10.1145/3368089.3409671,
author = {Yan, Shenao and Tao, Guanhong and Liu, Xuwei and Zhai, Juan and Ma, Shiqing and Xu, Lei and Zhang, Xiangyu},
title = {Correlations between Deep Neural Network Model Coverage Criteria and Model Quality},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409671},
abstract = {Inspired by the great success of using code coverage as guidance in software testing, a lot of neural network coverage criteria have been proposed to guide testing of neural network models (e.g., model accuracy under adversarial attacks). However, while the monotonic relation between code coverage and software quality has been supported by many seminal studies in software engineering, it remains largely unclear whether similar monotonicity exists between neural network model coverage and model quality. This paper sets out to answer this question. Specifically, this paper studies the correlation between DNN model quality and coverage criteria, effects of coverage guided adversarial example generation compared with gradient decent based methods, effectiveness of coverage based retraining compared with existing adversarial training, and the internal relationships among coverage criteria.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {775–787},
numpages = {13}
}

@inbook{10.1145/3341105.3373961,
author = {Cerioli, Maura and Leotta, Maurizio and Ricca, Filippo},
title = {What 5 Million Job Advertisements Tell Us about Testing: A Preliminary Empirical Investigation},
year = {2020},
isbn = {9781450368667},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341105.3373961},
abstract = {Software testing is a crucial part of business success to ensure final product quality. However, little concrete data exists on technical demands about it in the industry, mostly collected through personal opinion surveys on a restricted sample of professionals.In this paper, we used a different approach: we applied content analysis to a set of about five million job advertisements taken from a popular Web job-search engine. The analysis of job advertisements is more promising than surveys because the data are by far more numerous and distributed geographically.The content analysis results revealed four essential findings on the current practice of software testing: a) Companies search for about six times more Coders than Testers, b) Unit testing is the most required skill for Coders while Acceptance testing is the most popular for Testers, c) Automated testing dominates the job advertisement scene compared to Manual testing and, d) the most valuable testing tools and frameworks are Selenium, JUnit, and Cucumber for both Testers and Coders. We believe that these findings (and other related results from the content analysis study) will be useful for professionals, instructors, and researchers dealing with software testing.},
booktitle = {Proceedings of the 35th Annual ACM Symposium on Applied Computing},
pages = {1586–1594},
numpages = {9}
}

@inproceedings{10.1145/3383219.3383252,
author = {Florea, Raluca and Stray, Viktoria},
title = {A Qualitative Study of the Background, Skill Acquisition, and Learning Preferences of Software Testers},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383252},
doi = {10.1145/3383219.3383252},
abstract = {Context: There is an indisputable industrial need for highly skilled individuals in the role of software testers. However, little is known about the educational background of these professionals, their first contact with the role, their preferences in acquiring skills, the impediments they face, and their perception of the software testing role. Objective: In the current paper, we report on the background, skills, learning preferences, and role profiles as described by professionals in software testing, spanning over a significant number of industries, countries, and software development models. Method: We conducted 19 in-depth, semi-structured interviews with software testing practitioners, across eight industries. We performed a content and thematic analysis of the collected data. Results: The practitioners in software testing had diverse educational backgrounds, and their first contact with the testing role was accidental. Exploratory testing was the preferred testing technique, while curiosity was identified as the most important feature in their skill set. Our respondents collaborated extensively with the developers, whom they perceived as a learning source and symbiotic work partner. Conclusion: The professionals in software testing described their skills as a rather undefined heap of knowledge, increasing with each work task. They used mainly informal and hands-on learning approaches. They found it necessary for education providers to present information on software testing. Generally, companies assisted them well in their skill development but need to allocate sufficient time for the learning. We identified five specialties of the role: product owner in testing, UX tester, DevOps tester, test-script automator, and test-process coordinator.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {299–305},
numpages = {7},
keywords = {Hiring Software Testers, Skill Acquisition, Software Testing, Software Tester, Software Testing Profiles},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/2897010.2897013,
author = {McMinn, Phil and Harman, Mark and Fraser, Gordon and Kapfhammer, Gregory M.},
title = {Automated Search for Good Coverage Criteria: Moving from Code Coverage to Fault Coverage through Search-Based Software Engineering},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897013},
doi = {10.1145/2897010.2897013},
abstract = {We propose to use Search-Based Software Engineering to automatically evolve coverage criteria that are well correlated with fault revelation, through the use of existing fault databases. We explain how problems of bloat and overfitting can be ameliorated in our approach, and show how this new method will yield insight into faults --- as well as better guidance for Search-Based Software Testing.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {43–44},
numpages = {2},
location = {Austin, Texas},
series = {SBST '16}
}

@article{10.1145/3182659,
author = {Rodrigues, Davi Silva and Delamaro, M\'{a}rcio Eduardo and Corr\^{e}a, Cl\'{e}ber Gimenez and Nunes, F\'{a}tima L. S.},
title = {Using Genetic Algorithms in Test Data Generation: A Critical Systematic Mapping},
year = {2018},
issue_date = {March 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/3182659},
doi = {10.1145/3182659},
abstract = {Software testing activities account for a considerable portion of systems development cost and, for this reason, many studies have sought to automate these activities. Test data generation has a high cost reduction potential (especially for complex domain systems), since it can decrease human effort. Although several studies have been published about this subject, articles of reviews covering this topic usually focus only on specific domains. This article presents a systematic mapping aiming at providing a broad, albeit critical, overview of the literature in the topic of test data generation using genetic algorithms. The selected studies were categorized by software testing technique (structural, functional, or mutation testing) for which test data were generated and according to the most significantly adapted genetic algorithms aspects. The most used evaluation metrics and software testing techniques were identified. The results showed that genetic algorithms have been successfully applied to simple test data generation, but are rarely used to generate complex test data such as images, videos, sounds, and 3D (three-dimensional) models. From these results, we discuss some challenges and opportunities for research in this area.},
journal = {ACM Comput. Surv.},
month = {may},
articleno = {41},
numpages = {23},
keywords = {systematic mapping, genetic algorithms, Test data generation, test case generation, scoping study, evolutionary test, software testing}
}

@inproceedings{10.1145/3194718.3194726,
author = {Fredericks, Erik M.},
title = {An Empirical Analysis of the Mutation Operator for Run-Time Adaptive Testing in Self-Adaptive Systems},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194726},
doi = {10.1145/3194718.3194726},
abstract = {A self-adaptive system (SAS) can reconfigure at run time in response to uncertainty and/or adversity to continually deliver an acceptable level of service. An SAS can experience uncertainty during execution in terms of environmental conditions for which it was not explicitly designed as well as unanticipated combinations of system parameters that result from a self-reconfiguration or misunderstood requirements. Run-time testing provides assurance that an SAS continually behaves as it was designed even as the system reconfigures and the environment changes. Moreover, introducing adaptive capabilities via lightweight evolutionary algorithms into a run-time testing framework can enable an SAS to effectively update its test cases in response to uncertainty alongside the SAS's adaptation engine while still maintaining assurance that requirements are being satisfied. However, the impact of the evolutionary parameters that configure the search process for run-time testing may have a significant impact on test results. Therefore, this paper provides an empirical study that focuses on the mutation parameter that guides online evolution as applied to a run-time testing framework, in the context of an SAS.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {59–66},
numpages = {8},
keywords = {run-time testing, self-adaptive systems, evolutionary algorithms, search-based software testing},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@inbook{10.1145/3293882.3330579,
author = {Xie, Xiaofei and Ma, Lei and Juefei-Xu, Felix and Xue, Minhui and Chen, Hongxu and Liu, Yang and Zhao, Jianjun and Li, Bo and Yin, Jianxiong and See, Simon},
title = {DeepHunter: A Coverage-Guided Fuzz Testing Framework for Deep Neural Networks},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330579},
abstract = {The past decade has seen the great potential of applying deep neural network (DNN) based software to safety-critical scenarios, such as autonomous driving. Similar to traditional software, DNNs could exhibit incorrect behaviors, caused by hidden defects, leading to severe accidents and losses. In this paper, we propose DeepHunter, a coverage-guided fuzz testing framework for detecting potential defects of general-purpose DNNs. To this end, we first propose a metamorphic mutation strategy to generate new semantically preserved tests, and leverage multiple extensible coverage criteria as feedback to guide the test generation. We further propose a seed selection strategy that combines both diversity-based and recency-based seed selection. We implement and incorporate 5 existing testing criteria and 4 seed selection strategies in DeepHunter. Large-scale experiments demonstrate that (1) our metamorphic mutation strategy is useful to generate new valid tests with the same semantics as the original seed, by up to a 98% validity ratio; (2) the diversity-based seed selection generally weighs more than recency-based seed selection in boosting the coverage and in detecting defects; (3) DeepHunter outperforms the state of the arts by coverage as well as the quantity and diversity of defects identified; (4) guided by corner-region based criteria, DeepHunter is useful to capture defects during the DNN quantization for platform migration.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {146–157},
numpages = {12}
}

@inbook{10.1145/3293882.3330577,
author = {Liu, Kui and Koyuncu, Anil and Kim, Dongsun and Bissyand\'{e}, Tegawend\'{e} F.},
title = {TBar: Revisiting Template-Based Automated Program Repair},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330577},
abstract = {We revisit the performance of template-based APR to build comprehensive knowledge about the effectiveness of fix patterns, and to highlight the importance of complementary steps such as fault localization or donor code retrieval. To that end, we first investigate the literature to collect, summarize and label recurrently-used fix patterns. Based on the investigation, we build TBar, a straightforward APR tool that systematically attempts to apply these fix patterns to program bugs. We thoroughly evaluate TBar on the Defects4J benchmark. In particular, we assess the actual qualitative and quantitative diversity of fix patterns, as well as their effectiveness in yielding plausible or correct patches. Eventually, we find that, assuming a perfect fault localization, TBar correctly/plausibly fixes 74/101 bugs. Replicating a standard and practical pipeline of APR assessment, we demonstrate that TBar correctly fixes 43 bugs from Defects4J, an unprecedented performance in the literature (including all approaches, i.e., template-based, stochastic mutation-based or synthesis-based APR).},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {31–42},
numpages = {12}
}

@inproceedings{10.1145/3482909.3482915,
author = {Jorge, Dalton and Machado, Patricia and Andrade, Wilkerson},
title = {Investigating Test Smells in JavaScript Test Code},
year = {2021},
isbn = {9781450385039},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3482909.3482915},
doi = {10.1145/3482909.3482915},
abstract = { Writing automated test cases is a challenging and demanding activity. The test case itself is software that requires proper design to ensure it can be implemented and maintained as long as the production code evolves. Like code smells, test smells may indicate violations of principles that negatively affect the quality of test code design, making it difficult to comprehend and, consequently, impairing its proper use and evolution. This work aims to investigate the occurrence of test smells in JavaScript test code and whether their presence can be correlated with test code quality. We perform an empirical study using the STEEL tool where the test suites of 11 open-source JavaScript projects from the Github repository are analyzed to detect a set of previously cataloged test smells. We then investigate: i) which ones occur more frequently; ii) whether given test smells are likely to occur together, and iii) if the presence of certain test smells is related to classical bad design indicators on the test code. We found that the Duplicate Assert, Magic Number Test, Unknown Test and Conditional Test Logic smells are the most common in JavaScript test code, whereas the Mystery Guest, Ignored Test and Resource Optimism smells are the least common. Moreover, the Conditional Test Logic, Magic Number Test, Duplicate Assert and the Exception Handling smells may often appear together. Furthermore, there is a moderate to a strong positive correlation between some smells count and quality measures in the test code. We can conclude that test smells are frequently found in JavaScript test code, and their presence may be an indicator of low design quality.},
booktitle = {Brazilian Symposium on Systematic and Automated Software Testing},
pages = {36–45},
numpages = {10},
keywords = {quality metrics, javascript, test smells},
location = {Joinville, Brazil},
series = {SAST'21}
}

@inproceedings{10.1145/3338906.3338930,
author = {Li, Zenan and Ma, Xiaoxing and Xu, Chang and Cao, Chun and Xu, Jingwei and L\"{u}, Jian},
title = {Boosting Operational DNN Testing Efficiency through Conditioning},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338930},
doi = {10.1145/3338906.3338930},
abstract = {With the increasing adoption of Deep Neural Network (DNN) models as integral parts of software systems, efficient operational testing of DNNs is much in demand to ensure these models' actual performance in field conditions. A challenge is that the testing often needs to produce precise results with a very limited budget for labeling data collected in field.  Viewing software testing as a practice of reliability estimation through statistical sampling, we re-interpret the idea behind conventional structural coverages as conditioning for variance reduction. With this insight we propose an efficient DNN testing method based on the conditioning on the representation learned by the DNN model under testing. The representation is defined by the probability distribution of the output of neurons in the last hidden layer of the model. To sample from this high dimensional distribution in which the operational data are sparsely distributed, we design an algorithm leveraging cross entropy minimization.  Experiments with various DNN models and datasets were conducted to evaluate the general efficiency of the approach. The results show that, compared with simple random sampling, this approach requires only about a half of labeled inputs to achieve the same level of precision.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {499–509},
numpages = {11},
keywords = {Neural networks, Software testing, Coverage criteria},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/186258.187158,
author = {Richardson, Debra J.},
title = {TAOS: Testing with Analysis and Oracle Support},
year = {1994},
isbn = {0897916832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/186258.187158},
doi = {10.1145/186258.187158},
abstract = {Few would question that software testing is a necessary activity for assuring software quality, yet the typical testing process is a human intensive activity and as such, it is unproductive, error-prone, and often inadequately done. Moreover, testing is seldom given a prominent place in software development or maintenance processes, nor is it an integral part of them. Major productivity and quality enhancements can be achieved by automating the testing process through tool development and use and effectively incorporating it with development and maintenance processes.The TAOS toolkit, Testing with Analysis and Oracle Support, provides support for the testing process. It includes tools that automate many tasks in the testing process, including management and persistence of test artifacts and the relationships between those artifacts, test development, test execution, and test measurement. A unique aspect of TAOS is its support for test oracles and their use to verify behavioral correctness of test executions. TAOS also supports structural/dependence coverage, by measuring the adequacy of test criteria coverage, and regression testing, by identifying tests associated or dependent upon modified software artifacts. This is accomplished by integrating the ProDAG toolset, Program Dependence Analysis Graph, with TAOS, which supports the use of program dependence analysis in testing, debugging, and maintenance.This paper describes the TAOS toolkit and its capabilities as well as testing, debugging and maintenance processes based on program dependence analysis. We also describe our experience with the toolkit and discuss our future plans.},
booktitle = {Proceedings of the 1994 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {138–153},
numpages = {16},
location = {Seattle, Washington, USA},
series = {ISSTA '94}
}

@inproceedings{10.1145/1233341.1233357,
author = {Parveen, Tauhida and Tilley, Scott and Gonzalez, George},
title = {A Case Study in Test Management},
year = {2007},
isbn = {9781595936295},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1233341.1233357},
doi = {10.1145/1233341.1233357},
abstract = {Testing is an essential but often under-utilized area of software engineering. A variety of software testing techniques have been developed to effectively identify bugs in source code, yet these techniques are not always fully employed in practice. There are numerous reasons for this, including the difficulty in mastering the complexity of managing all of the test cases for large-scale projects. Test case management involves organizing testing artifacts (e.g., requirements traceability data, test cases, and expected results) in a systematic manner. To be successful, test case management requires a high degree of discipline to accommodate the large volume of artifacts under consideration. This paper presents the results of a case study in centralizing test artifacts in an industrial setting to aid better test management. Several of the challenges in adopting this approach are discussed. In response to these challenges, recommendations on how to better leverage test case management are offered.},
booktitle = {Proceedings of the 45th Annual Southeast Regional Conference},
pages = {82–87},
numpages = {6},
keywords = {software testing, test management},
location = {Winston-Salem, North Carolina},
series = {ACM-SE 45}
}

@inproceedings{10.1145/3383219.3383236,
author = {Petri\'{c}, Jean and Hall, Tracy and Bowes, David},
title = {Which Software Faults Are Tests Not Detecting?},
year = {2020},
isbn = {9781450377317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3383219.3383236},
doi = {10.1145/3383219.3383236},
abstract = {Context: Software testing plays an important role in assuring the reliability of systems. Assessing the efficacy of testing remains challenging with few established test effectiveness metrics. Those metrics that have been used (e.g. coverage and mutation analysis) have been criticised for insufficiently differentiating between the faults detected by tests. Objective: We investigate how effective tests are at detecting different types of faults and whether some types of fault evade tests more than others. Our aim is to suggest to developers specific ways in which their tests need to be improved to increase fault detection. Method: We investigate seven fault types and analyse how often each goes undetected in 10 open source systems. We statistically look for any relationship between the test set and faults. Results: Our results suggest that the fault detection rates of unit tests are relatively low, typically finding only about a half of all faults. In addition, conditional boundary and method call removals are less well detected by tests than other fault types. Conclusions: We conclude that the testing of these open source systems needs to be improved across the board. In addition, despite boundary cases being long known to attract faults, tests covering boundaries need particular improvement. Overall, we recommend that developers do not rely only on code coverage and mutation score to measure the effectiveness of their tests.},
booktitle = {Proceedings of the Evaluation and Assessment in Software Engineering},
pages = {160–169},
numpages = {10},
keywords = {unit tests, software testing, test effectiveness},
location = {Trondheim, Norway},
series = {EASE '20}
}

@inproceedings{10.1145/2897010.2897021,
author = {Sakti, Abdelilah and Pesant, Gilles and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {JTExpert at the Fourth Unit Testing Tool Competition},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897021},
doi = {10.1145/2897010.2897021},
abstract = {JTExpert is a software testing tool that automatically generates a whole test suite to satisfy the branch-coverage criterion. It takes as inputs a Java source code and its dependencies and automatically produces a test-case suite in JUnit format. In this paper, we summarize our results for the Unit Testing Tool Competition held at the fourth SBST Contest, where JTExpert received 931 points and was ranked third. We also analyze our tool's performance.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {37–40},
numpages = {4},
keywords = {test-case generation, unit testing, classes testing, static analysis, random testing},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.5555/2337223.2337414,
author = {Alshahwan, Nadia and Harman, Mark},
title = {Augmenting Test Suites Effectiveness by Increasing Output Diversity},
year = {2012},
isbn = {9781467310673},
publisher = {IEEE Press},
abstract = { The uniqueness (or otherwise) of test outputs ought to have a bearing on test effectiveness, yet it has not previously been studied. In this paper we introduce a novel test suite adequacy criterion based on output uniqueness. We propose 4 definitions of output uniqueness with varying degrees of strictness. We present a preliminary evaluation for web application testing that confirms that output uniqueness enhances fault-finding effectiveness. The approach outperforms random augmentation in fault finding ability by an overall average of 280% in 5 medium sized, real world web applications. },
booktitle = {Proceedings of the 34th International Conference on Software Engineering},
pages = {1345–1348},
numpages = {4},
location = {Zurich, Switzerland},
series = {ICSE '12}
}

@inproceedings{10.1145/2931037.2931051,
author = {Kochhar, Pavneet Singh and Xia, Xin and Lo, David and Li, Shanping},
title = {Practitioners' Expectations on Automated Fault Localization},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931051},
doi = {10.1145/2931037.2931051},
abstract = { Software engineering practitioners often spend significant amount of time and effort to debug. To help practitioners perform this crucial task, hundreds of papers have proposed various fault localization techniques. Fault localization helps practitioners to find the location of a defect given its symptoms (e.g., program failures). These localization techniques have pinpointed the locations of bugs of various systems of diverse sizes, with varying degrees of success, and for various usage scenarios. Unfortunately, it is unclear whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by surveying 386 practitioners from more than 30 countries across 5 continents about their expectations of research in fault localization. In particular, we investigated a number of factors that impact practitioners' willingness to adopt a fault localization technique. We then compared what practitioners need and the current state-of-research by performing a literature review of papers on fault localization techniques published in ICSE, FSE, ESEC-FSE, ISSTA, TSE, and TOSEM in the last 5 years (2011-2015). From this comparison, we highlight the directions where researchers need to put effort to develop fault localization techniques that matter to practitioners. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {165–176},
numpages = {12},
keywords = {Empirical Study, Fault Localization, Practitioners' Expectations},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.5555/2821339.2821356,
author = {Sakti, Abdelilah and Pesant, Gilles and Gu\'{e}h\'{e}neuc, Yann-Ga\"{e}l},
title = {JTExpert at the Third Unit Testing Tool Competition},
year = {2015},
publisher = {IEEE Press},
abstract = {JTExpert is a software testing tool that automatically generates a whole test suite to satisfy the branch-coverage criterion on a given Java source code. It takes as inputs a Java source code and its dependencies and automatically produces a test-case suite in JUnit format. In this paper, we summarize our results for the Unit Testing Tool Competition held at the third SBST Contest, where JTExpert receives 159.16 points and was ranked sixth of seven participating tools. We discuss the internal and external reasons that were behind the relatively poor score and ranking.},
booktitle = {Proceedings of the Eighth International Workshop on Search-Based Software Testing},
pages = {52–55},
numpages = {4},
keywords = {unit testing, classes testing, test-case generation, static analysis, random testing},
location = {Florence, Italy},
series = {SBST '15}
}

@inproceedings{10.1145/3236024.3275525,
author = {Liang, Jie and Jiang, Yu and Chen, Yuanliang and Wang, Mingzhe and Zhou, Chijin and Sun, Jiaguang},
title = {PAFL: Extend Fuzzing Optimizations of Single Mode to Industrial Parallel Mode},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3275525},
doi = {10.1145/3236024.3275525},
abstract = {Researchers have proposed many optimizations to improve the efficiency of fuzzing, and most optimized strategies work very well on their targets when running in single mode with instantiating one fuzzer instance. However, in real industrial practice, most fuzzers run in parallel mode with instantiating multiple fuzzer instances, and those optimizations unfortunately fail to maintain the efficiency improvements.  In this paper, we present PAFL, a framework that utilizes efficient guiding information synchronization and task division to extend those existing fuzzing optimizations of single mode to industrial parallel mode. With an additional data structure to store the guiding information, the synchronization ensures the information is shared and updated among different fuzzer instances timely. Then, the task division promotes the diversity of fuzzer instances by splitting the fuzzing task into several sub-tasks based on branch bitmap. We first evaluate PAFL using 12 different real-world programs from Google fuzzer-test-suite. Results show that in parallel mode, two AFL improvers–AFLFast and FairFuzz do not outperform AFL, which is different from the case in single mode. However, when augmented with PAFL, the performance of AFLFast and FairFuzz in parallel mode improves. They cover 8% and 17% more branches, trigger 79% and 52% more unique crashes. For further evaluation on more widely-used software systems from GitHub, optimized fuzzers augmented with PAFL find more real bugs, and 25 of which are security-critical vulnerabilities registered as CVEs in the US National Vulnerability Database.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {809–814},
numpages = {6},
keywords = {Fuzzing, Parallel, Software testing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2993288.2993301,
author = {Rojas, Isabel K. Villanes and Meireles, Silvia and Dias-Neto, Arilo Claudio},
title = {Cloud-Based Mobile App Testing Framework: Architecture, Implementation and Execution},
year = {2016},
isbn = {9781450347662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993288.2993301},
doi = {10.1145/2993288.2993301},
abstract = {The growth in the use of mobile devices is notorious due to the multiple functionalities they offer. The time between the release of new device models and mobile platform updates is very short, and this has a direct influence on the quality of mobile applications, because these applications need to be compatible with new mobile devices and the different versions of mobile platforms. As a negative consequence, the quality of mobile apps would be lower than expected. Therefore, to ensure mobile application quality, many services for mobile test are offered as Cloud Testing. Thus, this work proposes a Mobile Cloud Testing Framework, called AM-TaaS, that meets these needs. AM-TaaS facilitates the test environment setup and configuration and covers a range of mobile devices and platforms. We also describe the architecture and implementation of the proposed framework. With the use of AM-TaaS framework, it is possible to perform mobile app testing on different mobile emulators/devices.},
booktitle = {Proceedings of the 1st Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {10},
numpages = {10},
keywords = {Mobile testing, mobile cloud testing, automated testing},
location = {Maringa, Parana, Brazil},
series = {SAST}
}

@inproceedings{10.1145/2666539.2666569,
author = {Yan, Minzhi and Sun, Hailong and Liu, Xudong},
title = {ITest: Testing Software with Mobile Crowdsourcing},
year = {2014},
isbn = {9781450332248},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2666539.2666569},
doi = {10.1145/2666539.2666569},
abstract = { In recent years, a lot of crowdsourcing systems have emerged and lead to many successful crowdsourcing systems like Wiki-pedia, Amazon Mechanical Turk and Waze. In the field of software engineering, crowdtesting has acquired increased interest and adoption, especially among personal developers and smaller companies. In this paper, we present iTest which combines mobile crowdsourcing and software testing together to support the testing of mobile application and web services. iTest is a framework for software developers to submit their software and conveniently get the test results from the crowd testers. Firstly, we analyze the key problems need to be solved in a mobile crowdtesting platform; Secondly, we present the architecture of iTest framework; Thirdly, we introduce the workflow of testing web service in iTest and propose an algorithm for solving the tester selection problem mentioned in Section 2; Then the development kit to support testing mobile application is explained; Finally, we perform two experiments to illustrate that both the way to access network and tester's location influence the performance of web service. },
booktitle = {Proceedings of the 1st International Workshop on Crowd-Based Software Development Methods and Technologies},
pages = {19–24},
numpages = {6},
keywords = {mobile crowdsourcing, mobile application, web service, Software testing},
location = {Hong Kong, China},
series = {CrowdSoft 2014}
}

