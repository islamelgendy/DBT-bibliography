@inproceedings{10.1145/3477132.3483556,
author = {Fu, Xinwei and Kim, Wook-Hee and Shreepathi, Ajay Paddayuru and Ismail, Mohannad and Wadkar, Sunny and Lee, Dongyoon and Min, Changwoo},
title = {Witcher: Systematic Crash Consistency Testing for Non-Volatile Memory Key-Value Stores},
year = {2021},
isbn = {9781450387095},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3477132.3483556},
doi = {10.1145/3477132.3483556},
abstract = {The advent of non-volatile main memory (NVM) enables the development of crash-consistent software without paying storage stack overhead. However, building a correct crash-consistent program remains very challenging in the presence of a volatile cache. This paper presents Witcher, a systematic crash consistency testing framework, which detects both correctness and performance bugs in NVM-based persistent key-value stores and underlying NVM libraries, without test space explosion and without manual annotations or crash consistency checkers. To detect correctness bugs, Witcher automatically infers likely correctness conditions by analyzing data and control dependencies between NVM accesses. Then Witcher validates if any violation of them is a true crash consistency bug by checking output equivalence between executions with and without a crash. Moreover, Witcher detects performance bugs by analyzing the execution traces. Evaluation with 20 NVM key-value stores based on Intel's PMDK library shows that Witcher discovers 47 (36 new) correctness consistency bugs and 158 (113 new) performance bugs in both applications and PMDK.},
booktitle = {Proceedings of the ACM SIGOPS 28th Symposium on Operating Systems Principles},
pages = {100–115},
numpages = {16},
keywords = {Debugging, Testing, Crash Consistency, Non-volatile Memory},
location = {Virtual Event, Germany},
series = {SOSP '21}
}

@inproceedings{10.5555/2818754.2818787,
author = {Terragni, Valerio and Cheung, Shing-Chi and Zhang, Charles},
title = {RECONTEST: Effective Regression Testing of Concurrent Programs},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Concurrent programs proliferate as multi-core technologies advance. The regression testing of concurrent programs often requires running a failing test for weeks before catching a faulty interleaving, due to the myriad of possible interleavings of memory accesses arising from concurrent program executions. As a result, the conventional approach that selects a sub-set of test cases for regression testing without considering interleavings is insufficient In this paper we present ReConTest to address the problem by selecting the new interleavings that arise due to code changes. These interleavings must be explored in order to uncover regression bugs. ReConTest efficiently selects new interleavings by first identifying shared memory accesses that are affected by the changes, and then exploring only those problematic interleavings that contain at least one of these accesses. We have implemented ReConTest as an automated tool and evaluated it using 13 real-world concurrent program subjects. Our results show that ReConTest can significantly reduce the regression testing cost without missing any faulty interleavings induced by code changes.},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {246–256},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@article{10.1145/3428233,
author = {Zhang, Hengchu and Roth, Edo and Haeberlen, Andreas and Pierce, Benjamin C. and Roth, Aaron},
title = {Testing Differential Privacy with Dual Interpreters},
year = {2020},
issue_date = {November 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {OOPSLA},
url = {https://doi.org/10.1145/3428233},
doi = {10.1145/3428233},
abstract = {Applying differential privacy at scale requires convenient ways to check that programs computing with sensitive data appropriately preserve privacy. We propose here a fully automated framework for testing differential privacy, adapting a well-known “pointwise” technique from informal proofs of differential privacy. Our framework, called DPCheck, requires no programmer annotations, handles all previously verified or tested algorithms, and is the first fully automated framework to distinguish correct and buggy implementations of PrivTree, a probabilistically terminating algorithm that has not previously been mechanically checked. We analyze the probability of DPCheck mistakenly accepting a non-private program and prove that, theoretically, the probability of false acceptance can be made exponentially small by suitable choice of test size. We demonstrate DPCheck’s utility empirically by implementing all benchmark algorithms from prior work on mechanical verification of differential privacy, plus several others and their incorrect variants, and show DPCheck accepts the correct implementations and rejects the incorrect variants. We also demonstrate how DPCheck can be deployed in a practical workflow to test differentially privacy for the 2020 US Census Disclosure Avoidance System (DAS).},
journal = {Proc. ACM Program. Lang.},
month = {nov},
articleno = {165},
numpages = {26},
keywords = {symbolic execution, Differential privacy, testing}
}

@inproceedings{10.1145/3468264.3473920,
author = {Ayerdi, Jon and Terragni, Valerio and Arrieta, Aitor and Tonella, Paolo and Sagardui, Goiuria and Arratibel, Maite},
title = {Generating Metamorphic Relations for Cyber-Physical Systems with Genetic Programming: An Industrial Case Study},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3473920},
doi = {10.1145/3468264.3473920},
abstract = {One of the major challenges in the verification of complex industrial Cyber-Physical Systems is the difficulty of determining whether a particular system output or behaviour is correct or not, the so-called test oracle problem. Metamorphic testing alleviates the oracle problem by reasoning on the relations that are expected to hold among multiple executions of the system under test, which are known as Metamorphic Relations (MRs). However, the development of effective MRs is often challenging and requires the involvement of domain experts. In this paper, we present a case study aiming at automating this process. To this end, we implemented GAssertMRs, a tool to automatically generate MRs with genetic programming. We assess the cost-effectiveness of this tool in the context of an industrial case study from the elevation domain. Our experimental results show that in most cases GAssertMRs outperforms the other baselines, including manually generated MRs developed with the help of domain experts. We then describe the lessons learned from our experiments and we outline the future work for the adoption of this technique by industrial practitioners.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1264–1274},
numpages = {11},
keywords = {oracle generation, metamorphic testing, quality of service, mutation testing, evolutionary algorithm, genetic programming, oracle improvement, cyber physical systems},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@article{10.1145/3408056,
author = {Krishnamurthi, Shriram and Fisler, Kathi},
title = {Data-Centricity: A Challenge and Opportunity for Computing Education},
year = {2020},
issue_date = {August 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {63},
number = {8},
issn = {0001-0782},
url = {https://doi.org/10.1145/3408056},
doi = {10.1145/3408056},
abstract = {Rethinking the content of introductory computing around a data-centric approach to better engage and support a diversity of students.},
journal = {Commun. ACM},
month = {jul},
pages = {24–26},
numpages = {3}
}

@inproceedings{10.1145/3053600.3053614,
author = {Kaczmarski, Maciej and Perry, Philip and Murphy, John and Portillo-Dominguez, A. Omar},
title = {In-Test Adaptation of Workload in Enterprise Application Performance Testing},
year = {2017},
isbn = {9781450348997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3053600.3053614},
doi = {10.1145/3053600.3053614},
abstract = {Performance testing is used to assess if an enterprise application can fulfil its expected Service Level Agreements. However, since some performance issues depend on the input workloads, it is common to use time-consuming and complex iterative test methods, which heavily rely on human expertise. This paper presents an automated approach to dynamically adapt the workload so that issues (e.g. bottlenecks) can be identified more quickly as well as with less effort and expertise. We present promising results from an initial validation prototype indicating an 18-fold decrease in the test time without compromising the accuracy of the test results, while only introducing a marginal overhead in the system.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering Companion},
pages = {69–72},
numpages = {4},
keywords = {engineering, performance, testing, analysis},
location = {L'Aquila, Italy},
series = {ICPE '17 Companion}
}

@article{10.1145/353485.353486,
author = {Ritter, Frank E. and Baxter, Gordon D. and Jones, Gary and Young, Richard M.},
title = {Supporting Cognitive Models as Users},
year = {2000},
issue_date = {June 2000},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {2},
issn = {1073-0516},
url = {https://doi.org/10.1145/353485.353486},
doi = {10.1145/353485.353486},
abstract = {Cognitive models are computer programs that simulate human performance of cognitive skills. They have been useful to HCI by predicting task times, by assisting users, and by acting as surrogate users. If cognitive models could interact with the same interfaces that users do, the models would be easier to develop and would be easier to apply as interface testers. This approach can be encapsulated as a cognitive model interface management system (CMIMS), which is analogous to and based on a user interface management system (UIMS). We present five case studies using three different UIMSes. These show how models can interact with interfaces using an interaction mechanism that is designed to apply to all interfaces generated within a UIMS. These interaction mechanisms start to support and  constrain performance in the same ways that human performance is supported and constrained by interaction. Most existing UIMSes can and should be extended to create CMIMSes, and models can and should use CMIMSes to look at larger and more complex tasks. CMIMSes will help to further exploit the synergy between the disciplines of cognitive modeling and HCI by supporting cognitive models as users.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {jun},
pages = {141–173},
numpages = {33},
keywords = {usability engineering, cognitive modeling}
}

@inproceedings{10.1145/3408877.3432462,
author = {Blaine, Raymond W. and Blair, Jean R. S. and Chewar, Christa M. and Harrison, Rob and Raftery, James J. and Sobiesk, Edward},
title = {Creating a Multifarious Cyber Science Major},
year = {2021},
isbn = {9781450380621},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3408877.3432462},
doi = {10.1145/3408877.3432462},
abstract = {Existing approaches to computing-based cyber undergraduate majors typically take one of two forms: a broad exploration of both technical and human aspects, or a deep technical exploration of a single discipline relevant to cybersecurity. This paper describes the creation of a third approach--a multifarious major, consistent with Cybersecurity Curricula 2017, the ABET Cybersecurity Program Criteria, and the National Security Agency Center for Academic Excellence--Cyber Operations criteria. Our novel curriculum relies on a 10-course common foundation extended by one of five possible concentrations, each of which is delivered through a disciplinary lens and specialized into a highly relevant computing interest area serving society's diverse cyber needs. The journey began years ago when we infused cybersecurity education throughout our programs, seeking to keep offerings and extracurricular activities relevant in society's increasingly complex relationship with cyberspace. This paper details the overarching design principles, decision-making process, benchmarking, and feedback elicitation activities. A surprising key step was merging several curricula proposals into a single hybrid option. The new major attracted a strong initial cohort, meeting our enrollment goals and exceeding our diversity goals. We provide several recommendations for any institution embarking on a process of designing a new cyber-named major.},
booktitle = {Proceedings of the 52nd ACM Technical Symposium on Computer Science Education},
pages = {1205–1211},
numpages = {7},
keywords = {machine learning, cyber operations, curriculum development, cybersecurity, cyber science, cyber-physical systems, network services},
location = {Virtual Event, USA},
series = {SIGCSE '21}
}

@inproceedings{10.1145/1596655.1596682,
author = {Mostafa, Nagy and Krintz, Chandra},
title = {Tracking Performance across Software Revisions},
year = {2009},
isbn = {9781605585987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1596655.1596682},
doi = {10.1145/1596655.1596682},
abstract = {Repository-based revision control systems such as CVS, RCS, Subversion, and GIT, are extremely useful tools that enable software developers to concurrently modify source code, manage conflicting changes, and commit updates as new revisions. Such systems facilitate collaboration with and concurrent contribution to shared source code by large developer bases. In this work, we investigate a framework for "performance-aware" repository and revision control for Java programs. Our system automatically tracks behavioral differences across revisions to provide developers with feedback as to how their change impacts performance of the application. It does so as part of the repository commit process by profiling the performance of the program or component, and performing automatic analyses that identify differences in the dynamic behavior or performance between two code revisions.In this paper, we present our system that is based upon and extends prior work on calling context tree (CCT) profiling and performance differencing. Our framework couples the use of precise CCT information annotated with performance metrics and call-site information, with a simple tree comparison technique and novel heuristics that together target the cause of performance differences between code revisions without knowledge of program semantics. We evaluate the efficacy of the framework using a number of open source Java applications and present a case study in which we use the framework to distinguish two revisions of the popular FindBugs application.},
booktitle = {Proceedings of the 7th International Conference on Principles and Practice of Programming in Java},
pages = {162–171},
numpages = {10},
keywords = {performance-aware revision control, calling context tree, profiling},
location = {Calgary, Alberta, Canada},
series = {PPPJ '09}
}

@inproceedings{10.1145/2786805.2786850,
author = {Vasilescu, Bogdan and Yu, Yue and Wang, Huaimin and Devanbu, Premkumar and Filkov, Vladimir},
title = {Quality and Productivity Outcomes Relating to Continuous Integration in GitHub},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786850},
doi = {10.1145/2786805.2786850},
abstract = { Software processes comprise many steps; coding is followed by building, integration testing, system testing, deployment, operations, among others. Software process integration and automation have been areas of key concern in software engineering, ever since the pioneering work of Osterweil; market pressures for Agility, and open, decentralized, software development have provided additional pressures for progress in this area. But do these innovations actually help projects? Given the numerous confounding factors that can influence project performance, it can be a challenge to discern the effects of process integration and automation. Software project ecosystems such as GitHub provide a new opportunity in this regard: one can readily find large numbers of projects in various stages of process integration and automation, and gather data on various influencing factors as well as productivity and quality outcomes. In this paper we use large, historical data on process metrics and outcomes in GitHub projects to discern the effects of one specific innovation in process automation: continuous integration. Our main finding is that continuous integration improves the productivity of project teams, who can integrate more outside contributions, without an observable diminishment in code quality. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {805–816},
numpages = {12},
keywords = {pull requests, GitHub, Continuous integration},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3180155.3180210,
author = {Miranda, Breno and Cruciani, Emilio and Verdecchia, Roberto and Bertolino, Antonia},
title = {FAST Approaches to Scalable Similarity-Based Test Case Prioritization},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180210},
doi = {10.1145/3180155.3180210},
abstract = {Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {222–232},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.5555/3291168.3291173,
author = {Alquraan, Ahmed and Takruri, Hatem and Alfatafta, Mohammed and Al-Kiswany, Samer},
title = {An Analysis of Network-Partitioning Failures in Cloud Systems},
year = {2018},
isbn = {9781931971478},
publisher = {USENIX Association},
address = {USA},
abstract = {We present a comprehensive study of 136 system failures attributed to network-partitioning faults from 25 widely used distributed systems. We found that the majority of the failures led to catastrophic effects, such as data loss, reappearance of deleted data, broken locks, and system crashes. The majority of the failures can easily manifest once a network partition occurs: They require little to no client input, can be triggered by isolating a single node, and are deterministic. However, the number of test cases that one must consider is extremely large. Fortunately, we identify ordering, timing, and network fault characteristics that significantly simplify testing. Furthermore, we found that a significant number of the failures are due to design flaws in core system mechanisms.We found that the majority of the failures could have been avoided by design reviews, and could have been discovered by testing with network-partitioning fault injection. We built NEAT, a testing framework that simplifies the coordination of multiple clients and can inject different types of network-partitioning faults. We used NEAT to test seven popular systems and found and reported 32 failures.},
booktitle = {Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation},
pages = {51–68},
numpages = {18},
location = {Carlsbad, CA, USA},
series = {OSDI'18}
}

@article{10.1145/2018396.2018415,
author = {Kennedy, Ken and Koelbel, Charles and Zima, Hans},
title = {The Rise and Fall of High Performance Fortran},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2018396.2018415},
doi = {10.1145/2018396.2018415},
abstract = {HPF pioneered a high-level approach to parallel programming but failed to win over a broad user community.},
journal = {Commun. ACM},
month = {nov},
pages = {74–82},
numpages = {9}
}

@inproceedings{10.1145/2814270.2814282,
author = {Jensen, Casper S. and M\o{}ller, Anders and Raychev, Veselin and Dimitrov, Dimitar and Vechev, Martin},
title = {Stateless Model Checking of Event-Driven Applications},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814282},
doi = {10.1145/2814270.2814282},
abstract = { Modern event-driven applications, such as, web pages and mobile apps, rely on asynchrony to ensure smooth end-user experience. Unfortunately, even though these applications are executed by a single event-loop thread, they can still exhibit nondeterministic behaviors depending on the execution order of interfering asynchronous events. As in classic shared-memory concurrency, this nondeterminism makes it challenging to discover errors that manifest only in specific schedules of events. In this work we propose the first stateless model checker for event-driven applications, called R4. Our algorithm systematically explores the nondeterminism in the application and concisely exposes its overall effect, which is useful for bug discovery. The algorithm builds on a combination of three key insights: (i) a dynamic partial order reduction (DPOR) technique for reducing the search space, tailored to the domain of event-driven applications, (ii) conflict-reversal bounding based on a hypothesis that most errors occur with a small number of event reorderings, and (iii) approximate replay of event sequences, which is critical for separating harmless from harmful nondeterminism. We instantiate R4 for the domain of client-side web applications and use it to analyze event interference in a number of real-world programs. The experimental results indicate that the precision and overall exploration capabilities of our system significantly exceed that of existing techniques. },
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {57–73},
numpages = {17},
keywords = {web applications, event-driven applications, partial order reduction, Model checking, data races},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814282,
author = {Jensen, Casper S. and M\o{}ller, Anders and Raychev, Veselin and Dimitrov, Dimitar and Vechev, Martin},
title = {Stateless Model Checking of Event-Driven Applications},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814282},
doi = {10.1145/2858965.2814282},
abstract = { Modern event-driven applications, such as, web pages and mobile apps, rely on asynchrony to ensure smooth end-user experience. Unfortunately, even though these applications are executed by a single event-loop thread, they can still exhibit nondeterministic behaviors depending on the execution order of interfering asynchronous events. As in classic shared-memory concurrency, this nondeterminism makes it challenging to discover errors that manifest only in specific schedules of events. In this work we propose the first stateless model checker for event-driven applications, called R4. Our algorithm systematically explores the nondeterminism in the application and concisely exposes its overall effect, which is useful for bug discovery. The algorithm builds on a combination of three key insights: (i) a dynamic partial order reduction (DPOR) technique for reducing the search space, tailored to the domain of event-driven applications, (ii) conflict-reversal bounding based on a hypothesis that most errors occur with a small number of event reorderings, and (iii) approximate replay of event sequences, which is critical for separating harmless from harmful nondeterminism. We instantiate R4 for the domain of client-side web applications and use it to analyze event interference in a number of real-world programs. The experimental results indicate that the precision and overall exploration capabilities of our system significantly exceed that of existing techniques. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {57–73},
numpages = {17},
keywords = {Model checking, partial order reduction, event-driven applications, data races, web applications}
}

@article{10.1145/3361146,
author = {Hierons, Robert M. and Li, Miqing and Liu, Xiaohui and Parejo, Jose Antonio and Segura, Sergio and Yao, Xin},
title = {Many-Objective Test Suite Generation for Software Product Lines},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {1049-331X},
url = {https://doi.org/10.1145/3361146},
doi = {10.1145/3361146},
abstract = {A Software Product Line (SPL) is a set of products built from a number of features, the set of valid products being defined by a feature model. Typically, it does not make sense to test all products defined by an SPL and one instead chooses a set of products to test (test selection) and, ideally, derives a good order in which to test them (test prioritisation). Since one cannot know in advance which products will reveal faults, test selection and prioritisation are normally based on objective functions that are known to relate to likely effectiveness or cost. This article introduces a new technique, the grid-based evolution strategy (GrES), which considers several objective functions that assess a selection or prioritisation and aims to optimise on all of these. The problem is thus a many-objective optimisation problem. We use a new approach, in which all of the objective functions are considered but one (pairwise coverage) is seen as the most important. We also derive a novel evolution strategy based on domain knowledge. The results of the evaluation, on randomly generated and realistic feature models, were promising, with GrES outperforming previously proposed techniques and a range of many-objective optimisation algorithms.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jan},
articleno = {2},
numpages = {46},
keywords = {test selection, multi-objective optimisation, test prioritisation, Software product line}
}

@article{10.1145/2465787.2465799,
author = {Uzelac, Vladimir and Milenkovi\'{c}, Aleksandar},
title = {Hardware-Based Load Value Trace Filtering for On-the-Fly Debugging},
year = {2013},
issue_date = {May 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {2s},
issn = {1539-9087},
url = {https://doi.org/10.1145/2465787.2465799},
doi = {10.1145/2465787.2465799},
abstract = {Capturing program and data traces during program execution unobtrusively on-the-fly is crucial in debugging and testing of cyber-physical systems. However, tracing a complete program unobtrusively is often cost-prohibitive, requiring large on-chip trace buffers and wide trace ports. This article describes a new hardware-based load data value filtering technique called Cache First-access Tracking. Coupled with an effective variable encoding scheme, this technique achieves a significant reduction of load data value traces, from 5.86 to 56.39 times depending on the data cache size, thus enabling cost-effective, unobtrusive on-the-fly tracing and debugging.},
journal = {ACM Trans. Embed. Comput. Syst.},
month = {may},
articleno = {97},
numpages = {18},
keywords = {Debugging, trace compression, variable encoding, software debugger, load value filtering, trace module, program tracing}
}

@inproceedings{10.1145/2970276.2970351,
author = {Ohmann, Peter and Brown, David Bingham and Neelakandan, Naveen and Linderoth, Jeff and Liblit, Ben},
title = {Optimizing Customized Program Coverage},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970351},
doi = {10.1145/2970276.2970351},
abstract = { Program coverage is used across many stages of software development. While common during testing, program coverage has also found use outside the test lab, in production software. However, production software has stricter requirements on run-time overheads, and may limit possible program instrumentation. Thus, optimizing the placement of probes to gather program coverage is important.  We introduce and study the problem of customized program coverage optimization. We generalize previous work that optimizes for complete coverage instrumentation with a system that adapts optimization to customizable program coverage requirements. Specifically, our system allows a user to specify desired coverage locations and to limit legal instrumentation locations. We prove that the problem of determining optimal coverage probes is NP-hard, and we present a solution based on mixed integer linear programming. Due to the computational complexity of the problem, we also provide two practical approximation approaches. We evaluate the effectiveness of our approximations across a diverse set of benchmarks, and show that our techniques can substantially reduce instrumentation while allowing the user immense freedom in defining coverage requirements. When naive instrumentation is dense or expensive, our optimizations succeed in lowering execution time overheads. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {27–38},
numpages = {12},
keywords = {Debugging, program coverage, mixed integer linear optimization},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3180155.3180184,
author = {Patra, Jibesh and Dixit, Pooja N. and Pradel, Michael},
title = {ConflictJS: Finding and Understanding Conflicts between JavaScript Libraries},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180184},
doi = {10.1145/3180155.3180184},
abstract = {It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {741–751},
numpages = {11},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/508791.508833,
author = {Bella, Giampaolo and Menezes, Ronaldo and Whittaker, James},
title = {Editorial Message: Special Track on Computer Security},
year = {2002},
isbn = {1581134452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/508791.508833},
doi = {10.1145/508791.508833},
abstract = {The proliferation of network computing and especially the ubiquity of the Internet has made security one of the key areas in modern computing. This track aims at bringing together researchers working on applied issues in computer and information security, ranging from protocols and tools to policy and laws.Security is a multidisciplinary topic related to almost every aspect of computer science. From information security to mobile computation, from artificial intelligence to wireless communication, it is difficult to find a computer science area that is not concerned with security directly or indirectly.The Computer Security Track in the 2002 Symposium on Applied Computing is a demonstration of the diversity. The track focus is on practical (applied) aspects of computer security so as to fit well with the general objectives of the symposium.},
booktitle = {Proceedings of the 2002 ACM Symposium on Applied Computing},
pages = {194–195},
numpages = {2},
location = {Madrid, Spain},
series = {SAC '02}
}

@inproceedings{10.1145/326490.326508,
author = {Hathorn, Frederick C.},
title = {Structured Tasks},
year = {1989},
isbn = {9781450373203},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/326490.326508},
doi = {10.1145/326490.326508},
booktitle = {Proceedings of the Sixth Washington Ada Symposium on Ada},
pages = {63–74},
numpages = {12},
location = {McLean, Virginia, USA},
series = {WADAS '89}
}

