@inproceedings{10.1145/1831708.1831733,
author = {Santelices, Raul and Harrold, Mary Jean},
title = {Exploiting Program Dependencies for Scalable Multiple-Path Symbolic Execution},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831733},
doi = {10.1145/1831708.1831733},
abstract = {This paper presents a new technique, called Symbolic Program Decomposition (or SPD), for symbolic execution of multiple paths that is more scalable than existing techniques, which symbolically execute control-flow paths individually. SPD exploits control and data dependencies to avoid analyzing unnecessary combinations of subpaths. SPD can also compute an over-approximation of symbolic execution by abstracting away symbolic subterms arbitrarily, to further scale the analysis at the cost of precision. The paper also presents our implementation and empirical evaluation showing that SPD can achieve savings of orders of magnitude in the path-exploration costs of multiple-path symbolic execution. Finally, the paper presents a study that examines the use of SPD for a particular application: change analysis for test-suite augmentation.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {195–206},
numpages = {12},
keywords = {invariant detection, program analysis, path sensitive analysis, test generation, modular analysis, symbolic execution, data dependence, control dependence, change analysis, verification, path family, test suite augmentation, path condition},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/3395363.3397365,
author = {Kadron, undefinedsmet Burak and Rosner, Nicol\'{a}s and Bultan, Tevfik},
title = {Feedback-Driven Side-Channel Analysis for Networked Applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397365},
doi = {10.1145/3395363.3397365},
abstract = {Information leakage in software systems is a problem of growing importance. Networked applications can leak sensitive information even when they use encryption. For example, some characteristics of network packets, such as their size, timing and direction, are visible even for encrypted traffic. Patterns in these characteristics can be leveraged as side channels to extract information about secret values accessed by the application. In this paper, we present a new tool called AutoFeed for detecting and quantifying information leakage due to side channels in networked software applications. AutoFeed profiles the target system and automatically explores the input space, explores the space of output features that may leak information, quantifies the information leakage, and identifies the top-leaking features. Given a set of input mutators and a small number of initial inputs provided by the user, AutoFeed iteratively mutates inputs and periodically updates its leakage estimations to identify the features that leak the greatest amount of information about the secret of interest. AutoFeed uses a feedback loop for incremental profiling, and a stopping criterion that terminates the analysis when the leakage estimation for the top-leaking features converges. AutoFeed also automatically assigns weights to mutators in order to focus the search of the input space on exploring dimensions that are relevant to the leakage quantification. Our experimental evaluation on the benchmarks shows that AutoFeed is effective in detecting and quantifying information leaks in networked applications.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {260–271},
numpages = {12},
keywords = {network traffic analysis, input generation, dynamic program analysis, Side-channel analysis},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3395363.3397346,
author = {Lee, Seokhyun and Cha, Sooyoung and Lee, Dain and Oh, Hakjoo},
title = {Effective White-Box Testing of Deep Neural Networks with Adaptive Neuron-Selection Strategy},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397346},
doi = {10.1145/3395363.3397346},
abstract = {We present Adapt, a new white-box testing technique for deep neural networks. As deep neural networks are increasingly used in safety-first applications, testing their behavior systematically has become a critical problem. Accordingly, various testing techniques for deep neural networks have been proposed in recent years. However, neural network testing is still at an early stage and existing techniques are not yet sufficiently effective. In this paper, we aim to advance this field, in particular white-box testing approaches for neural networks, by identifying and addressing a key limitation of existing state-of-the-arts. We observe that the so-called neuron-selection strategy is a critical component of white-box testing and propose a new technique that effectively employs the strategy by continuously adapting it to the ongoing testing process. Experiments with real-world network models and datasets show that Adapt is remarkably more effective than existing testing techniques in terms of coverage and adversarial inputs found.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {165–176},
numpages = {12},
keywords = {White-box testing, Online learning, Deep neural networks},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/1831708.1831710,
author = {Godefroid, Patrice and Kinder, Johannes},
title = {Proving Memory Safety of Floating-Point Computations by Combining Static and Dynamic Program Analysis},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831710},
doi = {10.1145/1831708.1831710},
abstract = {Whitebox fuzzing is a novel form of security testing based on dynamic symbolic execution and constraint solving. Over the last couple of years, whitebox fuzzers have found many new security vulnerabilities (buffer overflows) in Windows and Linux applications, including codecs, image viewers and media players. Those types of applications tend to use floating-point instructions available on modern processors, yet existing whitebox fuzzers and SMT constraint solvers do not handle floating-point arithmetic. Are there new security vulnerabilities lurking in floating-point code?A naive solution would be to extend symbolic execution to floating-point (FP) instructions (months of work), extend SMT solvers to reason about FP constraints (months of work or more), and then face more complex constraints and an even worse path explosion problem. Instead, we propose an alternative approach, based on the rough intuition that FP code should only perform memory safe data-processing of the "payload" of an image or video file, while the non-FP part of the application should deal with buffer allocations and memory address computations, with only the latter being prone to buffer overflows and other security critical bugs. Our approach combines (1) a lightweight local path-insensitive "may" static analysis of FP instructions with (2) a high-precision whole-program path-sensitive "must" dynamic analysis of non-FP instructions. The aim of this combination is to prove memory safety of the FP part of each execution and a form of non-interference between the FP part and the non-FP part with respect to memory address computations.We have implemented our approach using two existing tools for, respectively, static and dynamic x86 binary analysis. We present preliminary results of experiments with standard JPEG, GIF and ANI Windows parsers. For a given test suite of diverse input files, our mixed static/dynamic analysis is able to prove memory safety of FP code in those parsers for a small upfront static analysis cost and a marginal runtime expense compared to regular dynamic symbolic execution.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {1–12},
numpages = {12},
keywords = {program verification, static and dynamic program analysis},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/1572272.1572291,
author = {Sinha, Saurabh and Shah, Hina and G\"{o}rg, Carsten and Jiang, Shujuan and Kim, Mijung and Harrold, Mary Jean},
title = {Fault Localization and Repair for Java Runtime Exceptions},
year = {2009},
isbn = {9781605583389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1572272.1572291},
doi = {10.1145/1572272.1572291},
abstract = {This paper presents a new approach for locating and repairing faults that cause runtime exceptions in Java programs. The approach handles runtime exceptions that involve a flow of an incorrect value that finally leads to the exception. This important class of exceptions includes exceptions related to dereferences of null pointers, arithmetic faults (e.g., ArithmeticException), and type faults (e.g., ArrayStoreException). Given a statement at which such an exception occurred, the technique combines dynamic analysis (using stack-trace information) with static backward data-flow analysis (beginning at the point where the runtime exception occurred) to identify the source statement at which an incorrect assignment was made; this information is required to locate the fault. The approach also identifies the source statements that may cause this same exception on other executions, along with the reference statements that may raise an exception in other executions because of this incorrect assignment; this information is required to repair the fault. The paper also presents an application of our technique to null pointer exceptions. Finally, the paper describes an implementation of the null-pointer-exception analysis and a set of studies that demonstrate the advantages of our approach for locating and repairing faults in the program.},
booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
pages = {153–164},
numpages = {12},
keywords = {static analysis, runtime exceptions, fault localization, null dereference},
location = {Chicago, IL, USA},
series = {ISSTA '09}
}

@inproceedings{10.1145/1565799.1565821,
author = {Milani, Masoud and Sadjadi, S. Masoud and Rangaswami, Raju and Clarke, Peter J. and Li, Tao},
title = {Research Experiences for Undergraduates: Autonomic Computing Research at FIU},
year = {2009},
isbn = {9781605582177},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1565799.1565821},
doi = {10.1145/1565799.1565821},
abstract = {According to Computing Research Association, during each year between 2003 and 2007, fewer than 3% of the US's Ph.D.s graduates in computer science and computer engineering were Hispanic or African American and fewer than 20% were women. Such an under-representation precludes the benefits of diversity in computer sciences research and industry and consequently compromises the competitiveness of the US economy. It is therefore imperative that undergraduate institutions introduce students from these groups to research at an early stage of their academic careers and to provide them with the tools necessary for success in graduate school. The School of Computing and Information Sciences (SCIS) at Florida International University (FIU) has been working to strengthen the pipeline of underrepresented students to graduate work in computer science by hosting an NSF sponsored Research Experiences for Undergraduates (REU) site for the past three years. Our REU site has hosted 30 undergraduate students, 23 of them were underrepresented including 8 females, 16 Hispanics, and 4 African Americans, who published 13 technical papers. Six of the ten students who have already graduated, have started their graduate studies.},
booktitle = {The Fifth Richard Tapia Celebration of Diversity in Computing Conference: Intellect, Initiatives, Insight, and Innovations},
pages = {93–97},
numpages = {5},
keywords = {underrepresented students, REU site, autonomic computing, research experiences for undergraduates},
location = {Portland, Oregon},
series = {TAPIA '09}
}

@inproceedings{10.1145/2483760.2483780,
author = {Li, Ding and Hao, Shuai and Halfond, William G. J. and Govindan, Ramesh},
title = {Calculating Source Line Level Energy Information for Android Applications},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483780},
doi = {10.1145/2483760.2483780},
abstract = { The popularity of mobile apps continues to grow as developers take advantage of the sensors and data available on mobile devices. However, the increased functionality comes with a higher energy cost, which can cause a problem for users on battery constrained mobile devices. To improve the energy consumption of mobile apps, developers need detailed information about the energy consumption of their applications. Existing techniques have drawbacks that limit their usefulness or provide information at too high of a level of granularity, such as components or methods. Our approach is able to calculate source line level energy consumption information. It does this by combining hardware-based power measurements with program analysis and statistical modeling. Our empirical evaluation of the approach shows that it is fast and accurate. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {78–89},
numpages = {12},
keywords = {Source line level, Android app, Energy measurement},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inproceedings{10.1145/1390630.1390648,
author = {Lincke, R\"{u}diger and Lundberg, Jonas and L\"{o}we, Welf},
title = {Comparing Software Metrics Tools},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390648},
doi = {10.1145/1390630.1390648},
abstract = {This paper shows that existing software metric tools interpret and implement the definitions of object-oriented software metrics differently. This delivers tool-dependent metrics results and has even implications on the results of analyses based on these metrics results. In short, the metrics-based assessment of a software system and measures taken to improve its design differ considerably from tool to tool. To support our case, we conducted an experiment with a number of commercial and free metrics tools. We calculated metrics values using the same set of standard metrics for three software systems of different sizes. Measurements show that, for the same software system and metrics, the metrics values are tool depended. We also defined a (simple) software quality model for "maintainability" based on the metrics selected. It defines a ranking of the classes that are most critical wrt. maintainability. Measurements show that even the ranking of classes in a software system is metrics tool dependent.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {131–142},
numpages = {12},
keywords = {software quality metrics, comparing tools},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@inproceedings{10.1145/3395363.3397369,
author = {Lutellier, Thibaud and Pham, Hung Viet and Pang, Lawrence and Li, Yitong and Wei, Moshi and Tan, Lin},
title = {CoCoNuT: Combining Context-Aware Neural Translation Models Using Ensemble for Program Repair},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397369},
doi = {10.1145/3395363.3397369},
abstract = {Automated generate-and-validate (GV) program repair techniques (APR) typically rely on hard-coded rules, thus only fixing bugs following specific fix patterns. These rules require a significant amount of manual effort to discover and it is hard to adapt these rules to different programming languages. To address these challenges, we propose a new G&amp;V technique—CoCoNuT, which uses ensemble learning on the combination of convolutional neural networks (CNNs) and a new context-aware neural machine translation (NMT) architecture to automatically fix bugs in multiple programming languages. To better represent the context of a bug, we introduce a new context-aware NMT architecture that represents the buggy source code and its surrounding context separately. CoCoNuT uses CNNs instead of recurrent neural networks (RNNs), since CNN layers can be stacked to extract hierarchical features and better model source code at different granularity levels (e.g., statements and functions). In addition, CoCoNuT takes advantage of the randomness in hyperparameter tuning to build multiple models that fix different bugs and combines these models using ensemble learning to fix more bugs. Our evaluation on six popular benchmarks for four programming languages (Java, C, Python, and JavaScript) shows that CoCoNuT correctly fixes (i.e., the first generated patch is semantically equivalent to the developer’s patch) 509 bugs, including 309 bugs that are fixed by none of the 27 techniques with which we compare.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {101–114},
numpages = {14},
keywords = {AI and Software Engineering, Automated program repair, Deep Learning, Neural Machine Translation},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inbook{10.1145/3293882.3338988,
author = {Fu, Xiaoqin},
title = {Towards Scalable Defense of Information Flow Security for Distributed Systems},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338988},
abstract = {It is particularly challenging to defend common distributed systems against security vulnerabilities because of the complexity and their large sizes. However, traditional solutions, that attack the information flow security problem, often fail for large, complex real-world distributed systems due to scalability problems. The problem would be even exacerbated for the online defense of continuously-running systems. My proposed research consists of three connected themes. First, I have developed metrics to help users understand and analyze the security characteristics of distributed systems at runtime in relation to their coupling measures. Then, I have also developed a highly scalable, cost-effective dynamic information flow analysis approach for distributed systems. It can detect implicit dependencies and find real security vulnerabilities in industrial distributed systems with practical portability and scalability. In order to thoroughly solve the scalability problem in general scenarios, I am developing a self-adaptive dynamic dependency analysis framework to monitor security issues during continuous running. In this proposal, I outline the three projects in a related manner as to how they consistently target the central objective of my thesis research.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {438–442},
numpages = {5}
}

@inbook{10.1145/3293882.3338990,
author = {Lee, Sungho},
title = {JNI Program Analysis with Automatically Extracted C Semantic Summary},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3338990},
abstract = {From Oracle JVM to Android Runtime, most Java runtime environments officially support Java Native Interface (JNI) for interaction between Java and C. Using JNI, developers can improve Java program performance or reuse existing libraries implemented in C. At the same time, differences between the languages can lead to various kinds of unexpected bugs when developers do not understand the differences or comprehensive interoperation semantics completely. Furthermore, existing program analysis techniques do not cover the interoperation, which can reduce the quality of JNI programs.  We propose a JNI program analysis technique that analyzes Java and C code of JNI programs using analyzers targeting each language respectively. The C analyzer generates a semantic summary for each C function callable from Java and the Java analyzer constructs call graphs using the semantic summaries and Java code. In addition to the call graph construction, we extend the analysis technique to detect four bug types that can occur in the interoperation between the languages. We believe that our approach would be able to detect genuine bugs as well as improve the quality of JNI programs.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {448–451},
numpages = {4}
}

@inproceedings{10.1145/3395363.3397381,
author = {Nie, Pengyu and Celik, Ahmet and Coley, Matthew and Milicevic, Aleksandar and Bell, Jonathan and Gligoric, Milos},
title = {Debugging the Performance of Maven’s Test Isolation: Experience Report},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397381},
doi = {10.1145/3395363.3397381},
abstract = {Testing is the most common approach used in industry for checking software correctness. Developers frequently practice reliable testing-executing individual tests in isolation from each other-to avoid test failures caused by test-order dependencies and shared state pollution (e.g., when tests mutate static fields). A common way of doing this is by running each test as a separate process. Unfortunately, this is known to introduce substantial overhead. This experience report describes our efforts to better understand the sources of this overhead and to create a system to confirm the minimal overhead possible. We found that different build systems use different mechanisms for communicating between these multiple processes, and that because of this design decision, running tests with some build systems could be faster than with others. Through this inquiry we discovered a significant performance bug in Apache Maven’s test running code, which slowed down test execution by on average 350 milliseconds per-test when compared to a competing build system, Ant. When used for testing real projects, this can result in a significant reduction in testing time. We submitted a patch for this bug which has been integrated into the Apache Maven build system, and describe our ongoing efforts to improve Maven’s test execution tooling.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {249–259},
numpages = {11},
keywords = {Build system, test isolation, Maven},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2610384.2610418,
author = {Lanzaro, Anna and Natella, Roberto and Winter, Stefan and Cotroneo, Domenico and Suri, Neeraj},
title = {An Empirical Study of Injected versus Actual Interface Errors},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610418},
doi = {10.1145/2610384.2610418},
abstract = { The reuse of software components is a common practice in commercial applications and increasingly appearing in safety critical systems as driven also by cost considerations. This practice puts dependability at risk, as differing operating conditions in different reuse scenarios may expose residual software faults in the components. Consequently, software fault injection techniques are used to assess how residual faults of reused software components may affect the system, and to identify appropriate counter-measures. As fault injection in components’ code suffers from a number of practical disadvantages, it is often replaced by error injection at the component interface level. However, it is still an open issue, whether such injected errors are actually representative of the effects of residual faults. To this end, we propose a method for analyzing how software faults turn into interface errors, with the ultimate aim of supporting more representative interface error injection experiments. Our analysis in the context of widely used software libraries reveals that existing interface error models are not suitable for emulating software faults, and provides useful insights for improving the representativeness of interface error injection. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {397–408},
numpages = {12},
keywords = {Software Faults/Errors, Off-The-Shelf Software, Software Fault/Error Injection, FMECA, Software Components, Experimental Dependability Assessment},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/2338965.2336767,
author = {Zhang, Fangfang and Jhi, Yoon-Chan and Wu, Dinghao and Liu, Peng and Zhu, Sencun},
title = {A First Step towards Algorithm Plagiarism Detection},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336767},
doi = {10.1145/2338965.2336767},
abstract = { In this work, we address the problem of algorithm plagiarism, which occurs when a plagiarist, violating intellectual property rights, steals others' algorithms and covertly implements them. In contrast to software plagiarism, which has been extensively studied, limited attention has been paid to algorithm plagiarism. In this paper, we propose two dynamic value-based approaches, namely N-version and annotation, for algorithm plagiarism detection. Our approaches are motivated by the observation that there exist some critical runtime values which are irreplaceable and uneliminatable for all implementations of the same algorithm. The N-version approach extracts such values by filtering out non-core values. The annotation approach leverages auxiliary information to flag important variables which contain core values. We also propose a value dependence graph based similarity metric in addition to the longest common subsequence based one, in order to address the potential value reordering attack. We have implemented a prototype and evaluated the proposed schemes on various algorithms. The results show that our approaches to algorithm plagiarism detection are practical, effective and resilient to many automatic obfuscation techniques. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {111–121},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/3213846.3213868,
author = {Noller, Yannic and Kersten, Rody and P\u{a}s\u{a}reanu, Corina S.},
title = {Badger: Complexity Analysis with Fuzzing and Symbolic Execution},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213868},
doi = {10.1145/3213846.3213868},
abstract = {Hybrid testing approaches that involve fuzz testing and symbolic execution have shown promising results in achieving high code coverage, uncovering subtle errors and vulnerabilities in a variety of software applications. In this paper we describe Badger - a new hybrid approach for complexity analysis, with the goal of discovering vulnerabilities which occur when the worst-case time or space complexity of an application is significantly higher than the average case.  Badger uses fuzz testing to generate a diverse set of inputs that aim to increase not only coverage but also a resource-related cost associated with each path. Since fuzzing may fail to execute deep program paths due to its limited knowledge about the conditions that influence these paths, we complement the analysis with a symbolic execution, which is also customized to search for paths that increase the resource-related cost. Symbolic execution is particularly good at generating inputs that satisfy various program conditions but by itself suffers from path explosion. Therefore, Badger uses fuzzing and symbolic execution in tandem, to leverage their benefits and overcome their weaknesses.  We implemented our approach for the analysis of Java programs, based on Kelinci and Symbolic PathFinder. We evaluated Badger on Java applications, showing that our approach is significantly faster in generating worst-case executions compared to fuzzing or symbolic execution on their own.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {322–332},
numpages = {11},
keywords = {Fuzzing, Symbolic Execution, Complexity Analysis, Denial-of-Service},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3395363.3397348,
author = {Mathis, Bj\"{o}rn and Gopinath, Rahul and Zeller, Andreas},
title = {Learning Input Tokens for Effective Fuzzing},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397348},
doi = {10.1145/3395363.3397348},
abstract = {Modern fuzzing tools like AFL operate at a lexical level: They explore the input space of tested programs one byte after another. For inputs with complex syntactical properties, this is very inefficient, as keywords and other tokens have to be composed one character at a time. Fuzzers thus allow to specify dictionaries listing possible tokens the input can be composed from; such dictionaries speed up fuzzers dramatically. Also, fuzzers make use of dynamic tainting to track input tokens and infer values that are expected in the input validation phase. Unfortunately, such tokens are usually implicitly converted to program specific values which causes a loss of the taints attached to the input data in the lexical phase. In this paper, we present a technique to extend dynamic tainting to not only track explicit data flows but also taint implicitly converted data without suffering from taint explosion. This extension makes it possible to augment existing techniques and automatically infer a set of tokens and seed inputs for the input language of a program given nothing but the source code. Specifically targeting the lexical analysis of an input processor, our lFuzzer test generator systematically explores branches of the lexical analysis, producing a set of tokens that fully cover all decisions seen. The resulting set of tokens can be directly used as a dictionary for fuzzing. Along with the token extraction seed inputs are generated which give further fuzzing processes a head start. In our experiments, the lFuzzer-AFL combination achieves up to 17% more coverage on complex input formats like json, lisp, tinyC, and JavaScript compared to AFL.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {27–37},
numpages = {11},
keywords = {parser, fuzzing, test input generation},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inbook{10.1109/ICSE-Companion52605.2021.00045,
author = {Berend, David},
title = {Distribution Awareness for AI System Testing},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00045},
abstract = {As Deep Learning (DL) is continuously adopted in many safety critical applications, its quality and reliability start to raise concerns. Similar to the traditional software development process, testing the DL software to uncover its defects at an early stage is an effective way to reduce risks after deployment. Although recent progress has been made in designing novel testing techniques for DL software, the distribution of generated test data is not taken into consideration. It is therefore hard to judge whether the identified errors are indeed meaningful errors to the DL application. Therefore, we propose a new distribution aware testing technique which aims to generate new unseen test cases relevant to the underlying DL system task. Our results show that this technique is able to filter up to 55.44% of error test case on CIFAR-10 and is 10.05% more effective in enhancing robustness.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {96–98},
numpages = {3}
}

@inproceedings{10.1145/1831708.1831717,
author = {Baah, George K. and Podgurski, Andy and Harrold, Mary Jean},
title = {Causal Inference for Statistical Fault Localization},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831717},
doi = {10.1145/1831708.1831717},
abstract = {This paper investigates the application of causal inference methodology for observational studies to software fault localization based on test outcomes and profiles. This methodology combines statistical techniques for counterfactual inference with causal graphical models to obtain causal-effect estimates that are not subject to severe confounding bias. The methodology applies Pearl's Back-Door Criterion to program dependence graphs to justify a linear model for estimating the causal effect of covering a given statement on the occurrence of failures. The paper also presents the analysis of several proposed-fault localization metrics and their relationships to our causal estimator. Finally, the paper presents empirical results demonstrating that our model significantly improves the effectiveness of fault localization.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {73–84},
numpages = {12},
keywords = {potential outcome model, program analysis, debugging, fault localization, causal inference},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/3395363.3397379,
author = {Riganelli, Oliviero and Mottadelli, Simone Paolo and Rota, Claudio and Micucci, Daniela and Mariani, Leonardo},
title = {Data Loss Detector: Automatically Revealing Data Loss Bugs in Android Apps},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397379},
doi = {10.1145/3395363.3397379},
abstract = {Android apps must work correctly even if their execution is interrupted by external events. For instance, an app must work properly even if a phone call is received, or after its layout is redrawn because the smartphone has been rotated. Since these events may require destroying, when the execution is interrupted, and recreating, when the execution is resumed, the foreground activity of the app, the only way to prevent the loss of state information is to save and restore it. This behavior must be explicitly implemented by app developers, who often miss to implement it properly, releasing apps affected by data loss problems, that is, apps that may lose state information when their execution is interrupted. Although several techniques can be used to automatically generate test cases for Android apps, the obtained test cases seldom include the interactions and the checks necessary to exercise and reveal data loss faults. To address this problem, this paper presents Data Loss Detector (DLD), a test case generation technique that integrates an exploration strategy, data-loss-revealing actions, and two customized oracle strategies for the detection of data loss failures. DLD revealed 75% of the faults in a benchmark of 54 Android app releases affected by 110 known data loss faults, and also revealed unknown data loss problems, outperforming competing approaches.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {141–152},
numpages = {12},
keywords = {test case generation, Android, data loss, validation, mobile apps},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inbook{10.1145/3460319.3464816,
author = {Li, Yuanchun and Zhang, Ziqi and Liu, Bingyan and Yang, Ziyue and Liu, Yunxin},
title = {ModelDiff: Testing-Based DNN Similarity Comparison for Model Reuse Detection},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464816},
abstract = {The knowledge of a deep learning model may be transferred to a student model, leading to intellectual property infringement or vulnerability propagation. Detecting such knowledge reuse is nontrivial because the suspect models may not be white-box accessible and/or may serve different tasks. In this paper, we propose ModelDiff, a testing-based approach to deep learning model similarity comparison. Instead of directly comparing the weights, activations, or outputs of two models, we compare their behavioral patterns on the same set of test inputs. Specifically, the behavioral pattern of a model is represented as a decision distance vector (DDV), in which each element is the distance between the model's reactions to a pair of inputs. The knowledge similarity between two models is measured with the cosine similarity between their DDVs. To evaluate ModelDiff, we created a benchmark that contains 144 pairs of models that cover most popular model reuse methods, including transfer learning, model compression, and model stealing. Our method achieved 91.7% correctness on the benchmark, which demonstrates the effectiveness of using ModelDiff for model reuse detection. A study on mobile deep learning apps has shown the feasibility of ModelDiff on real-world models.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {139–151},
numpages = {13}
}

