@article{10.1145/966221.966232,
author = {Orso, Alessandro and Porter, Adam},
title = {ICSE Workshop on Remote Analysis and Measurement of Software Systems (RAMSS)},
year = {2003},
issue_date = {November 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/966221.966232},
doi = {10.1145/966221.966232},
abstract = {The goal of this one-day workshop was to bring together researchers and practitioners interested in exploring how the characteristics of today's area of computing (e.g., high connectivity, substantial computing power for the average user, higher demand for and expectation of frequent software updates) can be leveraged to improve software quality and performance.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {10},
numpages = {1}
}

@inproceedings{10.1145/1368088.1368136,
author = {Siami Namin, Akbar and Andrews, James H. and Murdoch, Duncan J.},
title = {Sufficient Mutation Operators for Measuring Test Effectiveness},
year = {2008},
isbn = {9781605580791},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1368088.1368136},
doi = {10.1145/1368088.1368136},
abstract = {Mutants are automatically-generated, possibly faulty variants of programs. The mutation adequacy ratio of a test suite is the ratio of non-equivalent mutants it is able to identify to the total number of non-equivalent mutants. This ratio can be used as a measure of test effectiveness. However, it can be expensive to calculate, due to the large number of different mutation operators that have been proposed for generating the mutants.In this paper, we address the problem of finding a small set of mutation operators which is still sufficient for measuring test effectiveness. We do this by defining a statistical analysis procedure that allows us to identify such a set, together with an associated linear model that predicts mutation adequacy with high accuracy. We confirm the validity of our procedure through cross-validation and the application of other, alternative statistical analyses.},
booktitle = {Proceedings of the 30th International Conference on Software Engineering},
pages = {351–360},
numpages = {10},
keywords = {testing effectiveness, mutation analysis},
location = {Leipzig, Germany},
series = {ICSE '08}
}

@inproceedings{10.1145/2635868.2635888,
author = {Goffi, Alberto and Gorla, Alessandra and Mattavelli, Andrea and Pezz\`{e}, Mauro and Tonella, Paolo},
title = {Search-Based Synthesis of Equivalent Method Sequences},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635888},
doi = {10.1145/2635868.2635888},
abstract = { Software components are usually redundant, since their interface offers different operations that are equivalent in their functional behavior. Several reliability techniques exploit this redundancy to either detect or tolerate faults in software. Metamorphic testing, for instance, executes pairs of sequences of operations that are expected to produce equivalent results, and identifies faults in case of mismatching outcomes. Some popular fault tolerance and self-healing techniques execute redundant operations in an attempt to avoid failures at runtime. The common assumption of these techniques, though, is that such redundancy is known a priori. This means that the set of operations that are supposed to be equivalent in a given component should be available in the specifications. Unfortunately, inferring this information manually can be expensive and error prone. This paper proposes a search-based technique to synthesize sequences of method invocations that are equivalent to a target method within a finite set of execution scenarios. The experimental results obtained on 47 methods from 7 classes show that the proposed approach correctly identifies equivalent method sequences in the majority of the cases where redundancy was known to exist, with very few false positives. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {366–376},
numpages = {11},
keywords = {equivalent method sequences, search-based software engineering, specification mining, Redundancy},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/3338906.3338935,
author = {Koyuncu, Anil and Liu, Kui and Bissyand\'{e}, Tegawend\'{e} F. and Kim, Dongsun and Monperrus, Martin and Klein, Jacques and Le Traon, Yves},
title = {IFixR: Bug Report Driven Program Repair},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338935},
doi = {10.1145/3338906.3338935},
abstract = {Issue tracking systems are commonly used in modern software development for collecting feedback from users and developers. An ultimate automation target of software maintenance is then the systematization of patch generation for user-reported bugs. Although this ambition is aligned with the momentum of automated program repair, the literature has, so far, mostly focused on generate-and- validate setups where fault localization and patch generation are driven by a well-defined test suite. On the one hand, however, the common (yet strong) assumption on the existence of relevant test cases does not hold in practice for most development settings: many bugs are reported without the available test suite being able to reveal them. On the other hand, for many projects, the number of bug reports generally outstrips the resources available to triage them. Towards increasing the adoption of patch generation tools by practitioners, we investigate a new repair pipeline, iFixR, driven by bug reports: (1) bug reports are fed to an IR-based fault localizer; (2) patches are generated from fix patterns and validated via regression testing; (3) a prioritized list of generated patches is proposed to developers. We evaluate iFixR on the Defects4J dataset, which we enriched (i.e., faults are linked to bug reports) and carefully-reorganized (i.e., the timeline of test-cases is naturally split). iFixR generates genuine/plausible patches for 21/44 Defects4J faults with its IR-based fault localizer. iFixR accurately places a genuine/plausible patch among its top-5 recommendation for 8/13 of these faults (without using future test cases in generation-and-validation).},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {314–325},
numpages = {12},
keywords = {Information retrieval, fault localization, automatic patch generation},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/3236024.3264590,
author = {Angell, Rico and Johnson, Brittany and Brun, Yuriy and Meliou, Alexandra},
title = {Themis: Automatically Testing Software for Discrimination},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3264590},
doi = {10.1145/3236024.3264590},
abstract = {Bias in decisions made by modern software is becoming a common and serious problem. We present Themis, an automated test suite generator to measure two types of discrimination, including causal relationships between sensitive inputs and program behavior. We explain how Themis can measure discrimination and aid its debugging, describe a set of optimizations Themis uses to reduce test suite size, and demonstrate Themis' effectiveness on open-source software. Themis is open-source and all our evaluation data are available at http://fairness.cs.umass.edu/. See a video of Themis in action: https://youtu.be/brB8wkaUesY},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {871–875},
numpages = {5},
keywords = {discrimination testing, fairness testing, Software fairness, Themis, automated test generation, software bias, testing},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2896941.2896954,
author = {Kaulgud, Vikrant and Saxena, Amitabh and Podder, Sanjay and Sharma, Vibhu Saujanya and Dinakar, Chethana},
title = {Shifting Testing beyond the Deployment Boundary},
year = {2016},
isbn = {9781450341578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896941.2896954},
doi = {10.1145/2896941.2896954},
abstract = {Optimization of software testing has typically relied on a philosophy of 'shift left' where the focus is on using automation to start testing as early as possible in a software development life-cycle. With enterprises striving for continual business transformation, it becomes essential to focus on rapid and iterative software development. Moreover, understanding customer reactions, feedback and needs, and translating them as actionable insights to software development teams. This requires testing to 'shift right', i.e. move beyond the deployment boundary, and adopt a data-driven approach leveraging post-release production data and customer feedback. In this position paper we articulate the need for continuous, shift right testing, define a taxonomy for structuring testing types, analyses and data requirements. We end by providing a conceptual framework for applying shift right testing for Digital apps.},
booktitle = {Proceedings of the International Workshop on Continuous Software Evolution and Delivery},
pages = {30–33},
numpages = {4},
keywords = {continuous testing, shift right testing, customer feedback},
location = {Austin, Texas},
series = {CSED '16}
}

@inbook{10.1109/ICSE-SEET52601.2021.00028,
author = {Fraser, Gordon and Heuer, Ute and K\"{o}rber, Nina and Oberm\"{u}ller, Florian and Wasmeier, Ewald},
title = {LitterBox: A Linter for Scratch Programs},
year = {2021},
isbn = {9780738133201},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET52601.2021.00028},
abstract = {Creating programs with block-based programming languages like Scratch is easy and fun. Block-based programs can nevertheless contain bugs, in particular when learners have misconceptions about programming. Even when they do not, Scratch code is often of low quality and contains code smells, further inhibiting understanding, reuse, and fun. To address this problem, in this paper we introduce LitterBox, a linter for Scratch programs. Given a program or its public project ID, LitterBox checks the program against patterns of known bugs and code smells. For each issue identified, LitterBox provides not only the location in the code, but also a helpful explanation of the underlying reason and possible misconceptions. Learners can access LitterBox through an easy to use web interface with visual information about the errors in the block-code, while for researchers LitterBox provides a general, open source, and extensible framework for static analysis of Scratch programs.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Joint Track on Software Engineering Education and Training},
pages = {183–188},
numpages = {6}
}

@inproceedings{10.1145/1108792.1108801,
author = {Kumar, Naveen and Childers, Bruce R. and Soffa, Mary Lou},
title = {Low Overhead Program Monitoring and Profiling},
year = {2005},
isbn = {1595932399},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1108792.1108801},
doi = {10.1145/1108792.1108801},
abstract = {Program instrumentation, inserted either before or during execution, is rapidly becoming a necessary component of many systems. Instrumentation is commonly used to collect information for many diverse analysis applications, such as detecting program invariants, dynamic slicing and alias analysis, software security checking, and computer architecture modeling. Because instrumentation typically has a high run-time overhead, techniques are needed to mitigate the overheads. This paper describes "instrumentation optimizations" that reduce the overhead of profiling for program analysis. Our approach applies transformations to the instrumentation code that reduce the (1) number of instrumentation points executed, (2) cost of instrumentation probes, and (3) cost of instrumentation payload, while maintaining the semantics of the original instrumentation. We present the transformations and apply them for program profiling and computer architecture modeling. We evaluate the optimizations and show that the optimizations improve profiling performance by 1.26-2.63x and architecture modeling performance by 2-3.3x.},
booktitle = {Proceedings of the 6th ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering},
pages = {28–34},
numpages = {7},
keywords = {instrumentation optimization, profiling, dynamic binary translation, dynamic instrumentation},
location = {Lisbon, Portugal},
series = {PASTE '05}
}

@article{10.1145/1108768.1108801,
author = {Kumar, Naveen and Childers, Bruce R. and Soffa, Mary Lou},
title = {Low Overhead Program Monitoring and Profiling},
year = {2005},
issue_date = {January 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1108768.1108801},
doi = {10.1145/1108768.1108801},
abstract = {Program instrumentation, inserted either before or during execution, is rapidly becoming a necessary component of many systems. Instrumentation is commonly used to collect information for many diverse analysis applications, such as detecting program invariants, dynamic slicing and alias analysis, software security checking, and computer architecture modeling. Because instrumentation typically has a high run-time overhead, techniques are needed to mitigate the overheads. This paper describes "instrumentation optimizations" that reduce the overhead of profiling for program analysis. Our approach applies transformations to the instrumentation code that reduce the (1) number of instrumentation points executed, (2) cost of instrumentation probes, and (3) cost of instrumentation payload, while maintaining the semantics of the original instrumentation. We present the transformations and apply them for program profiling and computer architecture modeling. We evaluate the optimizations and show that the optimizations improve profiling performance by 1.26-2.63x and architecture modeling performance by 2-3.3x.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {sep},
pages = {28–34},
numpages = {7},
keywords = {dynamic binary translation, profiling, instrumentation optimization, dynamic instrumentation}
}

@article{10.1145/3133917,
author = {Donaldson, Alastair F. and Evrard, Hugues and Lascu, Andrei and Thomson, Paul},
title = {Automated Testing of Graphics Shader Compilers},
year = {2017},
issue_date = {October 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {1},
number = {OOPSLA},
url = {https://doi.org/10.1145/3133917},
doi = {10.1145/3133917},
abstract = { We present an automated technique for finding defects in compilers for graphics shading languages. key challenge in compiler testing is the lack of an oracle that classifies an output as correct or incorrect; this is particularly pertinent in graphics shader compilers where the output is a rendered image that is typically under-specified. Our method builds on recent successful techniques for compiler validation based on metamorphic testing, and leverages existing high-value graphics shaders to create sets of transformed shaders that should be semantically equivalent. Rendering mismatches are then indicative of shader compilation bugs. Deviant shaders are automatically minimized to identify, in each case, a minimal change to an original high-value shader that induces a shader compiler bug. We have implemented the approach as a tool, GLFuzz, targeting the OpenGL shading language, GLSL. Our experiments over a set of 17 GPU and driver configurations, spanning the main 7 GPU designers, have led to us finding and reporting more than 60 distinct bugs, covering all tested configurations. As well as defective rendering, these issues identify security-critical vulnerabilities that affect WebGL, including a significant remote information leak security bug where a malicious web page can capture the contents of other browser tabs, and a bug whereby visiting a malicious web page can lead to a ``blue screen of death'' under Windows 10. Our findings show that shader compiler defects are prevalent, and that metamorphic testing provides an effective means for detecting them automatically. },
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {93},
numpages = {29},
keywords = {OpenGL, GLSL, shaders, testing, compilers, GPUs}
}

@inproceedings{10.1145/3210459.3210481,
author = {Stratis, Panagiotis and Brown, Gordon},
title = {Assessing the Effect of Device-Based Test Scheduling on Heterogeneous Test Suite Execution},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210481},
doi = {10.1145/3210459.3210481},
abstract = {As software takes on more responsibility, it gets increasingly complex, requiring an extremely large number of tests for effective validation. This problem is more serious in heterogeneous system testing, where an application is required to be tested against a diverse collection of processors before is accepted for release.Test scheduling methodologies are concerned with scheduling the execution of tests in an order which enhances performance.In this paper we assess the effect of device-based test scheduling on the execution time of heterogeneous test suites. We utilize a test scheduling algorithm which improves load balance between the multiple devices of a heterogeneous system during test suite execution in an attempt to reduce its execution time. We conduct an empirical evaluation on a large-scaled test suite targeting implementations of the SYCL standard which has been developed by Codeplay Software. We found that a maximum of 25.42% speed-up is achieved by our test scheduling algorithm when compared to parallel test execution without test scheduling.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {193–198},
numpages = {6},
location = {Christchurch, New Zealand},
series = {EASE'18}
}

@inproceedings{10.1145/1287767.1287775,
author = {Varshney, Maneesh and Xu, Zhiguo and Mohan, Shrinivas and Yang, Yi and Xu, Defeng and Bagrodia, Rajive},
title = {WHYNET: A Framework for <i>in-Situ</i> Evaluation of Heterogeneous Mobile Wireless Systems},
year = {2007},
isbn = {9781595937384},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1287767.1287775},
doi = {10.1145/1287767.1287775},
abstract = {The design and implementation of wireless systems has been impeded by the lack of an evaluation framework that can provide an accurate understanding of middleware and application performance in the context of their interactions with system hardware and software, network architecture and configuration and wireless channel effects. In this paper we present a novel evaluation paradigm wherein the applications, middleware or sub-networks can be evaluated in-situ, in other words, as operational software that interfaces with the operating system and other applications, thus offering a fidelity equivalent to physical deployment. The physical environment in which such systems operate is modeled using high-fidelity simulations. This approach combines the fidelity of physical test beds with the benefits of scalability, repeatability of input parameters, and comprehensive parameter space evaluation - the known limitations of a physical test-bed. The framework design is extensible in that it allows configuring the desired components of a system with different modalities to suit a particular evaluation criterion. The implementation also addresses the key challenges in the interaction of the framework sub-components: seamless interfaces, time synchronization and preserving causality constraints. The benefits and applicability of the framework to diverse wireless contexts is demonstrated by means of various case studies in diverse wireless networks. In one case study, we show that a design exhibiting 4X improvement in network metrics may be actually degrading the application metric by 50%.},
booktitle = {Proceedings of the Second ACM International Workshop on Wireless Network Testbeds, Experimental Evaluation and Characterization},
pages = {35–42},
numpages = {8},
keywords = {heterogeneous networks, emulation, testbeds, WHYNET},
location = {Montreal, Quebec, Canada},
series = {WinTECH '07}
}

@inproceedings{10.1145/3192366.3192396,
author = {D'Elia, Daniele Cono and Demetrescu, Camil},
title = {On-Stack Replacement, Distilled},
year = {2018},
isbn = {9781450356985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3192366.3192396},
doi = {10.1145/3192366.3192396},
abstract = {On-stack replacement (OSR) is essential technology for adaptive optimization, allowing changes to code actively executing in a managed runtime. The engineering aspects of OSR are well-known among VM architects, with several implementations available to date. However, OSR is yet to be explored as a general means to transfer execution between related program versions, which can pave the road to unprecedented applications that stretch beyond VMs. We aim at filling this gap with a constructive and provably correct OSR framework, allowing a class of general-purpose transformation functions to yield a special-purpose replacement. We describe and evaluate an implementation of our technique in LLVM. As a novel application of OSR, we present a feasibility study on debugging of optimized code, showing how our techniques can be used to fix variables holding incorrect values at breakpoints due to optimizations.},
booktitle = {Proceedings of the 39th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {166–180},
numpages = {15},
keywords = {deoptimization, dynamic compilers, debugging},
location = {Philadelphia, PA, USA},
series = {PLDI 2018}
}

@article{10.1145/3296979.3192396,
author = {D'Elia, Daniele Cono and Demetrescu, Camil},
title = {On-Stack Replacement, Distilled},
year = {2018},
issue_date = {April 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296979.3192396},
doi = {10.1145/3296979.3192396},
abstract = {On-stack replacement (OSR) is essential technology for adaptive optimization, allowing changes to code actively executing in a managed runtime. The engineering aspects of OSR are well-known among VM architects, with several implementations available to date. However, OSR is yet to be explored as a general means to transfer execution between related program versions, which can pave the road to unprecedented applications that stretch beyond VMs. We aim at filling this gap with a constructive and provably correct OSR framework, allowing a class of general-purpose transformation functions to yield a special-purpose replacement. We describe and evaluate an implementation of our technique in LLVM. As a novel application of OSR, we present a feasibility study on debugging of optimized code, showing how our techniques can be used to fix variables holding incorrect values at breakpoints due to optimizations.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {166–180},
numpages = {15},
keywords = {debugging, deoptimization, dynamic compilers}
}

@inproceedings{10.1145/3180155.3180226,
author = {Chen, Chu and Tian, Cong and Duan, Zhenhua and Zhao, Liang},
title = {RFC-Directed Differential Testing of Certificate Validation in SSL/TLS Implementations},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180226},
doi = {10.1145/3180155.3180226},
abstract = {Certificate validation in Secure Socket Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS is correctly implemented. With this motivation, we propose a novel differential testing approach which is directed by the standard Request For Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e. certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {859–870},
numpages = {12},
keywords = {certificate validation, dynamic symbolic execution, differential testing, request for comments, SSL/TLS},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2786805.2803202,
author = {Pham, Raphael and Stoliar, Yauheni and Schneider, Kurt},
title = {Automatically Recommending Test Code Examples to Inexperienced Developers},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2803202},
doi = {10.1145/2786805.2803202},
abstract = { New graduates joining the software engineering workforce sometimes have trouble writing test code. Coming from university, they lack a hands-on approach to testing and have little experience with writing tests in a real-world setting. Software companies resort to costly training camps or mentoring initiatives. Not overcoming this lack of testing skills early on can hinder the newcomer’s professional progress in becoming a high-quality engineer. Studying open source developers, we found that they rely on a project’s pre-existing test code to learn how to write tests and adapt test code for their own use. We propose to strategically present useful and contextual test code examples from a project’s test suite to newcomers in order to facilitate learning and test writing. With an automatic suggestion mechanism for valuable test code, the newcomer is enabled to learn how senior developers write tests and copy it. Having access to suitable tests lowers the barrier for writing new tests. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {890–893},
numpages = {4},
keywords = {Newcomers, Recommendation, Examples, Testing},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1109/ASE.2011.6100094,
author = {Wei, Yi and Roth, Hannes and Furia, Carlo A. and Pei, Yu and Horton, Alexander and Steindorfer, Michael and Nordio, Martin and Meyer, Bertrand},
title = {Stateful Testing: Finding More Errors in Code and Contracts},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100094},
doi = {10.1109/ASE.2011.6100094},
abstract = {Automated random testing has shown to be an effective approach to finding faults but still faces a major unsolved issue: how to generate test inputs diverse enough to find many faults and find them quickly. Stateful testing, the automated testing technique introduced in this article, generates new test cases that improve an existing test suite. The generated test cases are designed to violate the dynamically inferred contracts (invariants) characterizing the existing test suite. As a consequence, they are in a good position to detect new faults, and also to improve the accuracy of the inferred contracts by discovering those that are unsound. Experiments on 13 data structure classes totalling over 28,000 lines of code demonstrate the effectiveness of stateful testing in improving over the results of long sessions of random testing: stateful testing found 68.4% new faults and improved the accuracy of automatically inferred contracts to over 99%, with just a 7% time overhead.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {440–443},
numpages = {4},
series = {ASE '11}
}

@article{10.1145/2927299.2940294,
author = {Beschastnikh, Ivan and Wang, Patty and Brun, Yuriy and Ernst, Michael D,},
title = {Debugging Distributed Systems: Challenges and Options for Validation and Debugging},
year = {2016},
issue_date = {March-April 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {1542-7730},
url = {https://doi.org/10.1145/2927299.2940294},
doi = {10.1145/2927299.2940294},
abstract = {Distributed systems pose unique challenges for software developers. Reasoning about concurrent activities of system nodes and even understanding the system’s communication topology can be difficult. A standard approach to gaining insight into system activity is to analyze system logs. Unfortunately, this can be a tedious and complex process. This article looks at several key features and debugging challenges that differentiate distributed systems from other kinds of software. The article presents several promising tools and ongoing research to help resolve these challenges.},
journal = {Queue},
month = {mar},
pages = {91–110},
numpages = {20}
}

@inproceedings{10.1145/2254064.2254075,
author = {Jin, Guoliang and Song, Linhai and Shi, Xiaoming and Scherpelz, Joel and Lu, Shan},
title = {Understanding and Detecting Real-World Performance Bugs},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254075},
doi = {10.1145/2254064.2254075},
abstract = {Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs.This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs.Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {77–88},
numpages = {12},
keywords = {rule-based bug detection, performance bugs, characteristics study},
location = {Beijing, China},
series = {PLDI '12}
}

@article{10.1145/2345156.2254075,
author = {Jin, Guoliang and Song, Linhai and Shi, Xiaoming and Scherpelz, Joel and Lu, Shan},
title = {Understanding and Detecting Real-World Performance Bugs},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345156.2254075},
doi = {10.1145/2345156.2254075},
abstract = {Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs.This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs.Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {77–88},
numpages = {12},
keywords = {characteristics study, performance bugs, rule-based bug detection}
}

@inproceedings{10.1145/3084226.3084244,
author = {Li, Wang and Li, Shanshan and Liao, Xiangke and Xu, Xiangyang and Zhou, Shulin and Jia, Zhouyang},
title = {ConfTest: Generating Comprehensive Misconfiguration for System Reaction Ability Evaluation},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084244},
doi = {10.1145/3084226.3084244},
abstract = {Misconfigurations are not only prevalent, but also costly on diagnosing and troubleshooting. Unlike software bugs, misconfigurations are more vulnerable to users' mistakes. Improving system reaction to misconfigurations would ease the burden of users' diagnoses. Such effort can greatly benefit from a comprehensive study of system reaction ability towards misconfigurations based on errors injection method. Unfortunately, few such studies have achieved the above goal in the past, primarily because they fail to provide rich error types or only rely on generic alternations to generate misconfigurations. In this paper, we studied 8 mature opensource and commercial software and summarized a fine-grained classification of option types. On the basis of this classification, we could extract syntactic and semantic constraints of each type to generate misconfigurations. We implemented a tool named ConfTest to conduct misconfiguration injection and further analyze system reaction abilities to various of misconfigurations. We carried out comprehensive analyses upon 4 open-source software systems. Our evaluation results show that our option classification covers over 96% of 1582 options from Httpd, Yum, PostgreSQL and MySQL.Our constraint is more fined-grained and the accuracy is more than 90% of of real constraints through manual verification. We compared the capability in finding bad system reactions between ConfTest and ConfErr, showing that the ConfTest can find nearly 3 times the bad reactions found by ConfErr.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {88–97},
numpages = {10},
keywords = {constraints, Misconfiguration, system reactions},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@inproceedings{10.1145/3377811.3381749,
author = {Lam, Wing and Mu\c{s}lu, K\i{}van\c{c} and Sajnani, Hitesh and Thummalapenta, Suresh},
title = {A Study on the Lifecycle of Flaky Tests},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3381749},
doi = {10.1145/3377811.3381749},
abstract = {During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects.This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft. We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78% without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they "fixed" a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be "fixes".},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1471–1482},
numpages = {12},
keywords = {empirical study, lifecycle, flaky test},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/2491627.2491651,
author = {Nakagawa, Elisa Yumi and Becker, Martin and Maldonado, Jos\'{e} Carlos},
title = {Towards a Process to Design Product Line Architectures Based on Reference Architectures},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491651},
doi = {10.1145/2491627.2491651},
abstract = {Software Product Line (SPL) has arisen as an approach for developing a family of software-intensive systems at lower costs, within shorter time, and with higher quality. In particular, SPL is supported by a product line architecture (sometimes also referred to as reference architecture) that captures the architectures of a product family. From another perspective, a special type of architecture that contains knowledge about a specific domain has been increasingly investigated, resulting in the research area of Reference Architecture. In spite of the positive impact of this type of architecture on reuse and productivity, the use of the knowledge contained in existing reference architectures in order to develop SPL has not been widely explored yet. The main contribution of this paper is to present a process, named ProSA-RA2PLA, that systematizes the use of reference architectures for building product line architectures. To illustrate the application of this process, we have built a product line architecture for an SPL of software testing tools using a reference architecture of that domain. Based on initial results, we have observed that benefits can be achieved, mainly regarding improvement in reuse and productivity to develop SPL.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {157–161},
numpages = {5},
keywords = {reference architecture, knowledge sharing},
location = {Tokyo, Japan},
series = {SPLC '13}
}

