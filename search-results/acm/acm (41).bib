@inproceedings{10.1109/ASE.2015.13,
author = {Di Nardo, Daniel and Pastore, Fabrizio and Arcuri, Andrea and Briand, Lionel},
title = {Evolutionary Robustness Testing of Data Processing Systems Using Models and Data Mutation},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.13},
doi = {10.1109/ASE.2015.13},
abstract = {System level testing of industrial data processing software poses several challenges. Input data can be very large, even in the order of gigabytes, and with complex constraints that define when an input is valid. Generating the right input data to stress the system for robustness properties (e.g. to test how faulty data is handled) is hence very complex, tedious and error prone when done manually. Unfortunately, this is the current practice in industry. In previous work, we defined a methodology to model the structure and the constraints of input data by using UML class diagrams and OCL constraints. Tests were automatically derived to cover predefined fault types in a fault model. In this paper, to obtain more effective system level test cases, we developed a novel search-based test generation tool. Experiments on a real-world, large industrial data processing system show that our automated approach can not only achieve better code coverage, but also accomplishes this using significantly smaller test suites.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {126–137},
numpages = {12},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/3287098.3287112,
author = {Suri, Venkata Ratnadeep and Rangaswamy, Nimmy and Joshi, Tanmay and Joshi, Meghna and Nanavati, Sneha},
title = {Tool Smiths in Off-Shored Work: Socio-Technical System of Quality Testing in India},
year = {2019},
isbn = {9781450361224},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3287098.3287112},
doi = {10.1145/3287098.3287112},
abstract = {Today, the consequence of viewing work automation as a face-off between human potential and smart technology denies the role of business acumen, tacit knowledge, market forces and social contexts that shape work environments around the world. Using a socio-technical framework, we examine the consequences arising out of the introduction of automation in the quality testing segment of the IT industry in India. We highlight key socio-economic parameters influencing the decisions to automate a testing environment. Next, by applying an ICTD lens, we analyze the ensuing discourse emerging from the voices of Quality Testers imbuing value into the testing job profile and juxtapose these discourses with current QT work practices. Finally, we highlight the importance of creating better QT work practices in tandem with training strategies that allow IT professionals to draw upon their implicit knowledge, critical thinking, computing skills, and business process knowledge, and combine it with automated QT testing procedures to add more value to QT profession.},
booktitle = {Proceedings of the Tenth International Conference on Information and Communication Technologies and Development},
articleno = {11},
numpages = {10},
keywords = {IT industry, ethnography, ICTD, India, quality testing, automation},
location = {Ahmedabad, India},
series = {ICTD '19}
}

@article{10.1145/357139.357140,
author = {Gannon, John and McMullin, Paul and Hamlet, Richard},
title = {Data Abstraction, Implementation, Specification, and Testing},
year = {1981},
issue_date = {July 1981},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {3},
issn = {0164-0925},
url = {https://doi.org/10.1145/357139.357140},
doi = {10.1145/357139.357140},
journal = {ACM Trans. Program. Lang. Syst.},
month = {jul},
pages = {211–223},
numpages = {13}
}

@inproceedings{10.1145/2463372.2463551,
author = {Bozkurt, Mustafa},
title = {Cost-Aware Pareto Optimal Test Suite Minimisation for Service-Centric Systems},
year = {2013},
isbn = {9781450319638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2463372.2463551},
doi = {10.1145/2463372.2463551},
abstract = {Runtime testing cost caused by service invocations is considered as one of the major limitations in Service-centric System Testing (ScST). Unfortunately, most of the existing work cannot achieve cost reduction at runtime as they perform offline testing. In this paper, we introduce a novel cost-aware pareto optimal test suite minimisation approach for ScST aimed at reducing runtime testing cost. In experimental analysis, the proposed approach achieved reductions between 69% and 98.6% in monetary cost of service invocations while retaining test suite coverage. The results also provided evidence for the effectiveness of the selected algorithm HNSGA-II over the two commonly used algorithms: Greedy and NSGA-II.},
booktitle = {Proceedings of the 15th Annual Conference on Genetic and Evolutionary Computation},
pages = {1429–1436},
numpages = {8},
keywords = {soa, service-centric systems testing, test suite minimisation, web services, multi-objective genetic algorithms},
location = {Amsterdam, The Netherlands},
series = {GECCO '13}
}

@inproceedings{10.5555/3106039.3106042,
author = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean and Shippey, Thomas and Turhan, Burak},
title = {How Good Are My Tests?},
year = {2017},
isbn = {9781538628072},
publisher = {IEEE Press},
abstract = {Background: Test quality is a prerequisite for achieving production system quality. While the concept of quality is multidimensional, most of the effort in testing context has been channelled towards measuring test effectiveness.Objective: While effectiveness of tests is certainly important, we aim to identify a core list of testing principles that also address other quality facets of testing, and to discuss how they can be quantified as indicators of test quality.Method: We have conducted a two-day workshop with our industry partners to come up with a list of relevant principles and best practices expected to result in high quality tests. We then utilised our academic and industrial training materials together with recommendations in practitioner oriented testing books to refine the list. We surveyed existing literature for potential metrics to quantify identified principles.Results: We have identified a list of 15 testing principles to capture the essence of testing goals and best practices from quality perspective. Eight principles do not map to existing test smells and we propose metrics for six of those. Further, we have identified additional potential metrics for the seven principles that partially map to test smells.Conclusion: We provide a core list of testing principles along with a discussion of possible ways to quantify them for assessing goodness of tests. We believe that our work would be useful for practitioners in assessing the quality of their tests from multiple perspectives including but not limited to maintainability, comprehension and simplicity.},
booktitle = {Proceedings of the 8th Workshop on Emerging Trends in Software Metrics},
pages = {9–14},
numpages = {6},
keywords = {unit testing, test quality, metrics},
location = {Buenos Aires, Argentina},
series = {WETSoM '17}
}

@inproceedings{10.1145/3412841.3442019,
author = {Zhao, Kunsong and Xu, Zhou and Yan, Meng and Tang, Yutian and Fan, Ming and Catolino, Gemma},
title = {Just-in-Time Defect Prediction for Android Apps via Imbalanced Deep Learning Model},
year = {2021},
isbn = {9781450381048},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3412841.3442019},
doi = {10.1145/3412841.3442019},
abstract = {Android mobile apps have played important roles in our daily life and work. To meet new requirements from users, the mobile apps encounter frequent updates, which involves in a large quantity of code commits. Previous studies proposed to apply Just-in-Time (JIT) defect prediction for mobile apps to timely identify whether new code commits can introduce defects into apps, aiming to assure the quality of mobile apps. In general, the number of defective commit instances is much fewer than that of clean ones, in other words, the defect data is class imbalanced. In this work, we propose a novel Imbalanced Deep Learning model, called IDL, to conduct JIT defect prediction task for Android mobile apps. More specifically, we introduce a state-of-the-art cost-sensitive cross-entropy loss function into the deep neural network to learn the high-level feature representation, in which the loss function alleviates the class imbalance issue by taking the prior probability of the two types of classes into account. We conduct experiments on a benchmark defect data consisting of 12 Android mobile apps. The results of rigorous experiments show that our proposed IDL model performs significantly better than 23 comparative imbalanced learning methods in terms of Matthews correlation coefficient performance indicator.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on Applied Computing},
pages = {1447–1454},
numpages = {8},
keywords = {JIT defect prediction, mobile apps, imbalanced learning},
location = {Virtual Event, Republic of Korea},
series = {SAC '21}
}

@inproceedings{10.1109/ASE.2019.00127,
author = {Xie, Xiaofei and Chen, Hongxu and Li, Yi and Ma, Lei and Liu, Yang and Zhao, Jianjun},
title = {Coverage-Guided Fuzzing for Feedforward Neural Networks},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00127},
doi = {10.1109/ASE.2019.00127},
abstract = {Deep neural network (DNN) has been widely applied to safety-critical scenarios such as autonomous vehicle, security surveillance, and cyber-physical control systems. Yet, the incorrect behaviors of DNNs can lead to severe accidents and tremendous losses due to hidden defects. In this paper, we present DeepHunter, a general-purpose fuzzing framework for detecting defects of DNNs. DeepHunter is inspired by traditional grey-box fuzzing and aims to increase the overall test coverage by applying adaptive heuristics according to runtime feedback. Specifically, DeepHunter provides a series of seed selection strategies, metamorphic mutation strategies, and testing criteria customized to DNN testing; all these components support multiple built-in configurations which are easy to extend. We evaluated DeepHunter on two popular datasets and the results demonstrate the effectiveness of DeepHunter in achieving coverage increase and detecting real defects. A video demonstration which showcases the main features of DeepHunter can be found at https://youtu.be/s5DfLErcgrc.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1162–1165},
numpages = {4},
location = {San Diego, California},
series = {ASE '19}
}

@inbook{10.1145/3368089.3417064,
author = {W\"{u}stholz, Valentin and Christakis, Maria},
title = {Harvey: A Greybox Fuzzer for Smart Contracts},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417064},
abstract = {We present Harvey, an industrial greybox fuzzer for smart contracts, which are programs managing accounts on a blockchain.  Greybox fuzzing is a lightweight test-generation approach that effectively detects bugs and security vulnerabilities. However, greybox fuzzers randomly mutate program inputs to exercise new paths; this makes it challenging to cover code that is guarded by narrow checks. Moreover, most real-world smart contracts transition through many different states during their lifetime, e.g., for every bid in an auction. To explore these states and thereby detect deep vulnerabilities, a greybox fuzzer would need to generate sequences of contract transactions, e.g., by creating bids from multiple users, while keeping the search space and test suite tractable.  In this paper, we explain how Harvey alleviates both challenges with two key techniques. First, Harvey extends standard greybox fuzzing with a method for predicting new inputs that are more likely to cover new paths or reveal vulnerabilities in smart contracts. Second, it fuzzes transaction sequences in a targeted and demand-driven way. We have evaluated our approach on 27 real-world contracts. Our experiments show that our techniques significantly increase Harvey's effectiveness in achieving high coverage and detecting vulnerabilities, in most cases orders-of-magnitude faster.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1398–1409},
numpages = {12}
}

@inproceedings{10.1145/2896921.2896934,
author = {Miranda, Breno and Bertolino, Antonia},
title = {Does Code Coverage Provide a Good Stopping Rule for Operational Profile Based Testing?},
year = {2016},
isbn = {9781450341516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896921.2896934},
doi = {10.1145/2896921.2896934},
abstract = {We introduce a new coverage measure, called the operational coverage, which is customized to the usage profile (count spectrum) of the entities to be covered. Operational coverage is proposed as an adequacy criterion for operational profile based testing, i.e., to assess the thoroughness of a black box test suite derived from the operational profile. To validate the approach we study the correlation between operational coverage of branches, statements, and functions, and the probability that the next test input will not fail. On the three subjects considered, we observed a moderate correlation in all cases (except a low correlation for function coverage for one subject), and consistently better results than traditional coverage measure.},
booktitle = {Proceedings of the 11th International Workshop on Automation of Software Test},
pages = {22–28},
numpages = {7},
keywords = {operational profile based testing, relative coverage, program spectra, coverage testing, operational coverage},
location = {Austin, Texas},
series = {AST '16}
}

@article{10.1145/3092566,
author = {Ghaffarian, Seyed Mohammad and Shahriari, Hamid Reza},
title = {Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques: A Survey},
year = {2017},
issue_date = {July 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3092566},
doi = {10.1145/3092566},
abstract = {Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.},
journal = {ACM Comput. Surv.},
month = {aug},
articleno = {56},
numpages = {36},
keywords = {data-mining, survey, review, software security, Software vulnerability analysis, software vulnerability discovery, machine-learning}
}

@article{10.1145/1195937.1195938,
author = {Saha, Goutam Kumar},
title = {Software Fault Avoidance Issues},
year = {2006},
issue_date = {November 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2006},
number = {November},
url = {https://doi.org/10.1145/1195937.1195938},
doi = {10.1145/1195937.1195938},
abstract = {This article aims to discuss various issues of software fault avoidance. Software fault avoidance aims to produce fault free software through various approaches having the common objective of reducing the number of latent defects in software programs.},
journal = {Ubiquity},
month = {nov},
articleno = {5},
numpages = {15}
}

@inproceedings{10.1109/ASE.2015.68,
author = {Zhang, Benwen and Hill, Emily and Clause, James},
title = {Automatically Generating Test Templates from Test Names},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.68},
doi = {10.1109/ASE.2015.68},
abstract = {Existing specification-based testing techniques require specifications that either do not exist or are too difficult to create. As a result, they often fall short of their goal of helping developers test expected behaviors. In this paper we present a novel, natural language-based approach that exploits the descriptive nature of test names to generate test templates. Similar to how modern IDEs simplify development by providing templates for common constructs such as loops, test templates can save time and lower the cognitive barrier for writing tests. The results of our evaluation show that the approach is feasible: despite the difficulty of the task, when test names contain a sufficient amount of information, the approach's accuracy is over 80% when parsing the relevant information from the test name and generating the template.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {506–511},
numpages = {6},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/1808266.1808285,
author = {Dulz, Winfried and German, Reinhard and Holpp, Stefan and G\"{o}tz, Helmut},
title = {Calculating the Usage Probabilities of Statistical Usage Models by Constraints Optimization},
year = {2010},
isbn = {9781605589701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808266.1808285},
doi = {10.1145/1808266.1808285},
abstract = {The systematic generation of test cases from statistical usage models has been investigated recently for specific application domains, such as wireless communications or automotive applications. For Markov chain usage models, the expected usage of a hardware/software system is represented by transitions between usage states and a usage profile, meaning probability values that are attached to the state transitions.In this paper, we explain how to calculate the profile probabilities for the Markov chain usage model from a set of linear usage constraints and by optimizing a convex polyhedron that represents the constrained solution space. Comparing the computed probability distributions of our polyhedron approach with the maximum entropy technique, which is the main technique used so far, illustrates that our results are more obvious to the intented constraint semantics. In order to demonstrate the applicability of our approach, workflow testing of a complex RIS/PACS system in the medical domain was carried through and has provided promising results.},
booktitle = {Proceedings of the 5th Workshop on Automation of Software Test},
pages = {127–134},
numpages = {8},
keywords = {optimizing usage probabilities, polyhedron optimization, Markov chain usage model, medical case study, profile generation, statistical usage testing, usage constraints},
location = {Cape Town, South Africa},
series = {AST '10}
}

@inproceedings{10.1145/2001576.2001825,
author = {Oh, Jungsup and Harman, Mark and Yoo, Shin},
title = {Transition Coverage Testing for Simulink/Stateflow Models Using Messy Genetic Algorithms},
year = {2011},
isbn = {9781450305570},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2001576.2001825},
doi = {10.1145/2001576.2001825},
abstract = {This paper introduces a messy-GA for transition coverage of Simulink/StateFlow models. We introduce a tool that implements our approach and evaluate it on three benchmark embedded system Simulink models. Our messy-GA is able to achieve statistically significantly better coverage when compared to both random search and to a commercial tool for Simulink/StateFlow model Testing.},
booktitle = {Proceedings of the 13th Annual Conference on Genetic and Evolutionary Computation},
pages = {1851–1858},
numpages = {8},
keywords = {search-based software engineering, model-based testing},
location = {Dublin, Ireland},
series = {GECCO '11}
}

@inproceedings{10.5555/782052.782069,
author = {Johnson, Morris S.},
title = {A Survey of Testing Techniques for Object-Oriented Systems},
year = {1996},
publisher = {IBM Press},
abstract = {Most research on object-oriented(OO) paradigms has been focused on analysis, design, and programming fundamentals. Testing the systems that are created with these paradigms has been considered an afterthought. Traditional testing techniques must be evaluated to determine if they are still useful with respect to object-oriented systems, and new techniques must be developed.This paper is a survey of research in testing object-oriented systems. It discusses the challenges to testing an OO system, the different levels of verification, and various testing techniques.},
booktitle = {Proceedings of the 1996 Conference of the Centre for Advanced Studies on Collaborative Research},
pages = {17},
location = {Toronto, Ontario, Canada},
series = {CASCON '96}
}

@inproceedings{10.1145/3009837.3009868,
author = {Lampropoulos, Leonidas and Gallois-Wong, Diane and Hri\c{t}cu, C\u{a}t\u{a}lin and Hughes, John and Pierce, Benjamin C. and Xia, Li-yao},
title = {Beginner's Luck: A Language for Property-Based Generators},
year = {2017},
isbn = {9781450346603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009837.3009868},
doi = {10.1145/3009837.3009868},
abstract = { Property-based random testing \`{a} la QuickCheck requires building efficient generators for well-distributed random data satisfying complex logical predicates, but writing these generators can be difficult and error prone. We propose a domain-specific language in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated. This language, called Luck, makes generators easier to write, read, and maintain.  We give Luck a formal semantics and prove several fundamental properties, including the soundness and completeness of random generation with respect to a standard predicate semantics. We evaluate Luck on common examples from the property-based testing literature and on two significant case studies, showing that it can be used in complex domains with comparable bug-finding effectiveness and a significant reduction in testing code size compared to handwritten generators. },
booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
pages = {114–129},
numpages = {16},
keywords = {narrowing, random testing, domain specific language, constraint solving, property-based testing},
location = {Paris, France},
series = {POPL 2017}
}

@article{10.1145/3093333.3009868,
author = {Lampropoulos, Leonidas and Gallois-Wong, Diane and Hri\c{t}cu, C\u{a}t\u{a}lin and Hughes, John and Pierce, Benjamin C. and Xia, Li-yao},
title = {Beginner's Luck: A Language for Property-Based Generators},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093333.3009868},
doi = {10.1145/3093333.3009868},
abstract = { Property-based random testing \`{a} la QuickCheck requires building efficient generators for well-distributed random data satisfying complex logical predicates, but writing these generators can be difficult and error prone. We propose a domain-specific language in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated. This language, called Luck, makes generators easier to write, read, and maintain.  We give Luck a formal semantics and prove several fundamental properties, including the soundness and completeness of random generation with respect to a standard predicate semantics. We evaluate Luck on common examples from the property-based testing literature and on two significant case studies, showing that it can be used in complex domains with comparable bug-finding effectiveness and a significant reduction in testing code size compared to handwritten generators. },
journal = {SIGPLAN Not.},
month = {jan},
pages = {114–129},
numpages = {16},
keywords = {property-based testing, domain specific language, constraint solving, random testing, narrowing}
}

@inproceedings{10.1145/1276958.1277173,
author = {Bryce, Ren\'{e}e C. and Colbourn, Charles J.},
title = {One-Test-at-a-Time Heuristic Search for Interaction Test Suites},
year = {2007},
isbn = {9781595936974},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1276958.1277173},
doi = {10.1145/1276958.1277173},
abstract = {Algorithms for the construction of software interaction test suites have focussed on the special case of pairwise coverage; less is known about efficiently constructing test suites for higher strength coverage. The combinatorial growth of t-tuples associated with higher strength hinders the efficacy of interaction testing. Test suites are inherently large, so testers may not run entire test suites. To address these problems, we combine a simple greedy algorithmallwith heuristic search to construct and dispense one test at a time. Our algorithm attempts to maximize the number of t-tuples covered by the earliest tests so that if a tester only runs a partial test suite, they test as many t-tuples as possible.allHeuristic search is shown to provide effective methods for achieving such coverage.},
booktitle = {Proceedings of the 9th Annual Conference on Genetic and Evolutionary Computation},
pages = {1082–1089},
numpages = {8},
keywords = {heuristic search, hill climbing, simulated annealing, software interaction testing, tabu search, t-way interaction testing, great flood, covering arrays, test suite prioritization},
location = {London, England},
series = {GECCO '07}
}

@inproceedings{10.1145/2330163.2330296,
author = {Le Goues, Claire and Weimer, Westley and Forrest, Stephanie},
title = {Representations and Operators for Improving Evolutionary Software Repair},
year = {2012},
isbn = {9781450311779},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2330163.2330296},
doi = {10.1145/2330163.2330296},
abstract = {Evolutionary computation is a promising technique for automating time-consuming and expensive software maintenance tasks, including bug repair. The success of this approach, however, depends at least partially on the choice of representation, fitness function, and operators. Previous work on evolutionary software repair has employed different approaches, but they have not yet been evaluated in depth. This paper investigates representation and operator choices for source-level evolutionary program repair in the GenProg framework [17], focusing on: (1) representation of individual variants, (2) crossover design, (3) mutation operators, and (4) search space definition. We evaluate empirically on a dataset comprising 8 C programs totaling over 5.1 million lines of code and containing 105 reproducible, human-confirmed defects. Our results provide concrete suggestions for operator and representation design choices for evolutionary program repair. When augmented to incorporate these suggestions, GenProg repairs 5 additional bugs (60 vs. 55 out of 105), with a decrease in repair time of 17-43% for the more difficult repair searches.},
booktitle = {Proceedings of the 14th Annual Conference on Genetic and Evolutionary Computation},
pages = {959–966},
numpages = {8},
keywords = {genetic programming, search-based software engineering, mutation, crossover, software repair, representation},
location = {Philadelphia, Pennsylvania, USA},
series = {GECCO '12}
}

@inproceedings{10.1145/3210459.3210464,
author = {Williams, Ashley},
title = {Using Reasoning Markers to Select the More Rigorous Software Practitioners' Online Content When Searching for Grey Literature},
year = {2018},
isbn = {9781450364034},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3210459.3210464},
doi = {10.1145/3210459.3210464},
abstract = {Background: Blog articles have potential value as a source of practitioner generated evidence in grey literature reviews: they could complement already accepted sources (e.g. interviews and focus groups). To be valuable to research, blog articles need to be relevant, rigorous and evidence-based.Objective: This paper focuses on the rigour of blog articles. We develop, evaluate and partially validate a set of reasoning markers that can be used to search for rigorous blog articles. We then demonstrate how these markers can be used in the online search of grey literature for software testing.Method: We identify discourse markers from literature and then select those that are explicit indicators of reasoning. We evaluate the set against a corpus of persuasive essays, and validate false negatives to refine our set further. We then demonstrate the use of the set in a search of grey literature on software testing.Results: The set of markers is reasonably successful at detecting reasoning within the corpus, achieving a precision of 89.6% in the first pass and 91.1% after refining our set. However, recall is low due to specifically focusing only on explicit indicators of reasoning (31.3% in the first pass and 38.7% in the second). Our overall F1-Score is 46.4% in the first pass and 54.3% in the second. This is acceptable for the time being as the current focus is on the quality of results retrieved from a keyword-based search. Improving the recall of results is left for future research.Conclusion: Our work provides a set of discourse markers that can be used to indicate the presence of reasoning in a blog article, and therefore provides an indication of the more rigorous blog article content. We intend to extend the work through considering the presence of evidence, and improving the relevance of blog articles found.},
booktitle = {Proceedings of the 22nd International Conference on Evaluation and Assessment in Software Engineering 2018},
pages = {46–56},
numpages = {11},
keywords = {blogs, evidence, Evidence based software engineering, grey literature reviews, discourse markers, argumentation, systematic literature reviews},
location = {Christchurch, New Zealand},
series = {EASE'18}
}

@inproceedings{10.1145/2934466.2946046,
author = {Arrieta, Aitor and Wang, Shuai and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-Based Test Case Selection of Cyber-Physical System Product Lines for Simulation-Based Validation},
year = {2016},
isbn = {9781450340502},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2934466.2946046},
doi = {10.1145/2934466.2946046},
abstract = {Cyber-Physical Systems (CPSs) are often tested at different test levels following "X-in-the-Loop" configurations: Model-, Software- and Hardware-in-the-loop (MiL, SiL and HiL). While MiL and SiL test levels aim at testing functional requirements at the system level, the HiL test level tests functional as well as non-functional requirements by performing a real-time simulation. As testing CPS product line configurations is costly due to the fact that there are many variants to test, test cases are long, the physical layer has to be simulated and co-simulation is often necessary. It is therefore extremely important to select the appropriate test cases that cover the objectives of each level in an allowable amount of time. We propose an efficient test case selection approach adapted to the "X-in-the-Loop" test levels. Search algorithms are employed to reduce the amount of time required to test configurations of CPS product lines while achieving the test objectives of each level. We empirically evaluate three commonly-used search algorithms, i.e., Genetic Algorithm (GA), Alternating Variable Method (AVM) and Greedy (Random Search (RS) is used as a baseline) by employing two case studies with the aim of integrating the best algorithm into our approach. Results suggest that as compared with RS, our approach can reduce the costs of testing CPS product line configurations by approximately 80% while improving the overall test quality.},
booktitle = {Proceedings of the 20th International Systems and Software Product Line Conference},
pages = {297–306},
numpages = {10},
keywords = {test case selection, cyber-physical system product lines, search-based software engineering},
location = {Beijing, China},
series = {SPLC '16}
}

