@article{10.1016/j.jss.2023.111674,
author = {De Angelis, Emanuele and De Angelis, Guglielmo and Pellegrini, Alessandro and Proietti, Maurizio},
title = {What makes test programs similar in microservices applications?},
year = {2023},
issue_date = {Jul 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {201},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111674},
doi = {10.1016/j.jss.2023.111674},
journal = {J. Syst. Softw.},
month = {jul},
numpages = {20},
keywords = {Automated reasoning, Program instrumentation, Symbolic execution, Test program similarity, Microservices architecture, Software testing}
}

@inproceedings{10.1145/3597926.3598072,
author = {Cheng, Mingfei and Zhou, Yuan and Xie, Xiaofei},
title = {BehAVExplor: Behavior Diversity Guided Testing for Autonomous Driving Systems},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598072},
doi = {10.1145/3597926.3598072},
abstract = {Testing Autonomous Driving Systems (ADSs) is a critical task for ensuring the reliability and safety of autonomous vehicles. Existing methods mainly focus on searching for safety violations while the diversity of the generated test cases is ignored, which may generate many redundant test cases and failures. Such redundant failures can reduce testing performance and increase failure analysis costs. In this paper, we present a novel behavior-guided fuzzing technique (BehAVExplor) to explore the different behaviors of the ego vehi- cle (i.e., the vehicle controlled by the ADS under test) and detect diverse violations. Specifically, we design an efficient unsupervised model, called BehaviorMiner, to characterize the behavior of the ego vehicle. BehaviorMiner extracts the temporal features from the given scenarios and performs a clustering-based abstraction to group behaviors with similar features into abstract states. A new test case will be added to the seed corpus if it triggers new behav- iors (e.g., cover new abstract states). Due to the potential conflict between the behavior diversity and the general violation feedback, we further propose an energy mechanism to guide the seed selec- tion and the mutation. The energy of a seed quantifies how good it is. We evaluated BehAVExplor on Apollo, an industrial-level ADS, and LGSVL simulation environment. Empirical evaluation results show that BehAVExplor can effectively find more diverse violations than the state-of-the-art.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {488–500},
numpages = {13},
keywords = {fuzzing, critical scenarios, behavior diversity, Autonomous driving systems, Apollo},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3644032.3644458,
author = {Mazouni, Quentin and Spieker, Helge and Gotlieb, Arnaud and Acher, Mathieu},
title = {Testing for Fault Diversity in Reinforcement Learning},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644458},
doi = {10.1145/3644032.3644458},
abstract = {Reinforcement Learning is the premier technique to approach sequential decision problems, including complex tasks such as driving cars and landing spacecraft. Among the software validation and verification practices, testing for functional fault detection is a convenient way to build trustworthiness in the learned decision model. While recent works seek to maximise the number of detected faults, none consider fault characterisation during the search for more diversity. We argue that policy testing should not find as many failures as possible (e.g., inputs that trigger similar car crashes) but rather aim at revealing as informative and diverse faults as possible in the model. In this paper, we explore the use of quality diversity optimisation to solve the problem of fault diversity in policy testing. Quality diversity (QD) optimisation is a type of evolutionary algorithm to solve hard combinatorial optimisation problems where high-quality diverse solutions are sought. We define and address the underlying challenges of adapting QD optimisation to the test of action policies. Furthermore, we compare classical QD optimisers to state-of-the-art frameworks dedicated to policy testing, both in terms of search efficiency and fault diversity. We show that QD optimisation, while being conceptually simple and generally applicable, finds effectively more diverse faults in the decision model, and conclude that QD-based policy testing is a promising approach.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {136–146},
numpages = {11},
keywords = {software testing, reinforcement learning, quality diversity},
location = {Lisbon, Portugal},
series = {AST '24}
}

@inproceedings{10.1145/3597926.3598036,
author = {Shi, Ensheng and Wang, Yanlin and Zhang, Hongyu and Du, Lun and Han, Shi and Zhang, Dongmei and Sun, Hongbin},
title = {Towards Efficient Fine-Tuning of Pre-trained Code Models: An Experimental Study and Beyond},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598036},
doi = {10.1145/3597926.3598036},
abstract = {Recently, fine-tuning pre-trained code models such as CodeBERT on downstream tasks has achieved great success in many software testing and analysis tasks. While effective and prevalent, fine-tuning the pre-trained parameters incurs a large computational cost. In this paper, we conduct an extensive experimental study to explore what happens to layer-wise pre-trained representations and their encoded code knowledge during fine-tuning. We then propose efficient alternatives to fine-tune the large pre-trained code model based on the above findings. Our experimental study shows that (1) lexical, syntactic and structural properties of source code are encoded in the lower, intermediate, and higher layers, respectively, while the semantic property spans across the entire model. (2) The process of fine-tuning preserves most of the code properties. Specifically, the basic code properties captured by lower and intermediate layers are still preserved during fine-tuning. Furthermore, we find that only the representations of the top two layers change most during fine-tuning for various downstream tasks. (3) Based on the above findings, we propose Telly to efficiently fine-tune pre-trained code models via layer freezing. The extensive experimental results on five various downstream tasks demonstrate that training parameters and the corresponding time cost are greatly reduced, while performances are similar or better.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {39–51},
numpages = {13},
keywords = {Representational Similarity Analysis, Probing Techniques, Pre-Trained Language Models, Empirical study, Efficient Fine-tuning},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3611643.3616266,
author = {Wang, Longtian and Xie, Xiaofei and Du, Xiaoning and Tian, Meng and Guo, Qing and Yang, Zheng and Shen, Chao},
title = {DistXplore: Distribution-Guided Testing for Evaluating and Enhancing Deep Learning Systems},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616266},
doi = {10.1145/3611643.3616266},
abstract = {Deep learning (DL) models are trained on sampled data, where the distribution of training data differs from that of real-world data (i.e., the distribution shift), which reduces the model's robustness. Various testing techniques have been proposed, including distribution-unaware and distribution-aware methods. However, distribution-unaware testing lacks effectiveness by not explicitly considering the distribution of test cases and may generate redundant errors (within same distribution). Distribution-aware testing techniques primarily focus on generating test cases that follow the training distribution, missing out-of-distribution data that may also be valid and should be considered in the testing process.  
In this paper, we propose a novel distribution-guided approach for generating valid test cases with diverse distributions, which can better evaluate the model's robustness (i.e., generating hard-to-detect errors) and enhance the model's robustness (i.e., enriching training data). Unlike existing testing techniques that optimize individual test cases, DistXplore optimizes test suites that represent specific distributions. To evaluate and enhance the model's robustness, we design two metrics: distribution difference, which maximizes the similarity in distribution between two different classes of data to generate hard-to-detect errors, and distribution diversity, which increase the distribution diversity of generated test cases for enhancing the model's robustness. To evaluate the effectiveness of DistXplore in model evaluation and enhancement, we compare DistXplore with 14 state-of-the-art baselines on 10 models across 4 datasets. The evaluation results show that DisXplore not only detects a larger number of errors (e.g., 2\texttimes{}+ on average). Furthermore, DistXplore achieves a higher improvement in empirical robustness (e.g., 5.2\% more accuracy improvement than the baselines on average).},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {68–80},
numpages = {13},
keywords = {Deep learning, distribution diversity, model enhancement, software testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@inproceedings{10.1145/3597926.3605238,
author = {Sarker, Laboni},
title = {Quantitative Symbolic Similarity Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3605238},
doi = {10.1145/3597926.3605238},
abstract = {Similarity analysis plays a crucial role in various software engineering tasks, such as detecting software changes, version merging, identifying plagiarism, and analyzing binary code. Equivalence analysis, a stricter form of similarity, focuses on determining whether different programs or versions of the same program behave identically. While extensive research exists on code and binary similarity as well as equivalence analysis, there is a lack of quantitative reasoning in these areas. Non-equivalence is a spectrum that requires deeper exploration, as it can manifest in different ways across the input domain space. This paper emphasizes the importance of quantitative reasoning on non-equivalence which arises due to semantic differences. By quantitatively reasoning about non-equivalence, it becomes possible to identify specific input ranges for which programs are equivalent or non-equivalent. We aim to address the gap in quantitative reasoning in symbolic similarity analysis, enabling a more comprehensive understanding of program behavior.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1549–1551},
numpages = {3},
keywords = {equivalence, model counting, quantitative analysis, similarity, symbolic execution},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3597926.3598051,
author = {Chen, Zhuo and Liu, Jie and Hu, Yubo and Wu, Lei and Zhou, Yajin and He, Yiling and Liao, Xianhao and Wang, Ke and Li, Jinku and Qin, Zhan},
title = {DeUEDroid: Detecting Underground Economy Apps Based on UTG Similarity},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598051},
doi = {10.1145/3597926.3598051},
abstract = {In recent years, the underground economy is proliferating in the mobile system. These underground economy apps (UEware for short) make profits from providing non-compliant services, especially in sensitive areas (e.g., gambling, porn, loan). Unlike traditional malware, most of them (over 80\%) do not have malicious payloads. Due to their unique characteristics, existing detection approaches  
cannot effectively and efficiently mitigate this emerging threat.  
To address this problem, we propose a novel approach to effectively and efficiently detect UEware by considering their UI transition graphs (UTGs). Based on the proposed approach, we design and implement a system, named DeUEDroid, to perform the detection. To evaluate DeUEDroid, we collect 25, 717 apps and build up the first large-scale ground-truth dataset (1, 700 apps) of UEware. The evaluation result based on the ground-truth dataset shows that DeUEDroid can cover new UI features and statically construct precise UTG. It achieves 98.22\% detection F1-score and 98.97\% classification accuracy, a significantly better performance than the traditional approaches. The evaluation result involving 24, 017 apps demonstrates the effectiveness and efficiency of UEware detection in real-world scenarios. Furthermore, the result also reveals that UEware are prevalent, i.e., 54\% apps in the wild and 11\% apps in the app stores are UEware. Our work sheds light on the future work of analyzing and detecting UEware. To engage the community, we have made our prototype system and the dataset available online.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {223–235},
numpages = {13},
keywords = {underground economy, machine learning, UI transition graph},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3597926.3598121,
author = {Xu, Xiangzhe and Feng, Shiwei and Ye, Yapeng and Shen, Guangyu and Su, Zian and Cheng, Siyuan and Tao, Guanhong and Shi, Qingkai and Zhang, Zhuo and Zhang, Xiangyu},
title = {Improving Binary Code Similarity Transformer Models by Semantics-Driven Instruction Deemphasis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3598121},
doi = {10.1145/3597926.3598121},
abstract = {Given a function in the binary executable form, binary code similarity analysis determines a set of similar functions from a large pool of candidate functions. These similar functions are usually compiled from the same source code with different compilation setups. Such analysis has a large number of applications, such as malware detection, code clone detection, and automatic software patching. The state-of-the art methods utilize complex Deep Learning models such as Transformer models. We observe that these models suffer from undesirable instruction distribution biases caused by specific compiler conventions. We develop a novel technique to detect such biases and repair them by removing the corresponding instructions from the dataset and finetuning the models. This entails synergy between Deep Learning model analysis and program analysis. Our results show that we can substantially improve the state-of-the-art models’ performance by up to 14.4\% in the most challenging cases where test data may be out of the distributions of training data.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1106–1118},
numpages = {13},
keywords = {Transformer, Program Analysis, Binary Similarity Analysis},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1145/3597926.3605233,
author = {Shrestha, Sohil Lal},
title = {Harnessing Large Language Models for Simulink Toolchain Testing and Developing Diverse Open-Source Corpora of Simulink Models for Metric and Evolution Analysis},
year = {2023},
isbn = {9798400702211},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3597926.3605233},
doi = {10.1145/3597926.3605233},
abstract = {MATLAB/Simulink is a de-facto standard tool in several safety-critical industries such as automotive, aerospace, healthcare, and industrial automation for system modeling and analysis, compiling models to code, and deploying code to embedded hardware. On one hand, testing cyber-physical system (CPS) development tools such as MathWorks’ Simulink is important as a bug in the toolchain may propagate to the artifacts they produce. On the other hand, it is equally important to understand modeling practices and model evolution to support engineers and scientists as they are widely used in design, simulation, and verification of CPS models. Existing work in this area is limited by two main factors, i.e., (1) inefficiencies of state-of-the-art testing schemes in finding critical tool-chain bugs and (2) the lack of a reusable corpus of public Simulink models. In my thesis, I propose to (1) curate a large reusable corpus of Simulink models to help understand modeling practices and model evolution and (2) leverage such a corpus with deep-learning based language models to test the toolchain.},
booktitle = {Proceedings of the 32nd ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {1541–1545},
numpages = {5},
keywords = {tool chain bugs, programming language modeling, open-source, model evolution, mining software repositories, deep learning, Simulink, GPT-2, Cyber-physical system development},
location = {Seattle, WA, USA},
series = {ISSTA 2023}
}

@inproceedings{10.1109/ICSE48619.2023.00217,
author = {Christian, Garrett and Woodlief, Trey and Elbaum, Sebastian},
title = {Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00217},
doi = {10.1109/ICSE48619.2023.00217},
abstract = {Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {2604–2616},
numpages = {13},
keywords = {machine learning, software testing and validation},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1007/978-981-99-8664-4_24,
author = {Zhu, Jin and Tao, Chuanqi and Guo, Hongjing and Ju, Yue},
title = {DeepTD: Diversity-Guided Deep Neural Network Test Generation},
year = {2023},
isbn = {978-981-99-8663-7},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-8664-4_24},
doi = {10.1007/978-981-99-8664-4_24},
abstract = {Coverage-guided Fuzz Testing (CGF) techniques have been applied to deep neural network (DNN) testing in recent years, generating a significant number of test samples to uncover inherent defects in DNN models. However, the effectiveness of CGF techniques that utilize structured coverage metrics as coverage criteria is currently being questioned. A few unstructured coverage metrics, such as surprise adequacy, only take into account the diversity of the test samples against the training set, while ignoring the diversity of the test samples themselves. In addition to this, the existing surprise adequacy metrics have some limitations in their applications. Therefore, this paper proposes DeepTD, a diversity-guided deep neural networks test generation method. Firstly, DeepTD selects high-loss test samples from each class on average, ensuring these test seeds possess a strong ability to reveal model errors. Then, DeepTD transforms these test seeds to enhance the diversity of the generated samples. Finally, Cluster-based Surprise Adequacy is designed to guide the generation of test samples. To evaluate the effectiveness of DeepTD, six DNN models are selected as subjects, covering two well-known image datasets. Experimental results demonstrate that the Cluster-based Surprise Adequacy outperforms the two existing metrics not only in computational efficiency but also in discovering more model defects. What’s more, the test samples generated by DeepTD are on average 6.04\% and 3.24\% more effective for model retraining in MNIST and CIFAR10 compared to baseline methods, respectively.},
booktitle = {Dependable Software Engineering. Theories, Tools, and Applications: 9th International Symposium, SETTA 2023, Nanjing, China, November 27–29, 2023, Proceedings},
pages = {419–433},
numpages = {15},
keywords = {Test generation, Test samples diversity, Deep neural network, Coverage-guided fuzz testing},
location = {Nanjing, China}
}

@inproceedings{10.1007/978-981-99-9785-5_35,
author = {Chang, Zhiwei and Zhang, Hanfeng and Yang, Yue and Jia, Yan and Xu, Sihan and Li, Tong and Liu, Zheli},
title = {Fuzzing Drone Control System Configurations Based on&nbsp;Quality-Diversity Enhanced Genetic Algorithm},
year = {2024},
isbn = {978-981-99-9784-8},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-981-99-9785-5_35},
doi = {10.1007/978-981-99-9785-5_35},
abstract = {As drones are becoming widely used in various fields, drone security is a growing challenge nowadays. Drone control systems use various configuration parameters to control their positions and attitudes. If these parameters are misconfigured, drones will fall into abnormal flight states, such as trajectory deviation and crash to the ground. Existing works mainly focus on system memory errors which lead to obvious system failure but don’t apply to drone flight state anomalies. This paper focuses on abnormal drone flight states caused by configuration parameter errors. We propose a novel state-guided fuzzing system called APFuzzer, which searches for incorrect configuration parameter values that would trigger abnormal flight states. To enhance the capability of searching for multiple optimal solutions, we design a quality-diversity enhanced genetic algorithm (QDGA) to mutate configurations to search for incorrect configuration parameter values and consider the effects of environmental factors and flight missions on the flight states. We evaluated APFuzzer on the drone control system ArduPilot and successfully searched 3389 incorrect configuration parameter values and triggered all predefined five abnormal flight states. In addition, APFuzzer automatically analyzed the fuzzing results and found five software bugs related to configurations.},
booktitle = {Artificial Intelligence Security and Privacy: First International Conference on Artificial Intelligence Security and Privacy, AIS&amp;P 2023, Guangzhou, China, December 3–5, 2023, Proceedings, Part I},
pages = {499–512},
numpages = {14},
keywords = {Drone security, Configuration parameters, Fuzzing, Quality diversity, Genetic algorithm},
location = {Guangzhou, China}
}

@article{10.1145/3571855,
author = {Nass, Michel and Al\'{e}groth, Emil and Feldt, Robert and Leotta, Maurizio and Ricca, Filippo},
title = {Similarity-based Web Element Localization for Robust Test Automation},
year = {2023},
issue_date = {May 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3571855},
doi = {10.1145/3571855},
abstract = {Non-robust (fragile) test execution is a commonly reported challenge in GUI-based test automation, despite much research and several proposed solutions. A test script needs to be resilient to (minor) changes in the tested application but, at the same time, fail when detecting potential issues that require investigation. Test script fragility is a multi-faceted problem. However, one crucial challenge is how to reliably identify and locate the correct target web elements when the website evolves between releases or otherwise fail and report an issue. This article proposes and evaluates a novel approach called similarity-based web element localization (Similo), which leverages information from multiple web element locator parameters to identify a target element using a weighted similarity score. This experimental study compares Similo to a baseline approach for web element localization. To get an extensive empirical basis, we target 48 of the most popular websites on the Internet in our evaluation. Robustness is considered by counting the number of web elements found in a recent website version compared to how many of these existed in an older version. Results of the experiment show that Similo outperforms the baseline; it failed to locate the correct target web element in 91 out of 801 considered cases (i.e., 11\%) compared to 214 failed cases (i.e., 27\%) for the baseline approach. The time efficiency of Similo was also considered, where the average time to locate a web element was determined to be 4 milliseconds. However, since the cost of web interactions (e.g., a click) is typically on the order of hundreds of milliseconds, the additional computational demands of Similo can be considered negligible. This study presents evidence that quantifying the similarity between multiple attributes of web elements when trying to locate them, as in our proposed Similo approach, is beneficial. With acceptable efficiency, Similo gives significantly higher effectiveness (i.e., robustness) than the baseline web element localization approach.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {75},
numpages = {30},
keywords = {XPath locators, web element locators, test case robustness, test automation, GUI testing}
}

@inproceedings{10.1145/3644032.3644455,
author = {Elgendy, Islam and Hierons, Robert and Mcminn, Phil},
title = {Evaluating String Distance Metrics for Reducing Automatically Generated Test Suites},
year = {2024},
isbn = {9798400705885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3644032.3644455},
doi = {10.1145/3644032.3644455},
abstract = {Regression test suites can have a large number of test cases, especially automatically generated ones, and tend to grow in size, making it costly to run the entire test suite. Test suite reduction aims to eliminate some test cases to reduce the test suite size and therefore reduce the cost of running it. In this paper, string distances on the text of the test cases are used as measures of similarity for reduction. A practical benefit of using string distance is that there is no need to run the test cases: the test suite source code is the only requirement, making the approach fast. We reduce test suites generated from Randoop and EvoSuite; two well-known test generation tools of Java programs. We implemented a string-based similarity reduction and compared it against random reduction. In the experiments, mutation scores using reduced test suites based on maximising string dissimilarity of test cases were higher than those for random reduction in over 70\% of the test suites generated. Also, the results showed that test suites generated by Randoop can be drastically reduced in one case by 99\% using the string-based similarity reduction approach while maintaining the fault-finding capabilities of the original test suite. Finally, on average, the normalised compression distance was found to be the best similarity metric choice in terms of fault-detection.},
booktitle = {Proceedings of the 5th ACM/IEEE International Conference on Automation of Software Test (AST 2024)},
pages = {171–181},
numpages = {11},
keywords = {test suite reduction, similarity-based testing, diversity-based testing, automatically generated tests},
location = {Lisbon, Portugal},
series = {AST '24}
}

@article{10.1016/j.cose.2023.103197,
author = {Wang, Wenpeng and Chen, Zhixiang and Zheng, Ziyang and Wang, Hui},
title = {An adaptive fuzzing method based on transformer and protocol similarity mutation},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {129},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103197},
doi = {10.1016/j.cose.2023.103197},
journal = {Comput. Secur.},
month = {jun},
numpages = {10},
keywords = {Industrial control protocols, Fuzzing, Modbus TCP, Vulnerability mining, Transformer}
}

@article{10.1109/TSE.2023.3309610,
author = {Huai, Yuqi and Almanee, Sumaya and Chen, Yuntianyi and Wu, Xiafa and Chen, Qi Alfred and Garcia, Joshua},
title = {&lt;sc&gt;scenoRITA&lt;/sc&gt;: Generating Diverse, Fully Mutable, Test Scenarios for Autonomous Vehicle Planning},
year = {2023},
issue_date = {Oct. 2023},
publisher = {IEEE Press},
volume = {49},
number = {10},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2023.3309610},
doi = {10.1109/TSE.2023.3309610},
abstract = {Autonomous Vehicles (AVs) leverage advanced sensing and networking technologies (e.g., camera, LiDAR, RADAR, GPS, DSRC, 5G, etc.) to enable safe and efficient driving without human drivers. Although still in its infancy, AV technology is becoming increasingly common and could radically transform our transportation system and by extension, our economy and society. As a result, there is tremendous global enthusiasm for research, development, and deployment of AVs, e.g., self-driving taxis and trucks from Waymo and Baidu. The current practice for testing AVs uses virtual tests—where AVs are tested in software simulations—since they offer a more efficient and safer alternative compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, systematically creating &lt;italic&gt;valid&lt;/italic&gt; and &lt;italic&gt;effective&lt;/italic&gt; tests for AV software remains a major challenge. To address this challenge, we introduce &lt;sc&gt;scenoRITA&lt;/sc&gt;, a test generation approach for AVs that uses an evolutionary algorithm with (1) a novel gene representation that allows obstacles to be &lt;italic&gt;fully mutable&lt;/italic&gt;, hence, resulting in more reported violations and more diverse scenarios, (2) 5 test oracles to determine both safety and motion sickness-inducing violations and (3) a novel technique to identify and eliminate duplicate tests. Our extensive evaluation shows that &lt;sc&gt;scenoRITA&lt;/sc&gt; can produce test scenarios that are more effective in revealing ADS bugs and more diverse in covering different parts of the map compared to other state-of-the-art test generation approaches.},
journal = {IEEE Trans. Softw. Eng.},
month = {aug},
pages = {4656–4676},
numpages = {21}
}

@inproceedings{10.1145/3551349.3559519,
author = {Ghanbari, Ali and Marcus, Andrian (Andi)},
title = {Shibboleth: Hybrid Patch Correctness Assessment in Automated Program Repair},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559519},
doi = {10.1145/3551349.3559519},
abstract = {Test-based generate-and-validate automated program repair (APR) systems generate many patches that pass the test suite without fixing the bug. The generated patches must be manually inspected by the developers, a task that tends to be time-consuming, thereby diminishing the role of APR in reducing debugging costs. We present the design and implementation of a novel tool, named Shibboleth, for automatic assessment of the patches generated by test-based generate-and-validate APR systems. Shibboleth leverages lightweight static and dynamic heuristics from both test and production code to rank and classify the patches. Shibboleth is based on the idea that the buggy program is almost correct and the bugs are small mistakes that require small changes to fix and specifically the fix does not remove the code implementing correct functionality of the program. Thus, the tool measures the impact of patches on both production code (via syntactic and semantic similarity) and test code (via code coverage) to separate the patches that result in similar programs and that do not remove desired program elements. We have evaluated Shibboleth on 1,871 patches, generated by 29 Java-based APR systems for Defects4J programs. The technique outperforms state-of-the-art raking and classification techniques. Specifically, in our ranking data set, in 66\% of the cases, Shibboleth ranks the correct patch in top-1 or top-2 positions and, in our classification data set, it achieves an accuracy and F1-score of 0.887 and 0.852, respectively, in classification mode. A demo video of the tool is available at https://bit.ly/3NvYJN8.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {166},
numpages = {4},
keywords = {Similarity, Patch Correctness Assessment, Branch Coverage, Automated Program Repair},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1145/3611643.3616337,
author = {Liu, Jiawei and Peng, Jinjun and Wang, Yuyao and Zhang, Lingming},
title = {NeuRI: Diversifying DNN Generation via Inductive Rule Inference},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616337},
doi = {10.1145/3611643.3616337},
abstract = {Deep Learning (DL) is prevalently used in various industries to improve decision-making and automate processes, driven by the ever-evolving DL libraries and compilers. The correctness of DL systems is crucial for trust in DL applications.  
As such, the recent wave of research has been studying the automated synthesis of test-cases (i.e., DNN models and their inputs) for fuzzing DL systems. However, existing model generators only subsume a limited number of operators, lacking the ability to pervasively model operator constraints.  
To address this challenge, we propose NeuRI, a fully automated approach for generating valid and diverse DL models composed of hundreds of types of operators. NeuRI adopts a three-step process:  
(i) collecting valid and invalid API traces from various sources;  
(ii) applying inductive program synthesis over the traces to infer the constraints for constructing valid models; and  
(iii) using hybrid model generation which incorporates both symbolic and concrete operators.  
Our evaluation shows that NeuRI improves branch coverage of TensorFlow and PyTorch by 24\% and 15\% over the state-of-the-art model-level fuzzers. NeuRI finds 100 new bugs for PyTorch and TensorFlow in four months, with 81 already fixed or confirmed. Of these, 9 bugs are labelled as high priority or security vulnerability, constituting 10\% of all high-priority bugs of the period.  
Open-source developers regard error-inducing tests reported by us as "high-quality" and "common in practice".},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {657–669},
numpages = {13},
keywords = {Compiler Testing, Deep Learning Compilers, Fuzzing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1109/TSE.2022.3170272,
author = {Viggiato, Markos and Paas, Dale and Buzon, Chris and Bezemer, Cor-Paul},
title = {Identifying Similar Test Cases That Are Specified in Natural Language},
year = {2023},
issue_date = {March 2023},
publisher = {IEEE Press},
volume = {49},
number = {3},
issn = {0098-5589},
url = {https://doi.org/10.1109/TSE.2022.3170272},
doi = {10.1109/TSE.2022.3170272},
abstract = {Software testing is still a manual process in many industries, despite the recent improvements in automated testing techniques. As a result, test cases (which consist of one or more test steps that need to be executed manually by the tester) are often specified in natural language by different employees and many redundant test cases might exist in the test suite. This increases the (already high) cost of test execution. Manually identifying similar test cases is a time-consuming and error-prone task. Therefore, in this paper, we propose an unsupervised approach to identify similar test cases. Our approach uses a combination of text embedding, text similarity and clustering techniques to identify similar test cases. We evaluate five different text embedding techniques, two text similarity metrics, and two clustering techniques to cluster similar test steps and three techniques to identify similar test cases from the test step clusters. Through an evaluation in an industrial setting, we showed that our approach achieves a high performance to cluster test steps (an F-score of 87.39%) and identify similar test cases (an F-score of 86.13%). Furthermore, a validation with developers indicates several different practical usages of our approach (such as identifying redundant test cases), which help to reduce the testing manual effort and time.},
journal = {IEEE Trans. Softw. Eng.},
month = {mar},
pages = {1027–1043},
numpages = {17}
}

@inproceedings{10.1145/3575693.3575707,
author = {Liu, Jiawei and Lin, Jinkun and Ruffy, Fabian and Tan, Cheng and Li, Jinyang and Panda, Aurojit and Zhang, Lingming},
title = {NNSmith: Generating Diverse and Valid Test Cases for Deep Learning Compilers},
year = {2023},
isbn = {9781450399166},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3575693.3575707},
doi = {10.1145/3575693.3575707},
abstract = {Deep-learning (DL) compilers such as TVM and TensorRT are increasingly being used to optimize deep neural network (DNN) models to meet performance, resource utilization and other requirements. Bugs in these compilers can result in models whose semantics differ from the original ones, producing incorrect results that corrupt the correctness of downstream applications. However, finding bugs in these compilers is challenging due to their complexity. In this work, we propose a new fuzz testing approach for finding bugs in deep-learning compilers. Our core approach consists of (i) generating diverse yet valid DNN test models that can exercise a large part of the compiler's transformation logic using light-weight operator specifications; (ii) performing gradient-based search to find model inputs that avoid any floating-point exceptional values during model execution, reducing the chance of missed bugs or false alarms; and (iii) using differential testing to identify bugs. We implemented this approach in NNSmith which has found 72 new bugs for TVM, TensorRT, ONNXRuntime, and PyTorch to date. Of these 58 have been confirmed and 51 have been fixed by their respective project maintainers.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {530–543},
numpages = {14},
keywords = {Fuzzing, Deep Learning Compilers, Compiler Testing},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings{10.1145/3611643.3616324,
author = {Qi, Xiaofang and Qian, Xiang and Li, Yanhui},
title = {Semantic Test Repair for Web Applications},
year = {2023},
isbn = {9798400703270},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3611643.3616324},
doi = {10.1145/3611643.3616324},
abstract = {Automation testing is widely used in the functional testing of web applications. However, during the evolution of web applications, such web test scripts tend to break. It is essential to repair such broken test scripts to make regression testing run successfully. As manual repairing is time-consuming and expensive, researchers focus on automatic repairing techniques. Empirical study shows that the web element locator is the leading cause of web test breakages. Most existing repair techniques utilize Document Object Model attributes or visual appearances of elements to find their location but neglect their semantic information. This paper proposes a novel semantic repair technique called Semantic Test Repair (Semter) for web test repair. Our approach captures relevant semantic information from test executions on the application’s basic version and locates target elements by calculating semantic similarity between elements to repair tests. Our approach can also repair test workflow due to web page additions or deletions by a local exploration in the updated version. We evaluated the efficacy of our technique on six real-world web applications compared with three baselines. Experimental results show that Semter achieves an 84\% average repair ratio within an acceptable time cost, significantly outperforming the state-of-the-art web test repair techniques.},
booktitle = {Proceedings of the 31st ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1190–1202},
numpages = {13},
keywords = {GUI Testing, Semantic Similarity, Test Repair, Web Testing},
location = {San Francisco, CA, USA},
series = {ESEC/FSE 2023}
}

@article{10.1016/j.engappai.2023.106110,
author = {Panda, Rama Ranjan and Nagwani, Naresh Kumar},
title = {An intuitionistic fuzzy representation based software bug severity prediction approach for imbalanced severity classes},
year = {2023},
issue_date = {Jun 2023},
publisher = {Pergamon Press, Inc.},
address = {USA},
volume = {122},
number = {C},
issn = {0952-1976},
url = {https://doi.org/10.1016/j.engappai.2023.106110},
doi = {10.1016/j.engappai.2023.106110},
journal = {Eng. Appl. Artif. Intell.},
month = {jun},
numpages = {18},
keywords = {Software reliability, Intuitionistic fuzzy similarity, Software maintenance, Severity prediction, Machine learning, Topic modeling}
}

@article{10.1016/j.jss.2023.111607,
author = {Jiang, Yuan and Su, Xiaohong and Treude, Christoph and Shang, Chao and Wang, Tiantian},
title = {Does Deep Learning improve the performance of duplicate bug report detection? An empirical study},
year = {2023},
issue_date = {Apr 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {198},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111607},
doi = {10.1016/j.jss.2023.111607},
journal = {J. Syst. Softw.},
month = {apr},
numpages = {26},
keywords = {Realistic evaluation, Similarity measure, Information retrieval, Deep learning, Duplicate bug report detection}
}

@article{10.1007/s10115-023-02000-7,
author = {Panda, Rama Ranjan and Nagwani, Naresh Kumar},
title = {Software bug priority prediction technique based on intuitionistic fuzzy representation and class imbalance learning},
year = {2023},
issue_date = {Mar 2024},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {66},
number = {3},
issn = {0219-1377},
url = {https://doi.org/10.1007/s10115-023-02000-7},
doi = {10.1007/s10115-023-02000-7},
abstract = {In modern times, the software industry is more focused on the timely release of high-quality software. Software bugs have a significant impact on software quality and reliability. To complete the bug triaging process on time, the triager has to understand each bug and assign the correct priority to it. However, the bugs are reported rapidly, with lots of uncertainty and irregularities in the bug tracking system. Furthermore, there are multiple priority labels that are semantically close to each other. As a result, the triager is confused while understanding and prioritizing the bugs. To address these problems, the research presents an intuitionistic fuzzy representation of topic features-based software bug priority prediction (IFTBPP) technique. Initially, the imbalanced priority classes of software bugs are balanced using the synthetic minority oversampling technique. Then, topic modeling is used to create topics and terms for software bugs. The intuitionistic fuzzy set is used on the topics to compute various grades of a bug belonging to multiple priority classes. Finally, the similarity of a newly reported bug is calculated using intuitionistic fuzzy similarity measures with multiple priority classes. All the experiments of IFTBPP are conducted on Eclipse, Mozilla, Apache, and NetBeans repositories and compared with other existing models. The accuracy values obtained by IFTBPP on these repositories are 92.5\%, 91.9\%, 89.2\%, and 93.9\%, whereas the corresponding F-measure values are 91.7\%, 91.3\%, 88.9\%, and 93.1\%.},
journal = {Knowl. Inf. Syst.},
month = {oct},
pages = {2135–2164},
numpages = {30},
keywords = {Software maintenance, Fuzzy modeling, Priority prediction, Intuitionistic fuzzy similarity, Topic modeling, Class imbalance learning}
}

@inproceedings{10.1145/3551349.3556919,
author = {Xie, Xiaoyuan and Yin, Pengbo and Chen, Songqiang},
title = {Boosting the Revealing of Detected Violations in Deep Learning Testing: A Diversity-Guided Method},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3556919},
doi = {10.1145/3551349.3556919},
abstract = {Due to the ability to bypass the oracle problem, Metamorphic Testing (MT) has been a popular technique to test deep learning (DL) software. However, no work has taken notice of the prioritization for Metamorphic test case Pairs (MPs), which is quite essential and beneficial to the effectiveness of MT in DL testing. When the fault-sensitive MPs apt to trigger violations and expose defects are not prioritized, the revealing of some detected violations can be greatly delayed or even missed to conceal critical defects. In this paper, we propose the first method to prioritize the MPs for DL software, so as to boost the revealing of detected violations in DL testing. Specifically, we devise a new type of metric to measure the execution diversity of DL software on MPs based on the distribution discrepancy of the neuron outputs. The fault-sensitive MPs are next prioritized based on the devised diversity metric. Comprehensive evaluation results show that the proposed prioritization method and diversity metric can effectively prioritize the fault-sensitive MPs, boost the revealing of detected violations, and even facilitate the selection and design of the effective Metamorphic Relations for the image classification DL software.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {17},
numpages = {13},
keywords = {test case selection and prioritization, metamorphic testing, diversity, deep learning testing},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@inproceedings{10.1109/ICSE-SEET58685.2023.00026,
author = {Gra\ss{}l, Isabella and Fraser, Gordon and Trieflinger, Stefan and Kuhrmann, Marco},
title = {Exposing Software Engineering Students to Stressful Projects: Does Diversity Matter?},
year = {2023},
isbn = {9798350322590},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEET58685.2023.00026},
doi = {10.1109/ICSE-SEET58685.2023.00026},
abstract = {Software development teams have to face stress caused by deadlines, staff turnover, or individual differences in commitment, expertise, and time zones. While students are typically taught the theory of software project management, their exposure to such stress factors is usually limited. However, preparing students for the stress they will have to endure once they work in project teams is important for their own sake, as well as for the sake of team performance in the face of stress. Team performance has been linked to the diversity of software development teams, but little is known about how diversity influences the stress experienced in teams. In order to shed light on this aspect, we provided students with the opportunity to self-experience the basics of project management in self-organizing teams, and studied the impact of six diversity dimensions on team performance, coping with stressors, and positive perceived learning effects. Three controlled experiments at two universities with a total of 65 participants suggest that the social background impacts the perceived stressors the most, while age and work experience have the highest impact on perceived learnings. Most diversity dimensions have a medium correlation with the quality of work, yet no significant relation to the team performance. This lays the foundation to improve students' training for software engineering teamwork based on their diversity-related needs and to create diversity-sensitive awareness among educators, employers and researchers.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering: Software Engineering Education and Training},
pages = {210–222},
numpages = {13},
keywords = {team work, diversity, project management, software engineering education},
location = {Melbourne, Australia},
series = {ICSE-SEET '23}
}

@inproceedings{10.1109/ICSE48619.2023.00146,
author = {Pan, Rongqi and Ghaleb, Taher A. and Briand, Lionel},
title = {ATM: Black-Box Test Case Minimization Based on Test Code Similarity and Evolutionary Search},
year = {2023},
isbn = {9781665457019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE48619.2023.00146},
doi = {10.1109/ICSE48619.2023.00146},
abstract = {Executing large test suites is time and resource consuming, sometimes impossible, and such test suites typically contain many redundant test cases. Hence, test case (suite) minimization is used to remove redundant test cases that are unlikely to detect new faults. However, most test case minimization techniques rely on code coverage (white-box), model-based features, or requirements specifications, which are not always (entirely) accessible by test engineers. Code coverage analysis also leads to scalability issues, especially when applied to large industrial systems. Recently, a set of novel techniques was proposed, called FAST-R, relying solely on test case code for test case minimization, which appeared to be much more efficient than white-box techniques. However, it achieved a comparable low fault detection capability for Java projects, thus making its application challenging in practice. In this paper, we propose ATM (AST-based Test case Minimizer), a similarity-based, search-based test case minimization technique, taking a specific budget as input, that also relies exclusively on the source code of test cases but attempts to achieve higher fault detection through finer-grained similarity analysis and a dedicated search algorithm. ATM transforms test case code into Abstract Syntax Trees (AST) and relies on four tree-based similarity measures to apply evolutionary search, specifically genetic algorithms, to minimize test cases. We evaluated the effectiveness and efficiency of ATM on a large dataset of 16 Java projects with 661 faulty versions using three budgets ranging from 25\% to 75\% of test suites. ATM achieved significantly higher fault detection rates (0.82 on average), compared to FAST-R (0.61 on average) and random minimization (0.52 on average), when running only 50\% of the test cases, within practically acceptable time (1.1 -- 4.3 hours, on average, per project version), given that minimization is only occasionally applied when many new test cases are created (major releases). Results achieved for other budgets were consistent.},
booktitle = {Proceedings of the 45th International Conference on Software Engineering},
pages = {1700–1711},
numpages = {12},
keywords = {black-box testing, genetic algorithm, AST, tree-based similarity, test suite reduction, test case minimization},
location = {Melbourne, Victoria, Australia},
series = {ICSE '23}
}

@inproceedings{10.1145/3551349.3559553,
author = {Priamo, Giacomo and D'Elia, Daniele Cono and Querzoni, Leonardo},
title = {Principled Composition of Function Variants for Dynamic Software Diversity and Program Protection},
year = {2023},
isbn = {9781450394758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3551349.3559553},
doi = {10.1145/3551349.3559553},
abstract = {Artificial diversification of a software program can be a versatile tool in a wide range of software engineering and security scenarios. For example, randomizing implementation aspects can increase the costs for attackers as it prevents them from benefiting of precise knowledge of their target. A promising angle for diversification can be having two runs of a program on the same input yield inherently diverse instruction traces. Inspired by on-stack replacement designs for managed runtimes, in this paper we study how to transform a C program to realize continuous transfers of control and program state among function variants as they run. We discuss the technical challenges toward such goal and propose effective compiler techniques for it that enable the re-use of existing techniques for static diversification with no modifications. We implement our approach in LLVM and evaluate it on both synthetic and real-world subjects.},
booktitle = {Proceedings of the 37th IEEE/ACM International Conference on Automated Software Engineering},
articleno = {183},
numpages = {5},
keywords = {on-stack replacement., obfuscation, hardening, Software diversity},
location = {Rochester, MI, USA},
series = {ASE '22}
}

@article{10.1145/3628158,
author = {Xiang, Yi and Huang, Han and Li, Sizhe and Li, Miqing and Luo, Chuan and Yang, Xiaowei},
title = {Automated Test Suite Generation for Software Product Lines Based on Quality-Diversity Optimization},
year = {2023},
issue_date = {February 2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {33},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3628158},
doi = {10.1145/3628158},
abstract = {A Software Product Line (SPL) is a set of software products that are built from a variability model. Real-world SPLs typically involve a vast number of valid products, making it impossible to individually test each of them. This arises the need for automated test suite generation, which was previously modeled as either a single-objective or a multi-objective optimization problem considering only objective functions. This article provides a completely different mathematical model by exploiting the benefits of Quality-Diversity (QD) optimization that is composed of not only an objective function (e.g., t-wise coverage or test suite diversity) but also a user-defined behavior space (e.g., the space with test suite size as its dimension). We argue that the new model is more suitable and generic than the two alternatives because it provides at a time a large set of diverse (measured in the behavior space) and high-performing solutions that can ease the decision-making process. We apply MAP-Elites, one of the most popular QD algorithms, to solve the model. The results of the evaluation, on both realistic and artificial SPLs, are promising, with MAP-Elites significantly and substantially outperforming both single- and multi-objective approaches, and also several state-of-the-art SPL testing tools. In summary, this article provides a new and promising perspective on the test suite generation for SPLs.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {46},
numpages = {52},
keywords = {Quality-Diversity (QD) optimization, automated test suite generation, Software Product Line}
}

@article{10.4018/IJSWIS.322392,
author = {Xia, Bing and Liu, Wenbo and He, Qudong and Liu, Fudong and Pang, Jianmin and Yang, RuiNan and Yin, JiaBin and Ge, YunXiang},
title = {Binary Vulnerability Similarity Detection Based on Function Parameter Dependency},
year = {2023},
issue_date = {Jun 2023},
publisher = {IGI Global},
address = {USA},
volume = {19},
number = {1},
issn = {1552-6283},
url = {https://doi.org/10.4018/IJSWIS.322392},
doi = {10.4018/IJSWIS.322392},
abstract = {Many existing works compute the binary vulnerability similarity based on binary procedure, which has coarse detection granularity and cannot locate the vulnerability trigger position accurately, and have a higher false positive rate, so a new binary vulnerability similarity detection method based on function parameter dependency in hazard API is proposed. First, convert the instructions of different architectures into an intermediate language, and use the compiler with a back-end optimizer to optimize and normalize the binary procedure. Then, locate the hazard API that appears in the binary procedure, and perform the function parameters dependency analysis to generate a set of parameter slices on the hazard API. Experiments show that the method has a higher recall rate (up to 14.3\% better than the baseline model) in real-world scenarios, and not only locates the triggering position of the vulnerability but also identifies the fixed vulnerability.},
journal = {Int. J. Semant. Web Inf. Syst.},
month = {apr},
pages = {1–16},
numpages = {16},
keywords = {Binary Similarity, Neural Networks, Parameter Slice, Vulnerability Detection}
}

@article{10.1016/j.cose.2023.103296,
author = {Cabrera-Arteaga, Javier and Monperrus, Martin and Toady, Tim and Baudry, Benoit},
title = {WebAssembly diversification for malware evasion},
year = {2023},
issue_date = {Aug 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {131},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103296},
doi = {10.1016/j.cose.2023.103296},
journal = {Comput. Secur.},
month = {aug},
numpages = {16},
keywords = {Malware evasion, Software diversification, Cryptojacking, WebAssembly}
}

@article{10.1016/j.jss.2023.111796,
author = {Zakeri-Nasrabadi, Morteza and Parsa, Saeed and Ramezani, Mohammad and Roy, Chanchal and Ekhtiarzadeh, Masoud},
title = {A systematic literature review on source code similarity measurement and clone detection: Techniques, applications, and challenges},
year = {2023},
issue_date = {Oct 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {204},
number = {C},
issn = {0164-1212},
url = {https://doi.org/10.1016/j.jss.2023.111796},
doi = {10.1016/j.jss.2023.111796},
journal = {J. Syst. Softw.},
month = {oct},
numpages = {33},
keywords = {Systematic literature review, Code recommendation, Plagiarism detection, Code clone, Source code similarity}
}

@article{10.1016/j.jksuci.2023.101563,
author = {Lee, Seonyeol and Choi, Hyun-Jae and Chae, Heung-Seok},
title = {Augmenting a round-trip path test suite to cover all event-pairs for testing more diverse usage scenarios},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Inc.},
address = {USA},
volume = {35},
number = {6},
issn = {1319-1578},
url = {https://doi.org/10.1016/j.jksuci.2023.101563},
doi = {10.1016/j.jksuci.2023.101563},
journal = {J. King Saud Univ. Comput. Inf. Sci.},
month = {jun},
numpages = {16},
keywords = {State-Based Testing, UML State Machine, Round-Trip Path Test Suite, Event-Pairs, Usage Scenario Testing}
}

@article{10.1016/j.is.2023.102211,
author = {Garcia, Marcia Tavares and Nunes, Marina Macedo and Fantinato, Marcelo and Peres, Sarajane Marques and Thom, Lucin\'{e}ia Heloisa},
title = {BPMN-Sim: A multilevel structural similarity technique for BPMN process models},
year = {2023},
issue_date = {Jun 2023},
publisher = {Elsevier Science Ltd.},
address = {GBR},
volume = {116},
number = {C},
issn = {0306-4379},
url = {https://doi.org/10.1016/j.is.2023.102211},
doi = {10.1016/j.is.2023.102211},
journal = {Inf. Syst.},
month = {jun},
numpages = {26},
keywords = {BPMN, Business process management, Business process, Process model, Process model similarity, Structural similarity}
}

@article{10.1016/j.cose.2024.103731,
author = {Cabrera-Arteaga, Javier and Fitzgerald, Nicholas and Monperrus, Martin and Baudry, Benoit},
title = {         Wasm-Mutate: Fast and effective binary diversification for WebAssembly},
year = {2024},
issue_date = {Apr 2024},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {139},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2024.103731},
doi = {10.1016/j.cose.2024.103731},
journal = {Comput. Secur.},
month = {may},
numpages = {16}
}

@article{10.1145/3583566,
author = {Li, Meiziniu and Cao, Jialun and Tian, Yongqiang and Li, Tsz On and Wen, Ming and Cheung, Shing-Chi},
title = {COMET: Coverage-guided Model Generation For Deep Learning Library Testing},
year = {2023},
issue_date = {September 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {5},
issn = {1049-331X},
url = {https://doi.org/10.1145/3583566},
doi = {10.1145/3583566},
abstract = {Recent deep learning (DL) applications are mostly built on top of DL libraries. The quality assurance of these libraries is critical to the dependable deployment of DL applications. Techniques have been proposed to generate various DL models and apply them to test these libraries. However, their test effectiveness is constrained by the diversity of layer API calls in their generated DL models. Our study reveals that these techniques can cover at most 34.1\% layer inputs, 25.9\% layer parameter values, and 15.6\% layer sequences. As a result, we find that many bugs arising from specific layer API calls (i.e., specific layer inputs, parameter values, or layer sequences) can be missed by existing techniques.Because of this limitation, we propose COMET to effectively generate DL models with diverse layer API calls for DL library testing. COMET: (1) designs a set of mutation operators and a coverage-based search algorithm to diversify layer inputs, layer parameter values, and layer sequences in DL models. (2) proposes a model synthesis method to boost the test efficiency without compromising the layer API call diversity. Our evaluation result shows that COMET outperforms baselines by covering twice as many layer inputs (69.7\% vs. 34.1\%), layer parameter values (50.2\% vs. 25.9\%), and layer sequences (39.0\% vs. 15.6\%) as those by the state-of-the-art. Moreover, COMET covers 3.4\% more library branches than those by existing techniques. Finally, COMET detects 32 new bugs in the latest version of eight popular DL libraries, including TensorFlow and MXNet, with 21 of them confirmed by DL library developers and seven of those confirmed bugs have been fixed by developers.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {jul},
articleno = {127},
numpages = {34},
keywords = {model diversity, model generation, library testing, Deep learning testing}
}

@inproceedings{10.5555/3666122.3666262,
author = {Qian, Zhaozhi and Davis, Rob and van der Schaar, Mihaela},
title = {Synthcity: a benchmark framework for diverse use cases of tabular synthetic data},
year = {2024},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Accessible high-quality data is the bread and butter of machine learning research, and the demand for data has exploded as larger and more advanced ML models are built across different domains. Yet, real data often contain sensitive information, subject to various biases, and are costly to acquire, which compromise their quality and accessibility. Synthetic data have thus emerged as a complement, sometimes even a replacement, to real data for ML training. However, the landscape of synthetic data research has been fragmented due to the large number of data modalities (e.g., tabular data, time series data, images, etc.) and various use cases (e.g., privacy, fairness, data augmentation, etc.). This poses practical challenges in comparing and selecting synthetic data generators in different problem settings. To this end, we develop Synthcity, an open-source Python library that allows researchers and practitioners to perform one-click benchmarking of synthetic data generators across data modalities and use cases. In addition, Synthcity's plug-in style API makes it easy to incorporate additional data generators into the framework. Beyond benchmarking, it also offers a single access point to a diverse range of cutting-edge data generators. Through examples on tabular data generation and data augmentation, we illustrate the general applicability of Synthcity, and the insight one can obtain.},
booktitle = {Proceedings of the 37th International Conference on Neural Information Processing Systems},
articleno = {140},
numpages = {16},
location = {New Orleans, LA, USA},
series = {NIPS '23}
}

@article{10.1145/3644388,
author = {Aghababaeyan, Zohreh and Abdellatif, Manel and Dadkhah, Mahboubeh and Briand, Lionel},
title = {DeepGD: A Multi-Objective Black-Box Test Selection Approach for Deep Neural Networks},
year = {2024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {1049-331X},
url = {https://doi.org/10.1145/3644388},
doi = {10.1145/3644388},
abstract = {Deep neural networks (DNNs) are widely used in various application domains such as image processing, speech recognition, and natural language processing. However, testing DNN models may be challenging due to the complexity and size of their input domain. Particularly, testing DNN models often requires generating or exploring large unlabeled datasets. In practice, DNN test oracles, which identify the correct outputs for inputs, often require expensive manual effort to label test data, possibly involving multiple experts to ensure labeling correctness. In this paper, we propose DeepGD, a black-box multi-objective test selection approach for DNN models. It reduces the cost of labeling by prioritizing the selection of test inputs with high fault-revealing power from large unlabeled datasets. DeepGD not only selects test inputs with high uncertainty scores to trigger as many mispredicted inputs as possible but also maximizes the probability of revealing distinct faults in the DNN model by selecting diverse mispredicted inputs. The experimental results conducted on four widely used datasets and five DNN models show that in terms of fault-revealing ability: (1) White-box, coverage-based approaches fare poorly, (2) DeepGD outperforms existing black-box test selection approaches in terms of fault detection, and (3) DeepGD also leads to better guidance for DNN model retraining when using selected inputs to augment the training set.},
note = {Just Accepted},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
keywords = {Deep Neural Network, Test Case Selection, DNN Fault Detection, Multi-Objective Optimization, Deep Learning Model Evaluation, Uncertainty Metrics, Diversity, Model Retraining Guidance}
}

@article{10.1016/j.cose.2023.103508,
author = {Gu, Yeming and Shu, Hui and Kang, Fei},
title = {BinAIV: Semantic-enhanced vulnerability detection for Linux x86 binaries},
year = {2023},
issue_date = {Dec 2023},
publisher = {Elsevier Advanced Technology Publications},
address = {GBR},
volume = {135},
number = {C},
issn = {0167-4048},
url = {https://doi.org/10.1016/j.cose.2023.103508},
doi = {10.1016/j.cose.2023.103508},
journal = {Comput. Secur.},
month = {dec},
numpages = {12},
keywords = {Function semantic, Vulnerability detection, Code similarity, Binary code, Deep learning}
}

