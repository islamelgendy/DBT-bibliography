@inproceedings{10.1145/3425174.3425210,
author = {Lima, Jackson A. Prado and Vergilio, Silvia R.},
title = {Multi-Armed Bandit Test Case Prioritization in Continuous Integration Environments: A Trade-off Analysis},
year = {2020},
isbn = {9781450387552},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3425174.3425210},
doi = {10.1145/3425174.3425210},
abstract = {Continuous Integration (CI) practices lead the software to be integrated and tested many times a day, usually subject to a test budget. To deal with this scenario, cost-effective test case prioritization techniques are required. COLEMAN is a Multi-Armed Bandit approach that learns from the test case failure-history the best prioritization order to maximize early fault detection. Reported results show that COLEMAN has reached promising results with different test budgets and spends, in the worst case, less than one second to execute. However, COLEMAN has not been evaluated against a search-based approach. Such an approach can generate near-optimal solutions but is not suitable to the CI budget because it takes too long to execute. Considering this fact, this paper analyses the trade-offs of the COLEMAN solutions in comparison with the near-optimal solutions generated by a Genetic Algorithm (GA). We use measures, which better fit with time constraints: Normalized Average Percentage of Faults Detected (NAPFD), Root-Mean-Square-Error (RMSE), and Prioritization Time. We use seven large-scale real-world software systems, and three different test budgets, 10%, 50%, and 80% of the total time required to execute the test set available for a CI cycle. COLEMAN obtains solutions near to the GA solutions in 90% of the cases, but scenarios with high volatility of test cases and a small number of cycles hamper the prioritization.},
booktitle = {Proceedings of the 5th Brazilian Symposium on Systematic and Automated Software Testing},
pages = {21–30},
numpages = {10},
keywords = {Continuous Integration environments, Test Case Prioritization, Multi-Armed Bandit},
location = {Natal, Brazil},
series = {SAST 20}
}

@inproceedings{10.1145/2610384.2610409,
author = {Roy Choudhary, Shauvik and Prasad, Mukul R. and Orso, Alessandro},
title = {Cross-Platform Feature Matching for Web Applications},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610409},
doi = {10.1145/2610384.2610409},
abstract = { With the emergence of new computing platforms, software written for traditional platforms is being re-targeted to reach the users on these new platforms. In particular, due to the proliferation of mobile computing devices, it is common practice for companies to build mobile-specific versions of their existing web applications to provide mobile users with a better experience. Because the differences between desktop and mobile versions of a web application are not only cosmetic, but can also include substantial rewrites of key components, it is not uncommon for these different versions to provide different sets of features. Whereas some of these differences are intentional, such as the addition of location-based features on mobile devices, others are not and can negatively affect the user experience, as confirmed by numerous user reports and complaints. Unfortunately, checking and maintaining the consistency of different versions of an application by hand is not only time consuming, but also error prone. To address this problem, and help developers in this difficult task, we propose an automated technique for matching features across different versions of a multi-platform web application. We implemented our technique in a tool, called FMAP, and used it to perform a preliminary empirical evaluation on nine real-world multi-platform web applications. The results of our evaluation are promising, as FMAP was able to correctly identify missing features between desktop and mobile versions of the web applications considered, as confirmed by our analysis of user reports and software fixes for these applications. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {82–92},
numpages = {11},
keywords = {Mobile Web, Cross-Platform},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/2771783.2771809,
author = {Gong, Liang and Pradel, Michael and Sridharan, Manu and Sen, Koushik},
title = {DLint: Dynamically Checking Bad Coding Practices in JavaScript},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771809},
doi = {10.1145/2771783.2771809},
abstract = { JavaScript has become one of the most popular programming languages, yet it is known for its suboptimal design. To effectively use JavaScript despite its design flaws, developers try to follow informal code quality rules that help avoid correctness, maintainability, performance, and security problems. Lightweight static analyses, implemented in "lint-like" tools, are widely used to find violations of these rules, but are of limited use because of the language's dynamic nature. This paper presents DLint, a dynamic analysis approach to check code quality rules in JavaScript. DLint consists of a generic framework and an extensible set of checkers that each addresses a particular rule. We formally describe and implement 28 checkers that address problems missed by state-of-the-art static approaches. Applying the approach in a comprehensive empirical study on over 200 popular web sites shows that static and dynamic checking complement each other. On average per web site, DLint detects 49 problems that are missed statically, including visible bugs on the web sites of IKEA, Hilton, eBay, and CNBC. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {94–105},
numpages = {12},
keywords = {metric, dynamic analysis, Code practice, DLint},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inbook{10.1145/3238147.3238187,
author = {Zhang, Mengshi and Zhang, Yuqun and Zhang, Lingming and Liu, Cong and Khurshid, Sarfraz},
title = {DeepRoad: GAN-Based Metamorphic Testing and Input Validation Framework for Autonomous Driving Systems},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238187},
abstract = {While Deep Neural Networks (DNNs) have established the fundamentals of image-based autonomous driving systems, they may exhibit erroneous behaviors and cause fatal accidents. To address the safety issues in autonomous driving systems, a recent set of testing techniques have been designed to automatically generate artificial driving scenes to enrich test suite, e.g., generating new input images transformed from the original ones. However, these techniques are insufficient due to two limitations: first, many such synthetic images often lack diversity of driving scenes, and hence compromise the resulting efficacy and reliability. Second, for machine-learning-based systems, a mismatch between training and application domain can dramatically degrade system accuracy, such that it is necessary to validate inputs for improving system robustness.  In this paper, we propose DeepRoad, an unsupervised DNN-based framework for automatically testing the consistency of DNN-based autonomous driving systems and online validation. First, DeepRoad automatically synthesizes large amounts of diverse driving scenes without using image transformation rules (e.g. scale, shear and rotation). In particular, DeepRoad is able to produce driving scenes with various weather conditions (including those with rather extreme conditions) by applying Generative Adversarial Networks (GANs) along with the corresponding real-world weather scenes. Second, DeepRoad utilizes metamorphic testing techniques to check the consistency of such systems using synthetic images. Third, DeepRoad validates input images for DNN-based systems by measuring the distance of the input and training images using their VGGNet features. We implement DeepRoad to test three well-recognized DNN-based autonomous driving systems in Udacity self-driving car challenge. The experimental results demonstrate that DeepRoad can detect thousands of inconsistent behaviors for these systems, and effectively validate input images to potentially enhance the system robustness as well.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {132–142},
numpages = {11}
}

@inproceedings{10.1145/3213846.3213856,
author = {Lee, Jaekwon and Kim, Dongsun and Bissyand\'{e}, Tegawend\'{e} F. and Jung, Woosung and Le Traon, Yves},
title = {Bench4BL: Reproducibility Study on the Performance of IR-Based Bug Localization},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213856},
doi = {10.1145/3213846.3213856},
abstract = {In recent years, the use of Information Retrieval (IR) techniques to automate the localization of buggy files, given a bug report, has shown promising results. The abundance of approaches in the literature, however, contrasts with the reality of IR-based bug localization (IRBL) adoption by developers (or even by the research community to complement other research approaches). Presumably, this situation is due to the lack of comprehensive evaluations for state-of-the-art approaches which offer insights into the actual performance of the techniques.  We report on a comprehensive reproduction study of six state-of-the-art IRBL techniques. This study applies not only subjects used in existing studies (old subjects) but also 46 new subjects (61,431 Java files and 9,459 bug reports) to the IRBL techniques. In addition, the study compares two different version matching (between bug reports and source code files) strategies to highlight some observations related to performance deterioration. We also vary test file inclusion to investigate the effectiveness of IRBL techniques on test files, or its noise impact on performance. Finally, we assess potential performance gain if duplicate bug reports are leveraged.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {61–72},
numpages = {12},
keywords = {information retrieval, bug localization, Reproducibility studies},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/3167132.3167289,
author = {Ognawala, Saahil and Hutzelmann, Thomas and Psallida, Eirini and Pretschner, Alexander},
title = {Improving Function Coverage with Munch: A Hybrid Fuzzing and Directed Symbolic Execution Approach},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167289},
doi = {10.1145/3167132.3167289},
abstract = {Fuzzing and symbolic execution are popular techniques for finding vulnerabilities and generating test-cases for programs. Fuzzing, a blackbox method that mutates seed input values, is generally incapable of generating diverse inputs that exercise all paths in the program. Due to the path-explosion problem and dependence on SMT solvers, symbolic execution may also not achieve high path coverage. A hybrid technique involving fuzzing and symbolic execution may achieve better function coverage than fuzzing or symbolic execution alone. In this paper, we present Munch, an open-source framework implementing two hybrid techniques based on fuzzing and symbolic execution. We empirically show using nine large open-source programs that overall, Munch achieves higher (in-depth) function coverage than symbolic execution or fuzzing alone. Using metrics based on total analyses time and number of queries issued to the SMT solver, we also show that Munch is more efficient at achieving better function coverage.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1475–1482},
numpages = {8},
keywords = {function coverage, software testing, compositional analysis, fuzzing, symbolic execution},
location = {Pau, France},
series = {SAC '18}
}

@inbook{10.1145/3460319.3464845,
author = {Pan, Weiyu and Chen, Zhenbang and Zhang, Guofeng and Luo, Yunlai and Zhang, Yufeng and Wang, Ji},
title = {Grammar-Agnostic Symbolic Execution by Token Symbolization},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464845},
abstract = {Parsing code exists extensively in software. Symbolic execution of complex parsing programs is challenging. The inputs generated by the symbolic execution using the byte-level symbolization are usually rejected by the parsing program, which dooms the effectiveness and efficiency of symbolic execution. Complex parsing programs usually adopt token-based input grammar checking. A token sequence represents one case of the input grammar. Based on this observation, we propose grammar-agnostic symbolic execution that can automatically generate token sequences to test complex parsing programs effectively and efficiently. Our method's key idea is to symbolize tokens instead of input bytes to improve the efficiency of symbolic execution. Technically, we propose a novel two-stage algorithm: the first stage collects the byte-level constraints of token values; the second stage employs token symbolization and the constraints collected in the first stage to generate the program inputs that are more possible to pass the parsing code.  We have implemented our method on a Java Pathfinder (JPF) based concolic execution engine. The results of the extensive experiments on real-world Java parsing programs demonstrate the effectiveness and efficiency in testing complex parsing programs. Our method detects 6 unknown bugs in the benchmark programs and achieves orders of magnitude speedup to find the same bugs.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {374–387},
numpages = {14}
}

@inproceedings{10.1145/3213846.3213855,
author = {Khazem, Kareem and Barr, Earl T. and Hosek, Petr},
title = {Making Data-Driven Porting Decisions with Tuscan},
year = {2018},
isbn = {9781450356992},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213846.3213855},
doi = {10.1145/3213846.3213855},
abstract = {Software typically outlives the platform that it was originally written for. To smooth the transition to new tools and platforms, programs should depend on the underlying platform as little as possible. In practice, however, software build processes are highly sensitive to their build platform, notably the implementation of the compiler and standard library. This makes it difficult to port existing, mature software to emerging platforms---web based runtimes like WebAssembly, resource-constrained environments for Internet-of-Things devices, or innovative new operating systems like Fuchsia.  We present Tuscan, a framework for conducting automatic, deterministic, reproducible tests on build systems. Tuscan is the first framework to solve the problem of reproducibly testing builds cross-platform at massive scale. We also wrote a build wrapper, Red, which hijacks builds to tolerate common failures that arise from platform dependence, allowing the test harness to discover errors later in the build. Authors of innovative platforms can use Tuscan and Red to test the extent of unportability in the software ecosystem, and to quantify the effort necessary to port legacy software.  We evaluated Tuscan by building an operating system distribution, consisting of 2,699 Red-wrapped programs, on four platforms, yielding a `catalog' of the most common portability errors. This catalog informs data-driven porting decisions and motivates changes to programs, build systems, and language standards; systematically quantifies problems that platform writers have hitherto discovered only on an ad-hoc basis; and forms the basis for a common substrate of portability fixes that developers can apply to their software.},
booktitle = {Proceedings of the 27th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {276–286},
numpages = {11},
keywords = {build systems, toolchains, portability},
location = {Amsterdam, Netherlands},
series = {ISSTA 2018}
}

@inproceedings{10.1145/1831708.1831716,
author = {Wei, Yi and Pei, Yu and Furia, Carlo A. and Silva, Lucas S. and Buchholz, Stefan and Meyer, Bertrand and Zeller, Andreas},
title = {Automated Fixing of Programs with Contracts},
year = {2010},
isbn = {9781605588230},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1831708.1831716},
doi = {10.1145/1831708.1831716},
abstract = {In program debugging, finding a failing run is only the first step; what about correcting the fault? Can we automate the second task as well as the first? The AutoFix-E tool automatically generates and validates fixes for software faults. The key insights behind AutoFix-E are to rely on contracts present in the software to ensure that the proposed fixes are semantically sound, and on state diagrams using an abstract notion of state based on the boolean queries of a class. Out of 42 faults found by an automatic testing tool in two widely used Eiffel libraries, AutoFix-E proposes successful fixes for 16 faults. Submitting some of these faults to experts shows that several of the proposed fixes are identical or close to fixes proposed by humans.},
booktitle = {Proceedings of the 19th International Symposium on Software Testing and Analysis},
pages = {61–72},
numpages = {12},
keywords = {automatic fixing, dynamic invariants, program synthesis, automatic debugging},
location = {Trento, Italy},
series = {ISSTA '10}
}

@inproceedings{10.1145/1390630.1390644,
author = {Dor, Nurit and Lev-Ami, Tal and Litvak, Shay and Sagiv, Mooly and Weiss, Dror},
title = {Customization Change Impact Analysis for Erp Professionals via Program Slicing},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390644},
doi = {10.1145/1390630.1390644},
abstract = {We describe a new tool that automatically identifies impact of customization changes, i.e., how changes affect software behavior. As opposed to existing static analysis tools that aim at aiding programmers or improve performance, our tool is designed for end-users without prior knowledge in programming. We utilize state-of-the-art static analysis algorithms for the programs within an Enterprise Resource Planning system (ERP). Key challenges in analyzing real world ERP programs are their significant size and the interdependency between programs. In particular, we describe and compare three customization change impact analyses for real-world programs, and a balancing algorithm built upon the three independent analyses. This paper presents PanayaImpactAnalysis (PanayaIA), a web on-demand tool, providing ERP professionals a clear view of the impact of a customization change on the system. In addition we report empirical results of PanayaIA when used by end-users on an ERP system of tens of millions LOCs.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {97–108},
numpages = {12},
keywords = {customization change impact analysis},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@inproceedings{10.1145/2610384.2610387,
author = {Pastore, Fabrizio and Mariani, Leonardo and Hyv\"{a}rinen, Antti E. J. and Fedyukovich, Grigory and Sharygina, Natasha and Sehestedt, Stephan and Muhammad, Ali},
title = {Verification-Aided Regression Testing},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610387},
doi = {10.1145/2610384.2610387},
abstract = { In this paper we present Verification-Aided Regression Testing (VART), a novel extension of regression testing that uses model checking to increase the fault revealing capability of existing test suites. The key idea in VART is to extend the use of test case executions from the conventional direct fault discovery to the generation of behavioral properties specific to the upgrade, by (i) automatically producing properties that are proved to hold for the base version of a program, (ii) automatically identifying and checking on the upgraded program only the properties that, according to the developers’ intention, must be preserved by the upgrade, and (iii) reporting the faults and the corresponding counter-examples that are not revealed by the regression tests. Our empirical study on both open source and industrial software systems shows that VART automatically produces properties that increase the effectiveness of testing by automatically detecting faults unnoticed by the existing regression test suites. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {37–48},
numpages = {12},
keywords = {dynamic analysis, Regression testing, model checking},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/1007512.1007531,
author = {Sullivan, Kevin and Yang, Jinlin and Coppit, David and Khurshid, Sarfraz and Jackson, Daniel},
title = {Software Assurance by Bounded Exhaustive Testing},
year = {2004},
isbn = {1581138202},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1007512.1007531},
doi = {10.1145/1007512.1007531},
abstract = {The contribution of this paper is an experiment that shows the potential value of a combination of selective reverse engineering to formal specifications and bounded exhaustive testing to improve the assurance levels of complex software. A key problem is to scale up test input generation so that meaningful results can be obtained. We present an approach, using Alloy and TestEra for test input generation, which we evaluate by experimental application to the Galileo dynamic fault tree analysis tool.},
booktitle = {Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {133–142},
numpages = {10},
keywords = {automated test case generation, TestEra, bounded exhaustive testing, formal methods, reverse engineering, specification-based testing},
location = {Boston, Massachusetts, USA},
series = {ISSTA '04}
}

@article{10.1145/1013886.1007531,
author = {Sullivan, Kevin and Yang, Jinlin and Coppit, David and Khurshid, Sarfraz and Jackson, Daniel},
title = {Software Assurance by Bounded Exhaustive Testing},
year = {2004},
issue_date = {July 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1013886.1007531},
doi = {10.1145/1013886.1007531},
abstract = {The contribution of this paper is an experiment that shows the potential value of a combination of selective reverse engineering to formal specifications and bounded exhaustive testing to improve the assurance levels of complex software. A key problem is to scale up test input generation so that meaningful results can be obtained. We present an approach, using Alloy and TestEra for test input generation, which we evaluate by experimental application to the Galileo dynamic fault tree analysis tool.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jul},
pages = {133–142},
numpages = {10},
keywords = {formal methods, TestEra, specification-based testing, bounded exhaustive testing, reverse engineering, automated test case generation}
}

@article{10.1145/2648787,
author = {Clarke, Peter J. and Davis, Debra and King, Tariq M. and Pava, Jairo and Jones, Edward L.},
title = {Integrating Testing into Software Engineering Courses Supported by a Collaborative Learning Environment},
year = {2014},
issue_date = {November 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {3},
url = {https://doi.org/10.1145/2648787},
doi = {10.1145/2648787},
abstract = {As software becomes more ubiquitous and complex, the cost of software bugs continues to grow at a staggering rate. To remedy this situation, there needs to be major improvement in the knowledge and application of software validation techniques. Although there are several software validation techniques, software testing continues to be one of the most widely used in industry. The high demand for software engineers in the next decade has resulted in more software engineering (SE) courses being offered in academic institutions. However, due to the number of topics to be covered in SE courses, little or no attention is given to software testing, resulting in students entering industry with little or no testing experience.We propose a minimally disruptive approach of integrating software testing into SE courses by providing students access to a collaborative learning environment containing learning materials on testing techniques and testing tools. In this article, we describe the learning environment and the studies conducted to measure the benefits accrued by students using the learning environment in the SE courses.},
journal = {ACM Trans. Comput. Educ.},
month = {oct},
articleno = {18},
numpages = {33},
keywords = {testing tutorials, code coverage, unit testing, software testing, Course management}
}

@inproceedings{10.1145/3368691.3368706,
author = {Qusef, Abdallah and Issa, Lana and Ayoubi, Eyad and Murad, Sharefa},
title = {Challenges and Opportunities in Cloud Testing},
year = {2019},
isbn = {9781450372848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368691.3368706},
doi = {10.1145/3368691.3368706},
abstract = {Cloud computing is attracting the interest of many businesses around the world by offering feasible solutions towards hosting software applications and providing convenient development and testing environments. Cloud computing has developed enormously that it changed the management practices of dealing with computer systems and services. Because of cloud computing; testing as a service (TaaS) was created. Despite the facilities provided by TaaS, it produced some issues and challenges, particularly in cloud-based testing environments. Those issues are needed to be addressed and fixed. This paper reviews and addresses a set of challenges and benefits of cloud testing among with a theoretical comparison between cloud-based testing environment and traditional testing for regular systems.},
booktitle = {Proceedings of the Second International Conference on Data Science, E-Learning and Information Systems},
articleno = {15},
numpages = {7},
keywords = {cloud computing, software testing, cloud testing},
location = {Dubai, United Arab Emirates},
series = {DATA '19}
}

@inproceedings{10.1145/3395363.3397361,
author = {Xu, Yifei and Xu, Zhengzi and Chen, Bihuan and Song, Fu and Liu, Yang and Liu, Ting},
title = {Patch Based Vulnerability Matching for Binary Programs},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397361},
doi = {10.1145/3395363.3397361},
abstract = {The binary-level function matching has been widely used to detect whether there are 1-day vulnerabilities in released programs. However, the high false positive is a challenge for current function matching solutions, since the vulnerable function is highly similar to its corresponding patched version. In this paper, the Binary X-Ray (BinXray), a patch based vulnerability matching approach, is proposed to identify the specific 1-day vulnerabilities in target programs accurately and effectively. In the preparing step, a basic block mapping algorithm is designed to extract the signature of a patch, by comparing the given vulnerable and patched programs. The signature is represented as a set of basic block traces. In the detection step, the patching semantics is applied to reduce irrelevant basic block traces to speed up the signature searching. The trace similarity is also designed to identify whether a target program is patched. In experiments, 12 real software projects related to 479 CVEs are collected. BinXray achieves 93.31% accuracy and the analysis time cost is only 296.17ms per function, outperforming the state-of-the-art works.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {376–387},
numpages = {12},
keywords = {Security, Vulnerability Matching, Patch Presence Identification, Binary Analysis},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/2931037.2931072,
author = {Linares-V\'{a}squez, Mario and Li, Boyang and Vendome, Christopher and Poshyvanyk, Denys},
title = {Documenting Database Usages and Schema Constraints in Database-Centric Applications},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931072},
doi = {10.1145/2931037.2931072},
abstract = { Database-centric applications (DCAs) usually rely on database operations over a large number of tables and attributes. Understanding how database tables and attributes are used to implement features in DCAs along with the constraints related to these usages is an important component of any DCA’s maintenance. However, manually documenting database related operations and their asynchronously evolving constraints in constantly changing source code is a hard and time-consuming problem. In this paper, we present a novel approach, namely DBScribe, aimed at automatically generating always up-to-date natural language descriptions of database operations and schema constraints in source code methods. DBScribe statically analyzes the code and database schema to detect database usages and then prop- agates these usages and schema constraints through the call-chains implementing database-related features. Finally, each method in these call-chains is automatically documented based on the underlying database usages and constraints.  We evaluated DBScribe in a study with 52 participants analyzing generated documentation for database-related methods in five open-source DCAs. Additionally, we evaluated the descriptions generated by DBScribe on two commercial DCAs involving original developers. The results for the studies involving open-source and commercial DCAs demonstrate that generated descriptions are accurate and useful while understanding database usages and constraints, in particular during maintenance tasks. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {270–281},
numpages = {12},
keywords = {SQL-data statements, Schema constraints, Documentation, Database-centric applications},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/2483760.2483778,
author = {Li, Mengchen and Chen, Yuanjun and Wang, Linzhang and Xu, Guoqing},
title = {Dynamically Validating Static Memory Leak Warnings},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483778},
doi = {10.1145/2483760.2483778},
abstract = { File Edit Options Buffers Tools TeX Help Memory leaks have significant impact on software availability, performance, and security. Static analysis has been widely used to find memory leaks in C/C++ programs. Although a static analysis is able to find all potential leaks in a program, it often reports a great number of false warnings. Manually validating these warnings is a daunting task, which significantly limits the practicality of the analysis. In this paper, we develop a novel dynamic technique that automatically validates and categorizes such warnings to unleash the power of static memory leak detectors. Our technique analyzes each warning that contains information regarding the leaking allocation site and the leaking path, generates test cases to cover the leaking path, and tracks objects created by the leaking allocation site. Eventually, warnings are classified into four categories: MUST-LEAK, LIKELY-NOT-LEAK, BLOAT, and MAY-LEAK. Warnings in MUST-LEAK are guaranteed by our analysis to be true leaks. Warnings in LIKELY-NOT-LEAK are highly likely to be false warnings. Although we cannot provide any formal guarantee that they are not leaks, we have high confidence that this is the case. Warnings in BLOAT are also not likely to be leaks but they should be fixed to improve performance. Using our approach, the developer's manual validation effort needs to be focused only on warnings in the category MAY-LEAK, which is often much smaller than the original set. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {112–122},
numpages = {11},
keywords = {Warning Classification, Concolic Testing, Memory Leaks},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inbook{10.1145/3377929.3390077,
author = {Derakhshanfar, Pouria and Devroey, Xavier and Zaidman, Andy and van Deursen, Arie and Panichella, Annibale},
title = {Crash Reproduction Using Helper Objectives},
year = {2020},
isbn = {9781450371278},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377929.3390077},
abstract = {Evolutionary-based crash reproduction techniques aid developers in their debugging practices by generating a test case that reproduces a crash given its stack trace. In these techniques, the search process is typically guided by a single search objective called Crash Distance. Previous studies have shown that current approaches could only reproduce a limited number of crashes due to a lack of diversity in the population during the search. In this study, we address this issue by applying Multi-Objectivization using Helper-Objectives (MO-HO) on crash reproduction. In particular, we add two helper-objectives to the Crash Distance to improve the diversity of the generated test cases and consequently enhance the guidance of the search process. We assessed MO-HO against the single-objective crash reproduction. Our results show that MO-HO can reproduce two additional crashes that were not previously reproducible by the single-objective approach.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference Companion},
pages = {309–310},
numpages = {2}
}

@inproceedings{10.1145/3395363.3397354,
author = {Pan, Minxue and Huang, An and Wang, Guoxin and Zhang, Tian and Li, Xuandong},
title = {Reinforcement Learning Based Curiosity-Driven Testing of Android Applications},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397354},
doi = {10.1145/3395363.3397354},
abstract = {Mobile applications play an important role in our daily life, while it still remains a challenge to guarantee their correctness. Model-based and systematic approaches have been applied to Android GUI testing. However, they do not show significant advantages over random approaches because of limitations such as imprecise models and poor scalability. In this paper, we propose Q-testing, a reinforcement learning based approach which benefits from both random and model-based approaches to automated testing of Android applications. Q-testing explores the Android apps with a curiosity-driven strategy that utilizes a memory set to record part of previously visited states and guides the testing towards unfamiliar functionalities. A state comparison module, which is a neural network trained by plenty of collected samples, is novelly employed to divide different states at the granularity of functional scenarios. It can determine the reinforcement learning reward in Q-testing and help the curiosity-driven strategy explore different functionalities efficiently. We conduct experiments on 50 open-source applications where Q-testing outperforms the state-of-the-art and state-of-practice Android GUI testing tools in terms of code coverage and fault detection. So far, 22 of our reported faults have been confirmed, among which 7 have been fixed.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {153–164},
numpages = {12},
keywords = {reinforcement learning, functional scenario division, Android app testing},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/3460319.3464812,
author = {Zhang, Yakun and Lv, Xiao and Dong, Haoyu and Dou, Wensheng and Han, Shi and Zhang, Dongmei and Wei, Jun and Ye, Dan},
title = {Semantic Table Structure Identification in Spreadsheets},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464812},
doi = {10.1145/3460319.3464812},
abstract = {Spreadsheets are widely used in various business tasks, and contain amounts of valuable data. However, spreadsheet tables are usually organized in a semi-structured way, and contain complicated semantic structures, e.g., header types and relations among headers. Lack of documented semantic table structures, existing data analysis and error detection tools can hardly understand spreadsheet tables. Therefore, identifying semantic table structures in spreadsheet tables is of great importance, and can greatly promote various analysis tasks on spreadsheets. In this paper, we propose Tasi (Table structure identification) to automatically identify semantic table structures in spreadsheets. Based on the contents, styles, and spatial locations in table headers, Tasi adopts a multi-classifier to predict potential header types and relations, and then integrates all header types and relations into consistent semantic table structures. We further propose TasiError, to detect spreadsheet errors based on the identified semantic table structures by Tasi. Our experiments on real-world spreadsheets show that, Tasi can precisely identify semantic table structures in spreadsheets, and TasiError can detect real-world spreadsheet errors with higher precision (75.2%) and recall (82.9%) than existing approaches.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {283–295},
numpages = {13},
keywords = {table structure, error detection, Spreadsheet},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

