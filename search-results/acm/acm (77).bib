@inproceedings{10.1145/1394504.1394507,
author = {Jetley, Raoul Praful and Jones, Paul L. and Anderson, Paul},
title = {Static Analysis of Medical Device Software Using CodeSonar},
year = {2008},
isbn = {9781595939241},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1394504.1394507},
doi = {10.1145/1394504.1394507},
abstract = {Post-market investigators at the United States Food and Drug Administration may need to review medical device software to assess its integrity. They have to do this with little or no prior knowledge of the software. Historically, the only way to perform such a review has been to manually search the code for potential sources of error --- a process that is both tedious and error-prone.Static analysis tools can improve this process by providing a means for automated error detection. By using symbolic execution techniques to explore execution paths of the software, static analysis provides complete, or almost complete, coverage of the code, and helps detect potentially fatal errors that may not easily be detected through conventional testing methods. Using automated static analysis tools can help reduce the effort involved in analysis and provide a more accurate assessment of the software.In this paper, we discuss CodeSonar, a whole-program interprocedural static analysis tool for C/C++ programs, and illustrate how it was used to facilitate error detection during a post-market investigation.},
booktitle = {Proceedings of the 2008 Workshop on Static Analysis},
pages = {22–29},
numpages = {8},
location = {Tucson, Arizona},
series = {SAW '08}
}

@inproceedings{10.1109/ICSE.2007.37,
author = {Pacheco, Carlos and Lahiri, Shuvendu K. and Ernst, Michael D. and Ball, Thomas},
title = {Feedback-Directed Random Test Generation},
year = {2007},
isbn = {0769528287},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2007.37},
doi = {10.1109/ICSE.2007.37},
abstract = {We present a technique that improves random test generation by incorporating feedback obtained from executing test inputs as they are created. Our technique builds inputs incrementally by randomly selecting a method call to apply and finding arguments from among previously-constructed inputs. As soon as an input is built, it is executed and checked against a set of contracts and filters. The result of the execution determines whether the input is redundant, illegal, contract-violating, or useful for generating more inputs. The technique outputs a test suite consisting of unit tests for the classes under test. Passing tests can be used to ensure that code contracts are preserved across program changes; failing tests (that violate one or more contract) point to potential errors that should be corrected. Our experimental results indicate that feedback-directed random test generation can outperform systematic and undirected random test generation, in terms of coverage and error detection. On four small but nontrivial data structures (used previously in the literature), our technique achieves higher or equal block and predicate coverage than model checking (with and without abstraction) and undirected random generation. On 14 large, widely-used libraries (comprising 780KLOC), feedback-directed random test generation finds many previously-unknown errors, not found by either model checking or undirected random generation.},
booktitle = {Proceedings of the 29th International Conference on Software Engineering},
pages = {75–84},
numpages = {10},
series = {ICSE '07}
}

@inproceedings{10.1145/2814270.2814281,
author = {Zheng, Yudi and Bulej, Lubom\'{\i}r and Binder, Walter},
title = {Accurate Profiling in the Presence of Dynamic Compilation},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814281},
doi = {10.1145/2814270.2814281},
abstract = { Many profilers based on bytecode instrumentation yield wrong results in the presence of an optimizing dynamic compiler, either due to not being aware of optimizations such as stack allocation and method inlining, or due to the inserted code disrupting such optimizations. To avoid such perturbations, we present a novel technique to make any profiler implemented at the bytecode level aware of optimizations performed by the dynamic compiler. We implement our approach in a state-of-the-art Java virtual machine and demonstrate its significance with concrete profilers. We quantify the impact of escape analysis on allocation profiling, object life-time analysis, and the impact of method inlining on callsite profiling. We illustrate how our approach enables new kinds of profilers, such as a profiler for non-inlined callsites, and a testing framework for locating performance bugs in dynamic compiler implementations. },
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {433–450},
numpages = {18},
keywords = {Dynamic compilers, profiling, bytecode instrumentation},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814281,
author = {Zheng, Yudi and Bulej, Lubom\'{\i}r and Binder, Walter},
title = {Accurate Profiling in the Presence of Dynamic Compilation},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814281},
doi = {10.1145/2858965.2814281},
abstract = { Many profilers based on bytecode instrumentation yield wrong results in the presence of an optimizing dynamic compiler, either due to not being aware of optimizations such as stack allocation and method inlining, or due to the inserted code disrupting such optimizations. To avoid such perturbations, we present a novel technique to make any profiler implemented at the bytecode level aware of optimizations performed by the dynamic compiler. We implement our approach in a state-of-the-art Java virtual machine and demonstrate its significance with concrete profilers. We quantify the impact of escape analysis on allocation profiling, object life-time analysis, and the impact of method inlining on callsite profiling. We illustrate how our approach enables new kinds of profilers, such as a profiler for non-inlined callsites, and a testing framework for locating performance bugs in dynamic compiler implementations. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {433–450},
numpages = {18},
keywords = {Dynamic compilers, bytecode instrumentation, profiling}
}

@inproceedings{10.1145/3468264.3468554,
author = {Wang, Wenyu and Yang, Wei and Xu, Tianyin and Xie, Tao},
title = {Vet: Identifying and Avoiding UI Exploration Tarpits},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468554},
doi = {10.1145/3468264.3468554},
abstract = {Despite over a decade of research, it is still challenging for mobile UI testing tools to achieve satisfactory effectiveness, especially on industrial apps with rich features and large code bases. Our experiences suggest that existing mobile UI testing tools are prone to exploration tarpits, where the tools get stuck with a small fraction of app functionalities for an extensive amount of time. For example, a tool logs out an app at early stages without being able to log back in, and since then the tool gets stuck with exploring the app’s pre-login functionalities (i.e., exploration tarpits) instead of its main functionalities. While tool vendors/users can manually hardcode rules for the tools to avoid specific exploration tarpits, these rules can hardly generalize, being fragile in face of diverted testing environments, fast app iterations, and the demand of batch testing product lines. To identify and resolve exploration tarpits, we propose VET, a general approach including a supporting system for the given specific Android UI testing tool on the given specific app under test (AUT). VET runs the tool on the AUT for some time and records UI traces, based on which VET identifies exploration tarpits by recognizing their patterns in the UI traces. VET then pinpoints the actions (e.g., clicking logout) or the screens that lead to or exhibit exploration tarpits. In subsequent test runs, VET guides the testing tool to prevent or recover from exploration tarpits. From our evaluation with state-of-the-art Android UI testing tools on popular industrial apps, VET identifies exploration tarpits that cost up to 98.6% testing time budget. These exploration tarpits reveal not only limitations in UI exploration strategies but also defects in tool implementations. VET automatically addresses the identified exploration tarpits, enabling each evaluated tool to achieve higher code coverage and improve crash-triggering capabilities.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {83–94},
numpages = {12},
keywords = {UI testing, trace analysis, Android testing},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3377811.3380357,
author = {Xia, Hao and Zhang, Yuan and Zhou, Yingtian and Chen, Xiaoting and Wang, Yang and Zhang, Xiangyu and Cui, Shuaishuai and Hong, Geng and Zhang, Xiaohan and Yang, Min and Yang, Zhemin},
title = {How Android Developers Handle Evolution-Induced API Compatibility Issues: A Large-Scale Study},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380357},
doi = {10.1145/3377811.3380357},
abstract = {As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them.In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in counter-measure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {886–898},
numpages = {13},
keywords = {Android app analysis, compatibility issues, API evolution},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@article{10.1145/3485464,
author = {Hough, Katherine and Bell, Jonathan},
title = {A Practical Approach for Dynamic Taint Tracking with Control-Flow Relationships},
year = {2021},
issue_date = {April 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3485464},
doi = {10.1145/3485464},
abstract = {Dynamic taint tracking, a technique that traces relationships between values as a program executes, has been used to support a variety of software engineering tasks. Some taint tracking systems only consider data flows and ignore control flows. As a result, relationships between some values are not reflected by the analysis. Many applications of taint tracking either benefit from or rely on these relationships being traced, but past works have found that tracking control flows resulted in over-tainting, dramatically reducing the precision of the taint tracking system. In this article, we introduce Conflux, alternative semantics for propagating taint tags along control flows. Conflux aims to reduce over-tainting by decreasing the scope of control flows and providing a heuristic for reducing loop-related over-tainting. We created a Java implementation of Conflux and performed a case study exploring the effect of Conflux on a concrete application of taint tracking, automated debugging. In addition to this case study, we evaluated Conflux’s accuracy using a novel benchmark consisting of popular, real-world programs. We compared Conflux against existing taint propagation policies, including a state-of-the-art approach for reducing control-flow-related over-tainting, finding that Conflux had the highest F1 score on 43 out of the 48 total tests.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {dec},
articleno = {26},
numpages = {43},
keywords = {Taint tracking, dynamic information flow, control flow analysis}
}

@inproceedings{10.1145/3503222.3507748,
author = {Zhang, Qian and Wang, Jiyuan and Xu, Guoqing Harry and Kim, Miryung},
title = {HeteroGen: Transpiling C to Heterogeneous HLS Code with Automated Test Generation and Program Repair},
year = {2022},
isbn = {9781450392051},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503222.3507748},
doi = {10.1145/3503222.3507748},
abstract = {Despite the trend of incorporating heterogeneity and specialization in hardware, the development of heterogeneous applications is limited to a handful of engineers with deep hardware expertise. We propose HeteroGen that takes C/C++ code as input and automatically generates an HLS version with test behavior preservation and better performance. Key to the success of HeteroGen is adapting the idea of search-based program repair to the heterogeneous computing domain, while addressing two technical challenges. First, the turn-around time of HLS compilation and simulation is much longer than the usual C/C++ compilation and execution time; therefore, HeteroGen applies pattern-oriented program edits guided by common fix patterns and their dependences. Second, behavior and performance checking requires testing, but test cases are often unavailable. Thus, HeteroGen auto-generates test inputs suitable for checking C to HLS-C conversion errors, while providing high branch coverage for the original C code.  An evaluation of HeteroGen shows that it produces an HLS-compatible version for nine out of ten real-world heterogeneous applications fully automatically, applying up to 438 lines of edits to produce an HLS version 1.63x faster than the original version.},
booktitle = {Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1017–1029},
numpages = {13},
keywords = {program repair, Heterogeneous applications, test generation},
location = {Lausanne, Switzerland},
series = {ASPLOS 2022}
}

@inproceedings{10.1145/98949.99159,
author = {Sarkar, Manojit},
title = {Runtime Debuggers for Parallel and Distributed Systems: A Uniform Design Approach},
year = {1990},
isbn = {0897913566},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/98949.99159},
doi = {10.1145/98949.99159},
booktitle = {Proceedings of the 28th Annual Southeast Regional Conference},
pages = {333–336},
numpages = {4},
location = {Greenville, South Carolina},
series = {ACM-SE 28}
}

@inproceedings{10.5555/3241639.3241650,
author = {Yang, Junfeng and Cui, Heming and Wu, Jingyue},
title = {Determinism is Overrated: What Really Makes Multithreaded Programs Hard to Get Right and What Can Be Done about It?},
year = {2013},
publisher = {USENIX Association},
address = {USA},
abstract = {Our accelerating computational demand and the rise of multicore hardware have made parallel programs, especially shared-memory multithreaded programs, increasingly pervasive and critical. Yet, these programs remain extremely difficult to write, test, analyze, debug, and verify. Conventional wisdom has attributed these difficulties to nondeterminism, and researchers have recently dedicated much effort to bringing determinism into multithreading. In this paper, we argue that determinism is not as useful as commonly perceived: it is neither sufficient nor necessary for reliability. We present our view on why multithreaded programs are difficult to get right, describe a promising approach we call stable multithreading to dramatically improve reliability, and summarize our last four years' research on building and applying stable multithreading systems.},
booktitle = {Proceedings of the 5th USENIX Conference on Hot Topics in Parallelism},
pages = {11},
numpages = {1},
location = {San Jose, CA},
series = {HotPar'13}
}

@inproceedings{10.1145/1244002.1244059,
author = {Chelius, Guillaume and Fraboulet, Antoine and Fleury, Eric},
title = {Worldsens: A Fast and Accurate Development Framework for Sensor Network Applications},
year = {2007},
isbn = {1595934804},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1244002.1244059},
doi = {10.1145/1244002.1244059},
abstract = {In this article, we present Worldsens, a fast and accurate development framework for sensor network applications. World-sens offers an integrated platform for the design, development, performance evaluation and profiling of applications. It relies on two simulators, WSNet and WSim, which are used throughout the application design and implementation, from the high level design choices to the implementation validation. WSNet is a modular event-driven wireless network simulator while WSim is a full platform hardware simulator which takes the target binary code as input and uses instruction cycles as time reference. WSNet and WSim can be used in conjunction to offer a distributed simulation of sensor networks with instruction and radio byte accuracy.},
booktitle = {Proceedings of the 2007 ACM Symposium on Applied Computing},
pages = {222–226},
numpages = {5},
keywords = {development framework, real time sensor network, sensor networks, simulation},
location = {Seoul, Korea},
series = {SAC '07}
}

@inproceedings{10.1145/3236024.3236039,
author = {Wei, Jiayi and Chen, Jia and Feng, Yu and Ferles, Kostas and Dillig, Isil},
title = {Singularity: Pattern Fuzzing for Worst Case Complexity},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236039},
doi = {10.1145/3236024.3236039},
abstract = {We describe a new blackbox complexity testing technique for determining the worst-case asymptotic complexity of a given application. The key idea is to look for an input pattern —rather than a concrete input— that maximizes the asymptotic resource usage of the target program. Because input patterns can be described concisely as programs in a restricted language, our method transforms the complexity testing problem to optimal program synthesis. In particular, we express these input patterns using a new model of computation called Recurrent Computation Graph (RCG) and solve the optimal synthesis problem by developing a genetic programming algorithm that operates on RCGs. We have implemented the proposed ideas in a tool called Singularityand evaluate it on a diverse set of benchmarks. Our evaluation shows that Singularitycan effectively discover the worst-case complexity of various algorithms and that it is more scalable compared to existing state-of-the-art techniques. Furthermore, our experiments also corroborate that Singularitycan discover previously unknown performance bugs and availability vulnerabilities in real-world applications such as Google Guava and JGraphT.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {213–223},
numpages = {11},
keywords = {fuzzing, optimal program synthesis, availability vulnerability, genetic programming, Complexity testing, performance bug},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3368089.3409683,
author = {Laaber, Christoph and W\"{u}rsten, Stefan and Gall, Harald C. and Leitner, Philipp},
title = {Dynamically Reconfiguring Software Microbenchmarks: Reducing Execution Time without Sacrificing Result Quality},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409683},
doi = {10.1145/3368089.3409683},
abstract = {Executing software microbenchmarks, a form of small-scale performance tests predominantly used for libraries and frameworks, is a costly endeavor. Full benchmark suites take up to multiple hours or days to execute, rendering frequent checks, e.g., as part of continuous integration (CI), infeasible. However, altering benchmark configurations to reduce execution time without considering the impact on result quality can lead to benchmark results that are not representative of the software’s true performance. We propose the first technique to dynamically stop software microbenchmark executions when their results are sufficiently stable. Our approach implements three statistical stoppage criteria and is capable of reducing Java Microbenchmark Harness (JMH) suite execution times by 48.4% to 86.0%. At the same time it retains the same result quality for 78.8% to 87.6% of the benchmarks, compared to executing the suite for the default duration. The proposed approach does not require developers to manually craft custom benchmark configurations; instead, it provides automated mechanisms for dynamic reconfiguration. Hence, making dynamic reconfiguration highly effective and efficient, potentially paving the way to inclusion of JMH microbenchmarks in CI.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {989–1001},
numpages = {13},
keywords = {JMH, performance testing, configuration, software benchmarking},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.5555/2819419.2819423,
author = {Delaitre, Aurelien and Stivalet, Bertrand and Fong, Elizabeth and Okun, Vadim},
title = {Evaluating Bug Finders: Test and Measurement of Static Code Analyzers},
year = {2015},
publisher = {IEEE Press},
abstract = {Software static analysis is one of many options for finding bugs in software. Like compilers, static analyzers take a program as input. This paper covers tools that examine source codewithout executing itand output bug reports. Static analysis is a complex and generally undecidable problem. Most tools resort to approximation to overcome these obstacles and it sometimes leads to incorrect results. Therefore, tool effectiveness needs to be evaluated. Several characteristics of the tools should be examined. First, what types of bugs can they find? Second, what proportion of bugs do they report? Third, what percentage of findings is correct? These questions can be answered by one or more metrics. But to calculate these, we need test cases having certain characteristics: statistical significance, ground truth, and relevance. Test cases with all three attributes are out of reach, but we can use combinations of only two to calculate the metrics.The results in this paper were collected during Static Analysis Tool Exposition (SATE) V, where participants ran 14 static analyzers on the test sets we provided and submitted their reports to us for analysis. Tools had considerably different support for most bug classes. Some tools discovered significantly more bugs than others or generated mostly accurate warnings, while others reported wrong findings more frequently. Using the metrics, an evaluator can compare candidates and select the tool that aligns best with his or her objectives. In addition, our results confirm that the bugs most commonly found by tools are among the most common and important bugs in software. We also observed that code complexity is a major hindrance for static analyzers and detailed which code constructs tools handle well and which impede their analysis.},
booktitle = {Proceedings of the First International Workshop on Complex FaUlts and Failures in LargE Software Systems},
pages = {14–20},
numpages = {7},
keywords = {static analysis tools, software faults, software assurance, software vulnerability},
location = {Florence, Italy},
series = {COUFLESS '15}
}

@inproceedings{10.1145/3453483.3454092,
author = {Donaldson, Alastair F. and Thomson, Paul and Teliman, Vasyl and Milizia, Stefano and Maselco, Andr\'{e} Perez and Karpi\'{n}ski, Antoni},
title = {Test-Case Reduction and Deduplication Almost for Free with Transformation-Based Compiler Testing},
year = {2021},
isbn = {9781450383912},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3453483.3454092},
doi = {10.1145/3453483.3454092},
abstract = {Recent transformation-based approaches to compiler testing look for mismatches between the results of pairs of equivalent programs, where one program is derived from the other by randomly applying semantics-preserving transformations. We present a formulation of transformation-based compiler testing that provides effective test-case reduction almost for free: if transformations are designed to be as small and independent as possible, standard delta debugging can be used to shrink a bug-inducing transformation sequence to a smaller subsequence that still triggers the bug. The bug can then be reported as a delta between an original and minimally-transformed program. Minimized transformation sequences can also be used to heuristically deduplicate a set of bug-inducing tests, recommending manual investigation of those that involve disparate types of transformations and thus may have different root causes. We demonstrate the effectiveness of our approach via a new tool, spirv-fuzz, the first compiler-testing tool for the SPIR-V intermediate representation that underpins the Vulkan GPU programming model.},
booktitle = {Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation},
pages = {1017–1032},
numpages = {16},
keywords = {SPIR-V, Compilers, metamorphic testing},
location = {Virtual, Canada},
series = {PLDI 2021}
}

@inproceedings{10.5555/998675.999415,
author = {Briand, L. C. and Labiche, Y. and Wang, Y.},
title = {Using Simulation to Empirically Investigate Test Coverage Criteria Based on Statechart},
year = {2004},
isbn = {0769521630},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {A number of testing strategies have been proposedusing state machines and statecharts as test models inorder to derive test sequences and validate classes orclass clusters. Though such criteria have the advantage ofbeing systematic, little is known on how cost effective theyare and how they compare to each other.This article presents a precise simulation and analysisprocedure to analyze the cost-effectiveness of statechart-basedtesting techniques. We then investigate, using thisprocedure, the cost and fault detection effectiveness ofadequate test sets for the most referenced coveragecriteria for statecharts on three different representativecase studies. Through the analysis of common results anddifferences across studies, we attempt to draw moregeneral conclusions regarding the costs and benefits ofusing the criteria under investigation.},
booktitle = {Proceedings of the 26th International Conference on Software Engineering},
pages = {86–95},
numpages = {10},
series = {ICSE '04}
}

@inproceedings{10.5555/2486788.2486799,
author = {Gomez, Lorenzo and Neamtiu, Iulian and Azim, Tanzirul and Millstein, Todd},
title = {RERAN: Timing- and Touch-Sensitive Record and Replay for Android},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { Touchscreen-based devices such as smartphones and tablets are gaining popularity but their rich input capabilities pose new development and testing complications. To alleviate this problem, we present an approach and tool named RERAN that permits record-and-replay for the Android smartphone platform. Existing GUI-level record-and-replay approaches are inadequate due to the expressiveness of the smartphone domain, in which applications support sophisticated GUI gestures, depend on inputs from a variety of sensors on the device, and have precise timing requirements among the various input events. We address these challenges by directly capturing the low-level event stream on the phone, which includes both GUI events and sensor events, and replaying it with microsecond accuracy. Moreover, RERAN does not require access to app source code, perform any app rewriting, or perform any modifications to the virtual machine or Android platform. We demonstrate RERAN’s applicability in a variety of scenarios, including (a) replaying 86 out of the Top-100 Android apps on Google Play; (b) reproducing bugs in popular apps, e.g., Firefox, Facebook, Quickoffice; and (c) fast-forwarding executions. We believe that our versatile approach can help both Android developers and researchers. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {72–81},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@article{10.1145/376134.376180,
author = {Shepard, Terry and Lamb, Margaret and Kelly, Diane},
title = {More Testing Should Be Taught},
year = {2001},
issue_date = {June 2001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {6},
issn = {0001-0782},
url = {https://doi.org/10.1145/376134.376180},
doi = {10.1145/376134.376180},
journal = {Commun. ACM},
month = {jun},
pages = {103–108},
numpages = {6}
}

@inproceedings{10.1145/3468264.3468623,
author = {Patra, Jibesh and Pradel, Michael},
title = {Semantic Bug Seeding: A Learning-Based Approach for Creating Realistic Bugs},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468623},
doi = {10.1145/3468264.3468623},
abstract = {When working on techniques to address the wide-spread problem of software bugs, one often faces the need for a large number of realistic bugs in real-world programs. Such bugs can either help evaluate an approach, e.g., in form of a bug benchmark or a suite of program mutations, or even help build the technique, e.g., in learning-based bug detection. Because gathering a large number of real bugs is difficult, a common approach is to rely on automatically seeded bugs. Prior work seeds bugs based on syntactic transformation patterns, which often results in unrealistic bugs and typically cannot introduce new, application-specific code tokens.  This paper presents SemSeed, a technique for automatically seeding bugs in a semantics-aware way. The key idea is to imitate how a given real-world bug would look like in other programs by semantically adapting the bug pattern to the local context. To reason about the semantics of pieces of code, our approach builds on learned token embeddings that encode the semantic similarities of identifiers and literals. Our evaluation with real-world JavaScript software shows that the approach effectively reproduces real bugs and clearly outperforms a semantics-unaware approach. The seeded bugs are useful as training data for learning-based bug detection, where they significantly improve the bug detection ability. Moreover, we show that SemSeed-created bugs complement existing mutation testing operators, and that our approach is efficient enough to seed hundreds of thousands of bugs within an hour.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {906–918},
numpages = {13},
keywords = {bug injection, token embeddings, machine learning, dataset, bugs},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/3384943.3409432,
author = {Qing, Sude and Liu, Xiulei and Zheng, Hongwei},
title = {An Assessment Framework for Distributed Ledger Technology in Financial Application},
year = {2020},
isbn = {9781450376105},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3384943.3409432},
doi = {10.1145/3384943.3409432},
abstract = {Distributed Ledger Technology (DLT) currently stands in the limelight. In many people's eyes, DLT has great potential to accelerate the digitalization process and substantially change the roles and services of central banks. On the other hands, as many new applications for DLT spring up, there isn't an assessment framework to evaluate DLT applied in the financial industry, leaving those systems to be easily under attack. This paper presents an initial DLT assessment framework for financial application. Through a suite of benchmarks, we show the evaluation results and explore the factors which affect the performance and security of DLT application.},
booktitle = {Proceedings of the 2nd ACM International Symposium on Blockchain and Secure Critical Infrastructure},
pages = {161–170},
numpages = {10},
keywords = {assessment framework, blockchain, distributed ledger technology, performance evaluation},
location = {Taipei, Taiwan},
series = {BSCI '20}
}

@inproceedings{10.1145/3377811.3380350,
author = {Chen, Dongjie and Jiang, Yanyan and Xu, Chang and Ma, Xiaoxing and Lu, Jian},
title = {Testing File System Implementations on Layered Models},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380350},
doi = {10.1145/3377811.3380350},
abstract = {Generating high-quality system call sequences is not only important to testing file system implementations, but also challenging due to the astronomically large input space. This paper introduces a new approach to the workload generation problem by building layered models and abstract workloads refinement. This approach is instantiated as a three-layer file system model for file system workload generation. In a short-period experiment run, sequential workloads (system call sequences) manifested over a thousand crashes in mainline Linux Kernel file systems, with 12 previously unknown bugs being reported. We also provide evidence that such workloads benefit other domain-specific testing techniques including crash consistency testing and concurrency testing.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {1483–1495},
numpages = {13},
keywords = {file system, workload generation, model-based testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

