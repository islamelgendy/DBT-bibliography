@inproceedings{10.1145/1453101.1453126,
author = {Halfond, William G. J. and Orso, Alessandro},
title = {Automated Identification of Parameter Mismatches in Web Applications},
year = {2008},
isbn = {9781595939951},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1453101.1453126},
doi = {10.1145/1453101.1453126},
abstract = {Quality assurance techniques for web applications have become increasingly important as web applications have gained in popularity and become an essential part of our daily lives. To integrate content and data from multiple sources, the components of a web application communicate extensively among themselves. Unlike traditional program modules, the components communicate through interfaces and invocations that are not explicitly declared. Because of this, the communication between two components can fail due to a parameter mismatch between the interface invoked by a calling component and the interface provided by the called component. Parameter mismatches can cause serious errors in the web application and are difficult to identify using traditional testing and verification techniques. To address this problem, we propose a static-analysis based approach for identifying parameter mismatches. We also present an empirical evaluation of the approach, which we performed on a set of real web applications. The results of the evaluation are promising; our approach discovered 133 parameter mismatches in the subject applications.},
booktitle = {Proceedings of the 16th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {181–191},
numpages = {11},
keywords = {web applications},
location = {Atlanta, Georgia},
series = {SIGSOFT '08/FSE-16}
}

@inproceedings{10.1109/MSR.2017.62,
author = {Beller, Moritz and Gousios, Georgios and Zaidman, Andy},
title = {Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub},
year = {2017},
isbn = {9781538615447},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/MSR.2017.62},
doi = {10.1109/MSR.2017.62},
abstract = {Continuous Integration (CI) has become a best practice of modern software development. Yet, at present, we have a shortfall of insight into the testing practices that are common in CI-based software development. In particular, we seek quantifiable evidence on how central testing is to the CI process, how strongly the project language influences testing, whether different integration environments are valuable and if testing on the CI can serve as a surrogate to local testing in the IDE. In an analysis of 2,640,825 Java and Ruby builds on Travis CI, we find that testing is the single most important reason why builds fail. Moreover, the programming language has a strong influence on both the number of executed tests, their run time, and proneness to fail. The use of multiple integration environments leads to 10% more failures being caught at build time. However, testing on Travis CI does not seem an adequate surrogate for running tests locally in the IDE. To further research on Travis CI with GitHub, we introduce TravisTorrent.},
booktitle = {Proceedings of the 14th International Conference on Mining Software Repositories},
pages = {356–367},
numpages = {12},
location = {Buenos Aires, Argentina},
series = {MSR '17}
}

@inproceedings{10.1109/ICSE.2009.5070560,
author = {Gousios, Georgios and Spinellis, Diomidis},
title = {Alitheia Core: An Extensible Software Quality Monitoring Platform},
year = {2009},
isbn = {9781424434534},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ICSE.2009.5070560},
doi = {10.1109/ICSE.2009.5070560},
abstract = {Research in the fields of software quality and maintainability requires the analysis of large quantities of data, which often originate from open source software projects. Pre-processing data, calculating metrics, and synthesizing composite results from a large corpus of project artefacts is a tedious and error prone task lacking direct scientific value. The Alitheia Core tool is an extensible platform for software quality analysis that is designed specifically to facilitate software engineering research on large and diverse data sources, by integrating data collection and preprocessing phases with an array of analysis services, and presenting the researcher with an easy to use extension mechanism. The system has been used to process several projects successfully, forming the basis of an emerging ecosystem of quality analysis tools.},
booktitle = {Proceedings of the 31st International Conference on Software Engineering},
pages = {579–582},
numpages = {4},
series = {ICSE '09}
}

@inproceedings{10.1145/2811681.2811691,
author = {Liu, Huai and Spichkova, Maria and Schmidt, Heinz W. and Ulrich, Andreas and Sauer, Horst and Wieghardt, Jan},
title = {Efficient Testing Based on Logical Architecture},
year = {2015},
isbn = {9781450337960},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2811681.2811691},
doi = {10.1145/2811681.2811691},
abstract = {The rapid increase of software-intensive systems' size and complexity makes it infeasible to exhaustively run testing on the low level of source code. Instead, the testing should be executed on the high level of system architecture, i.e., at a level where component or subsystems relate and interoperate or interact collectively with the system environment. Testing at this level is system testing, including hardware and software in union. Moreover, when integrating complex, distributed systems and providing support for conformance, interoperability and interoperation tests, we need to have an explicit test description. In this vision paper, we discuss (1) how to select tests from logical architecture, especially based on the dependencies within the system, and (2) how to represent the selected tests in explicit and readable manner, so that the software systems can be cost-efficiently maintained and evolved over their entire life-cycle. In addition, we further study the relevance between different tests, based on which, we can optimise the test suites for efficient testing, and propose optimal resource allocation strategies for cloud-based testing.},
booktitle = {Proceedings of the ASWEC 2015 24th Australasian Software Engineering Conference},
pages = {49–53},
numpages = {5},
keywords = {dependencies between services, testing, logical architecture},
location = {Adelaide, SA, Australia},
series = {ASWEC ' 15 Vol. II}
}

@inproceedings{10.1145/3185089.3185146,
author = {Zamli, Kamal Z. and Safieny, Norasyikin and Din, Fakhrud},
title = {Hybrid Test Redundancy Reduction Strategy Based on Global Neighborhood Algorithm and Simulated Annealing},
year = {2018},
isbn = {9781450354141},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3185089.3185146},
doi = {10.1145/3185089.3185146},
abstract = {Software testing is a critical part of software development. Often, test suite sizes grow significantly with subsequent modifications to the software over time resulting into potential redundancies. Test redundancies are undesirable as they incur costs and are not helpful to detect new bugs. Owing to time and resource constraints, test suite minimization strategies are often sought to remove those redundant test cases in an effort to ensure that each test can cover as much requirements as possible. There are already many works in the literature exploiting the greedy computational algorithms as well as the meta-heuristic algorithms, but no single strategy can claim dominance in terms of test data reduction over their counterparts. Furthermore, despite much useful work, existing strategies have not sufficiently explored the hybrid based meta-heuristic strategies. In order to improve the performance of existing strategies, hybridization is seen as the key to exploit the strength of more than one meta-heuristic algorithm. Given such prospects, this research explores a hybrid test redundancy reduction strategy based on Global Neighborhood Algorithm and Simulated Annealing, called GNA_SA. Overall, GNA_SA offers better reduction as compared to the original GNA and many existing works.},
booktitle = {Proceedings of the 2018 7th International Conference on Software and Computer Applications},
pages = {87–91},
numpages = {5},
keywords = {Global Neighborhood Algorithm, Test Redundancy Reduction, Simulated Annealing},
location = {Kuantan, Malaysia},
series = {ICSCA 2018}
}

@inproceedings{10.1145/2635868.2635872,
author = {Seo, Hyunmin and Kim, Sunghun},
title = {How We Get There: A Context-Guided Search Strategy in Concolic Testing},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635872},
doi = {10.1145/2635868.2635872},
abstract = { One of the biggest challenges in concolic testing, an automatic test generation technique, is its huge search space. Concolic testing generates next inputs by selecting branches from previous execution paths. However, a large number of candidate branches makes a simple exhaustive search infeasible, which often leads to poor test coverage. Several search strategies have been proposed to explore high-priority branches only. Each strategy applies different criteria to the branch selection process but most do not consider context, how we got to the branch, in the selection process. In this paper, we introduce a context-guided search (CGS) strategy. CGS looks at preceding branches in execution paths and selects a branch in a new context for the next input. We evaluate CGS with two publicly available concolic testing tools, CREST and CarFast, on six C subjects and six Java subjects. The experimental results show that CGS achieves the highest coverage of all twelve subjects and reaches a target coverage with a much smaller number of iterations on most subjects than other strategies. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {413–424},
numpages = {12},
keywords = {Concolic testing, symbolic execution, search strategies},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inbook{10.1145/3238147.3238159,
author = {Shen, Yuju and Jiang, Yanyan and Xu, Chang and Yu, Ping and Ma, Xiaoxing and Lu, Jian},
title = {ReScue: Crafting Regular Expression DoS Attacks},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238159},
abstract = {Regular expression (regex) with modern extensions is one of the most popular string processing tools. However, poorly-designed regexes can yield exponentially many matching steps, and lead to regex Denial-of-Service (ReDoS) attacks under well-conceived string inputs. This paper presents Rescue, a three-phase gray-box analytical technique, to automatically generate ReDoS strings to highlight vulnerabilities of given regexes. Rescue systematically seeds (by a genetic search), incubates (by another genetic search), and finally pumps (by a regex-dedicated algorithm) for generating strings with maximized search time. We implemenmted the Rescue tool and evaluated it against 29,088 practical regexes in real-world projects. The evaluation results show that Rescue found 49% more attack strings compared with the best existing technique, and applying Rescue to popular GitHub projects discovered ten previously unknown ReDoS vulnerabilities.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {225–235},
numpages = {11}
}

@inproceedings{10.1145/2491956.2462173,
author = {Chen, Yang and Groce, Alex and Zhang, Chaoqiang and Wong, Weng-Keen and Fern, Xiaoli and Eide, Eric and Regehr, John},
title = {Taming Compiler Fuzzers},
year = {2013},
isbn = {9781450320146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491956.2462173},
doi = {10.1145/2491956.2462173},
abstract = {Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.},
booktitle = {Proceedings of the 34th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {197–208},
numpages = {12},
keywords = {bug reporting, random testing, automated testing, compiler testing, fuzz testing, test-case reduction, compiler defect},
location = {Seattle, Washington, USA},
series = {PLDI '13}
}

@article{10.1145/2499370.2462173,
author = {Chen, Yang and Groce, Alex and Zhang, Chaoqiang and Wong, Weng-Keen and Fern, Xiaoli and Eide, Eric and Regehr, John},
title = {Taming Compiler Fuzzers},
year = {2013},
issue_date = {June 2013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2499370.2462173},
doi = {10.1145/2499370.2462173},
abstract = {Aggressive random testing tools ("fuzzers") are impressively effective at finding compiler bugs. For example, a single test-case generator has resulted in more than 1,700 bugs reported for a single JavaScript engine. However, fuzzers can be frustrating to use: they indiscriminately and repeatedly find bugs that may not be severe enough to fix right away. Currently, users filter out undesirable test cases using ad hoc methods such as disallowing problematic features in tests and grepping test results. This paper formulates and addresses the fuzzer taming problem: given a potentially large number of random test cases that trigger failures, order them such that diverse, interesting test cases are highly ranked. Our evaluation shows our ability to solve the fuzzer taming problem for 3,799 test cases triggering 46 bugs in a C compiler and 2,603 test cases triggering 28 bugs in a JavaScript engine.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {197–208},
numpages = {12},
keywords = {compiler defect, automated testing, compiler testing, bug reporting, fuzz testing, random testing, test-case reduction}
}

@inproceedings{10.1145/3084226.3084252,
author = {Raulamo-Jurvanen, P\"{a}ivi and M\"{a}ntyl\"{a}, Mika and Garousi, Vahid},
title = {Choosing the Right Test Automation Tool: A Grey Literature Review of Practitioner Sources},
year = {2017},
isbn = {9781450348041},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3084226.3084252},
doi = {10.1145/3084226.3084252},
abstract = {Background: Choosing the right software test automation tool is not trivial, and recent industrial surveys indicate lack of right tools as the main obstacle to test automation. Aim: In this paper, we study how practitioners tackle the problem of choosing the right test automation tool. Method: We synthesize the "voice" of the practitioners with a grey literature review originating from 53 different companies. The industry experts behind the sources had roles such as "Software Test Automation Architect", and "Principal Software Engineer". Results: Common consensus about the important criteria exists but those are not applied systematically. We summarize the scattered steps from individual sources by presenting a comprehensive process for tool evaluation with 12 steps and a total of 14 different criteria for choosing the right tool. Conclusions: The practitioners tend to have general interest in and be influenced by related grey literature as about 78% of our sources had at least 20 backlinks (a reference comparable to a citation) while the variation was between 3 and 759 backlinks. There is a plethora of different software testing tools available, yet the practitioners seem to prefer and adopt the widely known and used tools. The study helps to identify the potential pitfalls of existing processes and opportunities for comprehensive tool evaluation.},
booktitle = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering},
pages = {21–30},
numpages = {10},
keywords = {Grey literature review, test automation tool, software test automation, tool selection},
location = {Karlskrona, Sweden},
series = {EASE'17}
}

@inproceedings{10.1145/2970276.2970343,
author = {Ceccato, Mariano and Nguyen, Cu D. and Appelt, Dennis and Briand, Lionel C.},
title = {SOFIA: An Automated Security Oracle for Black-Box Testing of SQL-Injection Vulnerabilities},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970343},
doi = {10.1145/2970276.2970343},
abstract = { Security testing is a pivotal activity in engineering secure software. It consists of two phases: generating attack inputs to test the system, and assessing whether test executions expose any vulnerabilities. The latter phase is known as the security oracle problem.  In this work, we present SOFIA, a Security Oracle for SQL-Injection Vulnerabilities. SOFIA is programming-language and source-code independent, and can be used with various attack generation tools. Moreover, because it does not rely on known attacks for learning, SOFIA is meant to also detect types of SQLi attacks that might be unknown at learning time. The oracle challenge is recast as a one-class classification problem where we learn to characterise legitimate SQL statements to accurately distinguish them from SQLi attack statements.  We have carried out an experimental validation on six applications, among which two are large and widely-used. SOFIA was used to detect real SQLi vulnerabilities with inputs generated by three attack generation tools. The obtained results show that SOFIA is computationally fast and achieves a recall rate of 100% (i.e., missing no attacks) with a low false positive rate (0.6%). },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {167–177},
numpages = {11},
keywords = {SQL-injection, Security oracle, Security testing},
location = {Singapore, Singapore},
series = {ASE 2016}
}

@inproceedings{10.1145/3302424.3303948,
author = {Lochmann, Alexander and Schirmeier, Horst and Borghorst, Hendrik and Spinczyk, Olaf},
title = {LockDoc: Trace-Based Analysis of Locking in the Linux Kernel},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303948},
doi = {10.1145/3302424.3303948},
abstract = {For fine-grained synchronization of application and kernel threads, the Linux kernel provides a multitude of different locking mechanisms that are being used on various individually locked data structures. Understanding which locks are required in which order for a particular member variable of a kernel data structure has become truly difficult, even for Linux-kernel experts themselves.In this paper we introduce LockDoc -- an approach that, based on the analysis of execution traces of an instrumented Linux kernel, automatically deduces the most likely locking rule for all members of arbitrary kernel data structures. From these locking rules, LockDoc generates documentation that supports kernel developers and helps avoiding concurrency bugs. Additionally, the (very limited) existing documentation can be verified, and locking-rule violations -- potential bugs in the kernel code -- can be found.Our results include generated locking rules for previously predominantly undocumented member variables of 11 different Linux-kernel data structures. Manually inspecting the scarce source-code documentation for five of these data structures reveals that only 53 percent of the variables with a documented locking rule are actually consistently accessed with the required locks held. This indicates possible documentation or synchronization bugs in the Linux kernel, of which one has already been confirmed by kernel experts.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {11},
numpages = {15},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{10.1145/3180155.3180236,
author = {Sun, Chengnian and Li, Yuanbo and Zhang, Qirun and Gu, Tianxiao and Su, Zhendong},
title = {Perses: Syntax-Guided Program Reduction},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180236},
doi = {10.1145/3180155.3180236},
abstract = {Given a program P that exhibits a certain property Ψ (e.g., a C program that crashes GCC when it is being compiled), the goal of program reduction is to minimize P to a smaller variant P′ that still exhibits the same property, i.e., Ψ(P′). Program reduction is important and widely demanded for testing and debugging. For example, all compiler/interpreter development projects need effective program reduction to minimize failure-inducing test programs to ease debugging. However, state-of-the-art program reduction techniques --- notably Delta Debugging (DD), Hierarchical Delta Debugging (HDD), and C-Reduce --- do not perform well in terms of speed (reduction time) and quality (size of reduced programs), or are highly customized for certain languages and thus lack generality.This paper presents Perses, a novel framework for effective, efficient, and general program reduction. The key insight is to exploit, in a general manner, the formal syntax of the programs under reduction and ensure that each reduction step considers only smaller, syntactically valid variants to avoid futile efforts on syntactically invalid variants. Our framework supports not only deletion (as for DD and HDD), but also general, effective program transformations.We have designed and implemented Perses, and evaluated it for two language settings: C and Java. Our evaluation results on 20 C programs triggering bugs in GCC and Clang demonstrate Perses's strong practicality compared to the state-of-the-art: (1) smaller size --- Perses's results are respectively 2% and 45% in size of those from DD and HDD; and (2) shorter reduction time --- Perses takes 23% and 47% time taken by DD and HDD respectively. Even when compared to the highly customized and optimized C-Reduce for C/C++, Perses takes only 38-60% reduction time.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {361–371},
numpages = {11},
keywords = {program reduction, debugging, delta debugging},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1109/ASE.2019.00133,
author = {Mossberg, Mark and Manzano, Felipe and Hennenfent, Eric and Groce, Alex and Grieco, Gustavo and Feist, Josselin and Brunson, Trent and Dinaburg, Artem},
title = {Manticore: A User-Friendly Symbolic Execution Framework for Binaries and Smart Contracts},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00133},
doi = {10.1109/ASE.2019.00133},
abstract = {An effective way to maximize code coverage in software tests is through dynamic symbolic execution---a technique that uses constraint solving to systematically explore a program's state space. We introduce an open-source dynamic symbolic execution framework called Manticore for analyzing binaries and Ethereum smart contracts. Manticore's flexible architecture allows it to support both traditional and exotic execution environments, and its API allows users to customize their analysis. Here, we discuss Manticore's architecture and demonstrate the capabilities we have used to find bugs and verify the correctness of code for our commercial clients.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1186–1189},
numpages = {4},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3236024.3236029,
author = {Pauck, Felix and Bodden, Eric and Wehrheim, Heike},
title = {Do Android Taint Analysis Tools Keep Their Promises?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236029},
doi = {10.1145/3236024.3236029},
abstract = {In recent years, researchers have developed a number of tools to conduct taint analysis of Android applications. While all the respective papers aim at providing a thorough empirical evaluation, comparability is hindered by varying or unclear evaluation targets. Sometimes, the apps used for evaluation are not precisely described. In other cases, authors use an established benchmark but cover it only partially. In yet other cases, the evaluations differ in terms of the data leaks searched for, or lack a ground truth to compare against. All those limitations make it impossible to truly compare the tools based on those published evaluations.  We thus present ReproDroid, a framework allowing the accurate comparison of Android taint analysis tools. ReproDroid supports researchers in inferring the ground truth for data leaks in apps, in automatically applying tools to benchmarks, and in evaluating the obtained results. We use ReproDroid to comparatively evaluate on equal grounds the six prominent taint analysis tools Amandroid, DIALDroid, DidFail, DroidSafe, FlowDroid and IccTA. The results are largely positive although four tools violate some promises concerning features and accuracy. Finally, we contribute to the area of unbiased benchmarking with a new and improved version of the open test suite DroidBench.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {331–341},
numpages = {11},
keywords = {Reproducibility, Android Taint Analysis, Benchmarks, Tools, Empirical Studies},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/2648511.2648513,
author = {Harman, M. and Jia, Y. and Krinke, J. and Langdon, W. B. and Petke, J. and Zhang, Y.},
title = {Search Based Software Engineering for Software Product Line Engineering: A Survey and Directions for Future Work},
year = {2014},
isbn = {9781450327404},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2648511.2648513},
doi = {10.1145/2648511.2648513},
abstract = {This paper presents a survey of work on Search Based Software Engineering (SBSE) for Software Product Lines (SPLs). We have attempted to be comprehensive, in the sense that we have sought to include all papers that apply computational search techniques to problems in software product line engineering. Having surveyed the recent explosion in SBSE for SPL research activity, we highlight some directions for future work. We focus on suggestions for the development of recent advances in genetic improvement, showing how these might be exploited by SPL researchers and practitioners: Genetic improvement may grow new products with new functional and non-functional features and graft these into SPLs. It may also merge and parameterise multiple branches to cope with SPL branchmania.},
booktitle = {Proceedings of the 18th International Software Product Line Conference - Volume 1},
pages = {5–18},
numpages = {14},
keywords = {program synthesis, genetic programming, SPL, SBSE},
location = {Florence, Italy},
series = {SPLC '14}
}

@inproceedings{10.1145/3324884.3416590,
author = {Wang, Shangwen and Wen, Ming and Lin, Bo and Wu, Hongjun and Qin, Yihao and Zou, Deqing and Mao, Xiaoguang and Jin, Hai},
title = {Automated Patch Correctness Assessment: How Far Are We?},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416590},
doi = {10.1145/3324884.3416590},
abstract = {Test-based automated program repair (APR) has attracted huge attention from both industry and academia. Despite the significant progress made in recent studies, the overfitting problem (i.e., the generated patch is plausible but overfitting) is still a major and long-standing challenge. Therefore, plenty of techniques have been proposed to assess the correctness of patches either in the patch generation phase or in the evaluation of APR techniques. However, the effectiveness of existing techniques has not been systematically compared and little is known to their advantages and disadvantages. To fill this gap, we performed a large-scale empirical study in this paper. Specifically, we systematically investigated the effectiveness of existing automated patch correctness assessment techniques, including both static and dynamic ones, based on 902 patches automatically generated by 21 APR tools from 4 different categories. Our empirical study revealed the following major findings: (1) static code features with respect to patch syntax and semantics are generally effective in differentiating overfitting patches over correct ones; (2) dynamic techniques can generally achieve high precision while heuristics based on static code features are more effective towards recall; (3) existing techniques are more effective towards certain projects and types of APR techniques while less effective to the others; (4) existing techniques are highly complementary to each other. For instance, a single technique can only detect at most 53.5% of the overfitting patches while 93.3% of them can be detected by at least one technique when the oracle information is available. Based on our findings, we designed an integration strategy to first integrate static code features via learning, and then combine with others by the majority voting strategy. Our experiments show that the strategy can enhance the performance of existing patch correctness assessment techniques significantly.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {968–980},
numpages = {13},
keywords = {empirical assessment, patch correctness, program repair},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3281411.3281436,
author = {Zheng, Peng and Benson, Theophilus and Hu, Chengchen},
title = {P4Visor: Lightweight Virtualization and Composition Primitives for Building and Testing Modular Programs},
year = {2018},
isbn = {9781450360807},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3281411.3281436},
doi = {10.1145/3281411.3281436},
abstract = {Programmable data planes, PDPs, enable an unprecedented level of flexibility and have emerged as a promising alternative to existing data planes. Despite the rapid development and prototyping cycles that PDPs promote, the existing PDP ecosystem lacks appropriate abstractions and algorithms to support these rapid testing and deployment life-cycles. In this paper, we propose P4Visor, a lightweight virtualization abstraction that provides testing primitives as a first-order citizen of the PDP ecosystem. P4Visor can efficiently support multiple PDP programs through a combination of compiler optimizations and program analysis-based algorithms. P4Visor s algorithm improves over state-of-the-art techniques by significantly reducing the resource overheads associated with embedding numerous versions of a PDP program into hardware. To demonstrate the efficiency and viability of P4Visor, we implemented and evaluated P4Visor on both a software switch and an FPGA-based hardware switch using fourteen different PDP programs. Our results demonstrate that P4Visor introduces minimal overheads (less than 1%) and is one order of magnitude more efficient than existing PDPs primitives for concurrently supporting multiple programs.},
booktitle = {Proceedings of the 14th International Conference on Emerging Networking EXperiments and Technologies},
pages = {98–111},
numpages = {14},
keywords = {code merge, programmable data plane, testing},
location = {Heraklion, Greece},
series = {CoNEXT '18}
}

@inproceedings{10.1145/3009837.3009868,
author = {Lampropoulos, Leonidas and Gallois-Wong, Diane and Hri\c{t}cu, C\u{a}t\u{a}lin and Hughes, John and Pierce, Benjamin C. and Xia, Li-yao},
title = {Beginner's Luck: A Language for Property-Based Generators},
year = {2017},
isbn = {9781450346603},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3009837.3009868},
doi = {10.1145/3009837.3009868},
abstract = { Property-based random testing \`{a} la QuickCheck requires building efficient generators for well-distributed random data satisfying complex logical predicates, but writing these generators can be difficult and error prone. We propose a domain-specific language in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated. This language, called Luck, makes generators easier to write, read, and maintain.  We give Luck a formal semantics and prove several fundamental properties, including the soundness and completeness of random generation with respect to a standard predicate semantics. We evaluate Luck on common examples from the property-based testing literature and on two significant case studies, showing that it can be used in complex domains with comparable bug-finding effectiveness and a significant reduction in testing code size compared to handwritten generators. },
booktitle = {Proceedings of the 44th ACM SIGPLAN Symposium on Principles of Programming Languages},
pages = {114–129},
numpages = {16},
keywords = {constraint solving, narrowing, domain specific language, random testing, property-based testing},
location = {Paris, France},
series = {POPL 2017}
}

@article{10.1145/3093333.3009868,
author = {Lampropoulos, Leonidas and Gallois-Wong, Diane and Hri\c{t}cu, C\u{a}t\u{a}lin and Hughes, John and Pierce, Benjamin C. and Xia, Li-yao},
title = {Beginner's Luck: A Language for Property-Based Generators},
year = {2017},
issue_date = {January 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093333.3009868},
doi = {10.1145/3093333.3009868},
abstract = { Property-based random testing \`{a} la QuickCheck requires building efficient generators for well-distributed random data satisfying complex logical predicates, but writing these generators can be difficult and error prone. We propose a domain-specific language in which generators are conveniently expressed by decorating predicates with lightweight annotations to control both the distribution of generated values and the amount of constraint solving that happens before each variable is instantiated. This language, called Luck, makes generators easier to write, read, and maintain.  We give Luck a formal semantics and prove several fundamental properties, including the soundness and completeness of random generation with respect to a standard predicate semantics. We evaluate Luck on common examples from the property-based testing literature and on two significant case studies, showing that it can be used in complex domains with comparable bug-finding effectiveness and a significant reduction in testing code size compared to handwritten generators. },
journal = {SIGPLAN Not.},
month = {jan},
pages = {114–129},
numpages = {16},
keywords = {constraint solving, narrowing, property-based testing, domain specific language, random testing}
}

@inproceedings{10.5555/2820704.2820711,
author = {Tonella, Paolo and Tiella, Roberto},
title = {Weekly Round Trips from Norms to Requirements and Tests: An Industrial Experience Report},
year = {2015},
publisher = {IEEE Press},
abstract = {SEAC is a major software provider in Italy in the area of business management, with a focus on norms and human resources. SEAC is re-engineering their huge legacy system to C#/SQL Server. To minimise the risks associated with such reengineering project, SEAC has adopted an incremental and agile process model, which produces small and frequent releases of new, incremental modules that replace a portion of the legacy system at a time.Since the SEAC software handles business activities that are highly dependent on norms, such as the contracts of employees, the taxation of incomes and salaries, the pension contributions, one of the key challenges is to support a smooth transformation of norms into requirements, into code and eventually into test cases used to verify that norms have been implemented as prescribed by the law. The SE research unit at FBK has been involved to introduce a set of practices aimed at supporting such transformation, so as to improve the current process. We report the experience made during the project in this paper.},
booktitle = {Proceedings of the Second International Workshop on Requirements Engineering and Testing},
pages = {20–26},
numpages = {7},
keywords = {requirements and testing in practice, industrial experience, agile process},
location = {Florence, Italy},
series = {RET '15}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: Symbolic Execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {GraalVM, Compiler optimizations, Java, Symbolic execution},
location = {Virtual, UK},
series = {MPLR 2020}
}

