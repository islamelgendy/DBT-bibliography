@inproceedings{10.1145/2508075.2514881,
author = {Ohmann, Peter},
title = {CSI: Crash Scene Investigation},
year = {2013},
isbn = {9781450319959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508075.2514881},
doi = {10.1145/2508075.2514881},
abstract = {Prior work proposes inexpensive, tunable tracing of acyclic paths and callsite coverage to enhance post-failure memory dumps. To better understand this data, current work investigates the benefit of each piece of traced data independently, their interplay, future low-cost data to collect, and further analysis uses of the post-mortem data.},
booktitle = {Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, &amp; Applications: Software for Humanity},
pages = {123–124},
numpages = {2},
keywords = {core dumps, program slicing, failure analysis},
location = {Indianapolis, Indiana, USA},
series = {SPLASH '13}
}

@inproceedings{10.1145/2839509.2844626,
author = {Politz, Joe Gibbs and Collard, Joseph M. and Guha, Arjun and Fisler, Kathi and Krishnamurthi, Shriram},
title = {The Sweep: Essential Examples for In-Flow Peer Review},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844626},
doi = {10.1145/2839509.2844626},
abstract = {In in-flow peer review, students provide feedback to one another on intermediate artifacts on their way to a final submission. Prior work has studied examples and tests as a potentially useful initial artifact for review. Unfortunately, large test suites are onerous to produce and especially to review. We instead propose the notion of a sweep, an artificially constrained set of tests that illustrates common and interesting behavior. We present experimental data across several courses that show that sweeps have reasonable quality, and are also a good target for peer review; for example, students usually (over half the time) suggest new tests to one another in a review.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {243–248},
numpages = {6},
keywords = {testing, example-first programming, peer review, code review},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3106195.3106210,
author = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-Based Product Line Fault Detection Allocating Test Cases Iteratively},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106210},
doi = {10.1145/3106195.3106210},
abstract = {The large number of possible configurations makes it unfeasible to test every single system variant in a product line. Consequently, a small subset of the product line products must be selected, typically following combinatorial interaction testing approaches. Recently, many product line engineering approaches have considered the selection and prioritization of relevant products within the product line. In a further step, these products are thoroughly tested individually. However, the test cases that must be executed in each of the products are not always insignificant, and in systems such as Cyber-Physical System Product Lines (CPSPLs), their test execution time can vary from tens to thousands of seconds. This issue leads to spending a lot of time testing each individual product. To solve this problem we propose a search-based approach to perform the testing of product lines by allocating small number of test cases in each of the products. This approach increases the probability of detecting faults faster. Specifically, our search-based approach obtains a set of products, which are derived from using any state-of-the-art approach as inputs, and a set of attributed test cases. As an output a list of allocated test cases for each product is obtained. We also define a novel fitness function to guide the search and we propose corresponding crossover and mutation operators. The search and test process is iteratively repeated until the time budget is consumed. We performed an evaluation with a CPSPL as a case study. Results suggest that our approach can reduce the fault detection time by 61% and 65% on average when compared with the traditional test process and the Random Search algorithm respectively.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {123–132},
numpages = {10},
keywords = {Fault Detection, Product Line Testing, Search-based Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@article{10.1145/3360603,
author = {Celik, Ahmet and Nie, Pengyu and Rossbach, Christopher J. and Gligoric, Milos},
title = {Design, Implementation, and Application of GPU-Based Java Bytecode Interpreters},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360603},
doi = {10.1145/3360603},
abstract = {We present the design and implementation of GVM, the first system for executing Java bytecode entirely on GPUs. GVM is ideal for applications that execute a large number of short-living tasks, which share a significant fraction of their codebase and have similar execution time. GVM uses novel algorithms, scheduling, and data layout techniques to adapt to the massively parallel programming and execution model of GPUs. We apply GVM to generate and execute tests for Java projects. First, we implement a sequence-based test generation on top of GVM and design novel algorithms to avoid redundant test sequences. Second, we use GVM to execute randomly generated test cases. We evaluate GVM by comparing it with two existing Java bytecode interpreters (Oracle JVM and Java Pathfinder), as well as with the Oracle JVM with just-in-time (JIT) compiler, which has been engineered and optimized for over twenty years. Our evaluation shows that sequence-based test generation on GVM outperforms both Java Pathfinder and Oracle JVM interpreter. Additionally, our results show that GVM performs as well as running our parallel sequence-based test generation algorithm using JVM with JIT with many CPU threads. Furthermore, our evaluation on several classes from open-source projects shows that executing randomly generated tests on GVM outperforms sequential execution on JVM interpreter and JVM with JIT.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {177},
numpages = {28},
keywords = {Complete matching, Graphics Processing Unit, Shape matching, Java bytecode interpreter, Sequence-based test generation}
}

@inproceedings{10.1109/IWSESS.2009.5068454,
author = {El-Ghali, M. and Masri, W.},
title = {Intrusion Detection Using Signatures Extracted from Execution Profiles},
year = {2009},
isbn = {9781424437252},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IWSESS.2009.5068454},
doi = {10.1109/IWSESS.2009.5068454},
abstract = {An application based intrusion detection system is a security mechanism designed to detect malicious behavior that could compromise the security of a software application. Our aim is to develop such a system that operates on signatures extracted from execution profiles. These signatures are not descriptions of exploits, but instead are descriptions of the program conditions that lead to the exploitation of software vulnerabilities, i.e., they depend on the structure of the vulnerabilities themselves. A program vulnerability is generally induced by the execution of a combination of program statements. In this work we first analyze the execution profiles of a subject application in order to identify such suspicious combinations and consequently extract and define their corresponding signatures. Then, we insert probes in select locations in the application to enable online signature matching. To evaluate our technique, we implemented it for Java programs and applied it on Tomcat 3.0 in order to detect well-known attacks. Our results were promising, as no false negatives and a maximum of 4.5% false positives were observed, and the runtime overhead was less than 5%.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Software Engineering for Secure Systems},
pages = {17–24},
numpages = {8},
keywords = {program statements, Java programs, program analysis, online signature matching, intrusion detection system, software security, security mechanism, software vulnerability, malicious behavior, execution profiles, Tomcat 3.0, signature extraction},
series = {IWSESS '09}
}

@inproceedings{10.1145/3422392.3422511,
author = {de Almeida, Diego Rodrigues and Machado, Patr\'{\i}cia D. L. and Andrade, Wilkerson L.},
title = {ENVIAR: ENVIronment DAta SimulatoR},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422511},
doi = {10.1145/3422392.3422511},
abstract = {Context-aware applications (CAAs) sense and react to changes from the environment when performing their tasks. Testing such applications is challenging. Firstly, due to the number of combinations of possible events and values they can handle. Secondly, due to the different execution scenarios that may be hard to reproduce manually. We present the ENVIAR tool whose objective is to support test case generation and execution of Android CAAs. Generation combines a set of events that may trigger failure in such applications filtered by pairwise testing. Execution simulates the environment in which the CAA runs by sending mock data. Current version focus on Android events, GPS, and Internet sensor events and values. Empirical results have shown that the tool can generate test cases that detect context defects in real Android applications, particularly covering scenarios that would be tricky to run in a real environment.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {532–537},
numpages = {6},
keywords = {Testing automation, Context-aware application, Android},
location = {Natal, Brazil},
series = {SBES '20}
}

@inproceedings{10.1145/2304696.2304714,
author = {Yu, Jian and Han, Jun and Schneider, Jean-Guy and Hine, Cameron and Versteeg, Steve},
title = {A Virtual Deployment Testing Environment for Enterprise Software Systems},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304714},
doi = {10.1145/2304696.2304714},
abstract = {Modern enterprise software systems often need to interact with a large number of heterogeneous systems in an enterprise IT environment. The distributedness, large-scale-ness, and heterogeneity of such environment makes it difficult to test a system's quality attributes such as performance and scalability before it is actually deployed in the environment. In this paper, we present a Coloured Petri nets (CPN) based system behaviour emulation approach and a lightweight virtual testing framework for provisioning the deployment testing environment of an enterprise system so that its quality attributes, especially scalability, can be evaluated without physically connecting to the real production environment. This testing environment is scalable and has a flexible pluggable architecture to support the emulation of the behaviour of heterogeneous systems in the environment. To validate the feasibility of this approach, a CPN emulation model for LDAP has been developed and applied in testing the scalability of a real-life identity management system. An in-lab performance study has been conducted to demonstrate the effectiveness of this approach.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {101–110},
numpages = {10},
keywords = {deployment testing, petri nets, system emulation, enterprise software systems},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@inproceedings{10.5555/776816.776824,
author = {Harder, Michael and Mellen, Jeff and Ernst, Michael D.},
title = {Improving Test Suites via Operational Abstraction},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text.The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications) from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults.This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {60–71},
numpages = {12},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/3377811.3380323,
author = {Hong, Seongjoon and Lee, Junhee and Lee, Jeongsoo and Oh, Hakjoo},
title = {SAVER: Scalable, Precise, and Safe Memory-Error Repair},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380323},
doi = {10.1145/3377811.3380323},
abstract = {We present SAVER, a new memory-error repair technique for C programs. Memory errors such as memory leak, double-free, and use-after-free are highly prevalent and fixing them requires significant effort. Automated program repair techniques hold the promise of reducing this burden but the state-of-the-art is still unsatisfactory. In particular, no existing techniques are able to fix those errors in a scalable, precise, and safe way, all of which are required for a truly practical tool. SAVER aims to address these shortcomings. To this end, we propose a method based on a novel representation of the program called object flow graph, which summarizes the program's heap-related behavior using static analysis. We show that fixing memory errors can be formulated as a graph labeling problem over object flow graph and present an efficient algorithm. We evaluated SAVER in combination with Infer, an industrial-strength static bug-finder, and show that 74% of the reported errors can be fixed automatically for a range of open-source C programs.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {271–283},
numpages = {13},
keywords = {program repair, program analysis, memory errors, debugging},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/68210.69228,
author = {Forin, Alessandro},
title = {Debugging of Heterogeneous Parallel Systems},
year = {1988},
isbn = {0897912969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/68210.69228},
doi = {10.1145/68210.69228},
abstract = {The Agora system supports the development of heterogeneous parallel programs, e.g. programs written in multiple languages and running on heterogeneous machines. Agora has been used since September 1986 in a large distributed system [1]: Two versions of the application have been demonstrated in one year, contrary to the expectation of two years per one version. The simplicity in debugging is one of the reasons of the productivity speedup gained. This simplicity is due both to the deeper understanding that the debugger has of parallel systems, and to a novel feature: the ability to replay the execution of parallel systems built with Agora. A user is able to exactly repeat for any number of times and at a slower pace an execution that failed. This makes it easy to identify time-dependent errors, which are peculiar to parallel and distributed systems. The debugger can also be customized to support user defined synchronization primitives, which are built on top of the system provided ones. The Agora debugger tackles three set of problems that no parallel debugger in the past has simultaneously addressed: dealing with programming-in-the-large, multiple processes in different languages, and multiple machine architectures.},
booktitle = {Proceedings of the 1988 ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging},
pages = {130–140},
numpages = {11},
location = {Madison, Wisconsin, USA},
series = {PADD '88}
}

@article{10.1145/69215.69228,
author = {Forin, Alessandro},
title = {Debugging of Heterogeneous Parallel Systems},
year = {1988},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/69215.69228},
doi = {10.1145/69215.69228},
abstract = {The Agora system supports the development of heterogeneous parallel programs, e.g. programs written in multiple languages and running on heterogeneous machines. Agora has been used since September 1986 in a large distributed system [1]: Two versions of the application have been demonstrated in one year, contrary to the expectation of two years per one version. The simplicity in debugging is one of the reasons of the productivity speedup gained. This simplicity is due both to the deeper understanding that the debugger has of parallel systems, and to a novel feature: the ability to replay the execution of parallel systems built with Agora. A user is able to exactly repeat for any number of times and at a slower pace an execution that failed. This makes it easy to identify time-dependent errors, which are peculiar to parallel and distributed systems. The debugger can also be customized to support user defined synchronization primitives, which are built on top of the system provided ones. The Agora debugger tackles three set of problems that no parallel debugger in the past has simultaneously addressed: dealing with programming-in-the-large, multiple processes in different languages, and multiple machine architectures.},
journal = {SIGPLAN Not.},
month = {nov},
pages = {130–140},
numpages = {11}
}

@inproceedings{10.5555/2662572.2662578,
author = {de Oliveira Neto, Francisco Gomes and Feldt, Robert and Torkar, Richard and Machado, Patr\'{\i}cia D. L.},
title = {Searching for Models to Evaluate Software Technology},
year = {2013},
isbn = {9781467362849},
publisher = {IEEE Press},
abstract = {Modeling and abstraction is key in all engineering processes and have found extensive use also in software engineering. When developing new methodologies and techniques to support software engineers we want to evaluate them on realistic models. However, this is a challenge since (1) it is hard to get industry to give access to their models, and (2) we need a large number of models to systematically evaluate a technology. This paper proposes that search-based techniques can be used to search for models with desirable properties, which can then be used to systematically evaluate model-based technologies. By targeting properties seen in industrial models we can then get the best of both worlds: models that are similar to models used in industry but in quantities that allow extensive experimentation. To exemplify our ideas we consider a specific case in which a model generator is used to create models to test a regression test optimization technique.},
booktitle = {Proceedings of the 1st International Workshop on Combining Modelling and Search-Based Software Engineering},
pages = {12–15},
numpages = {4},
keywords = {search-based techniques, automatic model generation, model-based software engineering technology},
location = {San Francisco, California},
series = {CMSBSE '13}
}

@inproceedings{10.1109/ASE.2011.6100047,
author = {Nguyen, Hung Viet and Nguyen, Hoan Anh and Nguyen, Tung Thanh and Nguyen, Tien N.},
title = {Auto-Locating and Fix-Propagating for HTML Validation Errors to PHP Server-Side Code},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100047},
doi = {10.1109/ASE.2011.6100047},
abstract = {Checking/correcting HTML validation errors in Web pages is helpful for Web developers in finding/fixing bugs. However, existing validating/fixing tools work well only on static HTML pages and do not help fix the corresponding server code if validation errors are found in HTML pages, due to several challenges with dynamically generated pages in Web development. We propose PhpSync, a novel automatic locating/fixing tool for HTML validation errors in PHP-based Web applications. Given an HTML page produced by a server-side PHP program, PhpSync uses Tidy, an HTML validating/correcting tool to find the validation errors in that HTML page. If errors are detected, it leverages the fixes from Tidy in the given HTML page and propagates them to the corresponding location(s) in PHP code. Our core solutions include 1) a symbolic execution algorithm on the given PHP program to produce a single tree-based model, called D-model, which approximately represents its possible client page outputs, 2) an algorithm mapping any text in the given HTML page to the text(s) in the node(s) of the D-model and then to the PHP code, and 3) a fix-propagating algorithm from the fixes in the HTML page to the PHP code via the D-model and the mapping algorithm. Our empirical evaluation shows that on average, PhpSync achieves 96.7% accuracy in locating the corresponding locations in PHP code from client pages, and 95% accuracy in propagating the fixes to the server-side code.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {13–22},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/3180155.3180248,
author = {Dutra, Rafael and Laeufer, Kevin and Bachrach, Jonathan and Sen, Koushik},
title = {Efficient Sampling of SAT Solutions for Testing},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180248},
doi = {10.1145/3180155.3180248},
abstract = {In software and hardware testing, generating multiple inputs which satisfy a given set of constraints is an important problem with applications in fuzz testing and stimulus generation. However, it is a challenge to perform the sampling efficiently, while generating a diverse set of inputs which satisfy the constraints. We developed a new algorithm QuickSampler which requires a small number of solver calls to produce millions of samples which satisfy the constraints with high probability. We evaluate QuickSampler on large real-world benchmarks and show that it can produce unique valid solutions orders of magnitude faster than other state-of-the-art sampling tools, with a distribution which is reasonably close to uniform in practice.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {549–559},
numpages = {11},
keywords = {constraint-based testing, sampling, stimulus generation, constrained-random verification},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2972206.2972222,
author = {Hirzel, Matthias and Brachth\"{a}user, Jonathan Immanuel and Klaeren, Herbert},
title = {Prioritizing Regression Tests for Desktop and Web-Applications Based on the Execution Frequency of Modified Code},
year = {2016},
isbn = {9781450341356},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2972206.2972222},
doi = {10.1145/2972206.2972222},
abstract = {Regression testing can be very time expensive when running all available test cases. Test prioritization seeks to find faults early by reordering tests. Existing techniques decide in which order tests should be run based on coverage data, knowledge of code changes, historical data of prior test execution or a combination of them. Others postpone tests if similar ones are already selected for early execution. However, these approaches do not take into account that tests which appear similar still might explore different parts of the application's state space and thus can result in different test outcome. Approaches based on structural coverage or on historical data might ignore small tests focusing on behavior that rarely changes. In this paper, we present a novel prioritization technique that is based on the frequencies with which modified code parts are executed by the tests. Our technique assumes that multiple executions of a modified code part (under different contexts) have a higher chance to reveal faults than a single execution of this code. For this purpose, we use both the output of regression test selection as well as test traces obtained during test development. We propose multiple variants of our technique, including a feedback mechanism to optimize the prioritization order dynamically, and compare them in an evaluation of Java-based applications to existing approaches using the standard APFD metric. The results show that our technique is highly competitive.},
booktitle = {Proceedings of the 13th International Conference on Principles and Practices of Programming on the Java Platform: Virtual Machines, Languages, and Tools},
articleno = {11},
numpages = {12},
keywords = {Execution frequency, Test case prioritization, Regression testing, Empirical study, Fault detection effectiveness},
location = {Lugano, Switzerland},
series = {PPPJ '16}
}

@inproceedings{10.1145/3183440.3194954,
author = {Kim, Misoo and Lee, Eunseok},
title = {Are Information Retrieval-Based Bug Localization Techniques Trustworthy?},
year = {2018},
isbn = {9781450356633},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3183440.3194954},
doi = {10.1145/3183440.3194954},
abstract = {Information retrieval-based bug localization techniques are evaluated using datasets with an oracle. However, datasets can contain non-buggy files, which affect the reliability of these techniques. To investigate the impact of non-buggy files, we show that a test file can be regarded as a buggy file. Then, we determined if this file caused inaccuracies that would eventually affect the trustworthiness of previous techniques. We further analyzed the impact of test files on IR-based bug localization through three research questions. Our results show that the test files significantly impact the performance of the techniques. Furthermore, MAP increased by a maximum of 21%, and MRR decreased by a maximum of 13%.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering: Companion Proceeedings},
pages = {248–249},
numpages = {2},
keywords = {empirical study, bug report, information retrieval-based bug localization, test file, trustworthness},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/3324884.3416571,
author = {Guo, Qianyu and Xie, Xiaofei and Li, Yi and Zhang, Xiaoyu and Liu, Yang and Li, Xiaohong and Shen, Chao},
title = {Audee: Automated Testing for Deep Learning Frameworks},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416571},
doi = {10.1145/3324884.3416571},
abstract = {Deep learning (DL) has been applied widely, and the quality of DL system becomes crucial, especially for safety-critical applications. Existing work mainly focuses on the quality analysis of DL models, but lacks attention to the underlying frameworks on which all DL models depend. In this work, we propose Audee, a novel approach for testing DL frameworks and localizing bugs. Audee adopts a search-based approach and implements three different mutation strategies to generate diverse test cases by exploring combinations of model structures, parameters, weights and inputs. Audee is able to detect three types of bugs: logical bugs, crashes and Not-a-Number (NaN) errors. In particular, for logical bugs, Audee adopts a cross-reference check to detect behavioural inconsistencies across multiple frameworks (e.g., TensorFlow and PyTorch), which may indicate potential bugs in their implementations. For NaN errors, Audee adopts a heuristic-based approach to generate DNNs that tend to output outliers (i.e., too large or small values), and these values are likely to produce NaN. Furthermore, Audee leverages a causal-testing based technique to localize layers as well as parameters that cause inconsistencies or bugs. To evaluate the effectiveness of our approach, we applied Audee on testing four DL frameworks, i.e., TensorFlow, PyTorch, CNTK, and Theano. We generate a large number of DNNs which cover 25 widely-used APIs in the four frameworks. The results demonstrate that Audee is effective in detecting inconsistencies, crashes and NaN errors. In total, 26 unique unknown bugs were discovered, and 7 of them have already been confirmed or fixed by the developers.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {486–498},
numpages = {13},
keywords = {bug detection, deep learning frameworks, deep learning testing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2786805.2786877,
author = {Dhar, Aritra and Purandare, Rahul and Dhawan, Mohan and Rangaswamy, Suresh},
title = {CLOTHO: Saving Programs from Malformed Strings and Incorrect String-Handling},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786877},
doi = {10.1145/2786805.2786877},
abstract = { Software is susceptible to malformed data originating from untrusted sources. Occasionally the programming logic or constructs used are inappropriate to handle the varied constraints imposed by legal and well-formed data. Consequently, softwares may produce unexpected results or even crash. In this paper, we present CLOTHO, a novel hybrid approach that saves such softwares from crashing when failures originate from malformed strings or inappropriate handling of strings. CLOTHO statically analyses a program to identify statements that are vulnerable to failures related to associated string data. CLOTHO then generates patches that are likely to satisfy constraints on the data, and in case of failures produces program behavior which would be close to the expected. The precision of the patches is improved with the help of a dynamic analysis. We have implemented CLOTHO for the JAVA String API, and our evaluation based on several popular open-source libraries shows that CLOTHO generates patches that are semantically similar to the patches generated by the programmers in the later versions. Additionally, these patches are activated only when a failure is detected, and thus CLOTHO incurs no runtime overhead during normal execution, and negligible overhead in case of failures. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {555–566},
numpages = {12},
keywords = {Automatic Program Repair, Program Analysis, Strings},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@article{10.1145/2841425,
author = {Natella, Roberto and Cotroneo, Domenico and Madeira, Henrique S.},
title = {Assessing Dependability with Software Fault Injection: A Survey},
year = {2016},
issue_date = {February 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {48},
number = {3},
issn = {0360-0300},
url = {https://doi.org/10.1145/2841425},
doi = {10.1145/2841425},
abstract = {With the rise of software complexity, software-related accidents represent a significant threat for computer-based systems. Software Fault Injection is a method to anticipate worst-case scenarios caused by faulty software through the deliberate injection of software faults. This survey provides a comprehensive overview of the state of the art on Software Fault Injection to support researchers and practitioners in the selection of the approach that best fits their dependability assessment goals, and it discusses how these approaches have evolved to achieve fault representativeness, efficiency, and usability. The survey includes a description of relevant applications of Software Fault Injection in the context of fault-tolerant systems.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {44},
numpages = {55},
keywords = {Software faults, software fault tolerance, dependability assessment}
}

@inproceedings{10.1145/2786805.2786844,
author = {Karg\'{e}n, Ulf and Shahmehri, Nahid},
title = {Turning Programs against Each Other: High Coverage Fuzz-Testing Using Binary-Code Mutation and Dynamic Slicing},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786844},
doi = {10.1145/2786805.2786844},
abstract = { Mutation-based fuzzing is a popular and widely employed black-box testing technique for finding security and robustness bugs in software. It owes much of its success to its simplicity; a well-formed seed input is mutated, e.g. through random bit-flipping, to produce test inputs. While reducing the need for human effort, and enabling security testing even of closed-source programs with undocumented input formats, the simplicity of mutation-based fuzzing comes at the cost of poor code coverage. Often millions of iterations are needed, and the results are highly dependent on configuration parameters and the choice of seed inputs. In this paper we propose a novel method for automated generation of high-coverage test cases for robustness testing. Our method is based on the observation that, even for closed-source programs with proprietary input formats, an implementation that can generate well-formed inputs to the program is typically available. By systematically mutating the program code of such generating programs, we leverage information about the input format encoded in the generating program to produce high-coverage test inputs, capable of reaching deep states in the program under test. Our method works entirely at the machine-code level, enabling use-cases similar to traditional black-box fuzzing. We have implemented the method in our tool MutaGen, and evaluated it on 7 popular Linux programs. We found that, for most programs, our method improves code coverage by one order of magnitude or more, compared to two well-known mutation-based fuzzers. We also found a total of 8 unique bugs. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {782–792},
numpages = {11},
keywords = {Fuzz testing, program mutation, dynamic slicing, black-box, fuzzing},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3485832.3488028,
author = {Fang, Dongliang and Song, Zhanwei and Guan, Le and Liu, Puzhuo and Peng, Anni and Cheng, Kai and Zheng, Yaowen and Liu, Peng and Zhu, Hongsong and Sun, Limin},
title = {ICS3Fuzzer: A Framework for Discovering Protocol Implementation Bugs in ICS Supervisory Software by Fuzzing},
year = {2021},
isbn = {9781450385794},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3485832.3488028},
doi = {10.1145/3485832.3488028},
abstract = {The supervisory software is widely used in industrial control systems (ICSs) to manage field devices such as PLC controllers. Once compromised, it could be misused to control or manipulate these physical devices maliciously, endangering manufacturing process or even human lives. Therefore, extensive security testing of supervisory software is crucial for the safe operation of ICS. However, fuzzing ICS supervisory software is challenging due to the prevalent use of proprietary protocols. Without the knowledge of the program states and packet formats, it is difficult to enter the deep states for effective fuzzing. In this work, we present a fuzzing framework to automatically discover implementation bugs residing in the communication protocols between the supervisory software and the field devices. To avoid heavy human efforts in reverse-engineering the proprietary protocols, the proposed approach constructs a state-book based on the readily-available execution trace of the supervisory software and the corresponding inputs. Then, we propose a state selection algorithm to find the protocol states that are more likely to have bugs. Our fuzzer distributes more budget on those interesting states. To quickly reach the interesting states, traditional snapshot-based method does not work since the communication protocols are time sensitive. We address this issue by synchronously managing external events (GUI operations and network traffic) during the fuzzing loop. We have implemented a prototype and used it to fuzz the supervisory software of four popular ICS platforms. We have found 13 bugs and received 3 CVEs, 2 are classified as critical (CVSS3.x score CRITICAL 9.8) and affected 40 different products. },
booktitle = {Annual Computer Security Applications Conference},
pages = {849–860},
numpages = {12},
keywords = {Supervisory software, ICS security, fuzzing, protocol implementation bugs, GUI-driven fuzzer},
location = {Virtual Event, USA},
series = {ACSAC}
}

