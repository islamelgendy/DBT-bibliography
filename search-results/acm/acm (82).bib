@inproceedings{10.1145/2667190.2667194,
author = {Dantas, Henrique and Erkin, Zekeriya and Doerr, Christian and Hallie, Raymond and Bij, Gerrit van der},
title = {EFuzz: A Fuzzer for DLMS/COSEM Electricity Meters},
year = {2014},
isbn = {9781450331548},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2667190.2667194},
doi = {10.1145/2667190.2667194},
abstract = {Smart grids enable new functionalities like remote and micro management and consequently, provide increased efficiency, easy management and effectiveness of the entire power grid infrastructure. In order to achieve this, smart meters are attached to the communication network, collecting fine granular data. Unfortunately, as the smart meters are limited devices connected to the network and running software, they also make the whole smart grid more vulnerable than the traditional grids in term of software problems and even possible cyber attacks. In this paper, we work towards an increased software security of smart metering devices and propose a fuzzing framework, eFuzz, built on the generic fuzzing framework Peach to detect software problems. eFuzz tests smart metering devices based on the communication protocol DLMS/COSEM, the standard protocol used in Europe, for possible faults. Our experiments prove the effectiveness of using an automated fuzzing framework compared to resource demanding, human made software protocol inspections. As an example, eFuzz detected between 10 and 40 bugs in different configurations in less than 3 hours while a manual inspection takes weeks. We also investigate the quality of the eFuzz results by comparing with the traditional non-automated evaluation of the same device with respect to scope and efficiency. Our analysis shows that eFuzz is a powerful tool for security inspections for smart meters, and embedded systems in general.},
booktitle = {Proceedings of the 2nd Workshop on Smart Energy Grid Security},
pages = {31–38},
numpages = {8},
keywords = {automated testing, software vulnerabilities, fuzz testing},
location = {Scottsdale, Arizona, USA},
series = {SEGS '14}
}

@inproceedings{10.5555/2819261.2819279,
author = {Su, Fang-Hsiang and Bell, Jonathan and Murphy, Christian and Kaiser, Gail},
title = {Dynamic Inference of Likely Metamorphic Properties to Support Differential Testing},
year = {2015},
publisher = {IEEE Press},
abstract = {Metamorphic testing is an advanced technique to test programs without a true test oracle such as machine learning applications. Because these programs have no general oracle to identify their correctness, traditional testing techniques such as unit testing may not be helpful for developers to detect potential bugs. This paper presents a novel system, Kabu, which can dynamically infer properties of methods' states in programs that describe the characteristics of a method before and after transforming its input. These Metamorphic Properties (MPs) are pivotal to detecting potential bugs in programs without test oracles, but most previous work relies solely on human effort to identify them and only considers MPs between input parameters and output result (return value) of a program or method. This paper also proposes a testing concept, Metamorphic Differential Testing (MDT). By detecting different sets of MPs between different versions for the same method, Kabu reports potential bugs for human review. We have performed a preliminary evaluation of Kabu by comparing the MPs detected by humans with the MPs detected by Kabu. Our preliminary results are promising: Kabu can find more MPs than human developers, and MDT is effective at detecting function changes in methods.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {55–59},
numpages = {5},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.5555/2486788.2486805,
author = {Bounimova, Ella and Godefroid, Patrice and Molnar, David},
title = {Billions and Billions of Constraints: Whitebox Fuzz Testing in Production},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { We report experiences with constraint-based whitebox fuzz testing in production across hundreds of large Windows applications and over 500 machine years of computation from 2007 to 2013. Whitebox fuzzing leverages symbolic execution on binary traces and constraint solving to construct new inputs to a program. These inputs execute previously uncovered paths or trigger security vulnerabilities. Whitebox fuzzing has found one-third of all file fuzzing bugs during the development of Windows 7, saving millions of dollars in potential security vulnerabilities. The technique is in use today across multiple products at Microsoft. We describe key challenges with running whitebox fuzzing in production. We give principles for addressing these challenges and describe two new systems built from these principles: SAGAN, which collects data from every fuzzing run for further analysis, and JobCenter, which controls deployment of our whitebox fuzzing infrastructure across commodity virtual machines. Since June 2010, SAGAN has logged over 3.4 billion constraints solved, millions of symbolic executions, and tens of millions of test cases generated. Our work represents the largest scale deployment of whitebox fuzzing to date, including the largest usage ever for a Satisfiability Modulo Theories (SMT) solver. We present specific data analyses that improved our production use of whitebox fuzzing. Finally we report data on the performance of constraint solving and dynamic test generation that points toward future research problems. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {122–131},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/1723028.1723052,
author = {Allen, Andrew A. and Wu, Yali and Clarke, Peter J. and King, Tariq M. and Deng, Yi},
title = {An Autonomic Framework for User-Centric Communication Services},
year = {2009},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1723028.1723052},
doi = {10.1145/1723028.1723052},
abstract = {The diversity of communication media now available on IP networks presents opportunities to create elaborate collaborative communication applications. However, developing collaborative communication applications can be challenging when using the traditional stovepiped development approach with lengthy development cycle as well as limited utility. One proposed solution to this problem is the Communication Virtual Machine (CVM). CVM uses a user-centric communication (UCC) approach to reduce the complexity while offering operating simplicity to developers and users of collaborative communication services. CVM currently utilizes only one communication framework which limits the number, quality and types of services available.We extend the CVM to support multiple communication frameworks with a policy-driven approach for the selection and configuration of communication services. Users define policies that, through automation, can maintain high level goals by influencing the selection and configuration decisions. In this paper we provide a policy definition for UCC and a technique to evaluate these UCC policies. We also present our autonomic framework for UCC and experimental evaluation of the implementation.},
booktitle = {Proceedings of the 2009 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {203–215},
numpages = {13},
location = {Ontario, Canada},
series = {CASCON '09}
}

@inproceedings{10.1145/1181775.1181790,
author = {Gulavani, Bhargav S. and Henzinger, Thomas A. and Kannan, Yamini and Nori, Aditya V. and Rajamani, Sriram K.},
title = {SYNERGY: A New Algorithm for Property Checking},
year = {2006},
isbn = {1595934685},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1181775.1181790},
doi = {10.1145/1181775.1181790},
abstract = {We consider the problem if a given program satisfies a specified safety property. Interesting programs have infinite state spaces, with inputs ranging over infinite domains, and for these programs the property checking problem is undecidable. Two broad approaches to property checking are testing and verification. Testing tries to find inputs and executions which demonstrate violations of the property. Verification tries to construct a formal proof which shows that all executions of the program satisfy the property. Testing works best when errors are easy to find, but it is often difficult to achieve sufficient coverage for correct programs. On the other hand, verification methods are most successful when proofs are easy to find, but they are often inefficient at discovering errors. We propose a new algorithm, Synergy, which combines testing and verification. Synergy unifies several ideas from the literature, including counterexample-guided model checking, directed testing, and partition refinement.This paper presents a description of the Synergy algorithm, its theoretical properties, a comparison with related algorithms, and a prototype implementation called Yogi.},
booktitle = {Proceedings of the 14th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {117–127},
numpages = {11},
keywords = {software model checking, directed testing, abstraction refinement, testing},
location = {Portland, Oregon, USA},
series = {SIGSOFT '06/FSE-14}
}

@inproceedings{10.1145/2945408.2945411,
author = {Di Nitto, Elisabetta and Jamshidi, Pooyan and Guerriero, Michele and Spais, Ilias and Tamburri, Damian A.},
title = {A Software Architecture Framework for Quality-Aware DevOps},
year = {2016},
isbn = {9781450344111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945408.2945411},
doi = {10.1145/2945408.2945411},
abstract = { DevOps is an emerging software engineering strategy entailing the joined efforts of development and operations people, their concerns and best practices with the purpose of realising a coherent working group for increased software development and operations speed. To allow software architecture practitioners to enrich and properly elaborate their architecture specifications in a manner which is consistent with DevOps, we surveyed a number of DevOps stakeholders. We studied concerns and challenges to be tackled with respect to preparing a software architecture which is DevOps-ready, i.e., described in all details needed to enact DevOps scenarios. Subsequently, we introduce SQUID, that stands for Specification Quality In DevOps. SQUID is a software architecture framework that supports the model-based documentation of software architectures and their quality properties in DevOps scenarios with the goal of providing DevOps-ready software architecture descriptions. We illustrate our framework in a case-study in the Big Data domain. },
booktitle = {Proceedings of the 2nd International Workshop on Quality-Aware DevOps},
pages = {12–17},
numpages = {6},
keywords = {Architecture Frameworks, Model-Driven Design, QoS, QoD},
location = {Saarbr\"{u}cken, Germany},
series = {QUDOS 2016}
}

@inproceedings{10.5555/2819261.2819276,
author = {Padmanabhuni, Bindu Madhavi and Beng Kuan Tan, Hee},
title = {Light-Weight Rule-Based Test Case Generation for Detecting Buffer Overflow Vulnerabilities},
year = {2015},
publisher = {IEEE Press},
abstract = {Buffer overflow exploits form a substantial portion of input manipulation attacks as they are commonly found and are easy to exploit. Despite existence of many detection solutions, buffer overflow bugs are widely being reported in multitude of applications suggesting either inherent limitations in current solutions or problems with their adoption by the end-users. To address this, we propose a novel light-weight rule-based test case generation approach for detecting buffer overflows. The proposed approach uses information collected from static program analysis and pre-defined rules to generate test cases. Since the proposed approach uses only static analysis information and does not involve any constraint solving it is termed as lightweight. Our experimental evaluation on benchmark programs shows that the test inputs generated by the proposed approach are effective in detecting known bugs along with reporting some new bugs.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {48–52},
numpages = {5},
keywords = {test inputs, buffer overflows, control dependency, detection, data, vulnerability, static analysis},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.1145/2568225.2568303,
author = {Gopinath, Divya and Khurshid, Sarfraz and Saha, Diptikalyan and Chandra, Satish},
title = {Data-Guided Repair of Selection Statements},
year = {2014},
isbn = {9781450327565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2568225.2568303},
doi = {10.1145/2568225.2568303},
abstract = { Database-centric programs form the backbone of many enterprise systems. Fixing defects in such programs takes much human effort due to the interplay between imperative code and database-centric logic. This paper presents a novel data-driven approach for automated fixing of bugs in the selection condition of database statements (e.g., WHERE clause of SELECT statements) – a common form of bugs in such programs. Our key observation is that in real-world data, there is information latent in the distribution of data that can be useful to repair selection conditions efficiently. Given a faulty database program and input data, only a part of which induces the defect, our novelty is in determining the correct behavior for the defect-inducing data by taking advantage of the information revealed by the rest of the data. We accomplish this by employing semi-supervised learning to predict the correct behavior for defect-inducing data and by patching up any inaccuracies in the prediction by a SAT-based combinatorial search. Next, we learn a compact decision tree for the correct behavior, including the correct behavior on the defect-inducing data. This tree suggests a plausible fix to the selection condition. We demonstrate the feasibility of our approach on seven realworld examples. },
booktitle = {Proceedings of the 36th International Conference on Software Engineering},
pages = {243–253},
numpages = {11},
keywords = {Program Repair, Support Vector Machines, ABAP, Machine Learning, data-centric programs, SAT, Databases},
location = {Hyderabad, India},
series = {ICSE 2014}
}

@inproceedings{10.1145/2103380.2103383,
author = {Choi, Yongsuk and Choi, Jongmoo and Ha, Eunyong},
title = {RTRP: Right Time Right Place Kernel Analysis Tool},
year = {2011},
isbn = {9781450310871},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2103380.2103383},
doi = {10.1145/2103380.2103383},
abstract = {As the use of embedded Linux kernel increases in a variety of consumer electronics such as Smart phones and Internet TVs, the significance of effective kernel profiling and debugging tools also increases. In this paper, we propose a novel integrated kernel analysis tool. It supports two features, right time and right place feature, thus we call it as RTRP (Right Time Right Place) tool. The right time feature is supported by monitoring system performance on-line and by issuing a debugging trigger automatically at the time when the target system shows performance anomalies or misbehaviors. It is a kind of semantic-aware debugging based on the context of Linux kernel. In addition, the tool supports the right place feature by providing proper system information including the stack frame and call graphs that causes the problems, allowing developers to focus on solving the problems without being deviated by irrelevant information. Experimental results, conducted on a real embedded system equipped with a live image compression facility, have shown that the tool can notify appropriate information to identify the location of a device driver that causes overrun problems.},
booktitle = {Proceedings of the 2011 ACM Symposium on Research in Applied Computation},
pages = {7–12},
numpages = {6},
keywords = {analysis tool, instrumentation, embedded Linux},
location = {Miami, Florida},
series = {RACS '11}
}

@article{10.1145/3503509,
author = {Yang, Yanming and Xia, Xin and Lo, David and Bi, Tingting and Grundy, John and Yang, Xiaohu},
title = {Predictive Models in Software Engineering: Challenges and Opportunities},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3503509},
doi = {10.1145/3503509},
abstract = {Predictive models are one of the most important techniques that are widely applied in many areas of software engineering. There have been a large number of primary studies that apply predictive models and that present well-performed studies in various research domains, including software requirements, software design and development, testing and debugging, and software maintenance. This article is a first attempt to systematically organize knowledge in this area by surveying a body of 421 papers on predictive models published between 2009 and 2020. We describe the key models and approaches used, classify the different models, summarize the range of key application areas, and analyze research results. Based on our findings, we also propose a set of current challenges that still need to be addressed in future work and provide a proposed research road map for these opportunities.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {56},
numpages = {72},
keywords = {machine learning, Predictive models, survey, software engineering, deep learning}
}

@inproceedings{10.1145/3303084.3309494,
author = {Nishi, Masataka},
title = {Process Barrier for Predictable and Repeatable Concurrent Execution},
year = {2019},
isbn = {9781450362900},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3303084.3309494},
doi = {10.1145/3303084.3309494},
abstract = {We study on how to design, debug and verify and validate (V&amp;V) safety-critical control software running on shared-memory many-core platforms. Managing concurrency in a verifiable way is a certification requirement. The presented process barrier is a simple concurrency control mechanism that guarantees deadlock-freedom by-design and temporal separation of tasks, while allowing non-conflicting tasks to run in parallel. It is placed in a lock-free task queue (LFTQ) and a group of processors are allocated to compete to dequeue and execute the tasks registered in the LFTQ. The process barrier consists of a checker and limiter pair. A process that dequeues the checker monitors for completion of preceding tasks in the LFTQ that conflicts with a subsequent task in the LFTQ. The process dequeues the paired limiter from the LFTQ upon completion. All other processes that find the limiter at the head of the LFTQ periodically checks if the head of the LFTQ points to subsequent tasks which happens after the process that took the checker task dequeues the limiter. The mechanism manages concurrent execution of the registered tasks that conflict on data, shared resources and execution order in a way that becomes conflict equivalent to sequential execution. The trace of the concurrent execution and the consequent program state is repeatable. We can reuse existing toolchains for single-core platforms for debugging, testing and V&amp;V. The temporal behavior of the concurrent execution becomes predictable and the worst-case execution time (WCET) of it is bounded.},
booktitle = {Proceedings of the 10th International Workshop on Programming Models and Applications for Multicores and Manycores},
pages = {71–80},
numpages = {10},
keywords = {many-core, WCET, design portability, predictability, concurrency control},
location = {Washington, DC, USA},
series = {PMAM'19}
}

@article{10.1145/2018396.2018415,
author = {Kennedy, Ken and Koelbel, Charles and Zima, Hans},
title = {The Rise and Fall of High Performance Fortran},
year = {2011},
issue_date = {November 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {11},
issn = {0001-0782},
url = {https://doi.org/10.1145/2018396.2018415},
doi = {10.1145/2018396.2018415},
abstract = {HPF pioneered a high-level approach to parallel programming but failed to win over a broad user community.},
journal = {Commun. ACM},
month = {nov},
pages = {74–82},
numpages = {9}
}

@inproceedings{10.1145/2668332.2668337,
author = {Brouwers, Niels and Zuniga, Marco and Langendoen, Koen},
title = {NEAT: A Novel Energy Analysis Toolkit for Free-Roaming Smartphones},
year = {2014},
isbn = {9781450331432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2668332.2668337},
doi = {10.1145/2668332.2668337},
abstract = {Analyzing the power consumption of smartphones is difficult because of the complex interplay between soft- and hardware. Currently, researchers rely on mainly two options: external measurement tools, which are precise but constrain the mobility of the device and require the annotation of power traces; or modelling methods, which allow mobility and consider explicitly the state of events, but have less accuracy and lower sampling rates than external tools.We address the challenges of mobile power analysis with a novel power metering toolkit, called NEAT, which comprises a coin-sized power measurement board that fits inside a typical smartphone, and analysis software that automatically fuses the event logs taken from the phone with the obtained power trace. The combination of high-fidelity power measurements and detailed information about the state of the phone's hardware and software components allows for fine-grained analysis of complex and short-lived energy patterns.We equipped smartphones with NEAT and conducted various experiments to highlight (i) its accuracy with respect to model-based approaches, showing errors upwards of 20%; (ii) its ability to gather accurate and well annotated user-data "in the wild", which would be hard to do with current external meters; and (iii) the importance of having fine-granular and expressive traces by resolving kernel energy bugs.},
booktitle = {Proceedings of the 12th ACM Conference on Embedded Network Sensor Systems},
pages = {16–30},
numpages = {15},
keywords = {mobility, trace visualization, accuracy, power monitor},
location = {Memphis, Tennessee},
series = {SenSys '14}
}

@inproceedings{10.5555/3213187.3213191,
author = {Bosmans, Stig and Mercelis, Siegfried and Hellinckx, Peter and Denil, Joachim},
title = {Towards Evaluating Emergent Behavior of the Internet of Things Using Large Scale Simulation Techniques (Wip)},
year = {2018},
isbn = {9781510860209},
publisher = {Society for Computer Simulation International},
address = {San Diego, CA, USA},
abstract = {With the increase in Internet of Things devices and more decentralized architectures we see a new type of application gain importance, a type where local interactions between individual entities lead to a global emergent behavior, Emergent-based IoT (EBI) Systems. In this position paper we explore techniques to evaluate this emergent behavior in IoT applications. Because of the required scale and diversity this is not an easy task. Therefore, we mainly focus on a distributed simulation approach and provide an overview of possible techniques that could optimize the overall simulation performance. Our focus is both on modeling and simulation technology.},
booktitle = {Proceedings of the Theory of Modeling and Simulation Symposium},
pages = {1–8},
numpages = {8},
keywords = {emergent behavior, internet of things, parallel and distributed simulation, scalability},
location = {Baltimore, Maryland},
series = {TMS '18}
}

@inproceedings{10.1145/3213187.3213191,
author = {Bosmans, Stig and Mercelis, Siegfried and Hellinckx, Peter and Denil, Joachim},
title = {Towards Evaluating Emergent Behavior of the Internet of Things Using Large Scale Simulation Techniques (Wip)},
year = {2018},
isbn = {9781450364478},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3213187.3213191},
doi = {10.1145/3213187.3213191},
abstract = {With the increase in Internet of Things devices and more decentralized architectures we see a new type of application gain importance, a type where local interactions between individual entities lead to a global emergent behavior, Emergent-based IoT (EBI) Systems. In this position paper we explore techniques to evaluate this emergent behavior in IoT applications. Because of the required scale and diversity this is not an easy task. Therefore, we mainly focus on a distributed simulation approach and provide an overview of possible techniques that could optimize the overall simulation performance. Our focus is both on modeling and simulation technology.},
booktitle = {Proceedings of the 4th ACM International Conference of Computing for Engineering and Sciences},
articleno = {4},
numpages = {8},
keywords = {internet of things, parallel and distributed simulation, scalability, emergent behavior},
location = {Kuala Lumpur, Malaysia},
series = {ICCES'18}
}

@inproceedings{10.1145/3106237.3106253,
author = {Long, Fan and Amidon, Peter and Rinard, Martin},
title = {Automatic Inference of Code Transforms for Patch Generation},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3106253},
doi = {10.1145/3106237.3106253},
abstract = { We present a new system, Genesis, that processes human patches to automatically infer code transforms for automatic patch generation. We present results that characterize the effectiveness of the Genesis inference algorithms and the complete Genesis patch generation system working with real-world patches and defects collected from 372 Java projects. To the best of our knowledge, Genesis is the first system to automatically infer patch generation transforms or candidate patch search spaces from previous successful patches. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {727–739},
numpages = {13},
keywords = {Search space inference, Patch Generation, Code transform},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/2786805.2803193,
author = {V\"{o}st, Sebastian},
title = {Vehicle Level Continuous Integration in the Automotive Industry},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2803193},
doi = {10.1145/2786805.2803193},
abstract = { Embedded systems are omnipresent in the modern world. This naturally includes the automobile industry, where electronic functions are becoming prevalent. In the automotive domain, embedded systems today are highly distributed systems and manufactured in great numbers and variance. To ensure correct functionality, systematic integration and testing on the system level is key. In software engineering, continuous integration has been used with great success. In the automotive industry though, system tests are still performed in a big-bang integration style, which makes tracing and fixing errors very expensive and time-consuming. Thus, I want to investigate whether and how continuous integration can be applied to the automotive industry on the system level. Doing so, I present an adapted process of Continuous Integration including methods for test case specification and selection. I will apply this process as a pilot project in a production environment at BMW and evaluate the effectiveness by gathering both qualitative and quantitative data. From the gained experience, I will derive possible improvements to the process for future implementations and requirements on test hardware used for Continuous Integration. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {1026–1029},
numpages = {4},
keywords = {Automotive, testing, embedded, continuous integration},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.1145/3377811.3380370,
author = {Menghi, Claudio and Nejati, Shiva and Briand, Lionel and Parache, Yago Isasi},
title = {Approximation-Refinement Testing of Compute-Intensive Cyber-Physical Models: An Approach Based on System Identification},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380370},
doi = {10.1145/3377811.3380370},
abstract = {Black-box testing has been extensively applied to test models of Cyber-Physical systems (CPS) since these models are not often amenable to static and symbolic testing and verification. Black-box testing, however, requires to execute the model under test for a large number of candidate test inputs. This poses a challenge for a large and practically-important category of CPS models, known as compute-intensive CPS (CI-CPS) models, where a single simulation may take hours to complete. We propose a novel approach, namely ARIsTEO, to enable effective and efficient testing of CI-CPS models. Our approach embeds black-box testing into an iterative approximation-refinement loop. At the start, some sampled inputs and outputs of the CI-CPS model under test are used to generate a surrogate model that is faster to execute and can be subjected to black-box testing. Any failure-revealing test identified for the surrogate model is checked on the original model. If spurious, the test results are used to refine the surrogate model to be tested again. Otherwise, the test reveals a valid failure. We evaluated ARIsTEO by comparing it with S-Taliro, an open-source and industry-strength tool for testing CPS models. Our results, obtained based on five publicly-available CPS models, show that, on average, ARIsTEO is able to find 24% more requirements violations than S-Taliro and is 31% faster than S-Taliro in finding those violations. We further assessed the effectiveness and efficiency of ARIsTEO on a large industrial case study from the satellite domain. In contrast to S-Taliro, ARIsTEO successfully tested two different versions of this model and could identify three requirements violations, requiring four hours, on average, for each violation.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {372–384},
numpages = {13},
keywords = {falsification, robustness, model testing, cyber-physical systems, search-based testing},
location = {Seoul, South Korea},
series = {ICSE '20}
}

@inproceedings{10.1145/568760.568763,
author = {Pedrycz, Witold},
title = {Computational Intelligence as an Emerging Paradigm of Software Engineering},
year = {2002},
isbn = {1581135564},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/568760.568763},
doi = {10.1145/568760.568763},
abstract = {Software Engineering is inherently knowledge intensive. Software processes and products are human centered. The technology of Computational Intelligence (CI) intensively exploits various mechanisms of interaction with humans and processes domain knowledge with intent of building intelligent systems. As commonly perceived, CI dwells on three highly synergistic technologies of neural networks, fuzzy sets (or granular computing, in general) and evolutionary optimization. As the software complexity grows and the diversity of software systems skyrocket, it becomes apparent that there is a genuine need for a solid, efficient, designer-oriented vehicle to support software analysis, design, and implementation at various levels. The research agenda makes CI a highly compatible and appealing vehicle to address the needs of knowledge rich environment of Software Engineering. The objective of this study is to identify and discuss synergistic links emerging between Software Engineering and Computational Intelligence. We show how CI --- based models contribute to the methodology of constructing models of software processes and products. Several selected examples (including software cost estimation, quality, and software measures) are included.},
booktitle = {Proceedings of the 14th International Conference on Software Engineering and Knowledge Engineering},
pages = {7–14},
numpages = {8},
keywords = {uncertainty representation, data visualization, software quality, neural networks, genetic optimization, computational intelligence, synergy, granular computing},
location = {Ischia, Italy},
series = {SEKE '02}
}

@inproceedings{10.1145/237091.237111,
author = {Vander Zanden, Bradley T. and Venckus, Scott A.},
title = {An Empirical Study of Constraint Usage in Graphical Applications},
year = {1996},
isbn = {0897917987},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/237091.237111},
doi = {10.1145/237091.237111},
booktitle = {Proceedings of the 9th Annual ACM Symposium on User Interface Software and Technology},
pages = {137–146},
numpages = {10},
keywords = {debugging, profiling, one-way constraints, graphical applications, toolkits, optimization},
location = {Seattle, Washington, USA},
series = {UIST '96}
}

