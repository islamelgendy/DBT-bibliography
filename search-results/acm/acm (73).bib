@inbook{10.1145/3173162.3177153,
author = {Devecsery, David and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {Optimistic Hybrid Analysis: Accelerating Dynamic Analysis through Predicated Static Analysis},
year = {2018},
isbn = {9781450349116},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173162.3177153},
abstract = {Dynamic analysis tools, such as those that detect data-races, verify memory safety, and identify information flow, have become a vital part of testing and debugging complex software systems. While these tools are powerful, their slow speed often limits how effectively they can be deployed in practice. Hybrid analysis speeds up these tools by using static analysis to decrease the work performed during dynamic analysis. In this paper we argue that current hybrid analysis is needlessly hampered by an incorrect assumption that preserving the soundness of dynamic analysis requires an underlying sound static analysis. We observe that, even with unsound static analysis, it is possible to achieve sound dynamic analysis for the executions which fall within the set of states statically considered. This leads us to a new approach, called optimistic hybrid analysis. We first profile a small set of executions and generate a set of likely invariants that hold true during most, but not necessarily all, executions. Next, we apply a much more precise, but unsound, static analysis that assumes these invariants hold true. Finally, we run the resulting dynamic analysis speculatively while verifying whether the assumed invariants hold true during that particular execution; if not, the program is reexecuted with a traditional hybrid analysis. Optimistic hybrid analysis is as precise and sound as traditional dynamic analysis, but is typically much faster because (1) unsound static analysis can speed up dynamic analysis much more than sound static analysis can and (2) verifications rarely fail. We apply optimistic hybrid analysis to race detection and program slicing and achieve 1.8x over a state-of-the-art race detector (FastTrack) optimized with traditional hybrid analysis and 8.3x over a hybrid backward slicer (Giri).},
booktitle = {Proceedings of the Twenty-Third International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {348–362},
numpages = {15}
}

@article{10.1145/3296957.3177153,
author = {Devecsery, David and Chen, Peter M. and Flinn, Jason and Narayanasamy, Satish},
title = {Optimistic Hybrid Analysis: Accelerating Dynamic Analysis through Predicated Static Analysis},
year = {2018},
issue_date = {February 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {2},
issn = {0362-1340},
url = {https://doi.org/10.1145/3296957.3177153},
doi = {10.1145/3296957.3177153},
abstract = {Dynamic analysis tools, such as those that detect data-races, verify memory safety, and identify information flow, have become a vital part of testing and debugging complex software systems. While these tools are powerful, their slow speed often limits how effectively they can be deployed in practice. Hybrid analysis speeds up these tools by using static analysis to decrease the work performed during dynamic analysis. In this paper we argue that current hybrid analysis is needlessly hampered by an incorrect assumption that preserving the soundness of dynamic analysis requires an underlying sound static analysis. We observe that, even with unsound static analysis, it is possible to achieve sound dynamic analysis for the executions which fall within the set of states statically considered. This leads us to a new approach, called optimistic hybrid analysis. We first profile a small set of executions and generate a set of likely invariants that hold true during most, but not necessarily all, executions. Next, we apply a much more precise, but unsound, static analysis that assumes these invariants hold true. Finally, we run the resulting dynamic analysis speculatively while verifying whether the assumed invariants hold true during that particular execution; if not, the program is reexecuted with a traditional hybrid analysis. Optimistic hybrid analysis is as precise and sound as traditional dynamic analysis, but is typically much faster because (1) unsound static analysis can speed up dynamic analysis much more than sound static analysis can and (2) verifications rarely fail. We apply optimistic hybrid analysis to race detection and program slicing and achieve 1.8x over a state-of-the-art race detector (FastTrack) optimized with traditional hybrid analysis and 8.3x over a hybrid backward slicer (Giri).},
journal = {SIGPLAN Not.},
month = {mar},
pages = {348–362},
numpages = {15},
keywords = {program slicing, data-race detection, debugging, speculative execution, static analysis, dynamic analysis}
}

@article{10.1145/1764810.1764827,
author = {Nair, T.R. Gopalakrishnan and Suma, V.},
title = {A Paradigm for Metric Based Inspection Process for Enhancing Defect Management},
year = {2010},
issue_date = {May 2010},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {3},
issn = {0163-5948},
url = {https://doi.org/10.1145/1764810.1764827},
doi = {10.1145/1764810.1764827},
abstract = {Inspection process in software development plays a vital role in effective defect management. In order to have an appropriate measurement of the inspection process, we depend on a process metric called the Depth of Inspection (DI). DI enables the manager within the software community to identify and compare the level of inspection performed in various projects. An empirical study of several projects facilitated the evaluation of a set of process coefficients which are capable of predicting the DI values using multiple regression models. The industry observed DI value based on defect count and the DI value produced by the model are strongly matching. This supports the predictive capability of DI through process coefficients without depending on the prior estimation of the defect count.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1},
numpages = {6},
keywords = {software inspection, software defect management, software quality metrics, software process}
}

@inproceedings{10.1145/949344.949407,
author = {Fraser, Steven and Astels, Dave and Beck, Kent and Boehm, Barry and McGregor, John and Newkirk, James and Poole, Charlie},
title = {Discipline and Practices of TDD: (Test Driven Development)},
year = {2003},
isbn = {1581137516},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/949344.949407},
doi = {10.1145/949344.949407},
abstract = {This panel brings together practitioners with experience in Agile and XP methodologies to discuss the approaches and benefits of applying Test Driven Development (TDD). The goal of TDD is clean code that works. The mantra of TDD is: write a test; make it run; and make it right. Open questions to be addressed by the panel include: - How are TDD approaches to be applied to databases, GUIs, and distributed systems? What are the quantitative benchmarks that can demonstrate the value of TDD, and what are the best approaches to solve the ubiquitous issue of scalability.},
booktitle = {Companion of the 18th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {268–270},
numpages = {3},
keywords = {TDD, test-driven design, inspections, XP practices, refactor},
location = {Anaheim, CA, USA},
series = {OOPSLA '03}
}

@inproceedings{10.5555/776816.776948,
author = {de Lemos, Rog\'{e}rio and Gacek, Cristina and Romanovsky, Alexander},
title = {ICSE 2003 Workshop on Software Architectures for Dependable Systems},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This workshop summary gives a brief overview of a one-day workshop on "Software Architectures for Dependable Systems" held in conjunctions with ICSE 2003.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {781–782},
numpages = {2},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.5555/1035053.1035067,
author = {Renaud, Karen and Gray, Phil},
title = {Making Sense of Low-Level Usage Data to Understand User Activities},
year = {2004},
publisher = {South African Institute for Computer Scientists and Information Technologists},
address = {ZAF},
abstract = {Empirical studies of user activity, based on data collected from the systems with which users interact, present technical challenges related to the transformation of data streams to a form suitable for analysis. In this paper we discuss the particular challenges confronted during a study of user interruption behaviour based on low-level "keystroke" data and the ways in which these challenges were addressed. We also report on a method of data cleaning and analytical preparation that was developed and consider its effectiveness and potential applicability for similar studies.},
booktitle = {Proceedings of the 2004 Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists on IT Research in Developing Countries},
pages = {115–124},
numpages = {10},
keywords = {interruptions, flexibility, resumption lag activity, user activities, filtering, usage data, low-level data, transformation},
location = {Stellenbosch, Western Cape, South Africa},
series = {SAICSIT '04}
}

@inproceedings{10.1145/2701126.2701207,
author = {Kim, Jeongho and Park, Jonghee and Lee, Eunseok},
title = {A New Hybrid Algorithm for Software Fault Localization},
year = {2015},
isbn = {9781450333771},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2701126.2701207},
doi = {10.1145/2701126.2701207},
abstract = {We previously presented a spectrum-based fault localization (SFL) technique, which we named Hybrid, that localizes a bug by using the program hit spectra and test results. We also proposed a distinct mechanism for test data that enables the SFL algorithms to localize fault in a more precise manner than what would be possible with the original test data. However, there was a limitation of the Hybrid algorithm. In that the technique only showed better performance when using distinct test data. Therefore, in the current work, we improve over Hybrid by analyzing more than 30 types of existing algorithms. After choosing the appropriate algorithms, we adopted their specific strengths through experimentation. Finally, we developed the novel Combination Algorithms (CAL). In our experimental study, we used the Siemens test program to confirm that our technique was more precise than the state-of-the-art SFL algorithm D Star and Heuristic III for both original and distinct test data. In particular, our technique localizes a fault under 2 percent on average as well as decreases the coverage of the reading code by a third of the source code.},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication},
articleno = {50},
numpages = {8},
keywords = {fault localization, spectrum based fault localization, execution trace, program debugging, suspicious code},
location = {Bali, Indonesia},
series = {IMCOM '15}
}

@inproceedings{10.1145/3382025.3414967,
author = {Lima, Jackson A. Prado and Mendon\c{c}a, Willian D. F. and Vergilio, Silvia R. and Assun\c{c}\~{a}o, Wesley K. G.},
title = {Learning-Based Prioritization of Test Cases in Continuous Integration of Highly-Configurable Software},
year = {2020},
isbn = {9781450375696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382025.3414967},
doi = {10.1145/3382025.3414967},
abstract = {Continuous Integration (CI) is a practice widely adopted in the industry to allow frequent integration of code changes. During the CI process, many test cases are executed multiple times a day, subject to time constraints. In this scenario, a learning-based approach, named COLEMAN, has been successfully applied. COLEMAN allows earlier execution of the most promising test cases to reveal faults. This approach considers CI particularities such as time budget and volatility of test cases, related to the fact that test cases can be added/removed along the CI cycles. In the CI of Highly Configuration System (HCS), many product variants must be tested, each one with different configuration options, but having test cases that are common to or reused from other variants. In this context, we found, by analogy, another particularity, the volatility of variants, that is, some variants can be included/discontinued along CI cycles. Considering this context, this work introduces two strategies for the application of COLEMAN in the CI of HCS: the Variant Test Set Strategy (VTS) that relies on the test set specific for each variant, and the Whole Test Set Strategy (WST) that prioritizes the test set composed by the union of the test cases of all variants. Both strategies are evaluated in a real-world HCS, considering three test budgets. The results show that the proposed strategies are applicable regarding the time spent for prioritization. They perform similarly regarding early fault detection, but WTS better mitigates the problem of beginning without knowledge, and is more suitable when a new variant to be tested is added.},
booktitle = {Proceedings of the 24th ACM Conference on Systems and Software Product Line: Volume A - Volume A},
articleno = {31},
numpages = {11},
keywords = {family of products, software product line, test case prioritization, continuous integration},
location = {Montreal, Quebec, Canada},
series = {SPLC '20}
}

@inproceedings{10.1145/2851553.2851571,
author = {Sandoval Alcocer, Juan Pablo and Bergel, Alexandre and Valente, Marco Tulio},
title = {Learning from Source Code History to Identify Performance Failures},
year = {2016},
isbn = {9781450340809},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2851553.2851571},
doi = {10.1145/2851553.2851571},
abstract = {Source code changes may inadvertently introduce performance regressions. Benchmarking each software version is traditionally employed to identify performance regressions. Although effective, this exhaustive approach is hard to carry out in practice. This paper contrasts source code changes against performance variations. By analyzing 1,288 software versions from 17 open source projects, we identified 10 source code changes leading to a performance variation (improvement or regression). We have produced a cost model to infer whether a software commit introduces a performance variation by analyzing the source code and sampling the execution of a few versions. By profiling the execution of only 17% of the versions, our model is able to identify 83% of the performance regressions greater than 5% and 100% of the regressions greater than 50%.},
booktitle = {Proceedings of the 7th ACM/SPEC on International Conference on Performance Engineering},
pages = {37–48},
numpages = {12},
keywords = {performance evolution, performance variation, performance analysis},
location = {Delft, The Netherlands},
series = {ICPE '16}
}

@inproceedings{10.1109/MSR.2007.13,
author = {Weiss, Cathrin and Premraj, Rahul and Zimmermann, Thomas and Zeller, Andreas},
title = {How Long Will It Take to Fix This Bug?},
year = {2007},
isbn = {076952950X},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/MSR.2007.13},
doi = {10.1109/MSR.2007.13},
abstract = {Predicting the time and effort for a software problem has long been a difficult task. We present an approach that automatically predicts the fixing effort, i.e., the person-hours spent on fixing an issue. Our technique leverages existing issue tracking systems: given a new issue report, we use the Lucene framework to search for similar, earlier reports and use their average time as a prediction. Our approach thus allows for early effort estimation, helping in assigning issues and scheduling stable releases. We evaluated our approach using effort data from the JBoss project. Given a sufficient number of issues reports, our automatic predictions are close to the actual effort; for issues that are bugs, we are off by only one hour, beating na\"{\i}ve predictions by a factor of four.},
booktitle = {Proceedings of the Fourth International Workshop on Mining Software Repositories},
pages = {1},
series = {MSR '07}
}

@inproceedings{10.1145/3520312.3534868,
author = {Dolby, Julian and Tsay, Jason and Hirzel, Martin},
title = {Automatically Debugging AutoML Pipelines Using Maro: ML Automated Remediation Oracle},
year = {2022},
isbn = {9781450392730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3520312.3534868},
doi = {10.1145/3520312.3534868},
abstract = {Machine learning in practice often involves complex pipelines for data cleansing, feature engineering, preprocessing, and prediction. These pipelines are composed of operators, which have to be correctly connected and whose hyperparameters must be correctly configured. Unfortunately, it is quite common for certain combinations of datasets, operators, or hyperparameters to cause failures. Diagnosing and fixing those failures is tedious and error-prone and can seriously derail a data scientist's workflow. This paper describes an approach for automatically debugging an ML pipeline, explaining the failures, and producing a remediation. We implemented our approach, which builds on a combination of AutoML  
and SMT, in a tool called Maro. Maro works seamlessly with the familiar data science ecosystem including Python, Jupyter notebooks, scikit-learn, and AutoML tools such as Hyperopt. We empirically evaluate our tool and find that for most cases, a single remediation automatically fixes errors, produces no additional faults, and does not significantly impact optimal accuracy nor time to convergence.},
booktitle = {Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming},
pages = {60–69},
numpages = {10},
keywords = {Automated Remediation, AutoML, AI Debugging, Automated Debugging},
location = {San Diego, CA, USA},
series = {MAPS 2022}
}

@inproceedings{10.1109/AST.2007.5,
author = {Nilsson, Robert and Offutt, Jeff},
title = {Automated Testing of Timeliness: A Case Study},
year = {2007},
isbn = {0769529712},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/AST.2007.5},
doi = {10.1109/AST.2007.5},
abstract = {A problem with testing timeliness of real-time applications is the response-time dependency on the execution order of concurrent tasks. Conventional test methods ignore task interleaving and timing and thus do not help determine which execution orders need to be exercised to test temporal correctness. Model based mutation testing has been proposed to generate inputs and determine the execution orders that need to be verified to increase confidence in timeliness. This paper evaluate a mutation-based framework for automated testing of timeliness by applying it on a small control system running on Linux/RTAI. The experiments presented in this paper indicate that mutation-based test cases are more effective than random and stress tests in finding both naturally occurring and randomly seeded timeliness faults.},
booktitle = {Proceedings of the Second International Workshop on Automation of Software Test},
pages = {11},
series = {AST '07}
}

@inproceedings{10.1145/3338906.3340456,
author = {Babi\'{c}, Domagoj and Bucur, Stefan and Chen, Yaohui and Ivan\v{c}i\'{c}, Franjo and King, Tim and Kusano, Markus and Lemieux, Caroline and Szekeres, L\'{a}szl\'{o} and Wang, Wei},
title = {FUDGE: Fuzz Driver Generation at Scale},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340456},
doi = {10.1145/3338906.3340456},
abstract = {At Google we have found tens of thousands of security and robustness bugs by fuzzing C and C++ libraries. To fuzz a library, a fuzzer requires a fuzz driver—which exercises some library code—to which it can pass inputs. Unfortunately, writing fuzz drivers remains a primarily manual exercise, a major hindrance to the widespread adoption of fuzzing. In this paper, we address this major hindrance by introducing the Fudge system for automated fuzz driver generation. Fudge automatically generates fuzz driver candidates for libraries based on existing client code. We have used Fudge to generate thousands of new drivers for a wide variety of libraries. Each generated driver includes a synthesized C/C++ program and a corresponding build script, and is automatically analyzed for quality. Developers have integrated over 200 of these generated drivers into continuous fuzzing services and have committed to address reported security bugs. Further, several of these fuzz drivers have been upstreamed to open source projects and integrated into the OSS-Fuzz fuzzing infrastructure. Running these fuzz drivers has resulted in over 150 bug fixes, including the elimination of numerous exploitable security vulnerabilities.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {975–985},
numpages = {11},
keywords = {fuzzing, fuzz testing, software security, program slicing, automated test generation, testing, code synthesis},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/68210.69217,
author = {Bates, Peter},
title = {Debugging Heterogeneous Distributed Systems Using Event-Based Models of Behavior},
year = {1988},
isbn = {0897912969},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/68210.69217},
doi = {10.1145/68210.69217},
abstract = {Event Based Behavioral Abstraction (EBBA) is a high-level debugging approach which treats debugging as a process of creating models of actual behavior from the activity of the system and comparing these to models of expected system behavior. The differences between the actual and expected models are used to characterize erroneous system behavior and direct further investigation.A set of EBBA-based tools has been implemented that users can employ to construct libraries of behavior models and investigate the behavior of an errorful system through these models. EBBA evolves naturally as a cooperative distributed program that can take better advantage of computational power available in a network computer system to enhance debugging tool transparency, reduce latency and uncertainty for fundamental debugging activities and accommodate diverse, heterogeneous architectures.},
booktitle = {Proceedings of the 1988 ACM SIGPLAN and SIGOPS Workshop on Parallel and Distributed Debugging},
pages = {11–22},
numpages = {12},
location = {Madison, Wisconsin, USA},
series = {PADD '88}
}

@article{10.1145/69215.69217,
author = {Bates, Peter},
title = {Debugging Heterogeneous Distributed Systems Using Event-Based Models of Behavior},
year = {1988},
issue_date = {Jan. 1989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {24},
number = {1},
issn = {0362-1340},
url = {https://doi.org/10.1145/69215.69217},
doi = {10.1145/69215.69217},
abstract = {Event Based Behavioral Abstraction (EBBA) is a high-level debugging approach which treats debugging as a process of creating models of actual behavior from the activity of the system and comparing these to models of expected system behavior. The differences between the actual and expected models are used to characterize erroneous system behavior and direct further investigation.A set of EBBA-based tools has been implemented that users can employ to construct libraries of behavior models and investigate the behavior of an errorful system through these models. EBBA evolves naturally as a cooperative distributed program that can take better advantage of computational power available in a network computer system to enhance debugging tool transparency, reduce latency and uncertainty for fundamental debugging activities and accommodate diverse, heterogeneous architectures.},
journal = {SIGPLAN Not.},
month = {nov},
pages = {11–22},
numpages = {12}
}

@inproceedings{10.1145/1960314.1960321,
author = {Silva Filho, Roberto Silveira and Bronsard, Fran\c{c}ois and Hasling, William M.},
title = {Experiences Documenting and Preserving Software Constraints Using Aspects},
year = {2011},
isbn = {9781450306065},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1960314.1960321},
doi = {10.1145/1960314.1960321},
abstract = {Software systems are increasingly being built as compositions of reusable artifacts (components, frameworks, toolkits, plug-ins, APIs, etc) that have non-trivial usage constraints in the form of interface contracts, underlying assumptions and design composition rules. Satisfying these constraints is challenging: they are often not well documented; or they are difficult to integrate into the software development process in ways that allow their identification by developers; or they may not be enforced by existing tools and development environments. Aspect-Oriented Programming has been advocated as an approach to represent and enforce software constraints in code artifacts. Aspects can be used to detect constraint violations, or more pro-actively, to ensure that the constraints are satisfied without requiring the developer's attention. This paper discusses our experience using aspects to document and enforce software constraints in an industrial application, specifically TDE/UML, a model-driven software testing tool developed at SIEMENS. We present an analysis of common constraints found in our case study, a set of primitive aspects developed to help the enforcement of software constraints, and show how AOP has been incorporated into existing software development and governance approaches in the TDE/UML project. We conclude with a discussion of strengths and limitations of AspectJ in supporting these constraints.},
booktitle = {Proceedings of the Tenth International Conference on Aspect-Oriented Software Development Companion},
pages = {7–18},
numpages = {12},
keywords = {software architecture, architectural constraints, design documentation, aspect-oriented programming},
location = {Porto de Galinhas, Brazil},
series = {AOSD '11}
}

@article{10.1145/1276301.1276305,
author = {Hoag, Joseph E. and Thompson, Craig W.},
title = {A Parallel General-Purpose Synthetic Data Generator},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {36},
number = {1},
issn = {0163-5808},
url = {https://doi.org/10.1145/1276301.1276305},
doi = {10.1145/1276301.1276305},
abstract = {PSDG is a parallel synthetic data generator designed to generate "industrial sized" data sets quickly using cluster computing. PSDG depends on SDDL, a synthetic data description language that provides flexibility in the types of data we can generate.},
journal = {SIGMOD Rec.},
month = {mar},
pages = {19–24},
numpages = {6}
}

@inproceedings{10.1145/1753326.1753555,
author = {Chang, Tsung-Hsiang and Yeh, Tom and Miller, Robert C.},
title = {GUI Testing Using Computer Vision},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753555},
doi = {10.1145/1753326.1753555},
abstract = {Testing a GUI's visual behavior typically requires human testers to interact with the GUI and to observe whether the expected results of interaction are presented. This paper presents a new approach to GUI testing using computer vision for testers to automate their tasks. Testers can write a visual test script that uses images to specify which GUI components to interact with and what visual feedback to be observed. Testers can also generate visual test scripts by demonstration. By recording both input events and screen images, it is possible to extract the images of components interacted with and the visual feedback seen by the demonstrator, and generate a visual test script automatically. We show that a variety of GUI behavior can be tested using this approach. Also, we show how this approach can facilitate good testing practices such as unit testing, regression testing, and test-driven development.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1535–1544},
numpages = {10},
keywords = {gui testing, test by demonstration, gui automation},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@inproceedings{10.1145/3324884.3416572,
author = {Zhou, Chijin and Wang, Mingzhe and Liang, Jie and Liu, Zhe and Jiang, Yu},
title = {Zeror: Speed up Fuzzing with Coverage-Sensitive Tracing and Scheduling},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416572},
doi = {10.1145/3324884.3416572},
abstract = {Coverage-guided fuzzing is one of the most popular software testing techniques for vulnerability detection. While effective, current fuzzing methods suffer from significant performance penalty due to instrumentation overhead, which limits its practical use. Existing solutions improve the fuzzing speed by decreasing instrumentation overheads but sacrificing coverage accuracy, which results in unstable performance of vulnerability detection.In this paper, we propose a coverage-sensitive tracing and scheduling framework Zeror that can improve the performance of existing fuzzers, especially in their speed and vulnerability detection. The Zeror is mainly made up of two parts: (1) a self-modifying tracing mechanism to provide a zero-overhead instrumentation for more effective coverage collection, and (2) a real-time scheduling mechanism to support adaptive switch between the zero-overhead instrumented binary and the fully instrumented binary for better vulnerability detection. In this way, Zeror is able to decrease collection overhead and preserve fine-grained coverage for guidance.For evaluation, we implement a prototype of Zeror and evaluate it on Google fuzzer-test-suite, which consists of 24 widely-used applications. The results show that Zeror performs better than existing fuzzing speed-up frameworks such as Untracer and INSTRIM, improves the execution speed of the state-of-the-art fuzzers such as AFL and MOPT by 159.80%, helps them achieve better coverage (averagely 10.14% for AFL, 6.91% for MOPT) and detect vulnerabilities faster (averagely 29.00% for AFL, 46.99% for MOPT).},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {858–870},
numpages = {13},
keywords = {scheduling, coverage-sensitive tracing, coverage-guided fuzzing},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@article{10.1145/3360599,
author = {Kokologiannakis, Michalis and Raad, Azalea and Vafeiadis, Viktor},
title = {Effective Lock Handling in Stateless Model Checking},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360599},
doi = {10.1145/3360599},
abstract = {Stateless Model Checking (SMC) is a verification technique for concurrent programs that checks for safety violations by exploring all possible thread interleavings. SMC is usually coupled with Partial Order Reduction (POR), which exploits the independence of instructions to avoid redundant explorations when an equivalent one has already been considered. While effective POR techniques have been developed for many different memory models, they are only able to exploit independence at the instruction level, which makes them unsuitable for programs with coarse-grained synchronization mechanisms such as locks. We present a lock-aware POR algorithm, LAPOR, that exploits independence at both instruction and critical section levels. This enables LAPOR to explore exponentially fewer interleavings than the state-of-the-art techniques for programs that use locks conservatively. Our algorithm is sound, complete, and optimal, and can be used for verifying programs under several different memory models. We implement LAPOR in a tool and show that it can be exponentially faster than the state-of-the-art model checkers.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {173},
numpages = {26},
keywords = {weak memory models, mutual exclusion locks, Model checking}
}

@inproceedings{10.1145/3237009.3237015,
author = {Aumayr, Dominik and Marr, Stefan and B\'{e}ra, Cl\'{e}ment and Boix, Elisa Gonzalez and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {Efficient and Deterministic Record &amp; Replay for Actor Languages},
year = {2018},
isbn = {9781450364249},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3237009.3237015},
doi = {10.1145/3237009.3237015},
abstract = {With the ubiquity of parallel commodity hardware, developers turn to high-level concurrency models such as the actor model to lower the complexity of concurrent software. However, debugging concurrent software is hard, especially for concurrency models with a limited set of supporting tools. Such tools often deal only with the underlying threads and locks, which obscures the view on e.g. actors and messages and thereby introduces additional complexity.To improve on this situation, we present a low-overhead record &amp; replay approach for actor languages. It allows one to debug concurrency issues deterministically based on a previously recorded trace. Our evaluation shows that the average run-time overhead for tracing on benchmarks from the Savina suite is 10% (min. 0%, max. 20%). For Acme-Air, a modern web application, we see a maximum increase of 1% in latency for HTTP requests and about 1.4 MB/s of trace data. These results are a first step towards deterministic replay debugging of actor systems in production.},
booktitle = {Proceedings of the 15th International Conference on Managed Languages &amp; Runtimes},
articleno = {15},
numpages = {14},
keywords = {replay, actors, determinism, debugging, tracing, concurrency},
location = {Linz, Austria},
series = {ManLang '18}
}

@inproceedings{10.1109/ASE.2015.78,
author = {Filieri, Antonio and P\u{a}s\u{a}reanu, Corina S. and Yang, Guowei},
title = {Quantification of Software Changes through Probabilistic Symbolic Execution},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.78},
doi = {10.1109/ASE.2015.78},
abstract = {Characterizing software changes is fundamental for software maintenance. However existing techniques are imprecise leading to unnecessary maintenance efforts. We introduce a novel approach that computes a precise numeric characterization of program changes, which quantifies the likelihood of reaching target program events (e.g., assert violations or successful termination) and how that evolves with each program update, together with the percentage of inputs impacted by the change.This precise characterization leads to a natural ranking of different program changes based on their probability of execution and their impact on target events. The approach is based on model counting over the constraints collected with a symbolic execution of the program, and exploits the similarity between program versions to reduce cost and improve the quality of analysis results.We implemented our approach in the Symbolic PathFinder tool and illustrate it on several Java case studies, including the evaluation of different program repairs, mutants used in testing, or incremental analysis after a change.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {703–708},
numpages = {6},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

