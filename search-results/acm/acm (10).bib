@inproceedings{10.1145/3395363.3397384,
author = {Zhang, Yakun and Dou, Wensheng and Zhu, Jiaxin and Xu, Liang and Zhou, Zhiyong and Wei, Jun and Ye, Dan and Yang, Bo},
title = {Learning to Detect Table Clones in Spreadsheets},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397384},
doi = {10.1145/3395363.3397384},
abstract = {In order to speed up spreadsheet development productivity, end users can create a spreadsheet table by copying and modifying an existing one. These two tables share the similar computational semantics, and form a table clone. End users may modify the tables in a table clone, e.g., adding new rows and deleting columns, thus introducing structure changes into the table clone. Our empirical study on real-world spreadsheets shows that about 58.5% of table clones involve structure changes. However, existing table clone detection approaches in spreadsheets can only detect table clones with the same structures. Therefore, many table clones with structure changes cannot be detected. We observe that, although the tables in a table clone may be modified, they usually share the similar structures and formats, e.g., headers, formulas and background colors. Based on this observation, we propose LTC (Learning to detect Table Clones), to automatically detect table clones with or without structure changes. LTC utilizes the structure and format information from labeled table clones and non table clones to train a binary classifier. LTC first identifies tables in spreadsheets, and then uses the trained binary classifier to judge whether every two tables can form a table clone. Our experiments on real-world spreadsheets from the EUSES and Enron corpora show that, LTC can achieve a precision of 97.8% and recall of 92.1% in table clone detection, significantly outperforming the state-of-the-art technique (a precision of 37.5% and recall of 11.1%).},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {528–540},
numpages = {13},
keywords = {Spreadsheet, table clone, structure, format},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

@inproceedings{10.1145/1390630.1390653,
author = {Edwards, Alex and Tucker, Sean and Worms, S\'{e}bastien and Vaidya, Rahul and Demsky, Brian},
title = {AFID: An Automated Fault Identification Tool},
year = {2008},
isbn = {9781605580500},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390630.1390653},
doi = {10.1145/1390630.1390653},
abstract = {We present the Automatic Fault IDentification Tool (AFID). AFID automatically constructs repositories of real software faults by monitoring the software development process. AFID records both a fault revealing test case and a faulty version of the source code for any crashing faults that the developer discovers and a fault correcting source code change for any crashing faults that the developer corrects. The test cases are a significant contribution, because they enable new research that explores the dynamic behaviors of the software faults.AFID uses a ptrace-based monitoring mechanism to monitor both the compilation and execution of the application. The ptrace-based technique makes it straightforward for AFID to support a wide range of programming languages and compilers. Our benchmark results indicate that the monitoring overhead will be acceptable for most developers. We performed a short case study to evaluate how effectively the AFID tool records software faults. In our case study, AFID recorded 12 software faults from the 8 participants.},
booktitle = {Proceedings of the 2008 International Symposium on Software Testing and Analysis},
pages = {179–188},
numpages = {10},
keywords = {fault collection},
location = {Seattle, WA, USA},
series = {ISSTA '08}
}

@inproceedings{10.1145/2610384.2610390,
author = {Yandrapally, Rahulkrishna and Thummalapenta, Suresh and Sinha, Saurabh and Chandra, Satish},
title = {Robust Test Automation Using Contextual Clues},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610390},
doi = {10.1145/2610384.2610390},
abstract = { Despite the seemingly obvious advantage of test automation, significant skepticism exists in the industry regarding its cost-benefit tradeoffs. Test scripts for web applications are fragile: even small changes in the page layout can break a number of tests, requiring the expense of re-automating them. Moreover, a test script created for one browser cannot be relied upon to run on a different web browser: it requires duplicate effort to create and maintain versions of tests for a variety of browsers. Because of these hidden costs, organizations often fall back to manual testing.  We present a fresh solution to the problem of test-script fragility. Often, the root cause of test-script fragility is that, to identify UI elements on a page, tools typically record some metadata that depends on the internal representation of the page in a browser. Our technique eliminates metadata almost entirely. Instead, it identifies UI elements relative to other prominent elements on the page. The core of our technique automatically identifies a series of contextual clues that unambiguously identify a UI element, without recording anything about the internal representation.  Empirical evidence shows that our technique is highly accurate in computing contextual clues, and outperforms existing techniques in its resilience to UI changes as well as browser changes. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {304–314},
numpages = {11},
keywords = {GUI test automation, test-script fragility},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inproceedings{10.1145/2931037.2931064,
author = {Fagerstr\"{o}m, Mikael and Ismail, Emre Emir and Liebel, Grischa and Guliani, Rohit and Larsson, Fredrik and Nordling, Karin and Knauss, Eric and Pelliccione, Patrizio},
title = {Verdict Machinery: On the Need to Automatically Make Sense of Test Results},
year = {2016},
isbn = {9781450343909},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2931037.2931064},
doi = {10.1145/2931037.2931064},
abstract = { Along with technological developments and increasing competition there is a major incentive for companies to produce and market high quality products before their competitors. In order to conquer a bigger portion of the market share, companies have to ensure the quality of the product in a shorter time frame. To accomplish this task companies try to automate their test processes as much as possible. It is critical to investigate and understand the problems that occur during different stages of test automation processes. In this paper we report on a case study on automatic analysis of non-functional test results. We discuss challenges in the face of continuous integration and deployment and provide improvement suggestions based on interviews at a large company in Sweden. The key contributions of this work are filling the knowledge gap in research about performance regression test analysis automation and providing warning signs and a road map for the industry. },
booktitle = {Proceedings of the 25th International Symposium on Software Testing and Analysis},
pages = {225–234},
numpages = {10},
keywords = {Automation}, Verdict System, Performance regression test analysis, Non-Functional Testing Oracle},
location = {Saarbr\"{u}cken, Germany},
series = {ISSTA 2016}
}

@inproceedings{10.1145/3460319.3464814,
author = {Srivastava, Prashast and Payer, Mathias},
title = {Gramatron: Effective Grammar-Aware Fuzzing},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464814},
doi = {10.1145/3460319.3464814},
abstract = {Fuzzers aware of the input grammar can explore deeper program states using grammar-aware mutations. Existing grammar-aware fuzzers are ineffective at synthesizing complex bug triggers due to: (i) grammars introducing a sampling bias during input generation due to their structure, and (ii) the current mutation operators for parse trees performing localized small-scale changes. Gramatron uses grammar automatons in conjunction with aggressive mutation operators to synthesize complex bug triggers faster. We build grammar automatons to address the sampling bias. It restructures the grammar to allow for unbiased sampling from the input state space. We redesign grammar-aware mutation operators to be more aggressive, i.e., perform large-scale changes. Gramatron can consistently generate complex bug triggers in an efficient manner as compared to using conventional grammars with parse trees. Inputs generated from scratch by Gramatron have higher diversity as they achieve up to 24.2% more coverage relative to existing fuzzers. Gramatron makes input generation 98% faster and the input representations are 24% smaller. Our redesigned mutation operators are 6.4\texttimes{} more aggressive while still being 68% faster at performing these mutations. We evaluate Gramatron across three interpreters with 10 known bugs consisting of three complex bug triggers and seven simple bug triggers against two Nautilus variants. Gramatron finds all the complex bug triggers reliably and faster. For the simple bug triggers, Gramatron outperforms Nautilus four out of seven times. To demonstrate Gramatron’s effectiveness in the wild, we deployed Gramatron on three popular interpreters for a 10-day fuzzing campaign where it discovered 10 new vulnerabilities.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {244–256},
numpages = {13},
keywords = {grammar-aware, dynamic software analysis, Fuzzing},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inbook{10.1145/3293882.3330572,
author = {Kong, Pingfan and Li, Li and Gao, Jun and Bissyand\'{e}, Tegawend\'{e} F. and Klein, Jacques},
title = {Mining Android Crash Fixes in the Absence of Issue- and Change-Tracking Systems},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330572},
abstract = {Android apps are prone to crash. This often arises from the misuse of Android framework APIs, making it harder to debug since official Android documentation does not discuss thoroughly potential exceptions.Recently, the program repair community has also started to investigate the possibility to fix crashes automatically. Current results, however, apply to limited example cases. In both scenarios of repair, the main issue is the need for more example data to drive the fix processes due to the high cost in time and effort needed to collect and identify fix examples. We propose in this work a scalable approach, CraftDroid, to mine crash fixes by leveraging a set of 28 thousand carefully reconstructed app lineages from app markets, without the need for the app source code or issue reports. We developed a replicative testing approach that locates fixes among app versions which output different runtime logs with the exact same test inputs. Overall, we have mined 104 relevant crash fixes, further abstracted 17 fine-grained fix templates that are demonstrated to be effective for patching crashed apks. Finally, we release ReCBench, a benchmark consisting of 200 crashed apks and the crash replication scripts, which the community can explore for evaluating generated crash-inducing bug patches.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {78–89},
numpages = {12}
}

@inproceedings{10.1145/948542.948544,
author = {Meza, Juan C. and Oliva, Ricardo A.},
title = {An Object Oriented Library to Manage the Collection of Schittkowski Test Problems for Nonlinear Optimization},
year = {2003},
isbn = {1581137907},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/948542.948544},
doi = {10.1145/948542.948544},
abstract = {This paper describes a C++ library that allows handling these test problems in an object-oriented manner without replacing the original Fortran code. The class includes wrappers for evaluating the functions defining each problem, in addition to methods that allow the user to query whether certain features are present in a given test problem. This makes it possible to classify and select subsets of the problems according to criteria specific for the code being tested.},
booktitle = {Proceedings of the 2003 Conference on Diversity in Computing},
pages = {9–11},
numpages = {3},
keywords = {optimization algorithm testing, optimization test problems, nonlinear programming},
location = {Atlanta, Georgia, USA},
series = {TAPIA '03}
}

@inproceedings{10.1145/2993288.2993299,
author = {Magalh\~{a}es, Cl\'{a}udio and Barros, Fl\'{a}via and Mota, Alexandre and Maia, Eliot},
title = {Automatic Selection of Test Cases for Regression Testing},
year = {2016},
isbn = {9781450347662},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993288.2993299},
doi = {10.1145/2993288.2993299},
abstract = {Regression testing is a safety measure to attest that changes made on a system preserve prior accepted behavior. Identifying which test cases must compose a regression test suite in a certain development stage is tricky, particularly when one only has test cases and change requests described in natural language, and the execution of the test suite will be performed manually. That is the case of our industrial partner. We propose a selection of regression test cases based on information retrieval and implement as a web-service. In performed experiments, we show that we can improve the creation of regression test suites of our industrial partner by providing more effective test cases based on keywords analysis in an automatic way.},
booktitle = {Proceedings of the 1st Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {8},
numpages = {8},
keywords = {Regression testing, Information Retrieval, Test case selection},
location = {Maringa, Parana, Brazil},
series = {SAST}
}

@inproceedings{10.1145/1273463.1273475,
author = {Harman, Mark and McMinn, Phil},
title = {A Theoretical &amp; Empirical Analysis of Evolutionary Testing and Hill Climbing for Structural Test Data Generation},
year = {2007},
isbn = {9781595937346},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1273463.1273475},
doi = {10.1145/1273463.1273475},
abstract = {Evolutionary testing has been widely studied as a technique for automating the process of test case generation. However, to date, there has been no theoretical examination of when and why it works. Furthermore, the empirical evidence for the effectiveness of evolutionary testing consists largely of small scale laboratory studies. This paper presents a first theoretical analysis of the scenarios in which evolutionary algorithms are suitable for structural test case generation. The theory is backed up by an empirical study that considers real world programs, the search spaces of which are several orders of magnitude larger than those previously considered.},
booktitle = {Proceedings of the 2007 International Symposium on Software Testing and Analysis},
pages = {73–83},
numpages = {11},
keywords = {schema theory, genetic algorithms, automated test data generation, hill climbing, royal road, evolutionary testing},
location = {London, United Kingdom},
series = {ISSTA '07}
}

@inbook{10.1145/3460319.3469080,
author = {Hou, Yunhan and Liu, Jiawei and Wang, Daiwei and He, Jiawei and Fang, Chunrong and Chen, Zhenyu},
title = {TauMed: Test Augmentation of Deep Learning in Medical Diagnosis},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3469080},
abstract = {Deep learning has made great progress in medical diagnosis. However, due to data standardization and privacy restriction, the acquisition and sharing of medical image data have been hindered, leading to the unacceptable accuracy of some intelligent medical diagnosis models. Another concern is data quality. If insufficient quantity and low-quality data are used for training and testing medical diagnosis models, it may cause serious medical accidents. We always use data augmentation to deal with it, and one of the most representative ways is through mutation relation. However, although common mutation methods can increase the amount of medical data, the quality of the image cannot be guaranteed due to the particularity of medical image. Therefore, combined with the characteristics of medical images, we propose TauMed, which implements augmentation techniques based on a series of mutation rules and domain semantics on medical datasets to generate sufficient and high-quality images. Moreover, we chose the ResNet-50 model to experiment with the augmented dataset and compared the results with two main popular mutation tools. The experimental result indicates that TauMed can improve the classification accuracy of the model effectively, and the quality of augmented images is higher than the other two tools. Its video is at https://www.youtube.com/watch?v=O8W8I7U_eqk and TauMed can be used at http://121.196.124.158:9500/.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {674–677},
numpages = {4}
}

@inproceedings{10.1145/3092703.3098227,
author = {Yaneva, Vanya and Rajan, Ajitha and Dubach, Christophe},
title = {ParTeCL: Parallel Testing Using OpenCL},
year = {2017},
isbn = {9781450350761},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3092703.3098227},
doi = {10.1145/3092703.3098227},
abstract = {With the growing complexity of software, the number of test cases needed for effective validation is extremely large. Executing these large test suites is expensive and time consuming, putting an enormous pressure on the software development cycle. In previous work, we proposed using Graphics Processing Units (GPUs) to accelerate test execution by running test cases in parallel on the GPU threads. However, the complexity of GPU programming poses challenges to the usability and effectiveness of the proposed approach. In this paper we present ParTeCL - a compiler-assisted framework to automatically generate GPU code from sequential programs and execute their tests in parallel on the GPU. We show feasibilitiy and performance achieved when executing test suites for 9 programs from an industry standard benchmark suite on the GPU. ParTeCL achieves an average speedup of 16\texttimes{} when compared to a single CPU for these benchmarks.},
booktitle = {Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {384–387},
numpages = {4},
keywords = {Embedded software, Automated testing, Functional testing, Compilers, GPUs},
location = {Santa Barbara, CA, USA},
series = {ISSTA 2017}
}

@inproceedings{10.1145/2771783.2771789,
author = {Hothersall-Thomas, Charlie and Maffeis, Sergio and Novakovic, Chris},
title = {BrowserAudit: Automated Testing of Browser Security Features},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771789},
doi = {10.1145/2771783.2771789},
abstract = { The security of the client side of a web application relies on browser features such as cookies, the same-origin policy and HTTPS. As the client side grows increasingly powerful and sophisticated, browser vendors have stepped up their offering of security mechanisms which can be leveraged to protect it. These are often introduced experimentally and informally and, as adoption increases, gradually become standardised (e.g., CSP, CORS and HSTS). Considering the diverse landscape of browser vendors, releases, and customised versions for mobile and embedded devices, there is a compelling need for a systematic assessment of browser security. We present BrowserAudit, a tool for testing that a deployed browser enforces the guarantees implied by the main standardised and experimental security mechanisms. It includes more than 400 fully-automated tests that exercise a broad range of security features, helping web users, application developers and security researchers to make an informed security assessment of a deployed browser. We validate BrowserAudit by discovering both fresh and known security-related bugs in major browsers. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {37–47},
numpages = {11},
keywords = {Content Security Policy, Cross-Origin Resource Sharing, cookies, Web security, web browser testing, Same-Origin Policy, click-jacking},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/2338965.2336771,
author = {Yang, Guowei and P\u{a}s\u{a}reanu, Corina S. and Khurshid, Sarfraz},
title = {Memoized Symbolic Execution},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336771},
doi = {10.1145/2338965.2336771},
abstract = { This paper introduces memoized symbolic execution (Memoise), a new approach for more efficient application of forward symbolic execution, which is a well-studied technique for systematic exploration of program behaviors based on bounded execution paths. Our key insight is that application of symbolic execution often requires several successive runs of the technique on largely similar underlying problems, e.g., running it once to check a program to find a bug, fixing the bug, and running it again to check the modified program. Memoise introduces a trie-based data structure that stores the key elements of a run of symbolic execution. Maintenance of the trie during successive runs allows re-use of previously computed results of symbolic execution without the need for re-computing them as is traditionally done. Experiments using our prototype implementation of Memoise show the benefits it holds in various standard scenarios of using symbolic execution, e.g., with iterative deepening of exploration depth, to perform regression analysis, or to enhance coverage using heuristics. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {144–154},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inbook{10.1145/3293882.3330576,
author = {Padhye, Rohan and Lemieux, Caroline and Sen, Koushik and Papadakis, Mike and Le Traon, Yves},
title = {Semantic Fuzzing with Zest},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330576},
abstract = {Programs expecting structured inputs often consist of both a syntactic analysis stage, which parses raw input, and a semantic analysis stage, which conducts checks on the parsed input and executes the core logic of the program. Generator-based testing tools in the lineage of QuickCheck are a promising way to generate random syntactically valid test inputs for these programs. We present Zest, a technique which automatically guides QuickCheck-like random input generators to better explore the semantic analysis stage of test programs. Zest converts random-input generators into deterministic parametric input generators. We present the key insight that mutations in the untyped parameter domain map to structural mutations in the input domain. Zest leverages program feedback in the form of code coverage and input validity to perform feedback-directed parameter search. We evaluate Zest against AFL and QuickCheck on five Java programs: Maven, Ant, BCEL, Closure, and Rhino. Zest covers 1.03x-2.81x as many branches within the benchmarks' semantic analysis stages as baseline techniques. Further, we find 10 new bugs in the semantic analysis stages of these benchmarks. Zest is the most effective technique in finding these bugs reliably and quickly, requiring at most 10 minutes on average to find each bug.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {329–340},
numpages = {12}
}

@inproceedings{10.1145/2338965.2336769,
author = {Caballero, Juan and Grieco, Gustavo and Marron, Mark and Nappa, Antonio},
title = {Undangle: Early Detection of Dangling Pointers in Use-after-Free and Double-Free Vulnerabilities},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336769},
doi = {10.1145/2338965.2336769},
abstract = { Use-after-free vulnerabilities are rapidly growing in popularity, especially for exploiting web browsers. Use-after-free (and double-free) vulnerabilities are caused by a program operating on a dangling pointer. In this work we propose early detection, a novel runtime approach for finding and diagnosing use-after-free and double-free vulnerabilities. While previous work focuses on the creation of the vulnerability (i.e., the use of a dangling pointer), early detection shifts the focus to the creation of the dangling pointer(s) at the root of the vulnerability. Early detection increases the effectiveness of testing by identifying unsafe dangling pointers in executions where they are created but not used. It also accelerates vulnerability analysis and minimizes the risk of incomplete fixes, by automatically collecting information about all dangling pointers involved in the vulnerability. We implement our early detection technique in a tool called Undangle. We evaluate Undangle for vulnerability analysis on 8 real-world vulnerabilities. The analysis uncovers that two separate vulnerabilities in Firefox had a common root cause and that their patches did not completely fix the underlying bug. We also evaluate Undangle for testing on the Firefox web browser identifying a potential vulnerability. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {133–143},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/2483760.2483781,
author = {Devaki, Pranavadatta and Thummalapenta, Suresh and Singhania, Nimit and Sinha, Saurabh},
title = {Efficient and Flexible GUI Test Execution via Test Merging},
year = {2013},
isbn = {9781450321594},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2483760.2483781},
doi = {10.1145/2483760.2483781},
abstract = { As a test suite evolves, it can accumulate redundant tests. To address this problem, many test-suite reduction techniques, based on different measures of redundancy, have been developed. A more subtle problem, that can also cause test-suite bloat and that has not been addressed by existing research, is the accumulation of similar tests. Similar tests are not redundant by any measure; but, they contain many common actions that are executed repeatedly, which over a large test suite, can degrade execution time substantially.  We present a test merging technique for GUI tests. Given a test suite, the technique identifies the tests that can be merged and creates a merged test, which covers all the application states that are exercised individually by the tests, but with the redundant common steps executed only once. The key novelty in the merging technique is that it compares the dynamic states induced by the tests to identify a semantically meaningful interleaving of steps from different tests. The technique not only improves the efficiency of test execution, but also ensures that there is no loss in the fault-revealing power of the original tests. In the empirical studies, conducted using four open-source web applications and one proprietary enterprise web application, in which over $3300$ test cases and 19600 test steps were analyzed, the technique reduced the number of test steps by 29% and the test-execution time by 39%. },
booktitle = {Proceedings of the 2013 International Symposium on Software Testing and Analysis},
pages = {34–44},
numpages = {11},
keywords = {dynamic analysis, Test-suite reduction, test merging},
location = {Lugano, Switzerland},
series = {ISSTA 2013}
}

@inproceedings{10.1145/2610384.2610414,
author = {Li, Ding and Jin, Yuchen and Sahin, Cagri and Clause, James and Halfond, William G. J.},
title = {Integrated Energy-Directed Test Suite Optimization},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610414},
doi = {10.1145/2610384.2610414},
abstract = { In situ testing techniques have become an important means of ensuring the reliability of embedded systems after they are deployed in the field. However, these techniques do not help testers optimize the energy consumption of their in situ test suites, which can needlessly waste the limited battery power of these systems. In this work, we extend prior techniques for test suite minimization in such a way as to allow testers to generate energy-efficient, minimized test suites with only minimal modifications to their existing work flow. We perform an extensive empirical evaluation of our approach using the test suites provided for real world applications. The results of the evaluation show that our technique is effective at generating, in less than one second, test suites that consume up to 95% less energy while maintaining coverage of the testing requirements. },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {339–350},
numpages = {12},
keywords = {Test suites, Energy usage, Minimization},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

@inbook{10.1145/3293882.3330567,
author = {Tolksdorf, Sandro and Lehmann, Daniel and Pradel, Michael},
title = {Interactive Metamorphic Testing of Debuggers},
year = {2019},
isbn = {9781450362245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3293882.3330567},
abstract = {When improving their code, developers often turn to interactive debuggers. The correctness of these tools is crucial, because bugs in the debugger itself may mislead a developer, e.g., to believe that executed code is never reached or that a variable has another value than in the actual execution. Yet, debuggers are difficult to test because their input consists of both source code and a sequence of debugging actions, such as setting breakpoints or stepping through code. This paper presents the first metamorphic testing approach for debuggers. The key idea is to transform both the debugged code and the debugging actions in such a way that the behavior of the original and the transformed inputs should differ only in specific ways. For example, adding a breakpoint should not change the control flow of the debugged program. To support the interactive nature of debuggers, we introduce interactive metamorphic testing. It differs from traditional metamorphic testing by determining the input transformation and the expected behavioral change it causes while the program under test is running. Our evaluation applies the approach to the widely used debugger in the Chromium browser, where it finds eight previously unknown bugs with a true positive rate of 51%. All bugs have been confirmed by the developers, and one bug has even been marked as release-blocking.},
booktitle = {Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {273–283},
numpages = {11}
}

@inproceedings{10.1145/3128473.3128478,
author = {Magalh\~{a}es, Claudio and Andrade, Jo\~{a}o and Perrusi, Lucas and Mota, Alexandre},
title = {Evaluating an Automatic Text-Based Test Case Selection Using a Non-Instrumented Code Coverage Analysis},
year = {2017},
isbn = {9781450353021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128473.3128478},
doi = {10.1145/3128473.3128478},
abstract = {During development, systems may be tested several times. In general, a system evolves from change requests, aiming at improving its behavior in terms of new features as well as fixing failures. Thus, selecting the best test plan in terms of the closeness between test cases and the changed code and its dependencies is pursued by industry and academia. In this paper we measure the coverage achieved by an automatic test case selection based on information retrieval that relates change requests and test cases. But instead of using off-the-shelf coverage tools, like JaCoCo, we propose a way of obtaining code coverage of Android apk's without instrumentation. This was a basic requirement of our industrial partner. We performed some experiments on this industrial partner and promising results were obtained.},
booktitle = {Proceedings of the 2nd Brazilian Symposium on Systematic and Automated Software Testing},
articleno = {5},
numpages = {9},
keywords = {Test case selection and prioritization, Information Retrieval, Code coverage},
location = {Fortaleza, Brazil},
series = {SAST}
}

@inproceedings{10.1145/3395363.3397355,
author = {Liu, Hui and Shen, Mingzhu and Jin, Jiahao and Jiang, Yanjie},
title = {Automated Classification of Actions in Bug Reports of Mobile Apps},
year = {2020},
isbn = {9781450380089},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3395363.3397355},
doi = {10.1145/3395363.3397355},
abstract = {When users encounter problems with mobile apps, they may commit such problems to developers as bug reports. To facilitate the processing of bug reports, researchers proposed approaches to validate the reported issues automatically according to the steps to reproduce specified in bug reports. Although such approaches have achieved high success rate in reproducing the reported issues, they often rely on a predefined vocabulary to identify and classify actions in bug reports. However, such manually constructed vocabulary and classification have significant limitations. It is challenging for the vocabulary to cover all potential action words because users may describe the same action with different words. Besides that, classification of actions solely based on the action words could be inaccurate because the same action word, appearing in different contexts, may have different meaning and thus belongs to different action categories. To this end, in this paper we propose an automated approach, called MaCa, to identify and classify action words in Mobile apps’ bug reports. For a given bug report, it first identifies action words based on natural language processing. For each of the resulting action words, MaCa extracts its contexts, i.e., its enclosing segment, the associated UI target, and the type of its target element by both natural language processing and static analysis of the associated app. The action word and its contexts are then fed into a machine learning based classifier that predicts the category of the given action word in the given context. To train the classifier, we manually labelled 1,202 actions words from 525 bug reports that are associated with 207 apps. Our evaluation results on manually labelled data suggested that MaCa was accurate with high accuracy varying from 95% to 96.7%. We also investigated to what extent MaCa could further improve existing approaches (i.e., Yakusu and ReCDroid) in reproducing bug reports. Our evaluation results suggested that integrating MaCa into existing approaches significantly improved the success rates of ReCDroid and Yakusu by 22.7% = (69.2%-56.4%)/56.4% and 22.9%= (62.7%-51%)/51%, respectively.},
booktitle = {Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {128–140},
numpages = {13},
keywords = {Bug report, Mobile Testing, Test Case Generation, Classification},
location = {Virtual Event, USA},
series = {ISSTA 2020}
}

