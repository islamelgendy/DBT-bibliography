@inbook{10.1145/3368089.3417065,
author = {Kim, Jinhan and Ju, Jeongil and Feldt, Robert and Yoo, Shin},
title = {Reducing DNN Labelling Cost Using Surprise Adequacy: An Industrial Case Study for Autonomous Driving},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417065},
abstract = {Deep Neural Networks (DNNs) are rapidly being adopted by the automotive industry, due to their impressive performance in tasks that are essential for autonomous driving. Object segmentation is one such task: its aim is to precisely locate boundaries of objects and classify the identified objects, helping autonomous cars to recognise the road environment and the traffic situation. Not only is this task safety critical, but developing a DNN based object segmentation module presents a set of challenges that are significantly different from traditional development of safety critical software. The development process in use consists of multiple iterations of data collection, labelling, training, and evaluation. Among these stages, training and evaluation are computation intensive while data collection and labelling are manual labour intensive. This paper shows how development of DNN based object segmentation can be improved by exploiting the correlation between Surprise Adequacy (SA) and model performance. The correlation allows us to predict model performance for inputs without manually labelling them. This, in turn, enables understanding of model performance, more guided data collection, and informed decisions about further training. In our industrial case study the technique allows cost savings of up to 50% with negligible evaluation inaccuracy. Furthermore, engineers can trade off cost savings versus the tolerable level of inaccuracy depending on different development phases and scenarios.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1466–1476},
numpages = {11}
}

@inproceedings{10.1145/3266237.3266272,
author = {Durelli, Vinicius H. S. and Durelli, Rafael S. and Endo, Andre T. and Cirilo, Elder and Luiz, Washington and Rocha, Leonardo},
title = {Please Please Me: Does the Presence of Test Cases Influence Mobile App Users' Satisfaction?},
year = {2018},
isbn = {9781450365031},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3266237.3266272},
doi = {10.1145/3266237.3266272},
abstract = {Mobile application developers have started to realize that quality plays a vital role in increasing the popularity of mobile applications (apps), thereby directly influencing economical profit (in-app purchases revenue) and app-related success factors (i.e., number of downloads). Therefore, developers have become increasingly concerned with taking preemptive actions to ensure the quality of their apps. In general, developers have been relying on testing as their main quality assurance practice. However, little is known about how much mobile app testing contributes to increasing user level satisfaction. In this paper we investigate to what extent testing mobile apps contributes to achieving higher user satisfaction. To this end, we probed into whether there is a relation between having automated tests and overall user satisfaction. We looked into users ratings, which express their level of satisfaction with apps, and users reviews, which often include bug (i.e., fault) reports. By analyzing a quantitative indicator of user satisfaction (i.e., user rating), we found that there is no significant difference between apps with automated tests and apps that have been developed without test suites. We also applied sentiment analysis on user reviews to examine the main differences between apps with and without test suites. The results of our review-based sentiment analysis suggest that most apps with and without test suites score quite high for user satisfaction. In addition, we found that update-related problems are far more common in apps with test suites, while apps without test suites are likely to have battery-drain problems.},
booktitle = {Proceedings of the XXXII Brazilian Symposium on Software Engineering},
pages = {132–141},
numpages = {10},
keywords = {mobile application, sentiment analysis, software testing},
location = {Sao Carlos, Brazil},
series = {SBES '18}
}

@inproceedings{10.1145/1370143.1370148,
author = {Park, Shelly S. and Maurer, Frank},
title = {The Benefits and Challenges of Executable Acceptance Testing},
year = {2008},
isbn = {9781605580210},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370143.1370148},
doi = {10.1145/1370143.1370148},
abstract = {In this paper, we argue that executable acceptance test driven development (EATDD) allows tighter integration between the software requirements and the implementation. We argue that EATDD improves communication between all project stakeholders. We give an overview of why previous approaches to requirements specifications are less than impressive and how executable acceptance tests help fix problems. In addition, we argue for multi-modal executable acceptance tests and how it can help improve the requirements specification. We provide some of the immediate research questions that need to be addressed in order to push forward more wide-spread use of executable acceptance test driven development.},
booktitle = {Proceedings of the 2008 International Workshop on Scrutinizing Agile Practices or Shoot-out at the Agile Corral},
pages = {19–22},
numpages = {4},
keywords = {agile software engineering, executable requirements, automated software testing, executable acceptance test driven development},
location = {Leipzig, Germany},
series = {APOS '08}
}

@article{10.1145/3539738,
author = {Langdon, William B.},
title = {Deep Genetic Programming Trees Are Robust},
year = {2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {2688-299X},
url = {https://doi.org/10.1145/3539738},
doi = {10.1145/3539738},
abstract = {We sample the genetic programming tree search space and show it is smooth since many mutations on many test cases have little or no fitness impact. We generate uniformly at random high order polynomials composed of 12&nbsp;500 and 750&nbsp;000 additions and multiplications and follow the impact of small changes to them. From information theory 32&nbsp;bit floating point arithmetic is dissipative and even with 1501 test cases deep mutations seldom have any impact on fitness. Absolute difference between parent and child evaluation can grow as well as fall further from the code change location but the number of disrupted fitness tests falls monotonically. In many cases deeply nested expressions are robust to crossover syntax changes, bugs, errors, run time glitches, perturbations, etc., because their disruption falls to zero, and so it fails to propagate beyond the program.},
note = {Just Accepted},
journal = {ACM Trans. Evol. Learn. Optim.},
month = {may},
keywords = {FDP, correctness attraction, theory of bloat, self-similar fractal, self-organised criticality, neutral networks, FEP, invisible faults, introns, SBSE, mutational robustness, GP fitness landscape, SOC, information funnels, heritability, failed disruption propagation, error hiding, evolvability, software testing, diversity, software robustness, sandpile 1/f powerlaw, information theory}
}

@article{10.1145/2382756.2382783,
author = {do Carmo Machado, Ivan and McGregor, John D. and Santana de Almeida, Eduardo},
title = {Strategies for Testing Products in Software Product Lines},
year = {2012},
issue_date = {November 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {37},
number = {6},
issn = {0163-5948},
url = {https://doi.org/10.1145/2382756.2382783},
doi = {10.1145/2382756.2382783},
abstract = {The software product line engineering strategy enables the achievement of significant improvements in quality through reuse of carefully crafted software assets across multiple products. However, high levels of quality in the software product line assets, which are used to create products, must be accompanied by effective and efficient test strategies for the products in the software product line. The goal of this study is to understand which strategies for testing products in software product lines have been reported in the literature, enabling discussions on the significant issues, and also pointing out further research directions. A systematic literature review was carried out that identified two hundred seventy-three papers, published from the years 1998 and early in 2012. From such a set of papers, a systematic selection resulted in forty-one relevant papers. The analysis of the reported strategies comprised two important aspects: the selection of products for testing, and the actual test of products. The findings showed a range of strategies, dealing with both aspects, but few empirical evaluations of their effectiveness have been performed, which limits the inferences that can be drawn.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {nov},
pages = {1–8},
numpages = {8},
keywords = {software testing, systematic review, software product lines}
}

@inproceedings{10.1109/ICSE.2019.00048,
author = {Tomassi, David A. and Dmeiri, Naji and Wang, Yichen and Bhowmick, Antara and Liu, Yen-Chuan and Devanbu, Premkumar T. and Vasilescu, Bogdan and Rubio-Gonz\'{a}lez, Cindy},
title = {BugSwarm: Mining and Continuously Growing a Dataset of Reproducible Failures and Fixes},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2019.00048},
doi = {10.1109/ICSE.2019.00048},
abstract = {Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like Travis-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BugSwarm, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BugSwarm toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering},
pages = {339–349},
numpages = {11},
keywords = {program analysis, bug database, reproducibility, experiment infrastructure, software testing},
location = {Montreal, Quebec, Canada},
series = {ICSE '19}
}

@inproceedings{10.1145/1594156.1594164,
author = {Tuya, Javier and Su\'{a}rez-Cabal, M. Jos\'{e} and de la Riva, Claudio},
title = {Query-Aware Shrinking Test Databases},
year = {2009},
isbn = {9781605587066},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1594156.1594164},
doi = {10.1145/1594156.1594164},
abstract = {Keeping the test databases as small as possible leads to faster execution of tests and facilitates the task of completing the test cases and evaluating the actual outputs against the expected. In this paper we present an automated approach to database reduction that considers an initial database that may be a copy of a production database and the set of queries that are executed against it. The database is reduced in order to preserve the coverage of the data with respect to the queries attaining large reductions with very similar fault detection ability.},
booktitle = {Proceedings of the Second International Workshop on Testing Database Systems},
articleno = {6},
numpages = {6},
keywords = {database testing, data reduction, SQL coverage, test-suite reduction, MC/DC, software testing},
location = {Providence, Rhode Island},
series = {DBTest '09}
}

@inproceedings{10.1145/2491627.2491650,
author = {Xu, Zhihong and Cohen, Myra B. and Motycka, Wayne and Rothermel, Gregg},
title = {Continuous Test Suite Augmentation in Software Product Lines},
year = {2013},
isbn = {9781450319683},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491627.2491650},
doi = {10.1145/2491627.2491650},
abstract = {Software Product Line (SPL) engineering offers several advantages in the development of families of software products. There is still a need, however, to generate test cases for individual products in product lines more efficiently. In this paper we propose an approach, CONTESA, for generating test cases for SPLs using test suite augmentation. Instead of generating test cases for products independently, our approach generates new test cases for products in an order that allows it to build on test cases created for products tested earlier. In this work, we use a genetic algorithm to generate test cases, targeting branches not yet covered in each product, although other algorithms and coverage criteria could be utilized. We have evaluated CONTESA on two non-trivial SPLs, and have shown that CONTESA is more efficient and effective than an approach that generates test cases for products independently. A further evaluation shows that CONTESA is more effective at achieving coverage, and reveals as many faults as an existing feature-based testing approach.},
booktitle = {Proceedings of the 17th International Software Product Line Conference},
pages = {52–61},
numpages = {10},
keywords = {software product lines, software testing, test generation},
location = {Tokyo, Japan},
series = {SPLC '13}
}

@inbook{10.1145/3387940.3391459,
author = {Honarvar, Shahin and Mousavi, Mohammad Reza and Nagarajan, Rajagopal},
title = {Property-Based Testing of Quantum Programs in Q#},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391459},
abstract = {Property-based testing is a structured method for automated testing using program specifications. We report on the design and implementation of what is to our knowledge the first property-based framework for quantum programs. We review various aspects of our design concerning property-specification, test-case generation, and test result analysis. We also provide an overview of the implementation and its way of working. Finally, we present the result of applying our framework to some examples.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {430–435},
numpages = {6}
}

@article{10.5555/2037151.2037162,
author = {Slack, James M.},
title = {ModelTester: A Tool for Teaching Model-Based Testing},
year = {2011},
issue_date = {October 2011},
publisher = {Consortium for Computing Sciences in Colleges},
address = {Evansville, IN, USA},
volume = {27},
number = {1},
issn = {1937-4771},
abstract = {Model-based testing is an increasingly important software-testing technique that CS and IS students must become familiar with. With ModelTester, students write an extended finite-state machine model for the software under test (SUT). ModelTester then uses the model to generate test cases for the SUT. The SUT language can be nearly anything; Java and Python back ends are currently available, and other back ends can be created easily. Previous versions of ModelTester have been used successfully for four years in an upper-level software-testing course.},
journal = {J. Comput. Sci. Coll.},
month = {oct},
pages = {37–46},
numpages = {10}
}

@inproceedings{10.1145/3503181.3503185,
author = {Tsai, Wei-Tek and Zhang, Li and Hu, Shufeng},
title = {From Crowdsourced Software Development to Crowdtesting},
year = {2021},
isbn = {9781450395540},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503181.3503185},
doi = {10.1145/3503181.3503185},
abstract = { Crowdsourcing Software Development (CSD) has existed and developed for many years. Over the years, CSD has made new progress and changes. The original intention of CSD is to reduce the cost of software development. However, the crowdsourcing approach encountered difficulties in software development, so it turned to software testing. Crowdsourcing Software Testing (CST) has had some successful cases. It has many advantages, such as reducing the cost and time of software testing. In this paper, we analyze the problems that crowdsourcing faces in software development and come to the view that crowdsourcing is more suitable for software testing. We analyzed the Quadrilateral Co-petition Model in the CST platforms, and gave the methods for optimization. We also proposed a new software testing program that integrates open source sharing and crowdsourcing methods.},
booktitle = {5th International Conference on Crowd Science and Engineering},
pages = {18–23},
numpages = {6},
keywords = {crowdtesting, quadrilateral co-petition model, crowdsourced software development, open-source software},
location = {Jinan, China},
series = {ICCSE '21}
}

@inproceedings{10.1145/2786805.2786862,
author = {Feng, Yang and Chen, Zhenyu and Jones, James A. and Fang, Chunrong and Xu, Baowen},
title = {Test Report Prioritization to Assist Crowdsourced Testing},
year = {2015},
isbn = {9781450336758},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2786805.2786862},
doi = {10.1145/2786805.2786862},
abstract = { In crowdsourced testing, users can be incentivized to perform testing tasks and report their results, and because crowdsourced workers are often paid per task, there is a financial incentive to complete tasks quickly rather than well. These reports of the crowdsourced testing tasks are called "test reports" and are composed of simple natural language and screenshots. Back at the software-development organization, developers must manually inspect the test reports to judge their value for revealing faults. Due to the nature of crowdsourced work, the number of test reports are often difficult to comprehensively inspect and process. In order to help with this daunting task, we created the first technique of its kind, to the best of our knowledge, to prioritize test reports for manual inspection. Our technique utilizes two key strategies: (1) a diversity strategy to help developers inspect a wide variety of test reports and to avoid duplicates and wasted effort on falsely classified faulty behavior, and (2) a risk strategy to help developers identify test reports that may be more likely to be fault-revealing based on past observations. Together, these strategies form our DivRisk strategy to prioritize test reports in crowd- sourced testing. Three industrial projects have been used to evaluate the effectiveness of test report prioritization methods. The results of the empirical study show that: (1) DivRisk can significantly outperform random prioritization; (2) DivRisk can approximate the best theoretical result for a real-world industrial mobile application. In addition, we provide some practical guidelines of test report prioritization for crowdsourced testing based on the empirical study and our experiences. },
booktitle = {Proceedings of the 2015 10th Joint Meeting on Foundations of Software Engineering},
pages = {225–236},
numpages = {12},
keywords = {test report prioritization, test diversity, natural language processing, Crowdsourcing testing},
location = {Bergamo, Italy},
series = {ESEC/FSE 2015}
}

@inproceedings{10.5555/2525512.2525517,
author = {Sinnott, Richard O. and Bayliss, Christopher and Morandini, Luca and Tomko, Martin},
title = {Tools and Processes to Support the Development of a National Platform for Urban Research: Lessons (Being) Learnt from the AURIN Project},
year = {2013},
isbn = {9781921770258},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {The development of large-scale software systems remains a non-trivial endeavour. This is especially so when the software systems comprise services and resources coming from multiple distributed software groups, and where they are required to interoperate with heterogeneous, independent (and autonomous) distributed data providers. The use of software development and management tools to support this process is highly desirable. In this paper we focus on the software development and management systems that have been adopted within the national Australian Urban Research Infrastructure Network (AURIN - www.aurin.org.au) project. AURIN is tasked with developing a software platform to support research into the urban and built environment - a domain with many diverse software system and data needs. In particular, given that AURIN is tasked with integrating a large portfolio of sub-projects offering both software and data that needs to be integrated, deployed and managed by a core team at the University of Melbourne, we illustrate how tooling and support processes are used to manage the software development lifecycle and code/data integration from the distributed teams and data providers that are involved. Results from the project demonstrating the ongoing status are presented.},
booktitle = {Proceedings of the Eleventh Australasian Symposium on Parallel and Distributed Computing - Volume 140},
pages = {39–48},
numpages = {10},
keywords = {collaborative development environment, urban research, code management, software testing},
location = {Adelaide, Australia},
series = {AusPDC '13}
}

@inbook{10.1145/3387940.3391465,
author = {Kim, Seah and Yoo, Shin},
title = {Evaluating Surprise Adequacy for Question Answering},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391465},
abstract = {With the wide and rapid adoption of Deep Neural Networks (DNNs) in various domains, an urgent need to validate their behaviour has risen, resulting in various test adequacy metrics for DNNs. One of the metrics, Surprise Adequacy (SA), aims to measure how surprising a new input is based on the similarity to the data used for training. While SA has been evaluated to be effective for image classifiers based on Convolutional Neural Networks (CNNs), it has not been studied for the Natural Language Processing (NLP) domain. This paper applies SA to NLP, in particular to the question answering task: the aim is to investigate whether SA correlates well with the correctness of answers. An empirical evaluation using the widely used Stanford Question Answering Dataset (SQuAD) shows that SA can work well as a test adequacy metric for the question answering task.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {197–202},
numpages = {6}
}

@article{10.1145/3375572.3375582,
author = {Fontana, Francesca Arcelli and Perrouin, Gilles and Ampatzoglou, Apostolos and Archer, Mathieu and Walter, Bartosz and Cordy, Maxime and Palomba, Fabio and Devroey, Xavier},
title = {MALTESQUE 2019 Workshop Summary},
year = {2020},
issue_date = {January 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/3375572.3375582},
doi = {10.1145/3375572.3375582},
abstract = {Welcome to the third edition of the workshop on Machine Learn- ing Techniques for Software Quality Evaluation (MaLTeSQuE 2019), held in Tallinn, Estonia, August 27th, 2019, co-located with ESEC / FSE 2019. This year MALTESQUE merged with the MASES (Machine Learning and Software Engineering in Symbiosis) work- shop, co-located with the ASE 2018 conference. Ten papers from all over the world were submitted, seven of them were accepted. The program also featured a keynote by Lionel Briand on the use of machine learning to improve software testing.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {34–35},
numpages = {2}
}

@inproceedings{10.1145/2031759.2031770,
author = {Ridene, Youssef and Barbier, Franck},
title = {A Model-Driven Approach for Automating Mobile Applications Testing},
year = {2011},
isbn = {9781450306188},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2031759.2031770},
doi = {10.1145/2031759.2031770},
abstract = {Software testing faces up several challenges. One out of these is the opposition between time-to-market software delivery and the excessive length of testing activities. The latter results from the growth of the application complexity along with the diversity of handheld devices. The economical competition, branding impose zero-defect products, putting forward testing as an even more crucial activity. In this paper, we describe a Domain-Specific Modeling Language (DSML) built upon an industrial platform (a test bed) which aims to automate mobile application checking. A key characteristic of this DSML is its ability to cope with variability in the spirit of software product line engineering. We discuss this DSML as part of a tool suite enabling the test of remote devices having variable features.},
booktitle = {Proceedings of the 5th European Conference on Software Architecture: Companion Volume},
articleno = {9},
numpages = {7},
keywords = {variability management, testing, mobile software, model-driven engineering},
location = {Essen, Germany},
series = {ECSA '11}
}

@inbook{10.1145/3377812.3382166,
author = {Zhang, Chengyu},
title = {Stress Testing SMT Solvers via Type-Aware Mutation},
year = {2020},
isbn = {9781450371223},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377812.3382166},
abstract = {This paper introduces type-aware mutation, a simple, but effective methodology for stress testing Satisfiability Modulo Theories (SMT) solvers. The key idea is mutating the operators of the formula to generate test inputs for differential testing, while considering the types of the operators to ensure the mutants are still valid. The realization of type-aware mutation was evaluated on finding bugs in two state-of-the-art SMT solvers, Z3 and CVC4. During the three months of empirical evaluation, 101 unique, confirmed bugs were found by type-aware mutation, and 87 of them have been fixed. The testing efforts and bugs were well-appreciated by the developers.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering: Companion Proceedings},
pages = {119–121},
numpages = {3}
}

@inproceedings{10.1145/3368089.3417067,
author = {Li, Linyi and Li, Zhenwen and Zhang, Weijie and Zhou, Jun and Wang, Pengcheng and Wu, Jing and He, Guanghua and Zeng, Xia and Deng, Yuetang and Xie, Tao},
title = {Clustering Test Steps in Natural Language toward Automating Test Automation},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3417067},
doi = {10.1145/3368089.3417067},
abstract = {For large industrial applications, system test cases are still often described in natural language (NL), and their number can reach thousands. Test automation is to automatically execute the test cases. Achieving test automation typically requires substantial manual effort for creating executable test scripts from these NL test cases. In particular, given that each NL test case consists of a sequence of NL test steps, testers first implement a test API method for each test step and then write a test script for invoking these test API methods sequentially for test automation. Across different test cases, multiple test steps can share semantic similarities, supposedly mapped to the same API method. However, due to numerous test steps in various NL forms under manual inspection, testers may not realize those semantically similar test steps and thus waste effort to implement duplicate test API methods for them. To address this issue, in this paper, we propose a new approach based on natural language processing to cluster similar NL test steps together such that the test steps in each cluster can be mapped to the same test API method. Our approach includes domain-specific word embedding training along with measurement based on Relaxed Word Mover’sDistance to analyze the similarity of test steps. Our approach also includes a technique to combine hierarchical agglomerative clustering and K-means clustering post-refinement to derive high-quality and manually-adjustable clustering results. The evaluation results of our approach on a large industrial mobile app, WeChat, show that our approach can cluster the test steps with high accuracy, substantially reducing the number of clusters and thus reducing the downstream manual effort. In particular, compared with the baseline approach, our approach achieves 79.8% improvement on cluster quality, reducing 65.9% number of clusters, i.e., the number of test API methods to be implemented.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {1285–1295},
numpages = {11},
keywords = {software testing, natural language processing, Clustering},
location = {Virtual Event, USA},
series = {ESEC/FSE 2020}
}

@inproceedings{10.1145/974044.974046,
author = {Avritzer, Alberto and Ros, Johannes P. and Weyuker, Elaine J.},
title = {Estimating the CPU Utilization of a Rule-Based System},
year = {2004},
isbn = {1581136730},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/974044.974046},
doi = {10.1145/974044.974046},
abstract = {Rule-based software systems have become very common in telecommunications settings, particularly to monitor and control workflow management of large networks. At the same time, shorter deployment cycles are frequently necessary which has led to modifications being made to the rule base, without a full assessment of the impact of these new rules through extensive performance testing.An approach is presented that helps assess the performance of rule-based systems, in terms of its CPU utilization, by using modeling and analysis. A case study is presented applying this approach to a large rule-based system that is used to monitor a very large industrial telecommunications network.},
booktitle = {Proceedings of the 4th International Workshop on Software and Performance},
pages = {1–12},
numpages = {12},
keywords = {software performance testing, software testing, workload characterization},
location = {Redwood Shores, California},
series = {WOSP '04}
}

@article{10.1145/974043.974046,
author = {Avritzer, Alberto and Ros, Johannes P. and Weyuker, Elaine J.},
title = {Estimating the CPU Utilization of a Rule-Based System},
year = {2004},
issue_date = {January 2004},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {29},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/974043.974046},
doi = {10.1145/974043.974046},
abstract = {Rule-based software systems have become very common in telecommunications settings, particularly to monitor and control workflow management of large networks. At the same time, shorter deployment cycles are frequently necessary which has led to modifications being made to the rule base, without a full assessment of the impact of these new rules through extensive performance testing.An approach is presented that helps assess the performance of rule-based systems, in terms of its CPU utilization, by using modeling and analysis. A case study is presented applying this approach to a large rule-based system that is used to monitor a very large industrial telecommunications network.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {1–12},
numpages = {12},
keywords = {software performance testing, software testing, workload characterization}
}

@inproceedings{10.1145/3322798.3329253,
author = {Kim, Youngsoo and Kim, Jonghyun and Kim, Ikkyun and Kim, Hyunchul},
title = {Real-Time Multi-Process Tracing Decoder Architecture},
year = {2019},
isbn = {9781450367615},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3322798.3329253},
doi = {10.1145/3322798.3329253},
abstract = {Tracing is a form of logging by recording the execution information of programs. Since a large amount of data must be created and decoded in real time, a tracer composed mainly of dedicated hardware is widely used. Intel® PT records all information related to software execution from each hardware thread. When the execution of the corresponding software is completed, the accurate program flow can be indicated through the recorded trace data. The hardware trace program can be integrated into the operating system, but in the case of the Windows system, the kernel is not disclosed so tight integration is not achieved. Also, in a Windows environment, it can only trace a single process and do not provide a way to trace multiple process streams. In this paper, we propose a way of extending the PT trace program in order to overcome this shortcoming by supporting multi-process stream tracing in Windows environment.},
booktitle = {Proceedings of the ACM Workshop on Systems and Network Telemetry and Analytics},
pages = {49–52},
numpages = {4},
keywords = {processor trace, multi-stream decoder, software testing},
location = {Phoenix, AZ, USA},
series = {SNTA '19}
}

