@inproceedings{10.1109/ICSE.2017.26,
author = {Thom\'{e}, Julian and Shar, Lwin Khin and Bianculli, Domenico and Briand, Lionel},
title = {Search-Driven String Constraint Solving for Vulnerability Detection},
year = {2017},
isbn = {9781538638682},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE.2017.26},
doi = {10.1109/ICSE.2017.26},
abstract = {Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-the-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one; this leads to limited effectiveness in finding vulnerabilities.In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support.We have implemented the proposed search-driven constraint solving technique in the ACO-Solver tool, which we have evaluated in the context of injection and XSS vulnerability detection for Java Web applications. We have assessed the benefits and costs of combining the proposed technique with two state-of-the-art constraint solvers (Z3-str2 and CVC4). The experimental results, based on a benchmark with 104 constraints derived from nine realistic Web applications, show that our approach, when combined in a state-of-the-art solver, significantly improves the number of detected vulnerabilities (from 4.7% to 71.9% for Z3-str2, from 85.9% to 100.0% for CVC4), and solves several cases on which the solver fails when used stand-alone (46 more solved cases for Z3-str2, and 11 more for CVC4), while still keeping the execution time affordable in practice.},
booktitle = {Proceedings of the 39th International Conference on Software Engineering},
pages = {198–208},
numpages = {11},
keywords = {string constraint solving, vulnerability detection, search-based software engineering},
location = {Buenos Aires, Argentina},
series = {ICSE '17}
}

@inproceedings{10.1145/2095050.2095099,
author = {Kell, Stephen and Irwin, Conrad},
title = {Virtual Machines Should Be Invisible},
year = {2011},
isbn = {9781450311830},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2095050.2095099},
doi = {10.1145/2095050.2095099},
abstract = {Current VM designs prioritise implementor freedom and performance, at the expense of other concerns of the end programmer. We motivate an alternative approach to VM design aiming to be unobtrusive in general, and prioritising two key concerns specifically: foreign function interfacing and support for runtime analysis tools (such as debuggers, profilers etc.). We describe our experiences building a Python VM in this manner, and identify some simple constraints that help enable low-overhead foreign function interfacing and direct use of native tools. We then discuss how to extend this towards a higher-performance VM suitable for Java or similar languages.},
booktitle = {Proceedings of the Compilation of the Co-Located Workshops on DSM'11, TMC'11, AGERE! 2011, AOOPES'11, NEAT'11, &amp; VMIL'11},
pages = {289–296},
numpages = {8},
keywords = {native code, dwarf, debugging, python, ffi, interoperability},
location = {Portland, Oregon, USA},
series = {SPLASH '11 Workshops}
}

@inproceedings{10.5555/2663370.2663380,
author = {Betz, Robin M. and Walker, Ross C.},
title = {Implementing Continuous Integration Software in an Established Computational Chemistry Software Package},
year = {2013},
isbn = {9781467362610},
publisher = {IEEE Press},
abstract = {Continuous integration is the software engineering principle of rapid and automated development and testing. We identify several key points of continuous integration and demonstrate how they relate to the needs of computational science projects by discussing the implementation and relevance of these principles to AMBER, a large and widely used molecular dynamics software package. The use of a continuous integration server has both improved collaboration and communication between AMBER developers, who are globally distributed, as well as making failure and benchmark information that would be time consuming for individual developers to obtain by themselves, available in real time. Continuous integration servers currently available are aimed at the software engineering community and can be difficult to adapt to the needs of computational science projects, however as demonstrated in this paper the effort payoff can be rapid since uncommon errors are found and contributions from geographically separated researchers are unified into one easily-accessible web-based interface.},
booktitle = {Proceedings of the 5th International Workshop on Software Engineering for Computational Science and Engineering},
pages = {68–74},
numpages = {7},
location = {San Francisco, California},
series = {SE-CSE '13}
}

@inproceedings{10.5555/2819261.2819285,
author = {Winter, Stefan and Piper, Thorsten and Schwahn, Oliver and Natella, Roberto and Suri, Neeraj and Cotroneo, Domenico},
title = {GRINDER: On Reusability of Fault Injection Tools},
year = {2015},
publisher = {IEEE Press},
abstract = {Fault Injection (FI) is an established testing technique to assess the fault-tolerance of computer systems. FI tests are usually highly automated for efficiency and to prevent human error from affecting result reliability. Most existing FI automation tools have been built for a specific application domain, i.e., a certain system under test (SUT) and fault types to test the SUT against, which significantly restricts their reusability.To improve reusability, generalist fault injection tools have been developed to decouple SUT-independent functionality from SUT-specific code. Unfortunately, existing generalist tools often embed subtle and implicit assumptions about the target system that affect their reusability. Furthermore, no assessments have been conducted how much effort the SUT-specific adaptation of generalist tools entails in comparison to re-implementation from scratch. In this paper, we present GRINDER, an open-source, highly-reusable FI tool, and report on its applicability in two very different systems (the Android OS in an emulated environment, and a real-time AUTOSAR system) under four different FI scenarios.},
booktitle = {Proceedings of the 10th International Workshop on Automation of Software Test},
pages = {75–79},
numpages = {5},
keywords = {software reuse, fault injection, test automation, robustness testing, test tools},
location = {Florence, Italy},
series = {AST '15}
}

@inproceedings{10.1145/1941553.1941583,
author = {Wang, Zhaoguo and Liu, Ran and Chen, Yufei and Wu, Xi and Chen, Haibo and Zhang, Weihua and Zang, Binyu},
title = {COREMU: A Scalable and Portable Parallel Full-System Emulator},
year = {2011},
isbn = {9781450301190},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1941553.1941583},
doi = {10.1145/1941553.1941583},
abstract = {This paper presents the open-source COREMU, a scalable and portable parallel emulation framework that decouples the complexity of parallelizing full-system emulators from building a mature sequential one. The key observation is that CPU cores and devices in current (and likely future) multiprocessors are loosely-coupled and communicate through well-defined interfaces. Based on this observation, COREMU emulates multiple cores by creating multiple instances of existing sequential emulators, and uses a thin library layer to handle the inter-core and device communication and synchronization, to maintain a consistent view of system resources. COREMU also incorporates lightweight memory transactions, feedback-directed scheduling, lazy code invalidation and adaptive signal control to provide scalable performance. To make COREMU useful in practice, we also provide some preliminary tools and APIs that can help programmers to diagnose performance problems and (concurrency) bugs. A working prototype, which reuses the widely-used QEMU as the sequential emulator, is with only 2500 lines of code (LOCs) changes to QEMU. It currently supports x64 and ARM platforms, and can emulates up to 255 cores running commodity OSes with practical performance, while QEMU cannot scale above 32 cores. A set of performance evaluation against QEMU indicates that, COREMU has negligible uniprocessor emulation overhead, performs and scales significantly better than QEMU. We also show how COREMU could be used to diagnose performance problems and concurrency bugs of both OS kernel and parallel applications.},
booktitle = {Proceedings of the 16th ACM Symposium on Principles and Practice of Parallel Programming},
pages = {213–222},
numpages = {10},
keywords = {full-system emulator, parallel emulator, multicore},
location = {San Antonio, TX, USA},
series = {PPoPP '11}
}

@article{10.1145/2038037.1941583,
author = {Wang, Zhaoguo and Liu, Ran and Chen, Yufei and Wu, Xi and Chen, Haibo and Zhang, Weihua and Zang, Binyu},
title = {COREMU: A Scalable and Portable Parallel Full-System Emulator},
year = {2011},
issue_date = {August 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/2038037.1941583},
doi = {10.1145/2038037.1941583},
abstract = {This paper presents the open-source COREMU, a scalable and portable parallel emulation framework that decouples the complexity of parallelizing full-system emulators from building a mature sequential one. The key observation is that CPU cores and devices in current (and likely future) multiprocessors are loosely-coupled and communicate through well-defined interfaces. Based on this observation, COREMU emulates multiple cores by creating multiple instances of existing sequential emulators, and uses a thin library layer to handle the inter-core and device communication and synchronization, to maintain a consistent view of system resources. COREMU also incorporates lightweight memory transactions, feedback-directed scheduling, lazy code invalidation and adaptive signal control to provide scalable performance. To make COREMU useful in practice, we also provide some preliminary tools and APIs that can help programmers to diagnose performance problems and (concurrency) bugs. A working prototype, which reuses the widely-used QEMU as the sequential emulator, is with only 2500 lines of code (LOCs) changes to QEMU. It currently supports x64 and ARM platforms, and can emulates up to 255 cores running commodity OSes with practical performance, while QEMU cannot scale above 32 cores. A set of performance evaluation against QEMU indicates that, COREMU has negligible uniprocessor emulation overhead, performs and scales significantly better than QEMU. We also show how COREMU could be used to diagnose performance problems and concurrency bugs of both OS kernel and parallel applications.},
journal = {SIGPLAN Not.},
month = {feb},
pages = {213–222},
numpages = {10},
keywords = {multicore, full-system emulator, parallel emulator}
}

@inproceedings{10.1145/3275245.3275257,
author = {Siqueira, Bento R. and J\'{u}nior, Misael Costa and Ferrari, Fabiano C. and Santib\'{a}\~{n}ez, Daniel S. M. and Menotti, Ricardo and Camargo, Valter V.},
title = {Experimenting with a Multi-Approach Testing Strategy for Adaptive Systems},
year = {2018},
isbn = {9781450365659},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3275245.3275257},
doi = {10.1145/3275245.3275257},
abstract = {Context: Testing adaptive systems (ASs) is particularly challenging due to certain characteristics such as the high number of possible configurations, runtime adaptations and the interactions between the system and its surrounding environment. Therefore, the combination of different testing approaches in order to compose a strategy is expected to improve the quality of the designed test suites. Objective: To devise and experiment with a testing strategy for ASs that relies on particular characteristics of these systems. Method: We ranked testing approaches for ASs and devised a strategy that is composed of the three top-ranked ones. The rankings address the challenges that can be mitigated by the approaches, activities from a typical testing process, and characteristics observed in some AS implementations. The strategy was applied to two adaptive systems for mobile devices. Results: The approach was applied to both systems. We observed partial gains in terms of fault detection and structural coverage when results are analysed separately for each system, even though no improvements were obtained with the application of the third approach. Conclusion: The strategy, despite being incipient, is promising and motivates a deeper analysis of results and new experiment rounds. Furthermore, it can evolve as long as the rankings are updated with new approaches.},
booktitle = {Proceedings of the 17th Brazilian Symposium on Software Quality},
pages = {111–120},
numpages = {10},
keywords = {testing strategy, Adaptive systems, testing challenges},
location = {Curitiba, Brazil},
series = {SBQS}
}

@inproceedings{10.1145/2591062.2591192,
author = {Hoseini, Salman and Hamou-Lhadj, Abdelwahab and Desrosiers, Patrick and Tapp, Martin},
title = {Software Feature Location in Practice: Debugging Aircraft Simulation Systems},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591192},
doi = {10.1145/2591062.2591192},
abstract = { In this paper, we report on a study that we have conducted at CAE, one of the largest civil aircraft simulation companies in the world, in which we have developed a feature location approach to help software engineers debug simulation scenarios. A simulation scenario consists of a set of software components, configured in a certain way. A simulation fails when it does not behave as intended. This is typically a sign of a configuration problem. To detect configuration errors, we propose FELODE (Feature Location for Debugging), an approach that uses a single trace combined with user queries. When applied to CAE systems, FELODE achieves in average a precision of 50% and a recall of up to 100%. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {225–234},
numpages = {10},
keywords = {Avionic Systems, Debugging, Trace Analysis, Feature Location},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/2591062.2591179,
author = {Vierhauser, Michael and Rabiser, Rick and Gr\"{u}nbacher, Paul},
title = {A Case Study on Testing, Commissioning, and Operation of Very-Large-Scale Software Systems},
year = {2014},
isbn = {9781450327688},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2591062.2591179},
doi = {10.1145/2591062.2591179},
abstract = { An increasing number of software systems today are very-large-scale software systems (VLSS) with system-of-systems (SoS) architectures. Due to their heterogeneity and complexity VLSS are difficult to understand and analyze, which results in various challenges for development and evolution. Existing software engineering processes, methods, and tools do not sufficiently address the characteristics of VLSS. Also, there are only a few empirical studies on software engineering for VLSS. We report on results of an exploratory case study involving engineers and technical project managers of an industrial automation VLSS for metallurgical plants. The paper provides empirical evidence on how VLSS are tested, commissioned, and operated in practice. The paper discusses practical challenges and reports industrial requirements regarding process and tool support. In particular, software processes and tools need to provide general guidance at the VLSS level as well as specific methods and tools for systems that are part of the VLSS. Processes and tools need to support multi-disciplinary engineering across system boundaries. Furthermore, managing variability and evolution is success-critical in VLSS verification and validation. },
booktitle = {Companion Proceedings of the 36th International Conference on Software Engineering},
pages = {125–134},
numpages = {10},
keywords = {Case Study, Verification and Validation, Very-Large-Scale Software Systems},
location = {Hyderabad, India},
series = {ICSE Companion 2014}
}

@inproceedings{10.1145/3445814.3446727,
author = {Ahmad, Adil and Lee, Sangho and Fonseca, Pedro and Lee, Byoungyoung},
title = {Kard: Lightweight Data Race Detection with per-Thread Memory Protection},
year = {2021},
isbn = {9781450383172},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3445814.3446727},
doi = {10.1145/3445814.3446727},
abstract = {Finding data race bugs in multi-threaded programs has proven challenging. A promising direction is to use dynamic detectors that monitor the program’s execution for data races. However, despite extensive work on dynamic data race detection, most proposed systems for commodity hardware incur prohibitive overheads due to expensive compiler instrumentation of memory accesses; hence, they are not efficient enough to be used in all development and testing settings.  KARD is a lightweight system that dynamically detects data races caused by inconsistent lock usage—when a program concurrently accesses the same memory object using different locks or only some of the concurrent accesses are synchronized using a common lock. Unlike existing detectors, KARD does not monitor memory accesses using expensive compiler instrumentation. Instead, KARD leverages commodity per-thread memory protection, Intel Memory Protection Keys (MPK). Using MPK, KARD ensures that a shared object is only accessible to a single thread in its critical section, and captures all violating accesses from other concurrent threads. KARD overcomes various limitations of MPK by introducing key-enforced race detection, employing consolidated unique page allocation, carefully managing protection keys, and automatically pruning out non-racy or redundant violations. Our evaluation shows that KARD detects all data races caused by inconsistent lock usage and has a low geometric mean execution time overhead: 7.0% on PARSEC and SPLASH-2x benchmarks and 5.3% on a set of real-world applications (NGINX, memcached, pigz, and Aget).},
booktitle = {Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {647–660},
numpages = {14},
keywords = {memory protection, data race, lock, concurrency},
location = {Virtual, USA},
series = {ASPLOS 2021}
}

@inproceedings{10.1145/2491411.2491450,
author = {Machiry, Aravind and Tahiliani, Rohan and Naik, Mayur},
title = {Dynodroid: An Input Generation System for Android Apps},
year = {2013},
isbn = {9781450322379},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2491411.2491450},
doi = {10.1145/2491411.2491450},
abstract = { We present a system Dynodroid for generating relevant inputs to unmodified Android apps. Dynodroid views an app as an event-driven program that interacts with its environment by means of a sequence of events through the Android framework. By instrumenting the framework once and for all, Dynodroid monitors the reaction of an app upon each event in a lightweight manner, using it to guide the generation of the next event to the app. Dynodroid also allows interleaving events from machines, which are better at generating a large number of simple inputs, with events from humans, who are better at providing intelligent inputs.  We evaluated Dynodroid on 50 open-source Android apps, and compared it with two prevalent approaches: users manually exercising apps, and Monkey, a popular fuzzing tool. Dynodroid, humans, and Monkey covered 55%, 60%, and 53%, respectively, of each app's Java source code on average. Monkey took 20X more events on average than Dynodroid. Dynodroid also found 9 bugs in 7 of the 50 apps, and 6 bugs in 5 of the top 1,000 free apps on Google Play. },
booktitle = {Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering},
pages = {224–234},
numpages = {11},
keywords = {Android, testing event-driven programs, GUI testing},
location = {Saint Petersburg, Russia},
series = {ESEC/FSE 2013}
}

@inbook{10.1145/3468264.3468600,
author = {Wong, Chu-Pan and Santiesteban, Priscila and K\"{a}stner, Christian and Le Goues, Claire},
title = {VarFix: Balancing Edit Expressiveness and Search Effectiveness in Automated Program Repair},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468600},
abstract = {Automatically repairing a buggy program is essentially a search problem, searching for code transformations that pass a set of tests. Various search strategies have been explored, but they either navigate the search space in an ad hoc way using heuristics, or systemically but at the cost of limited edit expressiveness in the kinds of supported program edits. In this work, we explore the possibility of systematically navigating the search space without sacrificing edit expressiveness. The key enabler of this exploration is variational execution, a dynamic analysis technique that has been shown to be effective at exploring many similar executions in large search spaces. We evaluate our approach on IntroClassJava and Defects4J, showing that a systematic search is effective at leveraging and combining fixing ingredients to find patches, including many high-quality patches and multi-edit patches.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {354–366},
numpages = {13}
}

@inproceedings{10.1145/2307636.2307661,
author = {Pathak, Abhinav and Jindal, Abhilash and Hu, Y. Charlie and Midkiff, Samuel P.},
title = {What is Keeping My Phone Awake? Characterizing and Detecting No-Sleep Energy Bugs in Smartphone Apps},
year = {2012},
isbn = {9781450313018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2307636.2307661},
doi = {10.1145/2307636.2307661},
abstract = {Despite their immense popularity in recent years, smartphones are and will remain severely limited by their battery life. Preserving this critical resource has driven smartphone OSes to undergo a paradigm shift in power management: by default every component, including the CPU, stays off or in an idle state, unless the app explicitly instructs the OS to keep it on! Such a policy encumbers app developers to explicitly juggle power control APIs exported by the OS to keep the components on, during their active use by the app and off otherwise. The resulting power-encumbered programming unavoidably gives rise to a new class of software energy bugs on smartphones called no-sleep bugs, which arise from mis-handling power control APIs by apps or the framework and result in significant and unexpected battery drainage.This paper makes the first advances towards understanding and automatically detecting software energy bugs on smartphones. It makes the following three contributions: (1) we present the first comprehensive study of real world no-sleep energy bug characteristics; (2) we propose the first automatic solution to detect these bugs based on the classic reaching definitions dataflow analysis algorithm; (3) we provide experimental data showing that our tool accurately detected all 17 known instances of no-sleep bugs and found 34 new bugs in the 73 apps examined.},
booktitle = {Proceedings of the 10th International Conference on Mobile Systems, Applications, and Services},
pages = {267–280},
numpages = {14},
keywords = {smartphones, energy-bug, mobile, energy, nosleep-bug},
location = {Low Wood Bay, Lake District, UK},
series = {MobiSys '12}
}

@inproceedings{10.1145/2508075.2514881,
author = {Ohmann, Peter},
title = {CSI: Crash Scene Investigation},
year = {2013},
isbn = {9781450319959},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2508075.2514881},
doi = {10.1145/2508075.2514881},
abstract = {Prior work proposes inexpensive, tunable tracing of acyclic paths and callsite coverage to enhance post-failure memory dumps. To better understand this data, current work investigates the benefit of each piece of traced data independently, their interplay, future low-cost data to collect, and further analysis uses of the post-mortem data.},
booktitle = {Proceedings of the 2013 Companion Publication for Conference on Systems, Programming, &amp; Applications: Software for Humanity},
pages = {123–124},
numpages = {2},
keywords = {core dumps, program slicing, failure analysis},
location = {Indianapolis, Indiana, USA},
series = {SPLASH '13}
}

@inproceedings{10.1145/2839509.2844626,
author = {Politz, Joe Gibbs and Collard, Joseph M. and Guha, Arjun and Fisler, Kathi and Krishnamurthi, Shriram},
title = {The Sweep: Essential Examples for In-Flow Peer Review},
year = {2016},
isbn = {9781450336857},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2839509.2844626},
doi = {10.1145/2839509.2844626},
abstract = {In in-flow peer review, students provide feedback to one another on intermediate artifacts on their way to a final submission. Prior work has studied examples and tests as a potentially useful initial artifact for review. Unfortunately, large test suites are onerous to produce and especially to review. We instead propose the notion of a sweep, an artificially constrained set of tests that illustrates common and interesting behavior. We present experimental data across several courses that show that sweeps have reasonable quality, and are also a good target for peer review; for example, students usually (over half the time) suggest new tests to one another in a review.},
booktitle = {Proceedings of the 47th ACM Technical Symposium on Computing Science Education},
pages = {243–248},
numpages = {6},
keywords = {testing, example-first programming, peer review, code review},
location = {Memphis, Tennessee, USA},
series = {SIGCSE '16}
}

@inproceedings{10.1145/3106195.3106210,
author = {Markiegi, Urtzi and Arrieta, Aitor and Sagardui, Goiuria and Etxeberria, Leire},
title = {Search-Based Product Line Fault Detection Allocating Test Cases Iteratively},
year = {2017},
isbn = {9781450352215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106195.3106210},
doi = {10.1145/3106195.3106210},
abstract = {The large number of possible configurations makes it unfeasible to test every single system variant in a product line. Consequently, a small subset of the product line products must be selected, typically following combinatorial interaction testing approaches. Recently, many product line engineering approaches have considered the selection and prioritization of relevant products within the product line. In a further step, these products are thoroughly tested individually. However, the test cases that must be executed in each of the products are not always insignificant, and in systems such as Cyber-Physical System Product Lines (CPSPLs), their test execution time can vary from tens to thousands of seconds. This issue leads to spending a lot of time testing each individual product. To solve this problem we propose a search-based approach to perform the testing of product lines by allocating small number of test cases in each of the products. This approach increases the probability of detecting faults faster. Specifically, our search-based approach obtains a set of products, which are derived from using any state-of-the-art approach as inputs, and a set of attributed test cases. As an output a list of allocated test cases for each product is obtained. We also define a novel fitness function to guide the search and we propose corresponding crossover and mutation operators. The search and test process is iteratively repeated until the time budget is consumed. We performed an evaluation with a CPSPL as a case study. Results suggest that our approach can reduce the fault detection time by 61% and 65% on average when compared with the traditional test process and the Random Search algorithm respectively.},
booktitle = {Proceedings of the 21st International Systems and Software Product Line Conference - Volume A},
pages = {123–132},
numpages = {10},
keywords = {Fault Detection, Product Line Testing, Search-based Software Engineering},
location = {Sevilla, Spain},
series = {SPLC '17}
}

@inproceedings{10.1109/IWSESS.2009.5068454,
author = {El-Ghali, M. and Masri, W.},
title = {Intrusion Detection Using Signatures Extracted from Execution Profiles},
year = {2009},
isbn = {9781424437252},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/IWSESS.2009.5068454},
doi = {10.1109/IWSESS.2009.5068454},
abstract = {An application based intrusion detection system is a security mechanism designed to detect malicious behavior that could compromise the security of a software application. Our aim is to develop such a system that operates on signatures extracted from execution profiles. These signatures are not descriptions of exploits, but instead are descriptions of the program conditions that lead to the exploitation of software vulnerabilities, i.e., they depend on the structure of the vulnerabilities themselves. A program vulnerability is generally induced by the execution of a combination of program statements. In this work we first analyze the execution profiles of a subject application in order to identify such suspicious combinations and consequently extract and define their corresponding signatures. Then, we insert probes in select locations in the application to enable online signature matching. To evaluate our technique, we implemented it for Java programs and applied it on Tomcat 3.0 in order to detect well-known attacks. Our results were promising, as no false negatives and a maximum of 4.5% false positives were observed, and the runtime overhead was less than 5%.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Software Engineering for Secure Systems},
pages = {17–24},
numpages = {8},
keywords = {program statements, Java programs, program analysis, online signature matching, intrusion detection system, software security, security mechanism, software vulnerability, malicious behavior, execution profiles, Tomcat 3.0, signature extraction},
series = {IWSESS '09}
}

@article{10.1145/3360603,
author = {Celik, Ahmet and Nie, Pengyu and Rossbach, Christopher J. and Gligoric, Milos},
title = {Design, Implementation, and Application of GPU-Based Java Bytecode Interpreters},
year = {2019},
issue_date = {October 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {OOPSLA},
url = {https://doi.org/10.1145/3360603},
doi = {10.1145/3360603},
abstract = {We present the design and implementation of GVM, the first system for executing Java bytecode entirely on GPUs. GVM is ideal for applications that execute a large number of short-living tasks, which share a significant fraction of their codebase and have similar execution time. GVM uses novel algorithms, scheduling, and data layout techniques to adapt to the massively parallel programming and execution model of GPUs. We apply GVM to generate and execute tests for Java projects. First, we implement a sequence-based test generation on top of GVM and design novel algorithms to avoid redundant test sequences. Second, we use GVM to execute randomly generated test cases. We evaluate GVM by comparing it with two existing Java bytecode interpreters (Oracle JVM and Java Pathfinder), as well as with the Oracle JVM with just-in-time (JIT) compiler, which has been engineered and optimized for over twenty years. Our evaluation shows that sequence-based test generation on GVM outperforms both Java Pathfinder and Oracle JVM interpreter. Additionally, our results show that GVM performs as well as running our parallel sequence-based test generation algorithm using JVM with JIT with many CPU threads. Furthermore, our evaluation on several classes from open-source projects shows that executing randomly generated tests on GVM outperforms sequential execution on JVM interpreter and JVM with JIT.},
journal = {Proc. ACM Program. Lang.},
month = {oct},
articleno = {177},
numpages = {28},
keywords = {Complete matching, Graphics Processing Unit, Shape matching, Java bytecode interpreter, Sequence-based test generation}
}

@inproceedings{10.1145/2304696.2304714,
author = {Yu, Jian and Han, Jun and Schneider, Jean-Guy and Hine, Cameron and Versteeg, Steve},
title = {A Virtual Deployment Testing Environment for Enterprise Software Systems},
year = {2012},
isbn = {9781450313469},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2304696.2304714},
doi = {10.1145/2304696.2304714},
abstract = {Modern enterprise software systems often need to interact with a large number of heterogeneous systems in an enterprise IT environment. The distributedness, large-scale-ness, and heterogeneity of such environment makes it difficult to test a system's quality attributes such as performance and scalability before it is actually deployed in the environment. In this paper, we present a Coloured Petri nets (CPN) based system behaviour emulation approach and a lightweight virtual testing framework for provisioning the deployment testing environment of an enterprise system so that its quality attributes, especially scalability, can be evaluated without physically connecting to the real production environment. This testing environment is scalable and has a flexible pluggable architecture to support the emulation of the behaviour of heterogeneous systems in the environment. To validate the feasibility of this approach, a CPN emulation model for LDAP has been developed and applied in testing the scalability of a real-life identity management system. An in-lab performance study has been conducted to demonstrate the effectiveness of this approach.},
booktitle = {Proceedings of the 8th International ACM SIGSOFT Conference on Quality of Software Architectures},
pages = {101–110},
numpages = {10},
keywords = {deployment testing, petri nets, system emulation, enterprise software systems},
location = {Bertinoro, Italy},
series = {QoSA '12}
}

@inproceedings{10.5555/776816.776824,
author = {Harder, Michael and Mellen, Jeff and Ernst, Michael D.},
title = {Improving Test Suites via Operational Abstraction},
year = {2003},
isbn = {076951877X},
publisher = {IEEE Computer Society},
address = {USA},
abstract = {This paper presents the operational difference technique for generating, augmenting, and minimizing test suites. The technique is analogous to structural code coverage techniques, but it operates in the semantic domain of program properties rather than the syntactic domain of program text.The operational difference technique automatically selects test cases; it assumes only the existence of a source of test cases. The technique dynamically generates operational abstractions (which describe observed behavior and are syntactically identical to formal specifications) from test suite executions. Test suites can be generated by adding cases until the operational abstraction stops changing. The resulting test suites are as small, and detect as many faults, as suites with 100% branch coverage, and are better at detecting certain common faults.This paper also presents the area and stacking techniques for comparing test suite generation strategies; these techniques avoid bias due to test suite size.},
booktitle = {Proceedings of the 25th International Conference on Software Engineering},
pages = {60–71},
numpages = {12},
location = {Portland, Oregon},
series = {ICSE '03}
}

@inproceedings{10.1145/3422392.3422511,
author = {de Almeida, Diego Rodrigues and Machado, Patr\'{\i}cia D. L. and Andrade, Wilkerson L.},
title = {ENVIAR: ENVIronment DAta SimulatoR},
year = {2020},
isbn = {9781450387538},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3422392.3422511},
doi = {10.1145/3422392.3422511},
abstract = {Context-aware applications (CAAs) sense and react to changes from the environment when performing their tasks. Testing such applications is challenging. Firstly, due to the number of combinations of possible events and values they can handle. Secondly, due to the different execution scenarios that may be hard to reproduce manually. We present the ENVIAR tool whose objective is to support test case generation and execution of Android CAAs. Generation combines a set of events that may trigger failure in such applications filtered by pairwise testing. Execution simulates the environment in which the CAA runs by sending mock data. Current version focus on Android events, GPS, and Internet sensor events and values. Empirical results have shown that the tool can generate test cases that detect context defects in real Android applications, particularly covering scenarios that would be tricky to run in a real environment.},
booktitle = {Proceedings of the 34th Brazilian Symposium on Software Engineering},
pages = {532–537},
numpages = {6},
keywords = {Testing automation, Context-aware application, Android},
location = {Natal, Brazil},
series = {SBES '20}
}

