@inproceedings{10.1145/3524304.3524311,
author = {Zahir Ahmad, Mohd Zamri and Othman, Rozmie Razif and Ramli, Nuraminah and Rashid Ali, Mohd Shaiful Aziz},
title = {VS-TACO: A Tuned Version of Ant Colony Optimization for Generating Variable Strength Interaction in T-Way Testing Strategy},
year = {2022},
isbn = {9781450385770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524304.3524311},
doi = {10.1145/3524304.3524311},
abstract = {Ever since, software technologies have been through a rapid evolution. In a real application, the interaction between input variables may vary, thus the exhaustive testing is no longer practical since it is time-consuming and lead to combinatorial explosion. One of the strategies that able to cater fault due to the interaction is Ant Colony Optimization (ACO) algorithm. Typically, amount of ants in the ACO algorithm is fixed at certain number while the search space technique (i.e. to explore or exploit new possible solutions) is randomized for each iteration in the entire algorithm, are potentially affect the optimization's efficiency. Thus this paper proposes a new variant of ACO algorithm called as a tuned version of ACO for generating variable strength interaction in t-way testing strategy (VS-TACO). VS-TACO applied a Mamdani fuzzy logic in order to dynamically choose the number of ant and decide which search space technique to be used. Experiments that have been conducted on VS-TACO and benchmarked with other strategies, shows VS-TACO produce a competitive result in term of test suite size.},
booktitle = {2022 11th International Conference on Software and Computer Applications},
pages = {48–54},
numpages = {7},
keywords = {ant colony optimization algorithm, variable strength interaction testing, t-way testing},
location = {Melaka, Malaysia},
series = {ICSCA 2022}
}

@inproceedings{10.5555/1924943.1924955,
author = {Xiong, Weiwei and Park, Soyeon and Zhang, Jiaqi and Zhou, Yuanyuan and Ma, Zhiqiang},
title = {Ad Hoc Synchronization Considered Harmful},
year = {2010},
publisher = {USENIX Association},
address = {USA},
abstract = {Many synchronizations in existing multi-threaded programs are implemented in an ad hoc way. The first part of this paper does a comprehensive characteristic study of ad hoc synchronizations in concurrent programs. By studying 229 ad hoc synchronizations in 12 programs of various types (server, desktop and scientific), including Apache, MySQL, Mozilla, etc., we find several interesting and perhaps alarming characteristics: (1) Every studied application uses ad hoc synchronizations. Specifically, there are 6-83 ad hoc synchronizations in each program. (2) Ad hoc synchronizations are error-prone. Significant percentages (22-67%) of these ad hoc synchronizations introduced bugs or severe performance issues. (3) Ad hoc synchronization implementations are diverse and many of them cannot be easily recognized as synchronizations, i.e. have poor readability and maintainability.The second part of our work builds a tool called SyncFinder to automatically identify and annotate ad hoc synchronizations in concurrent programswritten in C/C++ to assist programmers in porting their code to better structured implementations, while also enabling other tools to recognize them as synchronizations. Our evaluation using 25 concurrent programs shows that, on average, SyncFinder can automatically identify 96% of ad hoc synchronizations with 6% false positives.We also build two use cases to leverage SyncFinder's auto-annotation. The first one uses annotation to detect 5 deadlocks (including 2 new ones) and 16 potential issues missed by previous analysis tools in Apache, MySQL and Mozilla. The second use case reduces Valgrind data race checker's false positive rates by 43-86%.},
booktitle = {Proceedings of the 9th USENIX Conference on Operating Systems Design and Implementation},
pages = {163–176},
numpages = {14},
location = {Vancouver, BC, Canada},
series = {OSDI'10}
}

@inproceedings{10.1145/1639622.1639624,
author = {Gopalakrishnan, Ganesh and Yang, Yu and Vakkalanka, Sarvani and Vo, Anh and Aananthakrishnan, Sriram and Szubzda, Grzegorz and Sawaya, Geof and Williams, Jason and Sharma, Subodh and DeLisi, Michael and Atzeni, Simone},
title = {Some Resources for Teaching Concurrency},
year = {2009},
isbn = {9781605586557},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1639622.1639624},
doi = {10.1145/1639622.1639624},
abstract = {With the increasing emphasis on exploiting concurrency efficiently and correctly, the lack of suitable pedagogical material for teaching concurrency is a growing problem. In this paper, we summarize a recently concluded class as well as some independent projects in the area of concurrency and multi-core computing that offer some insights to address this problem. We examine background papers, the teaching of low level concurrency, and the teaching of threading and message passing. The use of dynamic formal verification tools in a class setting is discussed in some detail. We conclude with a summary of pedagogical material being assembled, including exercises from a popular textbook on MPI solved using our dynamic verifier ISP. Our observation is that the teaching of concurrency is greatly facilitated by the use of dynamic push-button formal verification tools that can handle non-trivial concurrent programs. Given the growing number of publications on how to teach concurrency as well as employ new programming approaches, our work addresses the somewhat neglected topic of using modern dynamic formal verification methods within the context of widely used concurrency approaches and libraries.},
booktitle = {Proceedings of the 7th Workshop on Parallel and Distributed Systems: Testing, Analysis, and Debugging},
articleno = {2},
numpages = {6},
keywords = {computer science education, multi-core, concurrency, pthreads, dynamic verification, education, MPI, memory models},
location = {Chicago, Illinois},
series = {PADTAD '09}
}

@inbook{10.1109/ICSE-Companion52605.2021.00103,
author = {Makhshari, Amir and Mesbah, Ali},
title = {IoT Development in the Wild: Bug Taxonomy and Developer Challenges},
year = {2021},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-Companion52605.2021.00103},
abstract = {IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.},
booktitle = {Proceedings of the 43rd International Conference on Software Engineering: Companion Proceedings},
pages = {225–226},
numpages = {2}
}

@inproceedings{10.1145/3220134.3220138,
author = {Bowen, Judy and Khanal, Swikrit},
title = {Test Stub Generation from Interaction and Behavioural Models},
year = {2018},
isbn = {9781450358972},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3220134.3220138},
doi = {10.1145/3220134.3220138},
abstract = {Testing strategies for interactive systems require that we find suitable ways of incorporating tests of functionality with tests of interfaces and interactivity. For safety-critical interactive systems the problem is harder due to the necessity to ensure higher thresholds for testing to ensure safety is preserved. Conversely however, for safety-critical systems we are more likely to have a larger set of artefacts such as formal models, specifications etc. which can be used as the basis for test generation. The challenge is in finding ways of making use of (what may be a diverse set of) such models and specifications to assist with the testing process. In this paper we describe how we incorporate interactive system models with behavioural specifications to automatically generate test stubs. We show how the declarative test language Gherkin and its associated Cucumber tool can be integrated into an interactive system modelling environment to achieve this. The tool can either convert interaction models into behavioural models or vice versa, and both sets of models are then used to generate test stubs.},
booktitle = {Proceedings of the ACM SIGCHI Symposium on Engineering Interactive Computing Systems},
articleno = {7},
numpages = {6},
keywords = {Specifications, Testing, Behavioural Declarations, Interactive systems, Gherkin},
location = {Paris, France},
series = {EICS '18}
}

@inbook{10.1145/3278186.3278193,
author = {Hodov\'{a}n, Ren\'{a}ta and Kiss, \'{A}kos and Gyim\'{o}thy, Tibor},
title = {Grammarinator: A Grammar-Based Open Source Fuzzer},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278193},
abstract = {Fuzzing, or random testing, is an increasingly popular testing technique. The power of the approach lies in its ability to generate a large number of useful test cases without consuming expensive manpower. Furthermore, because of the randomness, it can often produce unusual cases that would be beyond the awareness of a human tester. In this paper, we present Grammarinator, a general purpose test generator tool that is able to utilize existing parser grammars as models. Since the model can act both as a parser and as a generator, the tool can provide the capabilities of both generation and mutation-based fuzzers. The presented tool is actively used to test various JavaScript engines and has found more than 100 unique issues.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {45–48},
numpages = {4}
}

@inproceedings{10.5555/602770.602889,
author = {Cheng, Doreen and Hood, Robert},
title = {A Portable Debugger for Parallel and Distributed Programs},
year = {1994},
isbn = {0818666056},
publisher = {IEEE Computer Society Press},
address = {Washington, DC, USA},
abstract = {We describe the design and implementation of a portable debugger for parallel and distributed programs. The design incorporates a client-server model in order to isolate non-portable debugger code from the user interface. The precise definition of a protocol for client-server interaction facilitates a high degree of client portability. Replication of server components permits the implementation of a debugger for distributed computations.Portability across message passing implementations is achieved with a protocol that specifies the interaction between a message-passing library and the debugger. This permits the same debugger to be used both on PVM and MPI programs.The process abstractions used for debugging message-passing programs can be adapted to debug HPF programs at the source level. This permits the meaningful display of information obscured in tool-generated code.},
booktitle = {Proceedings of the 1994 ACM/IEEE Conference on Supercomputing},
pages = {723–732},
numpages = {10},
location = {Washington, D.C.},
series = {Supercomputing '94}
}

@inproceedings{10.1109/ASE.2019.00057,
author = {Celik, Ahmet and Palmskog, Karl and Parovic, Marinela and Arias, Emilio Jes\'{u}s Gallego and Gligoric, Milos},
title = {Mutation Analysis for Coq},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00057},
doi = {10.1109/ASE.2019.00057},
abstract = {Mutation analysis, which introduces artificial defects into software systems, is the basis of mutation testing, a technique widely applied to evaluate and enhance the quality of test suites. However, despite the deep analogy between tests and formal proofs, mutation analysis has seldom been considered in the context of deductive verification. We propose mutation proving, a technique for analyzing verification projects that use proof assistants. We implemented our technique for the Coq proof assistant in a tool dubbed mCoq. mCoq applies a set of mutation operators to Coq definitions of functions and datatypes, inspired by operators previously proposed for functional programming languages. mCoq then checks proofs of lemmas affected by operator application. To make our technique feasible in practice, we implemented several optimizations in mCoq such as parallel proof checking. We applied mCoq to several medium and large scale Coq projects, and recorded whether proofs passed or failed when applying different mutation operators. We then qualitatively analyzed the mutants, finding many instances of incomplete specifications. For our evaluation, we made several improvements to serialization of Coq files and even discovered a notable bug in Coq itself, all acknowledged by developers. We believe mCoq can be useful both to proof engineers for improving the quality of their verification projects and to researchers for evaluating proof engineering techniques.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {539–551},
numpages = {13},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/2950290.2950344,
author = {Luo, Qi and Moran, Kevin and Poshyvanyk, Denys},
title = {A Large-Scale Empirical Comparison of Static and Dynamic Test Case Prioritization Techniques},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950344},
doi = {10.1145/2950290.2950344},
abstract = {The large body of existing research in Test Case Prioritization (TCP) techniques, can be broadly classified into two categories: dynamic techniques (that rely on run-time execution information) and static techniques (that operate directly on source and test code). Absent from this current body of work is a comprehensive study aimed at understanding and evaluating the static approaches and comparing them to dynamic approaches on a large set of projects. In this work, we perform the first extensive study aimed at empirically evaluating four static TCP techniques comparing them with state-of-research dynamic TCP techniques at different test-case granularities (e.g., method and class-level) in terms of effectiveness, efficiency and similarity of faults detected. This study was performed on 30 real-word Java programs encompassing 431 KLoC. In terms of effectiveness, we find that the static call-graph-based technique outperforms the other static techniques at test-class level, but the topic-model-based technique performs better at test-method level. In terms of efficiency, the static call-graph-based technique is also the most efficient when compared to other static techniques. When examining the similarity of faults detected for the four static techniques compared to the four dynamic ones, we find that on average, the faults uncovered by these two groups of techniques are quite dissimilar, with the top 10% of test cases agreeing on only 25% - 30% of detected faults. This prompts further research into the severity/importance of faults uncovered by these techniques, and into the potential for combining static and dynamic information for more effective approaches.},
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {559–570},
numpages = {12},
keywords = {dynamic, Regression testing, test case prioritization, static},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/3106237.3119876,
author = {Schuler, Andreas},
title = {Application of Search-Based Software Engineering Methodologies for Test Suite Optimization and Evolution in Mission Critical Mobile Application Development},
year = {2017},
isbn = {9781450351058},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3106237.3119876},
doi = {10.1145/3106237.3119876},
abstract = { The demand for high quality mobile applications is constantly rising, especially in mission critical settings. Thus, new software engineering methodologies are needed in order to ensure the desired quality of an application. The research presented proposes a quality assurance methodology for mobile applications through test automation by optimizing test suites. The desired goal is to find a minimal test suite while maintaining efficiency and reducing execution cost. Furthermore to avoid invalidating an optimized test suite as the system under test evolves, the approach further proposes to extract patterns from the applied changes to an application. The evaluation plan comprises a combination of an empirical and an industrial case study based on open source projects and an industrial project in the healthcare domain. It is expected that the presented approach supports the testing process on mobile application platforms. },
booktitle = {Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering},
pages = {1034–1037},
numpages = {4},
keywords = {mobile application development, test suite optimization, multi-objective optimization, test automation},
location = {Paderborn, Germany},
series = {ESEC/FSE 2017}
}

@inproceedings{10.1145/1188895.1188910,
author = {Di Fatta, Giuseppe and Leue, Stefan and Stegantova, Evghenia},
title = {Discriminative Pattern Mining in Software Fault Detection},
year = {2006},
isbn = {1595935843},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1188895.1188910},
doi = {10.1145/1188895.1188910},
abstract = {We present a method to enhance fault localization for software systems based on a frequent pattern mining algorithm. Our method is based on a large set of test cases for a given set of programs in which faults can be detected. The test executions are recorded as function call trees. Based on test oracles the tests can be classified into successful and failing tests. A frequent pattern mining algorithm is used to identify frequent subtrees in successful and failing test executions. This information is used to rank functions according to their likelihood of containing a fault. The ranking suggests an order in which to examine the functions during fault analysis. We validate our approach experimentally using a subset of Siemens benchmark programs.},
booktitle = {Proceedings of the 3rd International Workshop on Software Quality Assurance},
pages = {62–69},
numpages = {8},
keywords = {fault isolation, automated debugging},
location = {Portland, Oregon},
series = {SOQUA '06}
}

@inproceedings{10.1145/800191.805575,
author = {Herndon, Edwin S.},
title = {Computer Systems Planning - Tools &amp; Techniques},
year = {1976},
isbn = {9781450374897},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800191.805575},
doi = {10.1145/800191.805575},
abstract = {Computer Systems Planning for a large general purpose computer facility is discussed in this paper. An overview of the planning process is presented which emphasizes development of a workload forecast and setting planning objectives. Integrated use of accounting data, synthetic programs, a system simulation program and a software monitor is described. Experiences in planning at Johnson Space Center are used to illustrate pivotal issues in the planning process.},
booktitle = {Proceedings of the 1976 Annual Conference},
pages = {203–207},
numpages = {5},
location = {Houston, Texas, USA},
series = {ACM '76}
}

@inproceedings{10.1145/3239235.3240500,
author = {Jimenez, Matthieu and Checkam, Thiery Titcheu and Cordy, Maxime and Papadakis, Mike and Kintis, Marinos and Traon, Yves Le and Harman, Mark},
title = {Are Mutants Really Natural? A Study on How "Naturalness" Helps Mutant Selection},
year = {2018},
isbn = {9781450358231},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3239235.3240500},
doi = {10.1145/3239235.3240500},
abstract = {Background: Code is repetitive and predictable in a way that is similar to the natural language. This means that code is "natural" and this "naturalness" can be captured by natural language modelling techniques. Such models promise to capture the program semantics and identify source code parts that `smell', i.e., they are strange, badly written and are generally error-prone (likely to be defective). Aims: We investigate the use of natural language modelling techniques in mutation testing (a testing technique that uses artificial faults). We thus, seek to identify how well artificial faults simulate real ones and ultimately understand how natural the artificial faults can be. Our intuition is that natural mutants, i.e., mutants that are predictable (follow the implicit coding norms of developers), are semantically useful and generally valuable (to testers). We also expect that mutants located on unnatural code locations (which are generally linked with error-proneness) to be of higher value than those located on natural code locations. Method: Based on this idea, we propose mutant selection strategies that rank mutants according to a) their naturalness (naturalness of the mutated code), b) the naturalness of their locations (naturalness of the original program statements) and c) their impact on the naturalness of the code that they apply to (naturalness differences between original and mutated statements). We empirically evaluate these issues on a benchmark set of 5 open-source projects, involving more than 100k mutants and 230 real faults. Based on the fault set we estimate the utility (i.e. capability to reveal faults) of mutants selected on the basis of their naturalness, and compare it against the utility of randomly selected mutants. Results: Our analysis shows that there is no link between naturalness and the fault revelation utility of mutants. We also demonstrate that the naturalness-based mutant selection performs similar (slightly worse) to the random mutant selection. Conclusions: Our findings are negative but we consider them interesting as they confute a strong intuition, i.e., fault revelation is independent of the mutants' naturalness.},
booktitle = {Proceedings of the 12th ACM/IEEE International Symposium on Empirical Software Engineering and Measurement},
articleno = {3},
numpages = {10},
keywords = {mutation testing, language models, fault revelation},
location = {Oulu, Finland},
series = {ESEM '18}
}

@inproceedings{10.1145/1595696.1595766,
author = {Ashok, B. and Joy, Joseph and Liang, Hongkang and Rajamani, Sriram K. and Srinivasa, Gopal and Vangala, Vipindeep},
title = {DebugAdvisor: A Recommender System for Debugging},
year = {2009},
isbn = {9781605580012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1595696.1595766},
doi = {10.1145/1595696.1595766},
abstract = {In large software development projects, when a programmer is assigned a bug to fix, she typically spends a lot of time searching (in an ad-hoc manner) for instances from the past where similar bugs have been debugged, analyzed and resolved. Systematic search tools that allow the programmer to express the context of the current bug, and search through diverse data repositories associated with large projects can greatly improve the productivity of debugging This paper presents the design, implementation and experience from such a search tool called DebugAdvisor.The context of a bug includes all the information a programmer has about the bug, including natural language text, textual rendering of core dumps, debugger output etc. Our key insight is to allow the programmer to collate this entire context as a query to search for related information. Thus, DebugAdvisor allows the programmer to search using a fat query, which could be kilobytes of structured and unstructured data describing the contextual information for the current bug. Information retrieval in the presence of fat queries and variegated data repositories, all of which contain a mix of structured and unstructured data is a challenging problem. We present novel ideas to solve this problem.We have deployed DebugAdvisor to over 100 users inside Microsoft. In addition to standard metrics such as precision and recall, we present extensive qualitative and quantitative feedback from our users.},
booktitle = {Proceedings of the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
pages = {373–382},
numpages = {10},
keywords = {search, debugging, recommendation systems},
location = {Amsterdam, The Netherlands},
series = {ESEC/FSE '09}
}

@inproceedings{10.1145/3460120.3484813,
author = {He, Jingxuan and Sivanrupan, Gishor and Tsankov, Petar and Vechev, Martin},
title = {Learning to Explore Paths for Symbolic Execution},
year = {2021},
isbn = {9781450384544},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460120.3484813},
doi = {10.1145/3460120.3484813},
abstract = {Symbolic execution is a powerful technique that can generate tests steering program execution into desired paths. However, the scalability of symbolic execution is often limited by path explosion, i.e., the number of symbolic states representing the paths under exploration quickly explodes as execution goes on. Therefore, the effectiveness of symbolic execution engines hinges on the ability to select and explore the right symbolic states.In this work, we propose a novel learning-based strategy, called Learch, able to effectively select promising states for symbolic execution to tackle the path explosion problem. Learch directly estimates the contribution of each state towards the goal of maximizing coverage within a time budget, as opposed to relying on manually crafted heuristics based on simple statistics as a crude proxy for the objective. Moreover, Learch leverages existing heuristics in training data generation and feature extraction, and can thus benefit from any new expert-designed heuristics. We instantiated Learch in KLEE, a widely adopted symbolic execution engine. We evaluated Learch on a diverse set of programs, showing that Learch is practically effective: it covers more code and detects more security violations than existing manual heuristics, as well as combinations of those heuristics. We also show that using tests generated by Learch as initial fuzzing seeds enables the popular fuzzer AFL to find more paths and security violations.},
booktitle = {Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
pages = {2526–2540},
numpages = {15},
keywords = {symbolic execution, machine learning, fuzzing, program testing},
location = {Virtual Event, Republic of Korea},
series = {CCS '21}
}

@inproceedings{10.1145/1808266.1808285,
author = {Dulz, Winfried and German, Reinhard and Holpp, Stefan and G\"{o}tz, Helmut},
title = {Calculating the Usage Probabilities of Statistical Usage Models by Constraints Optimization},
year = {2010},
isbn = {9781605589701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1808266.1808285},
doi = {10.1145/1808266.1808285},
abstract = {The systematic generation of test cases from statistical usage models has been investigated recently for specific application domains, such as wireless communications or automotive applications. For Markov chain usage models, the expected usage of a hardware/software system is represented by transitions between usage states and a usage profile, meaning probability values that are attached to the state transitions.In this paper, we explain how to calculate the profile probabilities for the Markov chain usage model from a set of linear usage constraints and by optimizing a convex polyhedron that represents the constrained solution space. Comparing the computed probability distributions of our polyhedron approach with the maximum entropy technique, which is the main technique used so far, illustrates that our results are more obvious to the intented constraint semantics. In order to demonstrate the applicability of our approach, workflow testing of a complex RIS/PACS system in the medical domain was carried through and has provided promising results.},
booktitle = {Proceedings of the 5th Workshop on Automation of Software Test},
pages = {127–134},
numpages = {8},
keywords = {polyhedron optimization, medical case study, optimizing usage probabilities, Markov chain usage model, usage constraints, statistical usage testing, profile generation},
location = {Cape Town, South Africa},
series = {AST '10}
}

@inproceedings{10.1145/3341069.3341070,
author = {Xu, Hefang and Su, Caihong and Wu, Shaoyu and Tang, Dongping},
title = {Survey of Testing Methods of O2O Catering Platform},
year = {2019},
isbn = {9781450371858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3341069.3341070},
doi = {10.1145/3341069.3341070},
abstract = {The particularity of the Web application of catering O2O platform makes its testing challenging, but the current research on its testing is relatively weak compared with the research on its design and development.This paper summarizes the research progress of Web platform testing methods in recent years.The Basic test contents and technologies, typical test models and automated testing tools of existing platforms are summarized.The current research hotspots and difficulties are analyzed, including the optimization of test models, the improvement and development of automated testing tools, the guarantee of test comprehensiveness, safety, stability and efficiency.Finally, the future research directions of the testing methods of catering O2O Web platform are discussed from three aspects: testing content, testing methods and testing tools.},
booktitle = {Proceedings of the 2019 3rd High Performance Computing and Cluster Technologies Conference},
pages = {220–224},
numpages = {5},
keywords = {Automated Testing Tools, Catering O2O platform, Testing methods},
location = {Guangzhou, China},
series = {HPCCT 2019}
}

@inproceedings{10.1145/2889160.2889239,
author = {Salvaneschi, Paolo},
title = {System Testing of Repository-Style Software: An Experience Report},
year = {2016},
isbn = {9781450342056},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2889160.2889239},
doi = {10.1145/2889160.2889239},
abstract = {System testing based on a black box approach is a common industrial practice in information systems. Despite its widespread use, however, little guidance is available for testing engineers facing the problem of selecting the best test strategy. In previous work, we proposed to adopt functional models and related testing patterns according to the architectural style of the application under test. In this paper, we present an industrial study that applies this technique to system testing of repository based applications.We define a set of functional models abstracting different concerns of software applications: hierarchy of functions, business processes and states/transitions of application objects. The models are used to derive the functional test cases through the definition of test patterns. We applied this approach in an industrial context for over 5 years.In this paper, we analyze a data set of 37 test projects including about 22000 test cases and 1500 failures. We relate failures to the originating defect types. The study confirms that a system test strategy that uses multiple functional models according to the architectural style of the software application generates a better cost/benefit ratio than the use of just one model. The explanation is that -- despite a small overlap -- each model detects specific types of software defects. The results of the study can guide testing engineers in selecting the best system test strategy and significantly improve the efficiency of their work.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering Companion},
pages = {172–181},
numpages = {10},
keywords = {multiple models, system test, software defects},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2658761.2658768,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki},
title = {Efficient Testing of Software Product Lines via Centralization (Short Paper)},
year = {2014},
isbn = {9781450331616},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2658761.2658768},
doi = {10.1145/2658761.2658768},
abstract = { Software product line~(SPL) engineering manages families of software products that share common features. However, cost-effective test case generation for an SPL is challenging. Applying existing test case generation techniques to each product variant separately may test common code in a redundant way. Moreover, it is difficult to share the test results among multiple product variants. In this paper, we propose the use of centralization, which combines multiple product variants from the same SPL and generates test cases for the entire system. By taking into account all variants, our technique generally avoids generating redundant test cases for common software components. Our case study on three SPLs shows that compared with testing each variant independently, our technique is more efficient and achieves higher test coverage. },
booktitle = {Proceedings of the 2014 International Conference on Generative Programming: Concepts and Experiences},
pages = {49–52},
numpages = {4},
keywords = {automatic test generation, random testing, Software Product Lines},
location = {V\"{a}ster\r{a}s, Sweden},
series = {GPCE 2014}
}

@article{10.1145/2775053.2658768,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki},
title = {Efficient Testing of Software Product Lines via Centralization (Short Paper)},
year = {2014},
issue_date = {March 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {3},
issn = {0362-1340},
url = {https://doi.org/10.1145/2775053.2658768},
doi = {10.1145/2775053.2658768},
abstract = { Software product line~(SPL) engineering manages families of software products that share common features. However, cost-effective test case generation for an SPL is challenging. Applying existing test case generation techniques to each product variant separately may test common code in a redundant way. Moreover, it is difficult to share the test results among multiple product variants. In this paper, we propose the use of centralization, which combines multiple product variants from the same SPL and generates test cases for the entire system. By taking into account all variants, our technique generally avoids generating redundant test cases for common software components. Our case study on three SPLs shows that compared with testing each variant independently, our technique is more efficient and achieves higher test coverage. },
journal = {SIGPLAN Not.},
month = {sep},
pages = {49–52},
numpages = {4},
keywords = {automatic test generation, random testing, Software Product Lines}
}

@inproceedings{10.1145/3133841.3133842,
author = {Marr, Stefan and Torres Lopez, Carmen and Aumayr, Dominik and Gonzalez Boix, Elisa and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {A Concurrency-Agnostic Protocol for Multi-Paradigm Concurrent Debugging Tools},
year = {2017},
isbn = {9781450355261},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3133841.3133842},
doi = {10.1145/3133841.3133842},
abstract = { Today's complex software systems combine high-level concurrency models. Each model is used to solve a specific set of problems. Unfortunately, debuggers support only the low-level notions of threads and shared memory, forcing developers to reason about these notions instead of the high-level concurrency models they chose.  This paper proposes a concurrency-agnostic debugger protocol that decouples the debugger from the concurrency models employed by the target application. As a result, the underlying language runtime can define custom breakpoints, stepping operations, and execution events for each concurrency model it supports, and a debugger can expose them without having to be specifically adapted.  We evaluated the generality of the protocol by applying it to SOMns, a Newspeak implementation, which supports a diversity of concurrency models including communicating sequential processes, communicating event loops, threads and locks, fork/join parallelism, and software transactional memory. We implemented 21 breakpoints and 20 stepping operations for these concurrency models. For none of these, the debugger needed to be changed. Furthermore, we visualize all concurrent interactions independently of a specific concurrency model. To show that tooling for a specific concurrency model is possible, we visualize actor turns and message sends separately. },
booktitle = {Proceedings of the 13th ACM SIGPLAN International Symposium on on Dynamic Languages},
pages = {3–14},
numpages = {12},
keywords = {Stepping, Breakpoints, Debugging, Visualization, Concurrency, Tooling},
location = {Vancouver, BC, Canada},
series = {DLS 2017}
}

@article{10.1145/3170472.3133842,
author = {Marr, Stefan and Torres Lopez, Carmen and Aumayr, Dominik and Gonzalez Boix, Elisa and M\"{o}ssenb\"{o}ck, Hanspeter},
title = {A Concurrency-Agnostic Protocol for Multi-Paradigm Concurrent Debugging Tools},
year = {2017},
issue_date = {November 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/3170472.3133842},
doi = {10.1145/3170472.3133842},
abstract = { Today's complex software systems combine high-level concurrency models. Each model is used to solve a specific set of problems. Unfortunately, debuggers support only the low-level notions of threads and shared memory, forcing developers to reason about these notions instead of the high-level concurrency models they chose.  This paper proposes a concurrency-agnostic debugger protocol that decouples the debugger from the concurrency models employed by the target application. As a result, the underlying language runtime can define custom breakpoints, stepping operations, and execution events for each concurrency model it supports, and a debugger can expose them without having to be specifically adapted.  We evaluated the generality of the protocol by applying it to SOMns, a Newspeak implementation, which supports a diversity of concurrency models including communicating sequential processes, communicating event loops, threads and locks, fork/join parallelism, and software transactional memory. We implemented 21 breakpoints and 20 stepping operations for these concurrency models. For none of these, the debugger needed to be changed. Furthermore, we visualize all concurrent interactions independently of a specific concurrency model. To show that tooling for a specific concurrency model is possible, we visualize actor turns and message sends separately. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {3–14},
numpages = {12},
keywords = {Breakpoints, Tooling, Debugging, Visualization, Stepping, Concurrency}
}

