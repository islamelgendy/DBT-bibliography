@inproceedings{10.1145/3167132.3167247,
author = {Yoon, Hyunmin and Majeed, Shakaiba and Ryu, Minsoo},
title = {Exploring OS-Based Full-System Deterministic Replay},
year = {2018},
isbn = {9781450351911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3167132.3167247},
doi = {10.1145/3167132.3167247},
abstract = {Modern computer systems have various sources of nondeterminism such as external inputs, concurrency in software and hardware, asynchronous interrupts and timing variations. With these sources of nondeterminism, many errors and bugs can remain undetected during development, manifesting in the form of corrupt data, hangs, crashes or other catastrophic results. Deterministic full-system replay helps in identifying the cause of such failures by reproducing a previously happened execution. Existing full-system deterministic replay schemes are based on either a special hardware implementation or a virtualization platform. Though beneficial, either they require non-trivial modifications to hardware or suffer from lack of reproducibility. This paper presents an innovative operating system (OS) based replay framework called Software Black Box (SBB) which is the first attempt to provide full-system replay without requiring any special hardware implementation or virtualization. It can reproduce the entire execution of a computer system including, user-level processes, OS functions and device drivers with instruction level accuracy. We implemented a prototype of SBB for Linux operating system ported to the ARM uniprocessor environment and evaluated its performance using Phoronix benchmark suites and some networking workloads. The results are promising, making it suitable for many purposes including debugging, testing, security and performance analysis.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on Applied Computing},
pages = {1077–1086},
numpages = {10},
keywords = {record and replay, full-system replay, embedded systems, debugging},
location = {Pau, France},
series = {SAC '18}
}

@inproceedings{10.1145/2983990.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984038},
doi = {10.1145/2983990.2984038},
abstract = { Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions.  This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable.  We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed. },
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {849–863},
numpages = {15},
keywords = {automated testing, miscompilation, Compiler testing, equivalent program variants},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984038},
doi = {10.1145/3022671.2984038},
abstract = { Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions.  This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable.  We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {849–863},
numpages = {15},
keywords = {equivalent program variants, miscompilation, Compiler testing, automated testing}
}

@inproceedings{10.5555/2820282.2820292,
author = {Thung, Ferdian and Le, Xuan-Bach D. and Lo, David},
title = {Active Semi-Supervised Defect Categorization},
year = {2015},
publisher = {IEEE Press},
abstract = {Defects are inseparable part of software development and evolution. To better comprehend problems affecting a software system, developers often store historical defects and these defects can be categorized into families. IBM proposes Orthogonal Defect Categorization (ODC) which include various classifications of defects based on a number of orthogonal dimensions (e.g., symptoms and semantics of defects, root causes of defects, etc.). To help developers categorize defects, several approaches that employ machine learning have been proposed in the literature. Unfortunately, these approaches often require developers to manually label a large number of defect examples. In practice, manually labelling a large number of examples is both time-consuming and labor-intensive. Thus, reducing the onerous burden of manual labelling while still being able to achieve good performance is crucial towards the adoption of such approaches. To deal with this challenge, in this work, we propose an active semi-supervised defect prediction approach. It is performed by actively selecting a small subset of diverse and informative defect examples to label (i.e., active learning), and by making use of both labeled and unlabeled defect examples in the prediction model learning process (i.e., semi-supervised learning). Using this principle, our approach is able to learn a good model while minimizing the manual labeling effort.To evaluate the effectiveness of our approach, we make use of a benchmark dataset that contains 500 defects from three software systems that have been manually labelled into several families based on ODC. We investigate our approach's ability in achieving good classification performance, measured in terms of weighted precision, recall, F-measure, and AUC, when only a small number of manually labelled defect examples are available. Our experiment results show that our active semi-supervised defect categorization approach is able to achieve a weighted precision, recall, F-measure, and AUC of 0.651, 0.669, 0.623, and 0.710, respectively, when only 50 defects are manually labelled. Furthermore, it outperforms an existing active multi-class classification algorithm, proposed in the machine learning community, by a substantial margin.},
booktitle = {Proceedings of the 2015 IEEE 23rd International Conference on Program Comprehension},
pages = {60–70},
numpages = {11},
location = {Florence, Italy},
series = {ICPC '15}
}

@inproceedings{10.1145/2393596.2393607,
author = {Mani, Senthil and Catherine, Rose and Sinha, Vibha Singhal and Dubey, Avinava},
title = {AUSUM: Approach for Unsupervised Bug Report Summarization},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393607},
doi = {10.1145/2393596.2393607},
abstract = {In most software projects, resolved bugs are archived for future reference. These bug reports contain valuable information on the reported problem, investigation and resolution. When bug triaging, developers look for how similar problems were resolved in the past. Search over bug repository gives the developer a set of recommended bugs to look into. However, the developer still needs to manually peruse the contents of the recommended bugs which might vary in size from a couple of lines to thousands. Automatic summarization of bug reports is one way to reduce the amount of data a developer might need to go through. Prior work has presented learning based approaches for bug summarization. These approaches have the disadvantage of requiring large training set and being biased towards the data on which the model was learnt. In fact, maximum efficacy was reported when the model was trained and tested on bug reports from the same project. In this paper, we present the results of applying four unsupervised summarization techniques for bug summarization. Industrial bug reports typically contain a large amount of noise---email dump, chat transcripts, core-dump---useless sentences from the perspective of summarization. These derail the unsupervised approaches, which are optimized to work on more well-formed documents. We present an approach for noise reduction, which helps to improve the precision of summarization over the base technique (4% to 24% across subjects and base techniques). Importantly, by applying noise reduction, two of the unsupervised techniques became scalable for large sized bug reports.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {11},
numpages = {11},
keywords = {summarization, unsupervised, bug report},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1109/SESoS/WDES.2019.00011,
author = {Song, Jiyoung and T\o{}rring, Jacob O. and Hyun, Sangwon and Jee, Eunkyoung and Bae, Doo-Hwan},
title = {Slicing Executable System-of-Systems Models for Efficient Statistical Verification},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SESoS/WDES.2019.00011},
doi = {10.1109/SESoS/WDES.2019.00011},
abstract = {A System of Systems (SoS), composed of independent constituent systems, can create synergy among its systems to achieve a common goal. Many studies have used statistical model checking techniques to verify how well an SoS can achieve its goals. SoS models are usually complex and probabilistic, which makes statistical verification computationally expensive. To reduce this cost, dynamic slicing techniques can be applied to SoS models since both dynamic slicing and statistical verification focus on the models' execution samples. However, existing dynamic slicing techniques cannot guarantee executable accurate slices of SoS models when the models contain uncertainty. Therefore, we propose a hybrid slicing approach that combines dynamic backward slicing and modified observation-based slicing to produce accurate executable slices. Experimentation on the proposed technique found that the verification time was significantly reduced (47--56%), depending on the property, while preserving the verification results.},
booktitle = {Proceedings of the 7th International Workshop on Software Engineering for Systems-of-Systems and 13th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems},
pages = {18–25},
numpages = {8},
keywords = {model verification, model slicing, system-of-systems, statistical model checking},
location = {Montreal, Quebec, Canada},
series = {SESoS-WDES '19}
}

@inproceedings{10.1145/3300115.3309516,
author = {Kangas, Vilma and Pirttinen, Nea and Nygren, Henrik and Leinonen, Juho and Hellas, Arto},
title = {Does Creating Programming Assignments with Tests Lead to Improved Performance in Writing Unit Tests?},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3309516},
doi = {10.1145/3300115.3309516},
abstract = {We have constructed a tool, CrowdSorcerer, in which students create programming assignments, their model solutions and associated test cases using a simple input-output format. We have used the tool as a part of an introductory programming course with normal course activities such as programming assignments and a final exam. In our work, we focus on whether creating programming assignments and associated tests correlate with students' performance in a testing-related exam question. We study this through an analysis of the quality of student-written tests within the tool, measured using the number of test cases, line coverage and mutation coverage, and students' performance in testing related exam question, measured using exam points. Finally, we study whether previous programming experience correlates with how students act within the tool and within the testing related exam question.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {106–112},
numpages = {7},
keywords = {testing, crowdsourcing, assignment creation, educational data mining},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@article{10.1145/356876.356879,
author = {Adrion, W. Richards and Branstad, Martha A. and Cherniavsky, John C.},
title = {Validation, Verification, and Testing of Computer Software},
year = {1982},
issue_date = {June 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/356876.356879},
doi = {10.1145/356876.356879},
journal = {ACM Comput. Surv.},
month = {jun},
pages = {159–192},
numpages = {34}
}

@inproceedings{10.1109/ASE.2013.6693071,
author = {Xu, Zhihong and Hirzel, Martin and Rothermel, Gregg and Wu, Kun-Lung},
title = {Testing Properties of Dataflow Program Operators},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693071},
doi = {10.1109/ASE.2013.6693071},
abstract = {Dataflow programming languages, which represent programs as graphs of data streams and operators, are becoming increasingly popular and being used to create a wide array of commercial software applications. The dependability of programs written in these languages, as well as the systems used to compile and run these programs, hinges on the correctness of the semantic properties associated with operators. Unfortunately, these properties are often poorly defined, and frequently are not checked, and this can lead to a wide range of problems in the programs that use the operators. In this paper we present an approach for improving the dependability of dataflow programs by checking operators for necessary properties. Our approach is dynamic, and involves generating tests whose results are checked to determine whether specific properties hold or not. We present empirical data that shows that our approach is both effective and efficient at assessing the status of properties.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {103–113},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE'13}
}

@inbook{10.1145/3368089.3409755,
author = {Cha, Sooyoung and Oh, Hakjoo},
title = {Making Symbolic Execution Promising by Learning Aggressive State-Pruning Strategy},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409755},
abstract = {We present HOMI, a new technique to enhance symbolic execution by maintaining only a small number of promising states. In practice, symbolic execution typically maintains as many states as possible in a fear of losing important states. In this paper, however, we show that only a tiny subset of the states plays a significant role in increasing code coverage or reaching bug points. Based on this observation, HOMI aims to minimize the total number of states while keeping “promising” states during symbolic execution. We identify promising states by a learning algorithm that continuously updates the probabilistic pruning strategy based on data accumulated during the testing process. Experimental results show that HOMI greatly increases code coverage and the ability to find bugs of KLEE on open-source C programs.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {147–158},
numpages = {12}
}

@inproceedings{10.1145/1321631.1321722,
author = {Pugh, William and Ayewah, Nathaniel},
title = {Unit Testing Concurrent Software},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321722},
doi = {10.1145/1321631.1321722},
abstract = {There are many difficulties associated with developing correct multithreaded software, and many of the activities that are simple for single threaded software are exceptionally hard for multithreaded software. One such example is constructing unit tests involving multiple threads. Given, for example, a blocking queue implementation, writing a test case to show that it blocks and unblocks appropriately using existing testing frameworks is exceptionally hard. In this paper, we describe the MultithreadedTC framework which allows the construction of deterministic and repeatable unit tests for concurrent abstractions. This framework is not designed to test for synchronization errors that lead to rare probabilistic faults under concurrent stress. Rather, this framework allows us to demonstrate that code does provide specific concurrent functionality (e.g., a thread attempting to acquire a lock is blocked if another thread has the lock).We describe the framework and provide empirical comparisons against hand-coded tests designed for Sun's Java concurrency utilities library and against previous frameworks that addressed this same issue. The source code for this framework is available under an open source license.},
booktitle = {Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering},
pages = {513–516},
numpages = {4},
keywords = {JUnit test cases, java, multithreadedTC, testing framework, concurrent abstraction},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inproceedings{10.1145/2991079.2991103,
author = {Pewny, Jannik and Holz, Thorsten},
title = {EvilCoder: Automated Bug Insertion},
year = {2016},
isbn = {9781450347716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991079.2991103},
doi = {10.1145/2991079.2991103},
abstract = {The art of finding software vulnerabilities has been covered extensively in the literature and there is a huge body of work on this topic. In contrast, the intentional insertion of exploitable, security-critical bugs has received little (public) attention yet. Wanting more bugs seems to be counterproductive at first sight, but the comprehensive evaluation of bug-finding techniques suffers from a lack of ground truth and the scarcity of bugs.In this paper, we propose EvilCoder, a system to automatically find potentially vulnerable source code locations and modify the source code to be actually vulnerable. More specifically, we leverage automated program analysis techniques to find sensitive sinks which match typical bug patterns (e.g., a sensitive API function with a preceding sanity check), and try to find data-flow connections to user-controlled sources. We then transform the source code such that exploitation becomes possible, for example by removing or modifying input sanitization or other types of security checks. Our tool is designed to randomly pick vulnerable locations and possible modifications, such that it can generate numerous different vulnerabilities on the same software corpus. We evaluated our tool on several open-source projects such as for example libpng and vsftpd, where we found between 22 and 158 unique connected source-sink pairs per project. This translates to hundreds of potentially vulnerable data-flow paths and hundreds of bugs we can insert. We hope to support future bug-finding techniques by supplying freshly generated, bug-ridden test corpora so that such techniques can (finally) be evaluated and compared in a comprehensive and statistically meaningful way.},
booktitle = {Proceedings of the 32nd Annual Conference on Computer Security Applications},
pages = {214–225},
numpages = {12},
location = {Los Angeles, California, USA},
series = {ACSAC '16}
}

@inproceedings{10.1109/ASE.2011.6100059,
author = {Robinson, Brian and Ernst, Michael D. and Perkins, Jeff H. and Augustine, Vinay and Li, Nuo},
title = {Scaling up Automated Test Generation: Automatically Generating Maintainable Regression Unit Tests for Programs},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100059},
doi = {10.1109/ASE.2011.6100059},
abstract = {This paper presents an automatic technique for generating maintainable regression unit tests for programs. We found previous test generation techniques inadequate for two main reasons. First. they were designed for and evaluated upon libraries rather than applications. Second, they were designed to find bugs rather than to create maintainable regression test suites: the test suites that they generated were brittle and hard to understand. This paper presents a suite of techniques that address these problems by enhancing an existing unit test generation system. In experiments using an industrial system, the generated tests achieved good coverage and mutation kill score, were readable by the product's developers, and required few edits as the system under test evolved. While our evaluation is in the context of one test generator, we are aware of many research systems that suffer similar limitations, so our approach and observations are more generally relevant.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {23–32},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/3062341.3062364,
author = {Billes, Marina and M\o{}ller, Anders and Pradel, Michael},
title = {Systematic Black-Box Analysis of Collaborative Web Applications},
year = {2017},
isbn = {9781450349888},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3062341.3062364},
doi = {10.1145/3062341.3062364},
abstract = { Web applications, such as collaborative editors that allow multiple clients to concurrently interact on a shared resource, are difficult to implement correctly. Existing techniques for analyzing concurrent software do not scale to such complex systems or do not consider multiple interacting clients. This paper presents Simian, the first fully automated technique for systematically analyzing multi-client web applications.  Naively exploring all possible interactions between a set of clients of such applications is practically infeasible. Simian obtains scalability for real-world applications by using a two-phase black-box approach. The application code remains unknown to the analysis and is first explored systematically using a single client to infer potential conflicts between client events triggered in a specific context. The second phase synthesizes multi-client interactions targeted at triggering misbehavior that may result from the potential conflicts, and reports an inconsistency if the clients do not converge to a consistent state.  We evaluate the analysis on three widely used systems, Google Docs, Firepad, and ownCloud Documents, where it reports a variety of inconsistencies, such as incorrect formatting and misplaced text fragments. Moreover, we find that the two-phase approach runs 10x faster compared to exhaustive exploration, making systematic analysis practically applicable. },
booktitle = {Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {171–184},
numpages = {14},
keywords = {Testing, dynamic analysis, collaborative editing},
location = {Barcelona, Spain},
series = {PLDI 2017}
}

@article{10.1145/3140587.3062364,
author = {Billes, Marina and M\o{}ller, Anders and Pradel, Michael},
title = {Systematic Black-Box Analysis of Collaborative Web Applications},
year = {2017},
issue_date = {June 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/3140587.3062364},
doi = {10.1145/3140587.3062364},
abstract = { Web applications, such as collaborative editors that allow multiple clients to concurrently interact on a shared resource, are difficult to implement correctly. Existing techniques for analyzing concurrent software do not scale to such complex systems or do not consider multiple interacting clients. This paper presents Simian, the first fully automated technique for systematically analyzing multi-client web applications.  Naively exploring all possible interactions between a set of clients of such applications is practically infeasible. Simian obtains scalability for real-world applications by using a two-phase black-box approach. The application code remains unknown to the analysis and is first explored systematically using a single client to infer potential conflicts between client events triggered in a specific context. The second phase synthesizes multi-client interactions targeted at triggering misbehavior that may result from the potential conflicts, and reports an inconsistency if the clients do not converge to a consistent state.  We evaluate the analysis on three widely used systems, Google Docs, Firepad, and ownCloud Documents, where it reports a variety of inconsistencies, such as incorrect formatting and misplaced text fragments. Moreover, we find that the two-phase approach runs 10x faster compared to exhaustive exploration, making systematic analysis practically applicable. },
journal = {SIGPLAN Not.},
month = {jun},
pages = {171–184},
numpages = {14},
keywords = {dynamic analysis, Testing, collaborative editing}
}

@inproceedings{10.1145/3368308.3415419,
author = {Singleton, Larry and Zhao, Rui and Song, Myoungkyu and Siy, Harvey},
title = {CryptoTutor: Teaching Secure Coding Practices through Misuse Pattern Detection},
year = {2020},
isbn = {9781450370455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368308.3415419},
doi = {10.1145/3368308.3415419},
abstract = {Insecure program practices seriously threaten software security. Misusing security primitives in application-level code is not unusual. For example, in mobile banking apps, developers might store customers' privacy information in plaintext, leading to sensitive information leakage. To leverage cryptographic primitives, developers need to correctly select the cryptographic algorithm, appropriate parameters, and sometimes its post-process. While recent research discusses pitfalls in cryptography-related implementations, few academic programs integrate these concepts in their educational programs. One big challenge is the lack of automated guidance on how to utilize existing libraries for secure coding. In this paper, we discuss the prevalence of the problem, especially with respect to implementing programs that utilize cryptography, to motivate the need for better tool support for guidance in writing secure code. We present a tool, CryptoTutor, that can automatically flag common cryptographic misuses and suggest possible repairs. We discuss how tools like CryptoTutor can be integrated into programming courses at the college and pre-college levels.},
booktitle = {Proceedings of the 21st Annual Conference on Information Technology Education},
pages = {403–408},
numpages = {6},
keywords = {secure coding, cryptographic misuse, programming education},
location = {Virtual Event, USA},
series = {SIGITE '20}
}

@inproceedings{10.1145/3236024.3236072,
author = {Wang, Peipei and Stolee, Kathryn T.},
title = {How Well Are Regular Expressions Tested in the Wild?},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236072},
doi = {10.1145/3236024.3236072},
abstract = {Developers report testing their regular expressions less than the rest of their code. In this work, we explore how thoroughly tested regular expressions are by examining open source projects.  Using standard metrics of coverage, such as line and branch coverage, gives an incomplete picture of the test coverage of regular expressions. We adopt graph-based coverage metrics for the DFA representation of regular expressions, providing fine-grained test coverage metrics. Using over 15,000 tested regular expressions in 1,225 Java projects on GitHub, we measure node, edge, and edge-pair coverage. Our results show that only 17% of the regular expressions in the repositories are tested at all. For those that are tested, the median number of test inputs is two. For nearly 42% of the tested regular expressions, only one test input is used. Average node and edge coverage levels on the DFAs for tested regular expressions are 59% and 29%, respectively. Due to the lack of testing of regular expressions, we explore whether a string generation tool for regular expressions, Rex, achieves high coverage levels. With some exceptions, we found that tools such as Rex can be used to write test inputs with similar coverage to the developer tests.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {668–678},
numpages = {11},
keywords = {Regular expressions, Deterministic Finite Automaton, Test coverage metrics},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1145/3338906.3340459,
author = {Ivankovi\'{c}, Marko and Petrovi\'{c}, Goran and Just, Ren\'{e} and Fraser, Gordon},
title = {Code Coverage at Google},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3340459},
doi = {10.1145/3338906.3340459},
abstract = {Code coverage is a measure of the degree to which a test suite exercises a software system. Although coverage is well established in software engineering research, deployment in industry is often inhibited by the perceived usefulness and the computational costs of analyzing coverage at scale. At Google, coverage information is computed for one billion lines of code daily, for seven programming languages. A key aspect of making coverage information actionable is to apply it at the level of changesets and code review. This paper describes Google’s code coverage infrastructure and how the computed code coverage information is visualized and used. It also describes the challenges and solutions for adopting code coverage at scale. To study how code coverage is adopted and perceived by developers, this paper analyzes adoption rates, error rates, and average code coverage ratios over a five-year period, and it reports on 512 responses, received from surveying 3000 developers. Finally, this paper provides concrete suggestions for how to implement and use code coverage in an industrial setting.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {955–963},
numpages = {9},
keywords = {industrial study, coverage, test infrastructure},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2897073.2897085,
author = {Banerjee, Abhijeet and Guo, Hai-Feng and Roychoudhury, Abhik},
title = {Debugging Energy-Efficiency Related Field Failures in Mobile Apps},
year = {2016},
isbn = {9781450341783},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897073.2897085},
doi = {10.1145/2897073.2897085},
abstract = {Debugging field failures can be a challenging task for app-developers. Insufficient or unreliable information, improper assumptions and multitude of devices (smartphones) being used, are just some of the many factors that may contribute to its challenges. In this work, we design and develop an open-source framework that helps to communicate, localize and patch energy consumption related field failures in Android apps. Our framework consists of two sets of automated tools: one for the app-user to precisely record and report field failures observed in real-life apps, and the other assists the developer by automatically localizing the reported defects and suggesting patch locations. More specifically, the tools on the developer's side consist of an Eclipse-plugin that detects specific patterns of Android API calls, that are indicative of energy-inefficient behaviour. In our experiments with real-life apps we observed that our framework can localize defects in a short amount of time (~3 seconds), even for apps with thousands of lines-of-code. Additionally, the energy savings generated as a result of the patched defects are significant (observed energy savings of up to 29%). When comparing the patch locations suggested by our framework to the changes in the patched code from real-life app-repositories, we observed a significant correlation (changes suggested by our tool also appeared in the source-code commits where the reported defects were marked as fixed).},
booktitle = {Proceedings of the International Conference on Mobile Software Engineering and Systems},
pages = {127–138},
numpages = {12},
keywords = {debugging, energy-efficiency, mobile apps},
location = {Austin, Texas},
series = {MOBILESoft '16}
}

@inproceedings{10.1145/2635868.2635916,
author = {Bae, SungGyeong and Cho, Hyunghun and Lim, Inho and Ryu, Sukyoung},
title = {SAFEWAPI: Web API Misuse Detector for Web Applications},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635916},
doi = {10.1145/2635868.2635916},
abstract = { The evolution of Web 2.0 technologies makes web applications prevalent in various platforms including mobile devices and smart TVs. While one of the driving technologies of web applications is JavaScript, the extremely dynamic features of JavaScript make it very difficult to define and detect errors in JavaScript applications. The problem becomes more important and complicated for JavaScript web applications which may lead to severe security vulnerabilities. To help developers write safe JavaScript web applications using vendor-specific Web APIs, vendors specify their APIs often in Web IDL, which enables both API writers and users to communicate better by understanding the expected behaviors of the Web APIs. In this paper, we present SAFEWAPI, a tool to analyze Web APIs and JavaScript web applications that use the Web APIs and to detect possible misuses of Web APIs by the web applications. Even though the JavaScript language semantics allows to call a function defined with some parameters without any arguments, platform developers may require application writers to provide the exact number of arguments. Because the library functions in Web APIs expose their intended semantics clearly to web application developers unlike pure JavaScript functions, we can detect wrong uses of Web APIs precisely. For representative misuses of Web APIs defined by software quality assurance engineers, our SAFEWAPI detects such misuses in real-world JavaScript web applications. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {507–517},
numpages = {11},
keywords = {web application, JavaScript, static analysis, bug detection},
location = {Hong Kong, China},
series = {FSE 2014}
}

@inproceedings{10.1145/2739480.2754725,
author = {Guizzo, Giovani and Fritsche, Gian Mauricio and Vergilio, Silvia Regina and Pozo, Aurora Trinidad Ramirez},
title = {A Hyper-Heuristic for the Multi-Objective Integration and Test Order Problem},
year = {2015},
isbn = {9781450334723},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739480.2754725},
doi = {10.1145/2739480.2754725},
abstract = {Multi-objective evolutionary algorithms (MOEAs) have been efficiently applied to Search-Based Software Engineering (SBSE) problems. However, skilled software engineers waste significant effort designing such algorithms for a particular problem, adapting them, selecting operators and configuring parameters. Hyper-heuristics can help in these tasks by dynamically selecting or creating heuristics. Despite of such advantages, we observe a lack of works regarding this subject in the SBSE field. Considering this fact, this work introduces HITO, a Hyper-heuristic for the Integration and Test Order Problem. It includes a set of well-defined steps and is based on two selection functions (Choice Function and Multi-armed Bandit) to select the best low-level heuristic (combination of mutation and crossover operators) in each mating. To perform the selection, a quality measure is proposed to assess the performance of low-level heuristics throughout the evolutionary process. HITO was implemented using NSGA-II and evaluated to solve the integration and test order problem in seven systems. The introduced hyper-heuristic obtained the best results for all systems, when compared to a traditional algorithm.},
booktitle = {Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {1343–1350},
numpages = {8},
keywords = {multi-objective algorithm, hyper-heuristic, search-based software engineering},
location = {Madrid, Spain},
series = {GECCO '15}
}

@inproceedings{10.1145/2970276.2970358,
author = {Hilton, Michael and Tunnell, Timothy and Huang, Kai and Marinov, Darko and Dig, Danny},
title = {Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects},
year = {2016},
isbn = {9781450338455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2970276.2970358},
doi = {10.1145/2970276.2970358},
abstract = { Continuous integration (CI) systems automate the compilation, building, and testing of software. Despite CI rising as a big success story in automated software engineering, it has received almost no attention from the research community. For example, how widely is CI used in practice, and what are some costs and benefits associated with CI? Without answering such questions, developers, tool builders, and researchers make decisions based on folklore instead of data. In this paper, we use three complementary methods to study the usage of CI in open-source projects. To understand which CI systems developers use, we analyzed 34,544 open-source projects from GitHub. To understand how developers use CI, we analyzed 1,529,291 builds from the most commonly used CI system. To understand why projects use or do not use CI, we surveyed 442 developers. With this data, we answered several key questions related to the usage, costs, and benefits of CI. Among our results, we show evidence that supports the claim that CI helps projects release more often, that CI is widely adopted by the most popular projects, as well as finding that the overall percentage of projects using CI continues to grow, making it important and timely to focus more research on CI. },
booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
pages = {426–437},
numpages = {12},
keywords = {mining software repositories, continuous integration},
location = {Singapore, Singapore},
series = {ASE 2016}
}

