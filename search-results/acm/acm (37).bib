@inproceedings{10.1145/2905055.2905283,
author = {Johri, Prashant and Nasar, Md. and Das, Sanjoy and Kumar, Mithun},
title = {Open Source Software Reliability Growth Models for Distributed Environment Based on Component-Specific Testing-Efforts},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905283},
doi = {10.1145/2905055.2905283},
abstract = {Because of availability, redistributable, affordability, modifiability, of source code, free and no restriction in choice, open source is a favorite platform for lot of software industries and peoples, who consider using the power of extremely reliable and superior quality software. Numeouus SRGMs have been proposed to estimate the reliability of the software of OSSs; however, no one has proven to perform very well considering diverse project characteristics. In the models for OSSs, the error deletion experience for the reused and the newly developed components based on component-specific testing-effort is demonstrated. It is considered that there are several different types of faults for newly developed component and single type of faults for reused components for obtaining the unambiguous expressions for the mean number of individual types of errors. For OSSs system components testing-efforts have to be modeled separately for each and every component in the system. The total effort of the system is then calculated from the summation of component-specific testing-effort functions. We have employed MATLAB as implementation framework for performing all the estimations. Our approach partitions the testing effort with growth curves of varying nature among different components of the same OSS. To validate our analytical results, numerical illustrations have also been provided.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {75},
numpages = {9},
keywords = {Software Reliability Growth Models (SRGMs), Testing-effort function (TEF), Non Homogeneous Poisson Process (NHPP), Open Source Software (OSS), Distributed development environment (DDE)},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.1109/WI-IAT.2014.62,
author = {Freitas, Artur and Vieira, Renata},
title = {An Ontology for Guiding Performance Testing},
year = {2014},
isbn = {9781479941438},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/WI-IAT.2014.62},
doi = {10.1109/WI-IAT.2014.62},
abstract = {Software test is a technique to obtain information about software systems quality. Performance test is a type of software test that aims at evaluating software performance at a given load scenario, but it requires specialized knowledge about tools, activities and metrics of the domain. Since ontology is a promising knowledge representation technique, this paper presents a literature review to identify trends and compare researches of ontologies in the fields of software testing and software performance. Also, to investigate this issue from a practical perspective, it was developed an ontology for representing the core knowledge of performance testing. This paper presents the ontology and compare it with related ones. Then, semantic technologies are explored to demonstrate the practical feasibility of developing ontology-based applications for assisting testers with performance test planning and management.},
booktitle = {Proceedings of the 2014 IEEE/WIC/ACM International Joint Conferences on Web Intelligence (WI) and Intelligent Agent Technologies (IAT) - Volume 01},
pages = {400–407},
numpages = {8},
series = {WI-IAT '14}
}

@inproceedings{10.1145/3056662.3056672,
author = {Lin, Mengxiang and Hou, Xiaomei and Liu, Rui and Ge, Linyan},
title = {Enhancing Constraint Based Test Generation by Local Search},
year = {2017},
isbn = {9781450348577},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3056662.3056672},
doi = {10.1145/3056662.3056672},
abstract = {The core operation of symbolic execution based test data generation is to generate a path constraint in terms of input variables for a selected path. Solutions to the path constraint are test data which will be used to execute the target path. So far, the limitations of constraint solving still prevent its widespread use in practice. Most of the constraint solvers aim at a particular kind of constraint and might have no way to deal with complex path constraints derived in a real-world software application. To tackle the problem, we propose a hybrid solving strategy for path constraints containing multiple kinds of constraints. In particular, the path constraint is converted and divided into two parts that can be solved by different constraint solving techniques. A local search method is combined with a linear constraint solver to solve complex mathematical constraints. We have extended the symbolic execution engine KLEE with our approach and evaluated its effectiveness on a set of mathematical programs. The preliminary evaluation shows that our approach can solve a majority of complex path constraints in the experiments in reasonable time.},
booktitle = {Proceedings of the 6th International Conference on Software and Computer Applications},
pages = {154–158},
numpages = {5},
keywords = {local search, symbolic execution, constraint solving, automated test generation},
location = {Bangkok, Thailand},
series = {ICSCA '17}
}

@article{10.1145/1945023.1945034,
author = {Chen, Shimin and Gibbons, Phillip B. and Kozuch, Michael and Mowry, Todd C.},
title = {Log-Based Architectures: Using Multicore to Help Software Behave Correctly},
year = {2011},
issue_date = {January 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5980},
url = {https://doi.org/10.1145/1945023.1945034},
doi = {10.1145/1945023.1945034},
abstract = {While application performance and power-efficiency are both important, application correctness is even more important. In other words, if the application is misbehaving, it is little consolation that it is doing so quickly or power-efficiently. In the Log-Based Architectures (LBA) project, we are focusing on a challenging source of application misbehavior: software bugs, including obscure bugs that only cause problems during security attacks. To help detect and fix software bugs, we have been exploring techniques for accelerating dynamic program monitoring tools, which we call "lifeguards". Lifeguards are typically written today using dynamic binary instrumentation frameworks such as Valgrind or Pin. Due to the overheads of binary instrumentation, lifeguards that require instructiongrain information typically experience 30X-100X slowdowns, and hence it is only practical to use them during explicit debug cycles. The goal in the LBA project is to reduce these overheads to the point where lifeguards can run continuously on deployed code. To accomplish this, we propose hardware mechanisms to create a dynamic log of instruction-level events in the monitored application and stream this information to one or more software lifeguards running on separate cores on the same multicore processor. In this paper, we highlight techniques and features of LBA that reduce the slowdown to just 2%--51% for sequential programs and 28%--51% for parallel programs.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {feb},
pages = {84–91},
numpages = {8},
keywords = {parallel monitoring, lifeguards, program monitoring, software bugs, log-based architectures}
}

@inproceedings{10.1145/2896941.2896949,
author = {Wang, Xiaolin and Zeng, Hongwei},
title = {History-Based Dynamic Test Case Prioritization for Requirement Properties in Regression Testing},
year = {2016},
isbn = {9781450341578},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2896941.2896949},
doi = {10.1145/2896941.2896949},
abstract = {Regression testing is an important but extremely costly and time-consuming process. Because of limited resources in practice, test case prioritization focuses on the improvement of testing efficiency. However, traditional test case prioritization techniques emphasize only one-time testing without considering huge historical data generated in regression testing. This paper proposes an approach to prioritizing test cases based on historical data. Requirements are a significant factor in the testing process, the priorities of test cases are initialized based on requirement priorities in our history-based approach, and then are calculated dynamically according to historical data in regression testing. To evaluate our approach, an empirical study on an industrial system is conducted. Experimental results show an improved performance for our proposed method using measurements of Average Percentage of Faults Detected and Fault Detection Rate.},
booktitle = {Proceedings of the International Workshop on Continuous Software Evolution and Delivery},
pages = {41–47},
numpages = {7},
keywords = {test case prioritization, requirement property, regression testing, history data},
location = {Austin, Texas},
series = {CSED '16}
}

@inproceedings{10.1145/2804345.2804348,
author = {Amalfitano, Domenico and Amatucci, Nicola and Fasolino, Anna Rita and Tramontana, Porfirio},
title = {AGRippin: A Novel Search Based Testing Technique for Android Applications},
year = {2015},
isbn = {9781450338158},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2804345.2804348},
doi = {10.1145/2804345.2804348},
abstract = { Recent studies have shown a remarkable need for testing automation techniques in the context of mobile applications. The main contributions in literature in the field of testing automation regard techniques such as Capture/Replay, Model Based, Model Learning and Random techniques. Unfortunately, only the last two typologies of techniques are applicable if no previous knowledge about the application under testing is available. Random techniques are able to generate effective test suites (in terms of source code coverage) but they need a remarkable effort in terms of machine time and the tests they generate are quite inefficient due to their redundancy. Model Learning techniques generate more efficient test suites but often they do not not reach good levels of coverage. In order to generate test suites that are both effective and efficient, we propose in this paper AGRippin, a novel Search Based Testing technique founded on the combination of genetic and hill climbing techniques. We carried out a case study involving five open source Android applications that has demonstrated how the proposed technique is able to generate test suites that are more effective and efficient than the ones generated by a Model Learning technique. },
booktitle = {Proceedings of the 3rd International Workshop on Software Development Lifecycle for Mobile},
pages = {5–12},
numpages = {8},
keywords = {Genetic Algorithms, Android, Search Based Testing},
location = {Bergamo, Italy},
series = {DeMobile 2015}
}

@article{10.1145/3143561,
author = {Chen, Tsong Yueh and Kuo, Fei-Ching and Liu, Huai and Poon, Pak-Lok and Towey, Dave and Tse, T. H. and Zhou, Zhi Quan},
title = {Metamorphic Testing: A Review of Challenges and Opportunities},
year = {2018},
issue_date = {January 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {1},
issn = {0360-0300},
url = {https://doi.org/10.1145/3143561},
doi = {10.1145/3143561},
abstract = {Metamorphic testing is an approach to both test case generation and test result verification. A central element is a set of metamorphic relations, which are necessary properties of the target function or algorithm in relation to multiple inputs and their expected outputs. Since its first publication, we have witnessed a rapidly increasing body of work examining metamorphic testing from various perspectives, including metamorphic relation identification, test case generation, integration with other software engineering techniques, and the validation and evaluation of software systems. In this article, we review the current research of metamorphic testing and discuss the challenges yet to be addressed. We also present visions for further improvement of metamorphic testing and highlight opportunities for new research.},
journal = {ACM Comput. Surv.},
month = {jan},
articleno = {4},
numpages = {27},
keywords = {oracle problem, Metamorphic testing, test case generation, metamorphic relation}
}

@inproceedings{10.5555/2662413.2662427,
author = {Avancini, Andrea and Ceccato, Mariano},
title = {Security Testing of the Communication among Android Applications},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {An important reason behind the popularity of smartphones and tablets is the huge amount of available applications to download, to expand functionalities of the devices with brand new features. In fact, official stores provide a plethora of applications developed by third parties, for entertainment and business, most of which for free. However, confidential data (e.g., phone contacts, global GPS position, banking data and emails) could be disclosed by vulnerable applications. Sensitive applications should carefully validate exchanged data to avoid security problems.In this paper, we propose a novel testing approach to test communication among applications on mobile devices. We present a test case generation strategy and a testing adequacy criterion for Android applications. Our approach has been assessed on three widely used Android applications.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {57–63},
numpages = {7},
keywords = {security testing, testing, mobile applications},
location = {San Francisco, California},
series = {AST '13}
}

@inproceedings{10.1145/3377930.3390194,
author = {Albunian, Nasser and Fraser, Gordon and Sudholt, Dirk},
title = {Causes and Effects of Fitness Landscapes in Unit Test Generation},
year = {2020},
isbn = {9781450371285},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377930.3390194},
doi = {10.1145/3377930.3390194},
abstract = {Search-based unit test generation applies evolutionary search to maximize code coverage. Although the performance of this approach is often good, sometimes it is not, and how the fitness landscape affects this performance is poorly understood. This paper presents a thorough analysis of 331 Java classes by (i) characterizing their fitness landscape using six established fitness landscape measures, (ii) analyzing the impact of these fitness landscape measures on the search, and (iii) investigating the underlying properties of the source code influencing these measures. Our results reveal that classical indicators for rugged fitness landscapes suggest well searchable problems in the case of unit test generation, but the fitness landscape for most problem instances is dominated by detrimental plateaus. A closer look at the underlying source code suggests that these plateaus are frequently caused by code in private methods, methods throwing exceptions, and boolean flags. This suggests that inter-procedural distance metrics and testability transformations could improve search-based test generation.},
booktitle = {Proceedings of the 2020 Genetic and Evolutionary Computation Conference},
pages = {1204–1212},
numpages = {9},
keywords = {genetic algorithm, search-based test generation, empirical software engineering, fitness landscape analysis},
location = {Canc\'{u}n, Mexico},
series = {GECCO '20}
}

@inproceedings{10.1145/2884781.2884793,
author = {Medeiros, Fl\'{a}vio and K\"{a}stner, Christian and Ribeiro, M\'{a}rcio and Gheyi, Rohit and Apel, Sven},
title = {A Comparison of 10 Sampling Algorithms for Configurable Systems},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884793},
doi = {10.1145/2884781.2884793},
abstract = {Almost every software system provides configuration options to tailor the system to the target platform and application scenario. Often, this configurability renders the analysis of every individual system configuration infeasible. To address this problem, researchers have proposed a diverse set of sampling algorithms. We present a comparative study of 10 state-of-the-art sampling algorithms regarding their fault-detection capability and size of sample sets. The former is important to improve software quality and the latter to reduce the time of analysis. In a nutshell, we found that sampling algorithms with larger sample sets are able to detect higher numbers of faults, but simple algorithms with small sample sets, such as most-enabled-disabled, are the most efficient in most contexts. Furthermore, we observed that the limiting assumptions made in previous work influence the number of detected faults, the size of sample sets, and the ranking of algorithms. Finally, we have identified a number of technical challenges when trying to avoid the limiting assumptions, which questions the practicality of certain sampling algorithms.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {643–654},
numpages = {12},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/2905055.2905157,
author = {Khari, Manju and Kumar, Prabhat},
title = {A Novel Approach for Software Test Data Generation Using Cuckoo Algorithm},
year = {2016},
isbn = {9781450339629},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2905055.2905157},
doi = {10.1145/2905055.2905157},
abstract = {Automated software test data is a process of generating data that satisfies a given criterion. Search-based techniques are being effectively used in various fields of software engineering. Recently a new meta-heuristic search-based algorithm, cuckoo search, has been proposed. In this paper, we propose the application of this algorithm for generating test data. We validate the proposed algorithm against test functions and then compare its performance with those of Hill Climbing algorithms. The initial results prove the effectiveness of the proposed algorithm in test data generation.},
booktitle = {Proceedings of the Second International Conference on Information and Communication Technology for Competitive Strategies},
articleno = {98},
numpages = {6},
keywords = {Hill climbing, Optimization, Test data generation, search based testing, Cuckoo search},
location = {Udaipur, India},
series = {ICTCS '16}
}

@inproceedings{10.5555/2020619.2020642,
author = {Pantoquilho, Marta},
title = {Challenges in Testing and Validating Operational Spacecraft Simulators},
year = {2010},
publisher = {Society for Modeling &amp; Simulation International},
address = {Vista, CA},
abstract = {Flight Control Teams (FCT) at the European Space Operations Centre (ESOC) of the European Space Agency (ESA), start flight operations training and preparation on a specific Spacecraft (S/C), about 3 years prior to launch through, mainly, the preparation of flight operations procedures. The training ends just before launch and has its peak starting 6 months before it, through a critical specific training called Simulations Campaign. The FCT starts its preparation while the spacecraft itself is being built and is therefore unavailable for testing. In order to surpass the lack of a real spacecraft with which to test procedures and train, the FCT uses an Operational Spacecraft Simulator that closely models the spacecraft (with its systems and subsystems modelled to the detail).The procurement and management of the development, testing and validation of the Mission Data Systems, such as the Spacecraft Operational Simulator and the Mission Control System, for each specific ESA spacecraft is the responsibility of the Mission Data Systems Division of the Ground Segment Department (OPS-GD) at ESOC.A new operational Simulator is developed per spacecraft based on common reusable infrastructure software. The challenges in the development, testing and validation of complex Operational Spacecraft Simulators are manifold. The first and major challenge resides in the fact that the majority of the requirements on which the development is based, must be generic enough to cope with the lack of detailed Spacecraft specifications, since the Spacecraft itself is also under development. Also, if there are any changes in the Spacecraft systems or subsystems specification consequent changes in the models developed for the Simulator are expected.Generic requirements, however, make it difficult to specify and execute relevant test cases. In addition, the continuously evolving Spacecraft specifications imply constant changes in the test and verification process.This paper intends to illustrate the complex process of testing and verification of Operational Spacecraft Simulators, coping with the parallel development of the spacecraft being modelled and the consequences that this brings.},
booktitle = {Proceedings of the 2010 Conference on Grand Challenges in Modeling &amp; Simulation},
pages = {165–172},
numpages = {8},
keywords = {European space agency, validation, operational spacecraft simulator, testing},
location = {Ottawa, Ontario, Canada},
series = {GCMS '10}
}

@inproceedings{10.1145/1923947.1924029,
author = {Wong, Kenny and Mankovskii, Serge and Kontogiannis, Kostas and M\"{u}ller, Hausi A. and Mylopoulos, John},
title = {Integrated System Diagnosis and Root Cause Analysis},
year = {2010},
publisher = {IBM Corp.},
address = {USA},
url = {https://doi.org/10.1145/1923947.1924029},
doi = {10.1145/1923947.1924029},
abstract = {To support ever-growing numbers of business initiatives and users, companies form diverse software systems into mission-critical enterprise applications whose scale and complexity has caused unprecedented problem determination challenges. As modern systems increase in complexity, due to technologies such as virtualization, service-orientation, mobile computing, and cloud computing, system management has become an increasing concern. Computer system failures at, for example, Google, RIM, Wikipedia, and the TSX have massive, newsworthy effects on users.},
booktitle = {Proceedings of the 2010 Conference of the Center for Advanced Studies on Collaborative Research},
pages = {427–428},
numpages = {2},
location = {Toronto, Ontario, Canada},
series = {CASCON '10}
}

@inproceedings{10.5555/1402821.1402860,
author = {Nguyen, Cu D. and Perini, Anna and Tonella, Paolo},
title = {Ontology-Based Test Generation for Multiagent Systems},
year = {2008},
isbn = {9780981738123},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {This paper investigates software agents testing, and in particular how to automate test generation. We propose a novel approach, which takes advantage of agent interaction ontologies that define content semantic of agent interactions to: (i) generate test inputs; (ii) guide the exploration of the input space during generation; and, (iii) verify messages exchanged among agents with respect to the defined interaction ontology. We integrated the proposed approach into a testing framework, called eCAT, which can generate and evolve test cases automatically, and run them continuously.},
booktitle = {Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 3},
pages = {1315–1320},
numpages = {6},
keywords = {ontology-based test generation},
location = {Estoril, Portugal},
series = {AAMAS '08}
}

@inproceedings{10.1145/1370062.1370077,
author = {Nakagawa, Elisa Yumi and Maldonado, Jos\'{e} Carlos},
title = {Reference Architecture Knowledge Representation: An Experience},
year = {2008},
isbn = {9781605580388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1370062.1370077},
doi = {10.1145/1370062.1370077},
abstract = {Software architectures have played a significant role in determining the success of software systems. In spite of impact of the architectures to the software development and, as a consequence, to the software quality, there is not yet a consensus about which mechanisms work better to describe these architectures. In addition, despite the relevance of reference architectures as an artifact that comprises knowledge of a given domain and supports development of systems for that domain, issues related to their representation have not also had enough attention. In this perspective, this work intends to contribute with an experience of representing reference architectures aiming at easily sharing and reusing knowledge in order to develop software systems. A case study on software testing is presented illustrating our experience.},
booktitle = {Proceedings of the 3rd International Workshop on Sharing and Reusing Architectural Knowledge},
pages = {51–54},
numpages = {4},
keywords = {architectural view, reference architecture, architectural description},
location = {Leipzig, Germany},
series = {SHARK '08}
}

@inproceedings{10.1109/ASE.2015.49,
author = {Ma, Lei and Artho, Cyrille and Zhang, Cheng and Sato, Hiroyuki and Gmeiner, Johannes and Ramler, Rudolf},
title = {GRT: Program-Analysis-Guided Random Testing},
year = {2015},
isbn = {9781509000241},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2015.49},
doi = {10.1109/ASE.2015.49},
abstract = {We propose Guided Random Testing (GRT), which uses static and dynamic analysis to include information on program types, data, and dependencies in various stages of automated test generation. Static analysis extracts knowledge from the system under test. Test coverage is further improved through state fuzzing and continuous coverage analysis. We evaluated GRT on 32 real-world projects and found that GRT outperforms major peer techniques in terms of code coverage (by 13%) and mutation score (by 9%). On the four studied benchmarks of Defects4J, which contain 224 real faults, GRT also shows better fault detection capability than peer techniques, finding 147 faults (66 %). Furthermore, in an in-depth evaluation on the latest versions of ten popular real-world projects, GRT successfully detects over 20 unknown defects that were confirmed by developers.},
booktitle = {Proceedings of the 30th IEEE/ACM International Conference on Automated Software Engineering},
pages = {212–223},
numpages = {12},
keywords = {automatic test generation, static analysis, random testing, dynamic analysis},
location = {Lincoln, Nebraska},
series = {ASE '15}
}

@inproceedings{10.1145/1868328.1868357,
author = {Ostrand, Thomas J. and Weyuker, Elaine J. and Bell, Robert M.},
title = {Programmer-Based Fault Prediction},
year = {2010},
isbn = {9781450304047},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1868328.1868357},
doi = {10.1145/1868328.1868357},
abstract = {Background: Previous research has provided evidence that a combination of static code metrics and software history metrics can be used to predict with surprising success which files in the next release of a large system will have the largest numbers of defects. In contrast, very little research exists to indicate whether information about individual developers can profitably be used to improve predictions.Aims: We investigate whether files in a large system that are modified by an individual developer consistently contain either more or fewer faults than the average of all files in the system. The goal of the investigation is to determine whether information about which particular developer modified a file is able to improve defect predictions. We also continue an earlier study to evaluate the use of counts of the number of developers who modified a file as predictors of the file's future faultiness.Method: We analyzed change reports filed by 107 programmers for 16 releases of a system with 1,400,000 LOC and 3100 files. A "bug ratio" was defined for programmers, measuring the proportion of faulty files in release R out of all files modified by the programmer in release R-1. The study compares the bug ratios of individual programmers to the average bug ratio, and also assesses the consistency of the bug ratio across releases for individual programmers.Results: Bug ratios varied widely among all the programmers, as well as for many individual programmers across all the releases that they participated in. We found a statistically significant correlation between the bug ratios for programmers for the first half of changed files versus the ratios for the second half, indicating a measurable degree of persistence in the bug ratio. However, when the computation was repeated with the bug ratio controlled not only by release, but also by file size, the correlation disappeared. In addition to the bug ratios, we confirmed that counts of the cumulative number of different developers changing a file over its lifetime can help to improve predictions, while other developer counts are not helpful.Conclusions: The results from this preliminary study indicate that adding information to a model about which particular developer modified a file is not likely to improve defect predictions. The study is limited to a single large system, and its results may not hold more widely. The bug ratio is only one way of measuring the "fault-proneness" of an individual programmer's coding, and we intend to investigate other ways of evaluating bug introduction by individuals.},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
articleno = {19},
numpages = {10},
keywords = {fault-prone, empirical study, regression model, bug ratio, software faults, prediction},
location = {Timi\c{s}oara, Romania},
series = {PROMISE '10}
}

@inproceedings{10.1145/1217935.1217972,
author = {Yuan, Chun and Lao, Ni and Wen, Ji-Rong and Li, Jiwei and Zhang, Zheng and Wang, Yi-Min and Ma, Wei-Ying},
title = {Automated Known Problem Diagnosis with Event Traces},
year = {2006},
isbn = {1595933220},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1217935.1217972},
doi = {10.1145/1217935.1217972},
abstract = {Computer problem diagnosis remains a serious challenge to users and support professionals. Traditional troubleshooting methods relying heavily on human intervention make the process inefficient and the results inaccurate even for solved problems, which contribute significantly to user's dissatisfaction. We propose to use system behavior information such as system event traces to build correlations with solved problems, instead of using only vague text descriptions as in existing practices. The goal is to enable automatic identification of the root cause of a problem if it is a known one, which would further lead to its resolution. By applying statistical learning techniques to classifying system call sequences, we show our approach can achieve considerable accuracy of root cause recognition by studying four case examples.},
booktitle = {Proceedings of the 1st ACM SIGOPS/EuroSys European Conference on Computer Systems 2006},
pages = {375–388},
numpages = {14},
keywords = {support vector machine, system call sequences, root cause analysis},
location = {Leuven, Belgium},
series = {EuroSys '06}
}

@article{10.1145/1218063.1217972,
author = {Yuan, Chun and Lao, Ni and Wen, Ji-Rong and Li, Jiwei and Zhang, Zheng and Wang, Yi-Min and Ma, Wei-Ying},
title = {Automated Known Problem Diagnosis with Event Traces},
year = {2006},
issue_date = {October 2006},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {4},
issn = {0163-5980},
url = {https://doi.org/10.1145/1218063.1217972},
doi = {10.1145/1218063.1217972},
abstract = {Computer problem diagnosis remains a serious challenge to users and support professionals. Traditional troubleshooting methods relying heavily on human intervention make the process inefficient and the results inaccurate even for solved problems, which contribute significantly to user's dissatisfaction. We propose to use system behavior information such as system event traces to build correlations with solved problems, instead of using only vague text descriptions as in existing practices. The goal is to enable automatic identification of the root cause of a problem if it is a known one, which would further lead to its resolution. By applying statistical learning techniques to classifying system call sequences, we show our approach can achieve considerable accuracy of root cause recognition by studying four case examples.},
journal = {SIGOPS Oper. Syst. Rev.},
month = {apr},
pages = {375–388},
numpages = {14},
keywords = {system call sequences, root cause analysis, support vector machine}
}

@inproceedings{10.1145/2593783.2593791,
author = {You, Dongjiang and Amundson, Isaac and Hareland, Scott A. and Rayadurgam, Sanjai},
title = {Practical Aspects of Building a Constrained Random Test Framework for Safety-Critical Embedded Systems},
year = {2014},
isbn = {9781450328517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2593783.2593791},
doi = {10.1145/2593783.2593791},
abstract = { In the safety-critical embedded system industry, one of the key challenges is to demonstrate the robustness and dependability of the product prior to market release, which is typically done using various verification and validation (V&amp;V) strategies. Directed verification testing is a common strategy that performs black-box testing at the system level; however, it only samples a small set of specific system behaviors and requires heavily manual effort. In this paper, we describe our experience and lessons learned of applying the concept of constrained random testing on safety-critical embedded systems as a complimentary testing methodology. Constrained random testing enables us to cover many more system behaviors through random input variations, random fault injections, and automatic output comparisons. Additionally, it can reduce manual effort and increase confidence on the dependability of both firmware and hardware. },
booktitle = {Proceedings of the 1st International Workshop on Modern Software Engineering Methods for Industrial Automation},
pages = {17–25},
numpages = {9},
keywords = {practical experience, constrained random testing, Safety-critical embedded systems},
location = {Hyderabad, India},
series = {MoSEMInA 2014}
}

@inproceedings{10.1145/2950290.2983954,
author = {Busjaeger, Benjamin and Xie, Tao},
title = {Learning for Test Prioritization: An Industrial Case Study},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2983954},
doi = {10.1145/2950290.2983954},
abstract = { Modern cloud-software providers, such as Salesforce.com, increasingly adopt large-scale continuous integration environments. In such environments, assuring high developer productivity is strongly dependent on conducting testing efficiently and effectively. Specifically, to shorten feedback cycles, test prioritization is popularly used as an optimization mechanism for ranking tests to run by their likelihood of revealing failures. To apply test prioritization in industrial environments, we present a novel approach (tailored for practical applicability) that integrates multiple existing techniques via a systematic framework of machine learning to rank. Our initial empirical evaluation on a large real-world dataset from Salesforce.com shows that our approach significantly outperforms existing individual techniques. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {975–980},
numpages = {6},
keywords = {Regression testing, test prioritization, learning to rank},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

