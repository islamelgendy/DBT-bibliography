@inproceedings{10.1145/1083292.1083296,
author = {Wagner, Stefan and Seifert, Tilman},
title = {Software Quality Economics for Defect-Detection Techniques Using Failure Prediction},
year = {2005},
isbn = {1595931228},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083292.1083296},
doi = {10.1145/1083292.1083296},
abstract = {Defect-detection techniques, like reviews or tests, are still the prevalent method to assure the quality of software. However, the economics behind those techniques are not fully understood. It is not always obvious when and for how long to use which technique. A cost model for defect-detection techniques is proposed that uses a reliability model and expert opinion for cost estimations and predictions. It is detailed enough to allow fine-grained estimates but also can be used with different defect-detection techniques not only testing. An application of the model is shown using partly data from an industrial project.},
booktitle = {Proceedings of the Third Workshop on Software Quality},
pages = {1–6},
numpages = {6},
keywords = {reliability model, software quality costs},
location = {St. Louis, Missouri},
series = {3-WoSQ}
}

@article{10.1145/1082983.1083296,
author = {Wagner, Stefan and Seifert, Tilman},
title = {Software Quality Economics for Defect-Detection Techniques Using Failure Prediction},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1083296},
doi = {10.1145/1082983.1083296},
abstract = {Defect-detection techniques, like reviews or tests, are still the prevalent method to assure the quality of software. However, the economics behind those techniques are not fully understood. It is not always obvious when and for how long to use which technique. A cost model for defect-detection techniques is proposed that uses a reliability model and expert opinion for cost estimations and predictions. It is detailed enough to allow fine-grained estimates but also can be used with different defect-detection techniques not only testing. An application of the model is shown using partly data from an industrial project.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–6},
numpages = {6},
keywords = {software quality costs, reliability model}
}

@inproceedings{10.1145/1083231.1083244,
author = {Ruthruff, Joseph R. and Burnett, Margaret},
title = {Six Challenges in Supporting End-User Debugging},
year = {2005},
isbn = {1595931317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1083231.1083244},
doi = {10.1145/1083231.1083244},
abstract = {This paper summarizes six challenges in end-user programming that can impact the debugging efforts of end users. These challenges have been derived through our experiences and empirical investigation of interactive fault localization techniques in the spreadsheet paradigm. Our contributions reveal several insights into debugging techniques for end-user programmers, particularly fault localization techniques, that can help guide the direction of future end-user software engineering research.},
booktitle = {Proceedings of the First Workshop on End-User Software Engineering},
pages = {1–6},
numpages = {6},
keywords = {fault localization, end-user software engineering, end-user programming, debugging},
location = {St. Louis, Missouri},
series = {WEUSE I}
}

@article{10.1145/1082983.1083244,
author = {Ruthruff, Joseph R. and Burnett, Margaret},
title = {Six Challenges in Supporting End-User Debugging},
year = {2005},
issue_date = {July 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {4},
issn = {0163-5948},
url = {https://doi.org/10.1145/1082983.1083244},
doi = {10.1145/1082983.1083244},
abstract = {This paper summarizes six challenges in end-user programming that can impact the debugging efforts of end users. These challenges have been derived through our experiences and empirical investigation of interactive fault localization techniques in the spreadsheet paradigm. Our contributions reveal several insights into debugging techniques for end-user programmers, particularly fault localization techniques, that can help guide the direction of future end-user software engineering research.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {may},
pages = {1–6},
numpages = {6},
keywords = {debugging, end-user programming, fault localization, end-user software engineering}
}

@inproceedings{10.5555/2820518.2820550,
author = {Saha, Ripon K. and Lawall, Julia and Khurshid, Sarfraz and Perry, Dewayne E.},
title = {Are These Bugs Really "Normal"?},
year = {2015},
isbn = {9780769555942},
publisher = {IEEE Press},
abstract = {Understanding the severity of reported bugs is important in both research and practice. In particular, a number of recently proposed mining-based software engineering techniques predict bug severity, bug report quality, and bug-fix time, according to this information. Many bug tracking systems provide a field "severity" offering options such as "severe", "normal", and "minor", with "normal" as the default. However, there is a widespread perception that for many bug reports the label "normal" may not reflect the actual severity, because reporters may overlook setting the severity or may not feel confident enough to do so. In many cases, researchers ignore "normal" bug reports, and thus overlook a large percentage of the reports provided. On the other hand, treating them all together risks mixing reports that have very diverse properties. In this study, we investigate the extent to which "normal" bug reports actually have the "normal" severity. We find that many "normal" bug reports in practice are not normal. Furthermore, this misclassification can have a significant impact on the accuracy of mining-based tools and studies that rely on bug report severity information.},
booktitle = {Proceedings of the 12th Working Conference on Mining Software Repositories},
pages = {258–268},
numpages = {11},
keywords = {mining software repositories, bug tracking system, bug severity},
location = {Florence, Italy},
series = {MSR '15}
}

@article{10.1145/333630.333634,
author = {Devanbu, Premkumar T.},
title = {Re-Targetability in Software Tools},
year = {1999},
issue_date = {Fall 1999},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {3},
issn = {1559-6915},
url = {https://doi.org/10.1145/333630.333634},
doi = {10.1145/333630.333634},
abstract = {Software tool construction is a risky business, with uncertain rewards. Many tools never get used. This is a truism: software tools, however brilliantly conceived, well-designed, and meticulously constructed, have little impact unless they are actually adopted by real programmers. While there are no sure-fire ways of ensuring that a tool will be used, experience indicates that retargetability is an important enabler for wide adoption. In this paper, we elaborate on the need for retargetability in software tools, describe some mechanisms that have proven useful in our experience, and outline our future research in the broader area of inter-operability and retargetability.},
journal = {SIGAPP Appl. Comput. Rev.},
month = {sep},
pages = {19–26},
numpages = {8}
}

@inproceedings{10.1145/3472883.3486994,
author = {Huang, Lexiang and Zhu, Timothy},
title = {Tprof: Performance Profiling via Structural Aggregation and Automated Analysis of Distributed Systems Traces},
year = {2021},
isbn = {9781450386388},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3472883.3486994},
doi = {10.1145/3472883.3486994},
abstract = {The traditional approach for performance debugging relies upon performance profilers (e.g., gprof, VTune) that provide average function runtime information. These aggregate statistics help identify slow regions affecting the entire workload, but they are ill-suited for identifying slow regions that only impact a fraction of the workload, such as tail latency effects. This paper takes a new approach to performance profiling by utilizing distributed tracing systems (e.g., Dapper, Zipkin, Jaeger). Since traces provide detailed timing information on a per-request basis, it is possible to group and aggregate tracing data in many different ways to identify the slow parts of the system. Our new approach to trace aggregation uses the structure embedded within traces to hierarchically group similar traces and calculate increasingly detailed aggregate statistics based on how the traces are grouped. We also develop an automated tool for analyzing the hierarchy of statistics to identify the most likely performance issues. Our case study across two complex distributed systems illustrates how our tool is able to find multiple performance issues that lead to 10x and 28x performance improvements in terms of average and tail latency, respectively. Our comparison with a state-of-the-art industry tool shows that our tool can pinpoint performance slowdowns more accurately than current approaches.},
booktitle = {Proceedings of the ACM Symposium on Cloud Computing},
pages = {76–91},
numpages = {16},
keywords = {performance debugging, distributed systems tracing},
location = {Seattle, WA, USA},
series = {SoCC '21}
}

@inproceedings{10.1145/3427228.3427269,
author = {Das, Sanjeev and James, Kedrian and Werner, Jan and Antonakakis, Manos and Polychronakis, Michalis and Monrose, Fabian},
title = {A Flexible Framework for Expediting Bug Finding by Leveraging Past (Mis-)Behavior to Discover New Bugs},
year = {2020},
isbn = {9781450388580},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427228.3427269},
doi = {10.1145/3427228.3427269},
abstract = { Among various fuzzing approaches, coverage-guided grey-box fuzzing is perhaps the most prominent, due to its ease of use and effectiveness. Using this approach, the selection of inputs focuses on maximizing program coverage, e.g., in terms of the different branches that have been traversed. In this work, we begin with the observation that selecting any input that explores a new path, and giving equal weight to all paths, can lead to severe inefficiencies. For instance, although seemingly “new” crashes involving previously unexplored paths may be discovered, these often have the same root cause and actually correspond to the same bug. To address these inefficiencies, we introduce a framework that incorporates a tighter feedback loop to guide the fuzzing process in exploring truly diverse code paths. Our framework employs (i) a vulnerability-aware selection of coverage metrics for enhancing the effectiveness of code exploration, (ii) crash deduplication information for early feedback, and (iii) a configurable input culling strategy that interleaves multiple strategies to achieve comprehensiveness. A novel aspect of our work is the use of hardware performance counters to derive coverage metrics. We present an approach for assessing and selecting the hardware events that can be used as a meaningful coverage metric for a target program. The results of our empirical evaluation using real-world programs demonstrate the effectiveness of our approach: in some cases, we explore fewer than 50% of the paths compared to a base fuzzer (AFL, MOpt, and Fairfuzz), yet on average, we improve new bug discovery by 31%, and find the same bugs (as the base) 3.3 times faster. Moreover, although we specifically chose applications that have been subject to recent fuzzing campaigns, we still discovered 9 new vulnerabilities.},
booktitle = {Annual Computer Security Applications Conference},
pages = {345–359},
numpages = {15},
keywords = {Fuzzing, Machine Learning, Hardware Performance Counters},
location = {Austin, USA},
series = {ACSAC '20}
}

@inbook{10.1145/3387940.3391456,
author = {Kang, Sungmin and Feldt, Robert and Yoo, Shin},
title = {SINVAD: Search-Based Image Space Navigation for DNN Image Classifier Test Input Generation},
year = {2020},
isbn = {9781450379632},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3387940.3391456},
abstract = {The testing of Deep Neural Networks (DNNs) has become increasingly important as DNNs are widely adopted by safety critical systems. While many test adequacy criteria have been suggested, automated test input generation for many types of DNNs remains a challenge because the raw input space is too large to randomly sample or to navigate and search for plausible inputs. Consequently, current testing techniques for DNNs depend on small local perturbations to existing inputs, based on the metamorphic testing principle. We propose new ways to search not over the entire image space, but rather over a plausible input space that resembles the true training distribution. This space is constructed using Variational Autoencoders (VAEs), and navigated through their latent vector space. We show that this space helps efficiently produce test inputs that can reveal information about the robustness of DNNs when dealing with realistic tests, opening the field to meaningful exploration through the space of highly structured images.},
booktitle = {Proceedings of the IEEE/ACM 42nd International Conference on Software Engineering Workshops},
pages = {521–528},
numpages = {8}
}

@inproceedings{10.1145/2425415.2425422,
author = {Wang, Shuai and Gotlieb, Arnaud and Liaaen, Marius and Briand, Lionel C.},
title = {Automatic Selection of Test Execution Plans from a Video Conferencing System Product Line},
year = {2012},
isbn = {9781450318099},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2425415.2425422},
doi = {10.1145/2425415.2425422},
abstract = {The Cisco Video Conferencing Systems (VCS) Product Line is composed of many distinct products that can be configured in many different ways. The validation of this product line is currently performed manually during test plan design and test executions' scheduling. For example, the testing of a specific VCS product leads to the manual selection of a set of test cases to be executed and scheduled, depending on the functionalities that are available on the product. In this paper, we develop an alternative approach where the variability of the VCS Product Line is captured by a feature model, while the variability within the set of test cases is captured by a component family model. Using the well-known pure::variants tool approach that establishes links between those two models through restrictions, we can obtain relevant test cases automatically for the testing of a new VCS product. The novelty in this paper lies in the design of a large component family model that organizes a complex test cases structure. We envision a large gain in terms of man-power when a new product is issued and needs to be tested before being marketed.},
booktitle = {Proceedings of the VARiability for You Workshop: Variability Modeling Made Useful for Everyone},
pages = {32–37},
numpages = {6},
location = {Innsbruck, Austria},
series = {VARY '12}
}

@inproceedings{10.1145/3524304.3524310,
author = {Abaei, Golnoush and Tah, Wen Zhong and Toh, Jason Zhern Wee and Hor, Ethan Sheng Jian},
title = {Improving Software Fault Prediction in Imbalanced Datasets Using the Under-Sampling Approach},
year = {2022},
isbn = {9781450385770},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3524304.3524310},
doi = {10.1145/3524304.3524310},
abstract = {To make most software defect-free, a considerable amount of budget needs to be allocated to the software testing phase. As each day goes by, this budget slowly rises, as most software grows in size and complexity, which causes an issue for specific companies that cannot allocate sufficient resources towards testing. To tackle this, many researchers use machine learning methods to create software fault prediction models that can help detect defect-prone modules so that resources can be allocated more efficiently during testing. Although this is a feasible plan, the effectiveness of these machine learning models also depends on a few factors, such as the issue of data imbalance. There are many known techniques in class imbalance research that can potentially improve the performance of prediction models through processing the dataset before providing it as input. However, not all methods are compatible with one another. Before building a prediction model, the dataset undergoes the preprocessing step, the under-sampling, and the feature selection process. This study uses an under-sampling process by employing the Instance Hardness Threshold (IHT), which reduces the number of data present in the majority class. The performance of the proposed approach is evaluated based on eight machine learning algorithms by applying it to eight moderate and highly imbalanced NASA datasets. The results of our proposed approach show improvement in AUC and F1-Score by 33% and 26%, respectively, compared to other research work in some datasets.},
booktitle = {2022 11th International Conference on Software and Computer Applications},
pages = {41–47},
numpages = {7},
keywords = {Imbalanced Dataset, Software Fault Prediction, Testing, Under-sampling},
location = {Melaka, Malaysia},
series = {ICSCA 2022}
}

@inproceedings{10.1145/3190645.3190689,
author = {Woodson, Clinton and Hayes, Jane Huffman and Griffioen, Sarah},
title = {Towards Reproducible Research: Automatic Classification of Empirical Requirements Engineering Papers},
year = {2018},
isbn = {9781450356961},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3190645.3190689},
doi = {10.1145/3190645.3190689},
abstract = {Research must be reproducible in order to make an impact on science and to contribute to the body of knowledge in our field. Yet studies have shown that 70% of research from academic labs cannot be reproduced. In software engineering, and more specifically requirements engineering (RE), reproducible research is rare, with datasets not always available or methods not fully described. This lack of reproducible research hinders progress, with researchers having to replicate an experiment from scratch. A researcher starting out in RE has to sift through conference papers, finding ones that are empirical, then must look through the data available from the empirical paper (if any) to make a preliminary determination if the paper can be reproduced. This paper addresses two parts of that problem, identifying RE papers and identifying empirical papers within the RE papers. Recent RE and empirical conference papers were used to learn features and to build an automatic classifier to identify RE and empirical papers. We introduce the Empirical Requirements Research Classifier (ERRC) method, which uses natural language processing and machine learning to perform supervised classification of conference papers. We compare our method to a baseline keyword-based approach. To evaluate our approach, we examine sets of papers from the IEEE Requirements Engineering conference and the IEEE International Symposium on Software Testing and Analysis. We found that the ERRC method performed better than the baseline method in all but a few cases.},
booktitle = {Proceedings of the ACMSE 2018 Conference},
articleno = {8},
numpages = {7},
keywords = {text classification, information retrieval, requirements engineering, reproducible research, supervised classification learning, statistical analysis, empirical research, machine learning},
location = {Richmond, Kentucky},
series = {ACMSE '18}
}

@article{10.1145/1314234.1314235,
author = {Stafford, Tom and Chau, Patrick Y. K. and Schwarz, Andrew and Lanier, Clinton R.},
title = {From the Editors},
year = {2007},
issue_date = {November 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0095-0033},
url = {https://doi.org/10.1145/1314234.1314235},
doi = {10.1145/1314234.1314235},
abstract = {Interesting times and interesting research at Data Base this year. We'll be providing coverage of a new Chinese Information Systems conference, courtesy of Global Co-Editor Patrick Chau. We have an issue on software testing in process with Al Hevner, and this promises to provide some interesting new perspectives on an underutilized but critical research area in our field.We are expanding our global reach and service dramatically. You'll see important colleagues from distant lands on our masthead as board members, even editors. You'll notice increasing submissions from far parts of the global market. You'll find a new perspective on the practice of scholarship in Information Systems, worldwide, at ACM Data Base!This issue, welcome Guest Editor Ed Roche, who has brought a globe-spanning body of speculative and futuristic writing about emerging virtual worlds enabled by online technology. Authors from the four corners of the eWorld weigh in with discussions of where the Internet is taking us as an experiential and cultural milieu of modern life. The directions for research provided by this extraordinary overview of an emerging field of inquiry are innovative, visionary and dramatic.We hope you like it. We hope you'll send us work equally as innovative. We hope you'll tell your friends around the world about how easy it is to access ACM Data Base with our online portal and review system, as well, located at http://www.editorialmanager.com/sigmisdb.},
journal = {SIGMIS Database},
month = {oct},
pages = {5},
numpages = {1}
}

@inproceedings{10.1145/2950290.2950297,
author = {Liu, Yepang and Xu, Chang and Cheung, Shing-Chi and Terragni, Valerio},
title = {Understanding and Detecting Wake Lock Misuses for Android Applications},
year = {2016},
isbn = {9781450342186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2950290.2950297},
doi = {10.1145/2950290.2950297},
abstract = { Wake locks are widely used in Android apps to protect critical computations from being disrupted by device sleeping. Inappropriate use of wake locks often seriously impacts user experience. However, little is known on how wake locks are used in real-world Android apps and the impact of their misuses. To bridge the gap, we conducted a large-scale empirical study on 44,736 commercial and 31 open-source Android apps. By automated program analysis and manual investigation, we observed (1) common program points where wake locks are acquired and released, (2) 13 types of critical computational tasks that are often protected by wake locks, and (3) eight patterns of wake lock misuses that commonly cause functional and non-functional issues, only three of which had been studied by existing work. Based on our findings, we designed a static analysis technique, Elite, to detect two most common patterns of wake lock misuses. Our experiments on real-world subjects showed that Elite is effective and can outperform two state-of-the-art techniques. },
booktitle = {Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {396–409},
numpages = {14},
keywords = {Wake lock, lock necessity analysis, critical computation},
location = {Seattle, WA, USA},
series = {FSE 2016}
}

@inproceedings{10.1145/1526709.1526830,
author = {Mei, Lijun and Zhang, Zhenyu and Chan, W. K. and Tse, T. H.},
title = {Test Case Prioritization for Regression Testing of Service-Oriented Business Applications},
year = {2009},
isbn = {9781605584874},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1526709.1526830},
doi = {10.1145/1526709.1526830},
abstract = {Regression testing assures the quality of modified service-oriented business applications against unintended changes. However, a typical regression test suite is large in size. Earlier execution of those test cases that may detect failures is attractive. Many existing prioritization techniques order test cases according to their respective coverage of program statements in a previous version of the application. On the other hand, industrial service-oriented business applications are typically written in orchestration languages such as WS-BPEL and integrated with workflow steps and web services via XPath and WSDL. Faults in these artifacts may cause the application to extract wrong data from messages, leading to failures in service compositions. Surprisingly, current regression testing research hardly considers these artifacts. We propose a multilevel coverage model to capture the business process, XPath, and WSDL from the perspective of regression testing. We develop a family of test case prioritization techniques atop the model. Empirical results show that our techniques can achieve significantly higher rates of fault detection than existing techniques.},
booktitle = {Proceedings of the 18th International Conference on World Wide Web},
pages = {901–910},
numpages = {10},
keywords = {service orientation, XPath, test case prioritization, WSDL},
location = {Madrid, Spain},
series = {WWW '09}
}

@inproceedings{10.1145/2370216.2370410,
author = {Kang, Jeong Seok and Park, Hong Seong},
title = {Web-Based Automated Black-Box Testing Framework for Component Based Robot Software},
year = {2012},
isbn = {9781450312240},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2370216.2370410},
doi = {10.1145/2370216.2370410},
abstract = {Reliability of component based robot software depends on the quality of each component because any defective components will have a ripple effect on systems built with them. Thus, testing of unit component, composite component and component based robot software is critical for checking the correctness of the software functionality. This paper proposes Web-based automated black-box testing framework for component based robot software. The proposed framework provides automated testing service, testing for different robot platform and distributed test environment, and sharing test resources. We have implemented the proposed framework and evaluated it on the component based front guidance application to output a guidance statement by sensing front obstacles, and control robot wheels by sensing a back user.},
booktitle = {Proceedings of the 2012 ACM Conference on Ubiquitous Computing},
pages = {852–859},
numpages = {8},
keywords = {automated testing framework, component based robot software, black-box testing, specification based testing, distributed test environment},
location = {Pittsburgh, Pennsylvania},
series = {UbiComp '12}
}

@inproceedings{10.1145/1858996.1859059,
author = {Schulte, Eric and Forrest, Stephanie and Weimer, Westley},
title = {Automated Program Repair through the Evolution of Assembly Code},
year = {2010},
isbn = {9781450301169},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1858996.1859059},
doi = {10.1145/1858996.1859059},
abstract = {A method is described for automatically repairing legacy software at the assembly code level using evolutionary computation. The technique is demonstrated on Java byte code and x86 assembly programs, showing how to find program variations that correct defects while retaining desired behavior. Test cases are used to demonstrate the defect and define required functionality. The paper explores advantages of assembly-level repair over earlier work at the source code level - the ability to repair programs written in many different languages; and the ability to repair bugs that were previously intractable. The paper reports experimental results showing reasonable performance of assembly language repair even on non-trivial programs},
booktitle = {Proceedings of the IEEE/ACM International Conference on Automated Software Engineering},
pages = {313–316},
numpages = {4},
keywords = {evolutionary computation, assembly code, program repair, bytecode, fault localization, legacy software},
location = {Antwerp, Belgium},
series = {ASE '10}
}

@inproceedings{10.1145/2814270.2814303,
author = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin},
title = {Scalable Race Detection for Android Applications},
year = {2015},
isbn = {9781450336895},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2814270.2814303},
doi = {10.1145/2814270.2814303},
abstract = { We present a complete end-to-end dynamic analysis system for finding data races in mobile Android applications. The capabilities of our system significantly exceed the state of the art: our system can analyze real-world application interactions in minutes rather than hours, finds errors inherently beyond the reach of existing approaches, while still (critically) reporting very few false positives. Our system is based on three key concepts: (i) a thorough happens-before model of Android-specific concurrency, (ii) a scalable analysis algorithm for efficiently building and querying the happens-before graph, and (iii) an effective set of domain-specific filters that reduce the number of reported data races by several orders of magnitude. We evaluated the usability and performance of our system on 354 real-world Android applications (e.g., Facebook). Our system analyzes a minute of end-user interaction with the application in about 24 seconds, while current approaches take hours to complete. Inspecting the results for 8 large open-source applications revealed 15 harmful bugs of diverse kinds. Some of the bugs we reported were confirmed and fixed by developers. },
booktitle = {Proceedings of the 2015 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {332–348},
numpages = {17},
keywords = {Data Races, Happens-before, Non-determinism, Android},
location = {Pittsburgh, PA, USA},
series = {OOPSLA 2015}
}

@article{10.1145/2858965.2814303,
author = {Bielik, Pavol and Raychev, Veselin and Vechev, Martin},
title = {Scalable Race Detection for Android Applications},
year = {2015},
issue_date = {October 2015},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {50},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/2858965.2814303},
doi = {10.1145/2858965.2814303},
abstract = { We present a complete end-to-end dynamic analysis system for finding data races in mobile Android applications. The capabilities of our system significantly exceed the state of the art: our system can analyze real-world application interactions in minutes rather than hours, finds errors inherently beyond the reach of existing approaches, while still (critically) reporting very few false positives. Our system is based on three key concepts: (i) a thorough happens-before model of Android-specific concurrency, (ii) a scalable analysis algorithm for efficiently building and querying the happens-before graph, and (iii) an effective set of domain-specific filters that reduce the number of reported data races by several orders of magnitude. We evaluated the usability and performance of our system on 354 real-world Android applications (e.g., Facebook). Our system analyzes a minute of end-user interaction with the application in about 24 seconds, while current approaches take hours to complete. Inspecting the results for 8 large open-source applications revealed 15 harmful bugs of diverse kinds. Some of the bugs we reported were confirmed and fixed by developers. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {332–348},
numpages = {17},
keywords = {Non-determinism, Data Races, Happens-before, Android}
}

@inproceedings{10.1145/781131.781148,
author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
title = {Bug Isolation via Remote Program Sampling},
year = {2003},
isbn = {1581136625},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/781131.781148},
doi = {10.1145/781131.781148},
abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
booktitle = {Proceedings of the ACM SIGPLAN 2003 Conference on Programming Language Design and Implementation},
pages = {141–154},
numpages = {14},
keywords = {statistical debugging, bug isolation, assertions, random sampling, feature selection, logistic regression},
location = {San Diego, California, USA},
series = {PLDI '03}
}

@article{10.1145/780822.781148,
author = {Liblit, Ben and Aiken, Alex and Zheng, Alice X. and Jordan, Michael I.},
title = {Bug Isolation via Remote Program Sampling},
year = {2003},
issue_date = {May 2003},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {5},
issn = {0362-1340},
url = {https://doi.org/10.1145/780822.781148},
doi = {10.1145/780822.781148},
abstract = {We propose a low-overhead sampling infrastructure for gathering information from the executions experienced by a program's user community. Several example applications illustrate ways to use sampled instrumentation to isolate bugs. Assertion-dense code can be transformed to share the cost of assertions among many users. Lacking assertions, broad guesses can be made about predicates that predict program errors and a process of elimination used to whittle these down to the true bug. Finally, even for non-deterministic bugs such as memory corruption, statistical modeling based on logistic regression allows us to identify program behaviors that are strongly correlated with failure and are therefore likely places to look for the error.},
journal = {SIGPLAN Not.},
month = {may},
pages = {141–154},
numpages = {14},
keywords = {feature selection, bug isolation, statistical debugging, assertions, logistic regression, random sampling}
}

@inproceedings{10.1145/1566445.1566533,
author = {Pava, Jairo and Enoex, Courtney and Hernandez, Yanelis},
title = {A Self-Configuring Test Harness for Web Applications},
year = {2009},
isbn = {9781605584218},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1566445.1566533},
doi = {10.1145/1566445.1566533},
abstract = {As web applications become more complex and dependent on numerous web technologies, regression testing becomes a time-consuming and expensive endeavor. Many approaches have been proposed to automate the testing process; however few of the approaches utilize the concepts of autonomic computing to symplify the management of a test harness. In this paper, we introduce a self-configuring autonomic test harness. Our test harness monitors the client-side of a web application to detect which web technologies were used to implement the interface. It then automatically selects a testing tool capable of validating any technology-specific features, generates an automated test script, and runs it on the web application.Test generation is performed using a model-driven approach which encodes test cases in a platform independent manner, and transforms them into platform specific tests using a model of various web technologies. In this way, our approach supports regression testing from the web interface as the application migrates to new technologies. We also present the details of a prototype used to demonstrate the approach.},
booktitle = {Proceedings of the 47th Annual Southeast Regional Conference},
articleno = {66},
numpages = {6},
keywords = {regression testing, self testing, autonomic computing, model driven development, self-configuration},
location = {Clemson, South Carolina},
series = {ACM-SE 47}
}

@article{10.1145/1046931.1046946,
author = {Donat, Michael},
title = {Orchestrating an Automated Test Lab: Composing a Score Can Help Us Manage the Complexity of Testing Distributed Apps.},
year = {2005},
issue_date = {February 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {3},
number = {1},
issn = {1542-7730},
url = {https://doi.org/10.1145/1046931.1046946},
doi = {10.1145/1046931.1046946},
abstract = {Networking and the Internet are encouraging increasing levels of interaction and collaboration between people and their software. Whether users are playing games or composing legal documents, their applications need to manage the complex interleaving of actions from multiple machines over potentially unreliable connections. As an example, Silicon Chalk is a distributed application designed to enhance the in-class experience of instructors and students. Its distributed nature requires that we test with multiple machines. Manual testing is too tedious, expensive, and inconsistent to be effective. While automating our testing, however, we have found it very labor intensive to maintain a set of scripts describing each machine’s portion of a given test. Maintainability suffers because the test description is spread over several files.},
journal = {Queue},
month = {feb},
pages = {46–53},
numpages = {8}
}

@article{10.1145/3439775,
author = {Ore, John-Paul and Detweiler, Carrick and Elbaum, Sebastian},
title = {An Empirical Study on Type Annotations: Accuracy, Speed, and Suggestion Effectiveness},
year = {2021},
issue_date = {April 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/3439775},
doi = {10.1145/3439775},
abstract = {Type annotations connect variables to domain-specific types. They enable the power of type checking and can detect faults early. In practice, type annotations have a reputation of being burdensome to developers. We lack, however, an empirical understanding of how and why they are burdensome. Hence, we seek to measure the baseline accuracy and speed for developers making type annotations to previously unseen code. We also study the impact of one or more type suggestions. We conduct an empirical study of 97 developers using 20 randomly selected code artifacts from the robotics domain containing physical unit types. We find that subjects select the correct physical type with just 51% accuracy, and a single correct annotation takes about 2 minutes on average. Showing subjects a single suggestion has a strong and significant impact on accuracy both when correct and incorrect, while showing three suggestions retains the significant benefits without the negative effects. We also find that suggestions do not come with a time penalty. We require subjects to explain their annotation choices, and we qualitatively analyze their explanations. We find that identifier names and reasoning about code operations are the primary clues for selecting a type. We also examine two state-of-the-art automated type annotation systems and find opportunities for their improvement.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {feb},
articleno = {20},
numpages = {29},
keywords = {software reliability, robotic systems, program analysis, physical units, dimensional analysis, automated static analysis, annotations, Type checking}
}

