@inproceedings{10.1145/2254064.2254126,
author = {Pradel, Michael and Gross, Thomas R.},
title = {Fully Automatic and Precise Detection of Thread Safety Violations},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254126},
doi = {10.1145/2254064.2254126},
abstract = {Concurrent, object-oriented programs often use thread-safe library classes. Existing techniques for testing a thread-safe class either rely on tests using the class, on formal specifications, or on both. Unfortunately, these techniques often are not fully automatic as they involve the user in analyzing the output. This paper presents an automatic testing technique that reveals concurrency bugs in supposedly thread-safe classes. The analysis requires as input only the class under test and reports only true positives. The key idea is to generate tests in which multiple threads call methods on a shared instance of the tested class. If a concurrent test exhibits an exception or a deadlock that cannot be triggered in any linearized execution of the test, the analysis reports a thread safety violation. The approach is easily applicable, because it is independent of hand-written tests and explicit specifications. The analysis finds 15 concurrency bugs in popular Java libraries, including two previously unknown bugs in the Java standard library.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {521–530},
numpages = {10},
keywords = {testing, thread safety, concurrent test generation},
location = {Beijing, China},
series = {PLDI '12}
}

@article{10.1145/2345156.2254126,
author = {Pradel, Michael and Gross, Thomas R.},
title = {Fully Automatic and Precise Detection of Thread Safety Violations},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345156.2254126},
doi = {10.1145/2345156.2254126},
abstract = {Concurrent, object-oriented programs often use thread-safe library classes. Existing techniques for testing a thread-safe class either rely on tests using the class, on formal specifications, or on both. Unfortunately, these techniques often are not fully automatic as they involve the user in analyzing the output. This paper presents an automatic testing technique that reveals concurrency bugs in supposedly thread-safe classes. The analysis requires as input only the class under test and reports only true positives. The key idea is to generate tests in which multiple threads call methods on a shared instance of the tested class. If a concurrent test exhibits an exception or a deadlock that cannot be triggered in any linearized execution of the test, the analysis reports a thread safety violation. The approach is easily applicable, because it is independent of hand-written tests and explicit specifications. The analysis finds 15 concurrency bugs in popular Java libraries, including two previously unknown bugs in the Java standard library.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {521–530},
numpages = {10},
keywords = {thread safety, testing, concurrent test generation}
}

@inproceedings{10.1145/3196321.3196350,
author = {Vassallo, Carmine and Proksch, Sebastian and Zemp, Timothy and Gall, Harald C.},
title = {Un-Break My Build: Assisting Developers with Build Repair Hints},
year = {2018},
isbn = {9781450357142},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3196321.3196350},
doi = {10.1145/3196321.3196350},
abstract = {Continuous integration is an agile software development practice. Instead of integrating features right before a release, they are constantly being integrated in an automated build process. This shortens the release cycle, improves software quality, and reduces time to market. However, the whole process will come to a halt when a commit breaks the build, which can happen for several reasons, e.g., compilation errors or test failures, and fixing the build suddenly becomes a top priority. Developers not only have to find the cause of the build break and fix it, but they have to be quick in all of it to avoid a delay for others. Unfortunately, these steps require deep knowledge and are often time consuming. To support developers in fixing a build break, we propose Bart, a tool that summarizes the reasons of the build failure and suggests possible solutions found on the Internet. We will show in a case study with eight participants that developersfind Bart useful to understand build breaks and that using Bart substantially reduces the time to fix a build break, on average by 41%.},
booktitle = {Proceedings of the 26th Conference on Program Comprehension},
pages = {41–51},
numpages = {11},
keywords = {build break, error recovery, software development tools, software engineering, summarization, agile software development},
location = {Gothenburg, Sweden},
series = {ICPC '18}
}

@inproceedings{10.1145/1052898.1052915,
author = {McEachen, Nathan and Alexander, Roger T.},
title = {Distributing Classes with Woven Concerns: An Exploration of Potential Fault Scenarios},
year = {2005},
isbn = {1595930426},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1052898.1052915},
doi = {10.1145/1052898.1052915},
abstract = {Aspect-oriented programming (AOP) promises to benefit software engineering by providing a layer of abstraction that can modularize system-level concerns. AOP is still a very young area of research and has yet to receive mainstream acceptance in industry. As a result, the industry as a whole lacks experience and knowledge concerning long term maintenance issues with AOP in deployed commercial applications. Fault models that result from software maintenance in aspect-oriented software development (AOSD) are not nearly as well understood as they are for object-oriented software development (OOSD). This paper will explore some of the long-term maintenance issues that can occur with AspectJ, which is an implementation of AOP for the Java programming language. More specifically, the ability of AspectJ (as of version 1.2) to weave into existing bytecode that already contains woven aspects can create unexpected and potentially unsolvable problems. This will hopefully lead to further discussion in the research community that will result in a better understanding of the long-term maintenance issues inherent in AOSD.},
booktitle = {Proceedings of the 4th International Conference on Aspect-Oriented Software Development},
pages = {192–200},
numpages = {9},
keywords = {aspect interference, aspect-orientation, AspectJ, foreign aspects},
location = {Chicago, Illinois},
series = {AOSD '05}
}

@inproceedings{10.1145/2393596.2393607,
author = {Mani, Senthil and Catherine, Rose and Sinha, Vibha Singhal and Dubey, Avinava},
title = {AUSUM: Approach for Unsupervised Bug Report Summarization},
year = {2012},
isbn = {9781450316149},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2393596.2393607},
doi = {10.1145/2393596.2393607},
abstract = {In most software projects, resolved bugs are archived for future reference. These bug reports contain valuable information on the reported problem, investigation and resolution. When bug triaging, developers look for how similar problems were resolved in the past. Search over bug repository gives the developer a set of recommended bugs to look into. However, the developer still needs to manually peruse the contents of the recommended bugs which might vary in size from a couple of lines to thousands. Automatic summarization of bug reports is one way to reduce the amount of data a developer might need to go through. Prior work has presented learning based approaches for bug summarization. These approaches have the disadvantage of requiring large training set and being biased towards the data on which the model was learnt. In fact, maximum efficacy was reported when the model was trained and tested on bug reports from the same project. In this paper, we present the results of applying four unsupervised summarization techniques for bug summarization. Industrial bug reports typically contain a large amount of noise---email dump, chat transcripts, core-dump---useless sentences from the perspective of summarization. These derail the unsupervised approaches, which are optimized to work on more well-formed documents. We present an approach for noise reduction, which helps to improve the precision of summarization over the base technique (4% to 24% across subjects and base techniques). Importantly, by applying noise reduction, two of the unsupervised techniques became scalable for large sized bug reports.},
booktitle = {Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering},
articleno = {11},
numpages = {11},
keywords = {summarization, unsupervised, bug report},
location = {Cary, North Carolina},
series = {FSE '12}
}

@inproceedings{10.1145/800230.806990,
author = {Seyfer, Harlan K.},
title = {Tailoring Testing to a Specific Compiler—Experiences},
year = {1982},
isbn = {0897910745},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/800230.806990},
doi = {10.1145/800230.806990},
abstract = {The testing of the Univac UCS-Pascal compiler is described. Tests were acquired from various sources, converted from existing tests, and developed in house. Test development and execution using the Univac Test Controller System is illustrated with examples. The experiences gained from this and other compiler testing efforts are described.},
booktitle = {Proceedings of the 1982 SIGPLAN Symposium on Compiler Construction},
pages = {140–152},
numpages = {13},
location = {Boston, Massachusetts, USA},
series = {SIGPLAN '82}
}

@article{10.1145/872726.806990,
author = {Seyfer, Harlan K.},
title = {Tailoring Testing to a Specific Compiler—Experiences},
year = {1982},
issue_date = {June 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {17},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/872726.806990},
doi = {10.1145/872726.806990},
abstract = {The testing of the Univac UCS-Pascal compiler is described. Tests were acquired from various sources, converted from existing tests, and developed in house. Test development and execution using the Univac Test Controller System is illustrated with examples. The experiences gained from this and other compiler testing efforts are described.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {140–152},
numpages = {13}
}

@inproceedings{10.1145/3236024.3236060,
author = {Lin, Qingwei and Hsieh, Ken and Dang, Yingnong and Zhang, Hongyu and Sui, Kaixin and Xu, Yong and Lou, Jian-Guang and Li, Chenggang and Wu, Youjiang and Yao, Randolph and Chintalapati, Murali and Zhang, Dongmei},
title = {Predicting Node Failure in Cloud Service Systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236060},
doi = {10.1145/3236024.3236060},
abstract = {In recent years, many traditional software systems have migrated to cloud computing platforms and are provided as online services. The service quality matters because system failures could seriously affect business and user experience. A cloud service system typically contains a large number of computing nodes. In reality, nodes may fail and affect service availability. In this paper, we propose a failure prediction technique, which can predict the failure-proneness of a node in a cloud service system based on historical data, before node failure actually happens. The ability to predict faulty nodes enables the allocation and migration of virtual machines to the healthy nodes, therefore improving service availability. Predicting node failure in cloud service systems is challenging, because a node failure could be caused by a variety of reasons and reflected by many temporal and spatial signals. Furthermore, the failure data is highly imbalanced. To tackle these challenges, we propose MING, a novel technique that combines: 1) a LSTM model to incorporate the temporal data, 2) a Random Forest model to incorporate spatial data; 3) a ranking model that embeds the intermediate results of the two models as feature inputs and ranks the nodes by their failure-proneness, 4) a cost-sensitive function to identify the optimal threshold for selecting the faulty nodes. We evaluate our approach using real-world data collected from a cloud service system. The results confirm the effectiveness of the proposed approach. We have also successfully applied the proposed approach in real industrial practice.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {480–490},
numpages = {11},
keywords = {node failure, maintenance, service availability, Failure prediction, cloud service systems},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@inproceedings{10.1109/SESoS/WDES.2019.00011,
author = {Song, Jiyoung and T\o{}rring, Jacob O. and Hyun, Sangwon and Jee, Eunkyoung and Bae, Doo-Hwan},
title = {Slicing Executable System-of-Systems Models for Efficient Statistical Verification},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SESoS/WDES.2019.00011},
doi = {10.1109/SESoS/WDES.2019.00011},
abstract = {A System of Systems (SoS), composed of independent constituent systems, can create synergy among its systems to achieve a common goal. Many studies have used statistical model checking techniques to verify how well an SoS can achieve its goals. SoS models are usually complex and probabilistic, which makes statistical verification computationally expensive. To reduce this cost, dynamic slicing techniques can be applied to SoS models since both dynamic slicing and statistical verification focus on the models' execution samples. However, existing dynamic slicing techniques cannot guarantee executable accurate slices of SoS models when the models contain uncertainty. Therefore, we propose a hybrid slicing approach that combines dynamic backward slicing and modified observation-based slicing to produce accurate executable slices. Experimentation on the proposed technique found that the verification time was significantly reduced (47--56%), depending on the property, while preserving the verification results.},
booktitle = {Proceedings of the 7th International Workshop on Software Engineering for Systems-of-Systems and 13th Workshop on Distributed Software Development, Software Ecosystems and Systems-of-Systems},
pages = {18–25},
numpages = {8},
keywords = {model verification, model slicing, system-of-systems, statistical model checking},
location = {Montreal, Quebec, Canada},
series = {SESoS-WDES '19}
}

@inproceedings{10.1109/ASE.2013.6693071,
author = {Xu, Zhihong and Hirzel, Martin and Rothermel, Gregg and Wu, Kun-Lung},
title = {Testing Properties of Dataflow Program Operators},
year = {2013},
isbn = {9781479902156},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2013.6693071},
doi = {10.1109/ASE.2013.6693071},
abstract = {Dataflow programming languages, which represent programs as graphs of data streams and operators, are becoming increasingly popular and being used to create a wide array of commercial software applications. The dependability of programs written in these languages, as well as the systems used to compile and run these programs, hinges on the correctness of the semantic properties associated with operators. Unfortunately, these properties are often poorly defined, and frequently are not checked, and this can lead to a wide range of problems in the programs that use the operators. In this paper we present an approach for improving the dependability of dataflow programs by checking operators for necessary properties. Our approach is dynamic, and involves generating tests whose results are checked to determine whether specific properties hold or not. We present empirical data that shows that our approach is both effective and efficient at assessing the status of properties.},
booktitle = {Proceedings of the 28th IEEE/ACM International Conference on Automated Software Engineering},
pages = {103–113},
numpages = {11},
location = {Silicon Valley, CA, USA},
series = {ASE'13}
}

@inproceedings{10.1145/1321631.1321722,
author = {Pugh, William and Ayewah, Nathaniel},
title = {Unit Testing Concurrent Software},
year = {2007},
isbn = {9781595938824},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1321631.1321722},
doi = {10.1145/1321631.1321722},
abstract = {There are many difficulties associated with developing correct multithreaded software, and many of the activities that are simple for single threaded software are exceptionally hard for multithreaded software. One such example is constructing unit tests involving multiple threads. Given, for example, a blocking queue implementation, writing a test case to show that it blocks and unblocks appropriately using existing testing frameworks is exceptionally hard. In this paper, we describe the MultithreadedTC framework which allows the construction of deterministic and repeatable unit tests for concurrent abstractions. This framework is not designed to test for synchronization errors that lead to rare probabilistic faults under concurrent stress. Rather, this framework allows us to demonstrate that code does provide specific concurrent functionality (e.g., a thread attempting to acquire a lock is blocked if another thread has the lock).We describe the framework and provide empirical comparisons against hand-coded tests designed for Sun's Java concurrency utilities library and against previous frameworks that addressed this same issue. The source code for this framework is available under an open source license.},
booktitle = {Proceedings of the Twenty-Second IEEE/ACM International Conference on Automated Software Engineering},
pages = {513–516},
numpages = {4},
keywords = {JUnit test cases, java, multithreadedTC, testing framework, concurrent abstraction},
location = {Atlanta, Georgia, USA},
series = {ASE '07}
}

@inbook{10.1145/3368089.3409755,
author = {Cha, Sooyoung and Oh, Hakjoo},
title = {Making Symbolic Execution Promising by Learning Aggressive State-Pruning Strategy},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409755},
abstract = {We present HOMI, a new technique to enhance symbolic execution by maintaining only a small number of promising states. In practice, symbolic execution typically maintains as many states as possible in a fear of losing important states. In this paper, however, we show that only a tiny subset of the states plays a significant role in increasing code coverage or reaching bug points. Based on this observation, HOMI aims to minimize the total number of states while keeping “promising” states during symbolic execution. We identify promising states by a learning algorithm that continuously updates the probabilistic pruning strategy based on data accumulated during the testing process. Experimental results show that HOMI greatly increases code coverage and the ability to find bugs of KLEE on open-source C programs.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {147–158},
numpages = {12}
}

@inproceedings{10.1145/2983990.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
isbn = {9781450344449},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2983990.2984038},
doi = {10.1145/2983990.2984038},
abstract = { Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions.  This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable.  We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed. },
booktitle = {Proceedings of the 2016 ACM SIGPLAN International Conference on Object-Oriented Programming, Systems, Languages, and Applications},
pages = {849–863},
numpages = {15},
keywords = {automated testing, miscompilation, Compiler testing, equivalent program variants},
location = {Amsterdam, Netherlands},
series = {OOPSLA 2016}
}

@article{10.1145/3022671.2984038,
author = {Sun, Chengnian and Le, Vu and Su, Zhendong},
title = {Finding Compiler Bugs via Live Code Mutation},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {51},
number = {10},
issn = {0362-1340},
url = {https://doi.org/10.1145/3022671.2984038},
doi = {10.1145/3022671.2984038},
abstract = { Validating optimizing compilers is challenging because it is hard to generate valid test programs (i.e., those that do not expose any undefined behavior). Equivalence Modulo Inputs (EMI) is an effective, promising methodology to tackle this problem. Given a test program with some inputs, EMI mutates the program to derive variants that are semantically equivalent w.r.t. these inputs. The state-of-the-art instantiations of EMI are Orion and Athena, both of which rely on deleting code from or inserting code into code regions that are not executed under the inputs. Although both have demonstrated their ability in finding many bugs in GCC and LLVM, they are still limited due to their mutation strategies that operate only on dead code regions.  This paper presents a novel EMI technique that allows mutation in the entire program (i.e., both live and dead regions). By removing the restriction of mutating only the dead regions, our technique significantly increases the EMI variant space. It also helps to more thoroughly stress test compilers as compilers must optimize mutated live code, whereas mutated dead code might be eliminated. Finally, our technique also makes compiler bugs more noticeable as miscompilations on mutated dead code may not be observable.  We have realized the proposed technique in Hermes. The evaluation demonstrates Hermes’s effectiveness. In 13 months, Hermes found 168 confirmed, valid bugs in GCC and LLVM, of which 132 have already been fixed. },
journal = {SIGPLAN Not.},
month = {oct},
pages = {849–863},
numpages = {15},
keywords = {equivalent program variants, miscompilation, Compiler testing, automated testing}
}

@inproceedings{10.1145/3300115.3309516,
author = {Kangas, Vilma and Pirttinen, Nea and Nygren, Henrik and Leinonen, Juho and Hellas, Arto},
title = {Does Creating Programming Assignments with Tests Lead to Improved Performance in Writing Unit Tests?},
year = {2019},
isbn = {9781450362597},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3300115.3309516},
doi = {10.1145/3300115.3309516},
abstract = {We have constructed a tool, CrowdSorcerer, in which students create programming assignments, their model solutions and associated test cases using a simple input-output format. We have used the tool as a part of an introductory programming course with normal course activities such as programming assignments and a final exam. In our work, we focus on whether creating programming assignments and associated tests correlate with students' performance in a testing-related exam question. We study this through an analysis of the quality of student-written tests within the tool, measured using the number of test cases, line coverage and mutation coverage, and students' performance in testing related exam question, measured using exam points. Finally, we study whether previous programming experience correlates with how students act within the tool and within the testing related exam question.},
booktitle = {Proceedings of the ACM Conference on Global Computing Education},
pages = {106–112},
numpages = {7},
keywords = {testing, crowdsourcing, assignment creation, educational data mining},
location = {Chengdu,Sichuan, China},
series = {CompEd '19}
}

@article{10.1145/356876.356879,
author = {Adrion, W. Richards and Branstad, Martha A. and Cherniavsky, John C.},
title = {Validation, Verification, and Testing of Computer Software},
year = {1982},
issue_date = {June 1982},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {14},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/356876.356879},
doi = {10.1145/356876.356879},
journal = {ACM Comput. Surv.},
month = {jun},
pages = {159–192},
numpages = {34}
}

@inproceedings{10.1145/3368308.3415419,
author = {Singleton, Larry and Zhao, Rui and Song, Myoungkyu and Siy, Harvey},
title = {CryptoTutor: Teaching Secure Coding Practices through Misuse Pattern Detection},
year = {2020},
isbn = {9781450370455},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368308.3415419},
doi = {10.1145/3368308.3415419},
abstract = {Insecure program practices seriously threaten software security. Misusing security primitives in application-level code is not unusual. For example, in mobile banking apps, developers might store customers' privacy information in plaintext, leading to sensitive information leakage. To leverage cryptographic primitives, developers need to correctly select the cryptographic algorithm, appropriate parameters, and sometimes its post-process. While recent research discusses pitfalls in cryptography-related implementations, few academic programs integrate these concepts in their educational programs. One big challenge is the lack of automated guidance on how to utilize existing libraries for secure coding. In this paper, we discuss the prevalence of the problem, especially with respect to implementing programs that utilize cryptography, to motivate the need for better tool support for guidance in writing secure code. We present a tool, CryptoTutor, that can automatically flag common cryptographic misuses and suggest possible repairs. We discuss how tools like CryptoTutor can be integrated into programming courses at the college and pre-college levels.},
booktitle = {Proceedings of the 21st Annual Conference on Information Technology Education},
pages = {403–408},
numpages = {6},
keywords = {secure coding, cryptographic misuse, programming education},
location = {Virtual Event, USA},
series = {SIGITE '20}
}

@inproceedings{10.1145/2991079.2991103,
author = {Pewny, Jannik and Holz, Thorsten},
title = {EvilCoder: Automated Bug Insertion},
year = {2016},
isbn = {9781450347716},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2991079.2991103},
doi = {10.1145/2991079.2991103},
abstract = {The art of finding software vulnerabilities has been covered extensively in the literature and there is a huge body of work on this topic. In contrast, the intentional insertion of exploitable, security-critical bugs has received little (public) attention yet. Wanting more bugs seems to be counterproductive at first sight, but the comprehensive evaluation of bug-finding techniques suffers from a lack of ground truth and the scarcity of bugs.In this paper, we propose EvilCoder, a system to automatically find potentially vulnerable source code locations and modify the source code to be actually vulnerable. More specifically, we leverage automated program analysis techniques to find sensitive sinks which match typical bug patterns (e.g., a sensitive API function with a preceding sanity check), and try to find data-flow connections to user-controlled sources. We then transform the source code such that exploitation becomes possible, for example by removing or modifying input sanitization or other types of security checks. Our tool is designed to randomly pick vulnerable locations and possible modifications, such that it can generate numerous different vulnerabilities on the same software corpus. We evaluated our tool on several open-source projects such as for example libpng and vsftpd, where we found between 22 and 158 unique connected source-sink pairs per project. This translates to hundreds of potentially vulnerable data-flow paths and hundreds of bugs we can insert. We hope to support future bug-finding techniques by supplying freshly generated, bug-ridden test corpora so that such techniques can (finally) be evaluated and compared in a comprehensive and statistically meaningful way.},
booktitle = {Proceedings of the 32nd Annual Conference on Computer Security Applications},
pages = {214–225},
numpages = {12},
location = {Los Angeles, California, USA},
series = {ACSAC '16}
}

@inproceedings{10.1145/18927.18911,
author = {Skibbe, R. E.},
title = {A Practical Approach to the Evaluation of Microcode Systems},
year = {1985},
isbn = {0897911725},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/18927.18911},
doi = {10.1145/18927.18911},
abstract = {This paper describes a microcode-evaluation methodology. The supporting test tools were developed by the IBM General Products Division in Tucson, Arizona, to allow effective and comprehensive evaluations of microcode systems. The methodology has been used successfully by the Tucson Test Laboratory (TTL) during the past several years.The evaluation methodology is characterized by an integrated application of static and dynamic analysis techniques. These two modes of analysis are complementary and they allow a level of automation that can significantly enhance the productivity of a testing organization through the systematic application of automated testing techniques. The methodology also establishes a discipline for the microcode-testing process that promotes a formal program of defect removal. Of course, improving the process of removing defects produces a corresponding enhancement in product quality.},
booktitle = {Proceedings of the 18th Annual Workshop on Microprogramming},
pages = {47–56},
numpages = {10},
location = {Pacific Grove, California, USA},
series = {MICRO 18}
}

@article{10.1145/18906.18911,
author = {Skibbe, R. E.},
title = {A Practical Approach to the Evaluation of Microcode Systems},
year = {1985},
issue_date = {Dec. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {16},
number = {4},
issn = {1050-916X},
url = {https://doi.org/10.1145/18906.18911},
doi = {10.1145/18906.18911},
abstract = {This paper describes a microcode-evaluation methodology. The supporting test tools were developed by the IBM General Products Division in Tucson, Arizona, to allow effective and comprehensive evaluations of microcode systems. The methodology has been used successfully by the Tucson Test Laboratory (TTL) during the past several years.The evaluation methodology is characterized by an integrated application of static and dynamic analysis techniques. These two modes of analysis are complementary and they allow a level of automation that can significantly enhance the productivity of a testing organization through the systematic application of automated testing techniques. The methodology also establishes a discipline for the microcode-testing process that promotes a formal program of defect removal. Of course, improving the process of removing defects produces a corresponding enhancement in product quality.},
journal = {SIGMICRO Newsl.},
month = {dec},
pages = {47–56},
numpages = {10}
}

@inproceedings{10.1109/ASE.2011.6100059,
author = {Robinson, Brian and Ernst, Michael D. and Perkins, Jeff H. and Augustine, Vinay and Li, Nuo},
title = {Scaling up Automated Test Generation: Automatically Generating Maintainable Regression Unit Tests for Programs},
year = {2011},
isbn = {9781457716386},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/ASE.2011.6100059},
doi = {10.1109/ASE.2011.6100059},
abstract = {This paper presents an automatic technique for generating maintainable regression unit tests for programs. We found previous test generation techniques inadequate for two main reasons. First. they were designed for and evaluated upon libraries rather than applications. Second, they were designed to find bugs rather than to create maintainable regression test suites: the test suites that they generated were brittle and hard to understand. This paper presents a suite of techniques that address these problems by enhancing an existing unit test generation system. In experiments using an industrial system, the generated tests achieved good coverage and mutation kill score, were readable by the product's developers, and required few edits as the system under test evolved. While our evaluation is in the context of one test generator, we are aware of many research systems that suffer similar limitations, so our approach and observations are more generally relevant.},
booktitle = {Proceedings of the 2011 26th IEEE/ACM International Conference on Automated Software Engineering},
pages = {23–32},
numpages = {10},
series = {ASE '11}
}

@inproceedings{10.1145/3324884.3416585,
author = {Wang, Jiawei and Kuo, Tzu-yang and Li, Li and Zeller, Andreas},
title = {Assessing and Restoring Reproducibility of Jupyter Notebooks},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416585},
doi = {10.1145/3324884.3416585},
abstract = {Jupyter notebooks---documents that contain live code, equations, visualizations, and narrative text---now are among the most popular means to compute, present, discuss and disseminate scientific findings. In principle, Jupyter notebooks should easily allow to reproduce and extend scientific computations and their findings; but in practice, this is not the case. The individual code cells in Jupyter notebooks can be executed in any order, with identifier usages preceding their definitions and results preceding their computations. In a sample of 936 published notebooks that would be executable in principle, we found that 73% of them would not be reproducible with straightforward approaches, requiring humans to infer (and often guess) the order in which the authors created the cells.In this paper, we present an approach to (1) automatically satisfy dependencies between code cells to reconstruct possible execution orders of the cells; and (2) instrument code cells to mitigate the impact of non-reproducible statements (i.e., random functions) in Jupyter notebooks. Our Osiris prototype takes a notebook as input and outputs the possible execution schemes that reproduce the exact notebook results. In our sample, Osiris was able to reconstruct such schemes for 82.23% of all executable notebooks, which has more than three times better than the state-of-the-art; the resulting reordered code is valid program code and thus available for further testing and analysis.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {138–149},
numpages = {12},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/3468264.3468580,
author = {Lou, Yiling and Zhu, Qihao and Dong, Jinhao and Li, Xia and Sun, Zeyu and Hao, Dan and Zhang, Lu and Zhang, Lingming},
title = {Boosting Coverage-Based Fault Localization via Graph-Based Representation Learning},
year = {2021},
isbn = {9781450385626},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3468264.3468580},
doi = {10.1145/3468264.3468580},
abstract = {Coverage-based fault localization has been extensively studied in the literature due to its effectiveness and lightweightness for real-world systems. However, existing techniques often utilize coverage in an oversimplified way by abstracting detailed coverage into numbers of tests or boolean vectors, thus limiting their effectiveness in practice. In this work, we present a novel coverage-based fault localization technique, GRACE, which fully utilizes detailed coverage information with graph-based representation learning. Our intuition is that coverage can be regarded as connective relationships between tests and program entities, which can be inherently and integrally represented by a graph structure: with tests and program entities as nodes, while with coverage and code structures as edges. Therefore, we first propose a novel graph-based representation to reserve all detailed coverage information and fine-grained code structures into one graph. Then we leverage Gated Graph Neural Network to learn valuable features from the graph-based coverage representation and rank program entities in a listwise way. Our evaluation on the widely used benchmark Defects4J (V1.2.0) shows that GRACE significantly outperforms state-of-the-art coverage-based fault localization: GRACE localizes 195 bugs within Top-1 whereas the best compared technique can at most localize 166 bugs within Top-1. We further investigate the impact of each GRACE component and find that they all positively contribute to GRACE. In addition, our results also demonstrate that GRACE has learnt essential features from coverage, which are complementary to various information used in existing learning-based fault localization. Finally, we evaluate GRACE in the cross-project prediction scenario on extra 226 bugs from Defects4J (V2.0.0), and find that GRACE consistently outperforms state-of-the-art coverage-based techniques.},
booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {664–676},
numpages = {13},
keywords = {Representation Learning, Fault Localization, Graph Neural Network},
location = {Athens, Greece},
series = {ESEC/FSE 2021}
}

@inproceedings{10.1145/2635868.2635916,
author = {Bae, SungGyeong and Cho, Hyunghun and Lim, Inho and Ryu, Sukyoung},
title = {SAFEWAPI: Web API Misuse Detector for Web Applications},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635916},
doi = {10.1145/2635868.2635916},
abstract = { The evolution of Web 2.0 technologies makes web applications prevalent in various platforms including mobile devices and smart TVs. While one of the driving technologies of web applications is JavaScript, the extremely dynamic features of JavaScript make it very difficult to define and detect errors in JavaScript applications. The problem becomes more important and complicated for JavaScript web applications which may lead to severe security vulnerabilities. To help developers write safe JavaScript web applications using vendor-specific Web APIs, vendors specify their APIs often in Web IDL, which enables both API writers and users to communicate better by understanding the expected behaviors of the Web APIs. In this paper, we present SAFEWAPI, a tool to analyze Web APIs and JavaScript web applications that use the Web APIs and to detect possible misuses of Web APIs by the web applications. Even though the JavaScript language semantics allows to call a function defined with some parameters without any arguments, platform developers may require application writers to provide the exact number of arguments. Because the library functions in Web APIs expose their intended semantics clearly to web application developers unlike pure JavaScript functions, we can detect wrong uses of Web APIs precisely. For representative misuses of Web APIs defined by software quality assurance engineers, our SAFEWAPI detects such misuses in real-world JavaScript web applications. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {507–517},
numpages = {11},
keywords = {web application, JavaScript, static analysis, bug detection},
location = {Hong Kong, China},
series = {FSE 2014}
}

