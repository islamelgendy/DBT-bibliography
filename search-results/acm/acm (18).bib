@inproceedings{10.1145/3460319.3464825,
author = {Humbatova, Nargiz and Jahangirova, Gunel and Tonella, Paolo},
title = {DeepCrime: Mutation Testing of Deep Learning Systems Based on Real Faults},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464825},
doi = {10.1145/3460319.3464825},
abstract = {Deep Learning (DL) solutions are increasingly adopted, but how to test them remains a major open research problem. Existing and new testing techniques have been proposed for and adapted to DL systems, including mutation testing. However, no approach has investigated the possibility to simulate the effects of real DL faults by means of mutation operators. We have defined 35 DL mutation operators relying on 3 empirical studies about real faults in DL systems. We followed a systematic process to extract the mutation operators from the existing fault taxonomies, with a formal phase of conflict resolution in case of disagreement. We have implemented 24 of these DL mutation operators into DeepCrime, the first source-level pre-training mutation tool based on real DL faults. We have assessed our mutation operators to understand their characteristics: whether they produce interesting, i.e., killable but not trivial, mutations. Then, we have compared the sensitivity of our tool to the changes in the quality of test data with that of DeepMutation++, an existing post-training DL mutation tool.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {67–78},
numpages = {12},
keywords = {mutation testing, deep learning, real faults},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/2338965.2336775,
author = {Fry, Zachary P. and Landau, Bryan and Weimer, Westley},
title = {A Human Study of Patch Maintainability},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336775},
doi = {10.1145/2338965.2336775},
abstract = { Identifying and fixing defects is a crucial and expensive part of the software lifecycle. Measuring the quality of bug-fixing patches is a difficult task that affects both functional correctness and the future maintainability of the code base. Recent research interest in automatic patch generation makes a systematic understanding of patch maintainability and understandability even more critical.  We present a human study involving over 150 participants, 32 real-world defects, and 40 distinct patches. In the study, humans perform tasks that demonstrate their understanding of the control flow, state, and maintainability aspects of code patches. As a baseline we use both human-written patches that were later reverted and also patches that have stood the test of time to ground our results. To address any potential lack of readability with machine-generated patches, we propose a system wherein such patches are augmented with synthesized, human-readable documentation that summarizes their effects and context. Our results show that machine-generated patches are slightly less maintainable than human-written ones, but that trend reverses when machine patches are augmented with our synthesized documentation. Finally, we examine the relationship between code features (such as the ratio of variable uses to assignments) with participants' abilities to complete the study tasks and thus explain a portion of the broad concept of patch quality. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {177–187},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@article{10.1145/1459352.1459354,
author = {Hierons, Robert M. and Bogdanov, Kirill and Bowen, Jonathan P. and Cleaveland, Rance and Derrick, John and Dick, Jeremy and Gheorghe, Marian and Harman, Mark and Kapoor, Kalpesh and Krause, Paul and L\"{u}ttgen, Gerald and Simons, Anthony J. H. and Vilkomir, Sergiy and Woodward, Martin R. and Zedan, Hussein},
title = {Using Formal Specifications to Support Testing},
year = {2009},
issue_date = {February 2009},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {41},
number = {2},
issn = {0360-0300},
url = {https://doi.org/10.1145/1459352.1459354},
doi = {10.1145/1459352.1459354},
abstract = {Formal methods and testing are two important approaches that assist in the development of high-quality software. While traditionally these approaches have been seen as rivals, in recent years a new consensus has developed in which they are seen as complementary. This article reviews the state of the art regarding ways in which the presence of a formal specification can be used to assist testing.},
journal = {ACM Comput. Surv.},
month = {feb},
articleno = {9},
numpages = {76},
keywords = {Software testing, formal methods}
}

@inproceedings{10.1145/1272680.1272683,
author = {Duarte, Alexandre N\'{o}brega and Nyczyk, Piotr and Retico, Antonio and Vicinanza, Domenico},
title = {Global Grid Monitoring: The EGEE/WLCG Case},
year = {2007},
isbn = {9781595937162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1272680.1272683},
doi = {10.1145/1272680.1272683},
abstract = {Grids have the potential to revolutionize computing by providing ubiquitous, on demand access to computational services and resources. However, grid systems are extremely large, complex and prone to failures. In this paper we present a tool able to to check if a given grid service works as expected for a given user or set of users on the different resources available on a grid. Our solution deals with the grid services as single components that should produce an expected output to a pre-defined input, what is quite similar to unit testing. Our tool, called Service Availability Monitoring or SAM, is being currently used to monitor some of the largest (maybe the largest) production grids available today.},
booktitle = {Proceedings of the 2007 Workshop on Grid Monitoring},
pages = {9–16},
numpages = {8},
keywords = {grid monitoring, EGEE, service availability monitoring, software testing, WLCG},
location = {Monterey, California, USA},
series = {GMW '07}
}

@inproceedings{10.1145/3180155.3180164,
author = {Bell, Jonathan and Legunsen, Owolabi and Hilton, Michael and Eloussi, Lamyaa and Yung, Tifany and Marinov, Darko},
title = {D<span class="smallcaps SmallerCapital">e</span>F<span class="smallcaps SmallerCapital">laker</span>: Automatically Detecting Flaky Tests},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180164},
doi = {10.1145/3180155.3180164},
abstract = {Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle.We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1, 874 flaky tests from 4, 846 failures, with a low false alarm rate (1.5%). DeFlaker had a higher recall (95.5% vs. 23%) of confirmed flaky tests than Maven's default flaky test detector.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {433–444},
numpages = {12},
keywords = {flaky tests, software testing, code coverage},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@inproceedings{10.1145/2338965.2336768,
author = {Canali, Davide and Lanzi, Andrea and Balzarotti, Davide and Kruegel, Christopher and Christodorescu, Mihai and Kirda, Engin},
title = {A Quantitative Study of Accuracy in System Call-Based Malware Detection},
year = {2012},
isbn = {9781450314541},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338965.2336768},
doi = {10.1145/2338965.2336768},
abstract = { Over the last decade, there has been a significant increase in the number and sophistication of malware-related attacks and infections. Many detection techniques have been proposed to mitigate the malware threat. A running theme among existing detection techniques is the similar promises of high detection rates, in spite of the wildly different models (or specification classes) of malicious activity used. In addition, the lack of a common testing methodology and the limited datasets used in the experiments make difficult to compare these models in order to determine which ones yield the best detection accuracy. In this paper, we present a systematic approach to measure how the choice of behavioral models influences the quality of a malware detector. We tackle this problem by executing a large number of testing experiments, in which we explored the parameter space of over 200 different models, corresponding to more than 220 million of signatures. Our results suggest that commonly held beliefs about simple models are incorrect in how they relate changes in complexity to changes in detection accuracy. This implies that accuracy is non-linear across the model space, and that analytical reasoning is insufficient for finding an optimal model, and has to be supplemented by testing and empirical measurements. },
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
pages = {122–132},
numpages = {11},
location = {Minneapolis, MN, USA},
series = {ISSTA 2012}
}

@inproceedings{10.1145/2897010.2897018,
author = {Rueda, Urko and Just, Ren\'{e} and Galeotti, Juan P. and Vos, Tanja E. J.},
title = {Unit Testing Tool Competition: Round Four},
year = {2016},
isbn = {9781450341660},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2897010.2897018},
doi = {10.1145/2897010.2897018},
abstract = {This paper describes the methodology and results of the 4th edition of the Java Unit Testing Tool Competition. This year's competition features a number of infrastructure improvements, new test effectiveness metrics, and the evaluation of the test generation tools for multiple time budgets. Overall, the competition evaluated four automated test generation tools. This paper details the methodology and contains the full results of the competition.},
booktitle = {Proceedings of the 9th International Workshop on Search-Based Software Testing},
pages = {19–28},
numpages = {10},
keywords = {benchmark, Java, tool competition, mutation testing, defects4j, automated unit testing},
location = {Austin, Texas},
series = {SBST '16}
}

@inproceedings{10.1145/3278186.3278190,
author = {Vercammen, Sten and Ghafari, Mohammad and Demeyer, Serge and Borg, Markus},
title = {Goal-Oriented Mutation Testing with Focal Methods},
year = {2018},
isbn = {9781450360531},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3278186.3278190},
doi = {10.1145/3278186.3278190},
abstract = {Mutation testing is the state-of-the-art technique for assessing the fault-detection capacity of a test suite. Unfortunately, mutation testing consumes enormous computing resources because it runs the whole test suite for each and every injected mutant. In this paper we explore fine-grained traceability links at method level (named focal methods), to reduce the execution time of mutation testing and to verify the quality of the test cases for each individual method, instead of the usually verified overall test suite quality. Validation of our approach on the open source Apache Ant project shows a speed-up of 573.5x for the mutants located in focal methods with a quality score of 80%.},
booktitle = {Proceedings of the 9th ACM SIGSOFT International Workshop on Automating TEST Case Design, Selection, and Evaluation},
pages = {23–30},
numpages = {8},
keywords = {Mutation testing, Focal methods, Feasibility study, Software testing},
location = {Lake Buena Vista, FL, USA},
series = {A-TEST 2018}
}

@inproceedings{10.1145/3460319.3464818,
author = {Lyu, Yingjun and Volokh, Sasha and Halfond, William G. J. and Tripp, Omer},
title = {SAND: A Static Analysis Approach for Detecting SQL Antipatterns},
year = {2021},
isbn = {9781450384599},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3460319.3464818},
doi = {10.1145/3460319.3464818},
abstract = {Local databases underpin important features in many mobile applications, such as responsiveness in the face of poor connectivity. However, failure to use such databases correctly can lead to high resource consumption or even security vulnerabilities. We present SAND, an extensible static analysis approach that checks for misuse of local databases, also known as SQL antipatterns, in mobile apps. SAND features novel abstractions for common forms of application/database interactions, which enables concise and precise specification of the antipatterns that SAND checks for. To validate the efficacy of SAND, we have experimented with a diverse suite of 1,000 Android apps. We show that the abstractions that power SAND allow concise specification of all the known antipatterns from the literature (12-74 LOC), and that the antipatterns are modeled accurately (99.4-100% precision). As for performance, SAND requires on average 41 seconds to complete a scan on a mobile app.},
booktitle = {Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis},
pages = {270–282},
numpages = {13},
keywords = {database, performance, security, Mobile applications},
location = {Virtual, Denmark},
series = {ISSTA 2021}
}

@inproceedings{10.1145/2884781.2884787,
author = {Gopinath, Rahul and Alipour, Mohammad Amin and Ahmed, Iftekhar and Jensen, Carlos and Groce, Alex},
title = {On the Limits of Mutation Reduction Strategies},
year = {2016},
isbn = {9781450339001},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2884781.2884787},
doi = {10.1145/2884781.2884787},
abstract = {Although mutation analysis is considered the best way to evaluate the effectiveness of a test suite, hefty computational cost often limits its use. To address this problem, various mutation reduction strategies have been proposed, all seeking to reduce the number of mutants while maintaining the representativeness of an exhaustive mutation analysis. While research has focused on the reduction achieved, the effectiveness of these strategies in selecting representative mutants, and the limits in doing so have not been investigated, either theoretically or empirically.We investigate the practical limits to the effectiveness of mutation reduction strategies, and provide a simple theoretical framework for thinking about the absolute limits. Our results show that the limit in improvement of effectiveness over random sampling for real-world open source programs is a mean of only 13.078%. Interestingly, there is no limit to the improvement that can be made by addition of new mutation operators.Given that this is the maximum that can be achieved with perfect advance knowledge of mutation kills, what can be practically achieved may be much worse. We conclude that more effort should be focused on enhancing mutations than removing operators in the name of selective mutation for questionable benefit.},
booktitle = {Proceedings of the 38th International Conference on Software Engineering},
pages = {511–522},
numpages = {12},
keywords = {software testing, statistical analysis, theoretical analysis, mutation analysis},
location = {Austin, Texas},
series = {ICSE '16}
}

@inproceedings{10.1145/1543834.1543839,
author = {Cao, Yang and Hu, Chunhua and Li, Luming},
title = {Search-Based Multi-Paths Test Data Generation for Structure-Oriented Testing},
year = {2009},
isbn = {9781605583266},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1543834.1543839},
doi = {10.1145/1543834.1543839},
abstract = {This paper presents a new fitness function to generate test data for a specific single path, which is different from the predicate distance applied by most test data generators based on genetic algorithms (GAs). We define a similarity between the target path and execution path to evaluate the quality of the populations. The problem of the most existing generators is to search only one target data a time, wasting plenty of available interim data. We construct another fitness function combined with the single path function, which can drive GA to complete covering multi-paths to avoid the reduplicate searching and utilize the interim populations for different paths.Several experiments are taken to examine the effectiveness of both the single path and multi-path fitness functions, which evaluate the functions' performance with the convergence ability and consumed time. Results show that the two functions perform well compared with other two typical path-oriented functions and the multi-paths approach retrenches the searching actually.},
booktitle = {Proceedings of the First ACM/SIGEVO Summit on Genetic and Evolutionary Computation},
pages = {25–32},
numpages = {8},
keywords = {path testing, software testing, automation test, structure-oriented, genetic algorithms},
location = {Shanghai, China},
series = {GEC '09}
}

@inproceedings{10.1145/2493394.2493402,
author = {Shams, Zalia and Edwards, Stephen H.},
title = {Toward Practical Mutation Analysis for Evaluating the Quality of Student-Written Software Tests},
year = {2013},
isbn = {9781450322430},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2493394.2493402},
doi = {10.1145/2493394.2493402},
abstract = {Software testing is being added to programming courses at many schools, but current assessment techniques for evaluating student-written tests are imperfect. Code coverage measures are typically used in practice, but they have limitations and sometimes overestimate the true quality of tests. Others have proposed using mutation analysis instead, but mutation analysis poses a number of practical obstacles to classroom use. This paper describes a new approach to mutation analysis of student-written tests that is more practical for educational use, especially in an automated grading context. This approach combines several techniques to produce a novel solution that addresses the shortcomings raised by more traditional mutation analysis. An evaluation of this approach in the context of both CS1 and CS2 courses illustrates how it differs from code coverage analysis. At the same time, however, the evaluation results also raise questions of concern for CS educators regarding the relative value of more comprehensive assessment of test quality, the value of more open-ended assignments that offer significant design freedom for students, the cost of providing higher-quality reference solutions in order to support better quality assessment, and the cost of supporting assignments that require more intensive testing, such as GUI assignments.},
booktitle = {Proceedings of the Ninth Annual International ACM Conference on International Computing Education Research},
pages = {53–58},
numpages = {6},
keywords = {test coverage, programming assignments, mutation testing, automated assessment, reflection, automated grading, test-driven development, software testing, bytecode transformation},
location = {San Diego, San California, USA},
series = {ICER '13}
}

@inproceedings{10.1145/3194718.3194723,
author = {Gay, Gregory},
title = {Multifaceted Test Suite Generation Using Primary and Supporting Fitness Functions},
year = {2018},
isbn = {9781450357418},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194718.3194723},
doi = {10.1145/3194718.3194723},
abstract = {Dozens of criteria have been proposed to judge testing adequacy. Such criteria are important, as they guide automated generation efforts. Yet, the current use of such criteria in automated generation contrasts how such criteria are used by humans. For a human, coverage is part of a multifaceted combination of testing strategies. In automated generation, coverage is typically the goal, and a single fitness function is applied at one time. We propose that the key to improving the fault detection efficacy of search-based test generation approaches lies in a targeted, multifaceted approach pairing primary fitness functions that effectively explore the structure of the class under test with lightweight supporting fitness functions that target particular scenarios likely to trigger an observable failure.This report summarizes our findings to date, details the hypothesis of primary and supporting fitness functions, and identifies outstanding research challenges related to multifaceted test suite generation. We hope to inspire new advances in search-based test generation that could benefit our software-powered society.},
booktitle = {Proceedings of the 11th International Workshop on Search-Based Software Testing},
pages = {2–5},
numpages = {4},
keywords = {adequacy criteria, automated test generation, search-based test generation},
location = {Gothenburg, Sweden},
series = {SBST '18}
}

@inproceedings{10.1109/SBST.2019.00015,
author = {Prasetya, I. S. W. B.},
title = {Random Testing with Austere Budgeting in T3: Benchmarking at SBST2019 Testing Tool Contest},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/SBST.2019.00015},
doi = {10.1109/SBST.2019.00015},
abstract = {Random testing has the benefit of being simple to implement. However, if left unmanaged, random testing can easily exhaust the allocated computation time without being effective. In a previous work we extended an automated random testing tool called T3 with a budget management algorithm. Given a time budget and a target class with multiple methods to test, the algorithm tries to spread the budget over the methods by dividing it into multiple budget slices. While this guarantees a certain measure of fairness, the algorithm was programmed towards using up the budget. This does deliver extra coverage, but beyond a certain point this becomes quite costly. In an attempt to improve this we consider an austere budgeting policy as an alternative. While this work is still in progress, we decided to participate in the SBST Unit Testing Tool Competition 2019 to obtain insight on the performance of the policy. This paper discussed its results.},
booktitle = {Proceedings of the 12th International Workshop on Search-Based Software Testing},
pages = {21–24},
numpages = {4},
keywords = {automated unit testing Java, random testing, automated testing with budget, fuzzing with budget},
location = {Montreal, Quebec, Canada},
series = {SBST '19}
}

@inbook{10.1145/3238147.3238183,
author = {Hilton, Michael and Bell, Jonathan and Marinov, Darko},
title = {A Large-Scale Study of Test Coverage Evolution},
year = {2018},
isbn = {9781450359375},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3238147.3238183},
abstract = {Statement coverage is commonly used as a measure of test suite quality. Coverage is often used as a part of a code review process: if a patch decreases overall coverage, or is itself not covered, then the patch is scrutinized more closely. Traditional studies of how coverage changes with code evolution have examined the overall coverage of the entire program, and more recent work directly examines the coverage of patches (changed statements). We present an evaluation much larger than prior studies and moreover consider a new, important kind of change --- coverage changes of unchanged statements. We present a large-scale evaluation of code coverage evolution over 7,816 builds of 47 projects written in popular languages including Java, Python, and Scala. We find that in large, mature projects, simply measuring the change to statement coverage does not capture the nuances of code evolution. Going beyond considering statement coverage as a simple ratio, we examine how the set of statements covered evolves between project revisions. We present and study new ways to assess the impact of a patch on a project's test suite quality that both separates coverage of the patch from coverage of the non-patch, and separates changes in coverage from changes in the set of statements covered.},
booktitle = {Proceedings of the 33rd ACM/IEEE International Conference on Automated Software Engineering},
pages = {53–63},
numpages = {11}
}

@inproceedings{10.1145/3324884.3415299,
author = {Derakhshanfar, Pouria and Devroey, Xavier and Panichella, Annibale and Zaidman, Andy and van Deursen, Arie},
title = {Botsing, a Search-Based Crash Reproduction Framework for Java},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3415299},
doi = {10.1145/3324884.3415299},
abstract = {Approaches for automatic crash reproduction aim to generate test cases that reproduce crashes starting from the crash stack traces. These tests help developers during their debugging practices. One of the most promising techniques in this research field leverages search-based software testing techniques for generating crash reproducing test cases. In this paper, we introduce Botsing, an open-source search-based crash reproduction framework for Java. Botsing implements state-of-the-art and novel approaches for crash reproduction. The well-documented architecture of Botsing makes it an easy-to-extend framework, and can hence be used for implementing new approaches to improve crash reproduction. We have applied Botsing to a wide range of crashes collected from open source systems. Furthermore, we conducted a qualitative assessment of the crash-reproducing test cases with our industrial partners. In both cases, Botsing could reproduce a notable amount of the given stack traces.Demo. video: https://www.youtube.com/watch?v=k6XaQjHqe48Botsing website: https://stamp-project.github.io/botsing/},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1278–1282},
numpages = {5},
keywords = {botsing, search-based software testing, crash reproduction},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/1572272.1572296,
author = {Yoo, Shin and Harman, Mark and Tonella, Paolo and Susi, Angelo},
title = {Clustering Test Cases to Achieve Effective and Scalable Prioritisation Incorporating Expert Knowledge},
year = {2009},
isbn = {9781605583389},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1572272.1572296},
doi = {10.1145/1572272.1572296},
abstract = {Pair-wise comparison has been successfully utilised in order to prioritise test cases by exploiting the rich, valuable and unique knowledge of the tester. However, the prohibitively large cost of the pair-wise comparison method prevents it from being applied to large test suites. In this paper, we introduce a cluster-based test case prioritisation technique. By clustering test cases, based on their dynamic runtime behaviour, we can reduce the required number of pair-wise comparisons significantly. The approach is evaluated on seven test suites ranging in size from 154 to 1,061 test cases. We present an empirical study that shows that the resulting prioritisation is more effective than existing coverage-based prioritisation techniques in terms of rate of fault detection. Perhaps surprisingly, the paper also demonstrates that clustering (even without human input) can outperform unclustered coverage-based technologies, and discusses an automated process that can be used to determine whether the application of the proposed approach would yield improvement.},
booktitle = {Proceedings of the Eighteenth International Symposium on Software Testing and Analysis},
pages = {201–212},
numpages = {12},
keywords = {clustering, test case prioritisation, ahp},
location = {Chicago, IL, USA},
series = {ISSTA '09}
}

@inproceedings{10.1145/2771783.2771799,
author = {Yue, Tao and Ali, Shaukat and Zhang, Man},
title = {RTCM: A Natural Language Based, Automated, and Practical Test Case Generation Framework},
year = {2015},
isbn = {9781450336208},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2771783.2771799},
doi = {10.1145/2771783.2771799},
abstract = { Based on our experience of collaborating with industry, we observed that test case generation usually relies on test case specifications (TCSs), commonly written in natural language, specifying test cases of a System Under Test at a high level of abstraction. In practice, TCSs are commonly used by test engineers as reference documents to perform these activities: 1) Manually executing test cases in TCSs; 2) Manually coding test cases in a test scripting language for automated test case execution. In the latter case, the gap between TCSs and executable test cases has to be filled by test engineers, requiring a significant amount of coding effort and domain knowledge. Motivated by the above observations from the industry, we first propose, in this paper, a TCS language, named as Restricted Test Case Modeling (RTCM), which is based on natural language and composed of an easy-to-use template, a set of restriction rules and keywords. Second, we propose a test case generation tool (aToucan4Test), which takes TCSs in RTCM as input and generates either manual test cases or automatically executable test cases, based on various coverage criteria defined on RTCM. To assess the applicability of RTCM, we manually modeled two industrial case studies and examined 30 automatically generated TCSs. To evaluate aToucan4Test, we modeled three subsystems of a Video Conferencing System developed by Cisco Systems, Norway and automatically generated executable test cases. These test cases were successfully executed on two commercial software versions. In the paper, we also discuss our experience of applying RTCM and aToucan4Test in an industrial context and compare our approach with other model-based testing methodologies. },
booktitle = {Proceedings of the 2015 International Symposium on Software Testing and Analysis},
pages = {397–408},
numpages = {12},
keywords = {Model, RUCM, Test Case Generation, Test Case Specification},
location = {Baltimore, MD, USA},
series = {ISSTA 2015}
}

@inproceedings{10.1145/1985793.1986053,
author = {Foster, Howard and Bertolino, Antonia and Li, J. Jenny},
title = {Sixth International Workshop on Automation of Software Test (AST 2011)},
year = {2011},
isbn = {9781450304450},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1985793.1986053},
doi = {10.1145/1985793.1986053},
abstract = {The Sixth International Workshop on Automation of Software Test (AST 2011) is associated with the 33rd International Conference on Software Engineering (ICSE 2011). This edition of AST was focused on the special theme of Software Design and the Automation of Software Test and authors were encouraged to submit work in this area. The workshop covers two days with presentations of regular research papers, industrial case studies and experience reports. The workshop also aims to have extensive discussions on collaborative solutions in the form of charette sessions. This paper summarizes the organization of the workshop, the special theme, as well as the sessions.},
booktitle = {Proceedings of the 33rd International Conference on Software Engineering},
pages = {1216–1217},
numpages = {2},
keywords = {automation of software test, software design, software tools, software testing},
location = {Waikiki, Honolulu, HI, USA},
series = {ICSE '11}
}

@inproceedings{10.1145/2610384.2610417,
author = {Weitz, Konstantin and Kim, Gene and Srisakaokul, Siwakorn and Ernst, Michael D.},
title = {A Type System for Format Strings},
year = {2014},
isbn = {9781450326452},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2610384.2610417},
doi = {10.1145/2610384.2610417},
abstract = { Most programming languages support format strings, but their use is error-prone. Using the wrong format string syntax, or passing the wrong number or type of arguments, leads to unintelligible text output, program crashes, or security vulnerabilities.  This paper presents a type system that guarantees that calls to format string APIs will never fail. In Java, this means that the API will not throw exceptions. In C, this means that the API will not return negative values, corrupt memory, etc.  We instantiated this type system for Java’s Formatter API, and evaluated it on 6 large and well-maintained open-source projects. Format string bugs are common in practice (our type system found 104 bugs), and the annotation burden on the user of our type system is low (on average, for every bug found, only 1.0 annotations need to be written). },
booktitle = {Proceedings of the 2014 International Symposium on Software Testing and Analysis},
pages = {127–137},
numpages = {11},
keywords = {static analysis, Format string, printf, type system},
location = {San Jose, CA, USA},
series = {ISSTA 2014}
}

