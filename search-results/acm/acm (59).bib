@inproceedings{10.1145/2254064.2254075,
author = {Jin, Guoliang and Song, Linhai and Shi, Xiaoming and Scherpelz, Joel and Lu, Shan},
title = {Understanding and Detecting Real-World Performance Bugs},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254075},
doi = {10.1145/2254064.2254075},
abstract = {Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs.This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs.Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {77–88},
numpages = {12},
keywords = {rule-based bug detection, performance bugs, characteristics study},
location = {Beijing, China},
series = {PLDI '12}
}

@article{10.1145/2345156.2254075,
author = {Jin, Guoliang and Song, Linhai and Shi, Xiaoming and Scherpelz, Joel and Lu, Shan},
title = {Understanding and Detecting Real-World Performance Bugs},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345156.2254075},
doi = {10.1145/2345156.2254075},
abstract = {Developers frequently use inefficient code sequences that could be fixed by simple patches. These inefficient code sequences can cause significant performance degradation and resource waste, referred to as performance bugs. Meager increases in single threaded performance in the multi-core era and increasing emphasis on energy efficiency call for more effort in tackling performance bugs.This paper conducts a comprehensive study of 110 real-world performance bugs that are randomly sampled from five representative software suites (Apache, Chrome, GCC, Mozilla, and MySQL). The findings of this study provide guidance for future work to avoid, expose, detect, and fix performance bugs.Guided by our characteristics study, efficiency rules are extracted from 25 patches and are used to detect performance bugs. 332 previously unknown performance problems are found in the latest versions of MySQL, Apache, and Mozilla applications, including 219 performance problems found by applying rules across applications.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {77–88},
numpages = {12},
keywords = {characteristics study, performance bugs, rule-based bug detection}
}

@inproceedings{10.1145/3302424.3303948,
author = {Lochmann, Alexander and Schirmeier, Horst and Borghorst, Hendrik and Spinczyk, Olaf},
title = {LockDoc: Trace-Based Analysis of Locking in the Linux Kernel},
year = {2019},
isbn = {9781450362818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3302424.3303948},
doi = {10.1145/3302424.3303948},
abstract = {For fine-grained synchronization of application and kernel threads, the Linux kernel provides a multitude of different locking mechanisms that are being used on various individually locked data structures. Understanding which locks are required in which order for a particular member variable of a kernel data structure has become truly difficult, even for Linux-kernel experts themselves.In this paper we introduce LockDoc -- an approach that, based on the analysis of execution traces of an instrumented Linux kernel, automatically deduces the most likely locking rule for all members of arbitrary kernel data structures. From these locking rules, LockDoc generates documentation that supports kernel developers and helps avoiding concurrency bugs. Additionally, the (very limited) existing documentation can be verified, and locking-rule violations -- potential bugs in the kernel code -- can be found.Our results include generated locking rules for previously predominantly undocumented member variables of 11 different Linux-kernel data structures. Manually inspecting the scarce source-code documentation for five of these data structures reveals that only 53 percent of the variables with a documented locking rule are actually consistently accessed with the required locks held. This indicates possible documentation or synchronization bugs in the Linux kernel, of which one has already been confirmed by kernel experts.},
booktitle = {Proceedings of the Fourteenth EuroSys Conference 2019},
articleno = {11},
numpages = {15},
location = {Dresden, Germany},
series = {EuroSys '19}
}

@inproceedings{10.1145/3180155.3180236,
author = {Sun, Chengnian and Li, Yuanbo and Zhang, Qirun and Gu, Tianxiao and Su, Zhendong},
title = {Perses: Syntax-Guided Program Reduction},
year = {2018},
isbn = {9781450356381},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3180155.3180236},
doi = {10.1145/3180155.3180236},
abstract = {Given a program P that exhibits a certain property Ψ (e.g., a C program that crashes GCC when it is being compiled), the goal of program reduction is to minimize P to a smaller variant P′ that still exhibits the same property, i.e., Ψ(P′). Program reduction is important and widely demanded for testing and debugging. For example, all compiler/interpreter development projects need effective program reduction to minimize failure-inducing test programs to ease debugging. However, state-of-the-art program reduction techniques --- notably Delta Debugging (DD), Hierarchical Delta Debugging (HDD), and C-Reduce --- do not perform well in terms of speed (reduction time) and quality (size of reduced programs), or are highly customized for certain languages and thus lack generality.This paper presents Perses, a novel framework for effective, efficient, and general program reduction. The key insight is to exploit, in a general manner, the formal syntax of the programs under reduction and ensure that each reduction step considers only smaller, syntactically valid variants to avoid futile efforts on syntactically invalid variants. Our framework supports not only deletion (as for DD and HDD), but also general, effective program transformations.We have designed and implemented Perses, and evaluated it for two language settings: C and Java. Our evaluation results on 20 C programs triggering bugs in GCC and Clang demonstrate Perses's strong practicality compared to the state-of-the-art: (1) smaller size --- Perses's results are respectively 2% and 45% in size of those from DD and HDD; and (2) shorter reduction time --- Perses takes 23% and 47% time taken by DD and HDD respectively. Even when compared to the highly customized and optimized C-Reduce for C/C++, Perses takes only 38-60% reduction time.},
booktitle = {Proceedings of the 40th International Conference on Software Engineering},
pages = {361–371},
numpages = {11},
keywords = {program reduction, debugging, delta debugging},
location = {Gothenburg, Sweden},
series = {ICSE '18}
}

@article{10.1145/1039174.1039193,
author = {Bhansali, P. V.},
title = {Software Safety: Current Status and Future Direction},
year = {2005},
issue_date = {January 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {1},
issn = {0163-5948},
url = {https://doi.org/10.1145/1039174.1039193},
doi = {10.1145/1039174.1039193},
abstract = {This paper describes the current status of software safety in terms of research and existing standards. It highlights the differences between various standards set up by government agencies to accomplish the same safety objectives. For example, European standards tend to place more emphasis on static analysis whereas American standards prefer dynamic testing to verify the software. An optimal verification approach is still a debatable issue in the software safety community. As for future direction, the author believes that the key to making safer and cheaper software is to have better requirements validation that ensure that the requirements are correct and complete before the design and coding phases begin.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {jan},
pages = {3},
numpages = {3},
keywords = {software safety, system safety, validation, verification}
}

@inproceedings{10.1145/2427376.2427378,
author = {Eriksson, Anders and Lindstr\"{o}m, Birgitta and Andler, Sten F. and Offutt, Jeff},
title = {Model Transformation Impact on Test Artifacts: An Empirical Study},
year = {2012},
isbn = {9781450318013},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2427376.2427378},
doi = {10.1145/2427376.2427378},
abstract = {Development environments that support Model-Driven Development often focus on model-level functional testing, enabling verification of design models against their specifications. However, developers of safety-critical software systems are also required to show that tests cover the structure of the implementation. Unfortunately, the implementation structure can diverge from the model depending on choices such as the model compiler or target language. Therefore, structural coverage at the model level may not guarantee coverage of the implementation.We present results from an industrial experiment that demonstrates the model-compiler effect on test artifacts in xtUML models when these models are transformed into C++. Test artifacts, i.e., predicates and clauses, are used to satisfy the structural code coverage criterion, in this case MCDC, which is required by the US Federal Aviation Administration. The results of the experiment show not only that the implementation contains more test artifacts than the model, but also that the test artifacts can be deterministically enumerated during translation. The analysis identifies two major sources for these additional test artifacts.},
booktitle = {Proceedings of the Workshop on Model-Driven Engineering, Verification and Validation},
pages = {5–10},
numpages = {6},
keywords = {model transformation, model coverage, structural code coverage, model-driven development, xtUML, MCDC},
location = {Innsbruck, Austria},
series = {MoDeVVa '12}
}

@inproceedings{10.1145/2046707.2046736,
author = {Doup\'{e}, Adam and Boe, Bryce and Kruegel, Christopher and Vigna, Giovanni},
title = {Fear the EAR: Discovering and Mitigating Execution after Redirect Vulnerabilities},
year = {2011},
isbn = {9781450309486},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2046707.2046736},
doi = {10.1145/2046707.2046736},
abstract = {The complexity of modern web applications makes it difficult for developers to fully understand the security implications of their code. Attackers exploit the resulting security vulnerabilities to gain unauthorized access to the web application environment. Previous research into web application vulnerabilities has mostly focused on input validation flaws, such as cross site scripting and SQL injection, while logic flaws have received comparably less attention. In this paper, we present a comprehensive study of a relatively unknown logic flaw in web applications, which we call Execution After Redirect, or EAR. A web application developer can introduce an EAR by calling a redirect method under the assumption that execution will halt. A vulnerability occurs when server-side execution continues after the developer's intended halting point, which can lead to broken/insufficient access controls and information leakage. We start with an analysis of how susceptible applications written in nine web frameworks are to EAR vulnerabilities. We then discuss the results from the EAR challenge contained within the 2010 International Capture the Flag Competition. Finally, we present an open-source, white-box, static analysis tool to detect EARs in Ruby on Rails web applications. This tool found 3,944 EAR instances in 18,127 open-source applications. Finally, we describe an approach to prevent EARs in web frameworks.},
booktitle = {Proceedings of the 18th ACM Conference on Computer and Communications Security},
pages = {251–262},
numpages = {12},
keywords = {execution after redirect, web applications, static analysis},
location = {Chicago, Illinois, USA},
series = {CCS '11}
}

@inproceedings{10.1145/1352135.1352315,
author = {Janzen, David and Saiedian, Hossein},
title = {Test-Driven Learning in Early Programming Courses},
year = {2008},
isbn = {9781595937995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1352135.1352315},
doi = {10.1145/1352135.1352315},
abstract = {Coercing new programmers to adopt disciplined development practices such as thorough unit testing is a challenging endeavor. Test-driven development (TDD) has been proposed as a solution to improve both software design and testing. Test-driven learning (TDL) has been proposed as a pedagogical approach for teaching TDD without imposing significant additional instruction time.This research evaluates the effects of students using a test-first (TDD) versus test-last approach in early programming courses, and considers the use of TDL on a limited basis in CS1 and CS2. Software testing, programmer productivity, programmer performance, and programmer opinions are compared between test-first and test-last programming groups. Results from this research indicate that a test-first approach can increase student testing and programmer performance, but that early programmers are very reluctant to adopt a test-first approach, even after having positive experiences using TDD. Further, this research demonstrates that TDL can be applied in CS1/2, but suggests that a more pervasive implementation of TDL may be necessary to motivate and establish disciplined testing practice among early programmers.},
booktitle = {Proceedings of the 39th SIGCSE Technical Symposium on Computer Science Education},
pages = {532–536},
numpages = {5},
keywords = {pedagogy, cs1, test-driven learning, test-driven development},
location = {Portland, OR, USA},
series = {SIGCSE '08}
}

@article{10.1145/1352322.1352315,
author = {Janzen, David and Saiedian, Hossein},
title = {Test-Driven Learning in Early Programming Courses},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {1},
issn = {0097-8418},
url = {https://doi.org/10.1145/1352322.1352315},
doi = {10.1145/1352322.1352315},
abstract = {Coercing new programmers to adopt disciplined development practices such as thorough unit testing is a challenging endeavor. Test-driven development (TDD) has been proposed as a solution to improve both software design and testing. Test-driven learning (TDL) has been proposed as a pedagogical approach for teaching TDD without imposing significant additional instruction time.This research evaluates the effects of students using a test-first (TDD) versus test-last approach in early programming courses, and considers the use of TDL on a limited basis in CS1 and CS2. Software testing, programmer productivity, programmer performance, and programmer opinions are compared between test-first and test-last programming groups. Results from this research indicate that a test-first approach can increase student testing and programmer performance, but that early programmers are very reluctant to adopt a test-first approach, even after having positive experiences using TDD. Further, this research demonstrates that TDL can be applied in CS1/2, but suggests that a more pervasive implementation of TDL may be necessary to motivate and establish disciplined testing practice among early programmers.},
journal = {SIGCSE Bull.},
month = {mar},
pages = {532–536},
numpages = {5},
keywords = {cs1, test-driven development, test-driven learning, pedagogy}
}

@inproceedings{10.1145/2254064.2254104,
author = {Regehr, John and Chen, Yang and Cuoq, Pascal and Eide, Eric and Ellison, Chucky and Yang, Xuejun},
title = {Test-Case Reduction for C Compiler Bugs},
year = {2012},
isbn = {9781450312059},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2254064.2254104},
doi = {10.1145/2254064.2254104},
abstract = {To report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated test-case reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domain-specific test-case reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by compiler developers. We conclude that effective program reduction requires more than straightforward delta debugging.},
booktitle = {Proceedings of the 33rd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {335–346},
numpages = {12},
keywords = {random testing, compiler testing, bug reporting, compiler defect, test-case minimization, automated testing},
location = {Beijing, China},
series = {PLDI '12}
}

@article{10.1145/2345156.2254104,
author = {Regehr, John and Chen, Yang and Cuoq, Pascal and Eide, Eric and Ellison, Chucky and Yang, Xuejun},
title = {Test-Case Reduction for C Compiler Bugs},
year = {2012},
issue_date = {June 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {47},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2345156.2254104},
doi = {10.1145/2345156.2254104},
abstract = {To report a compiler bug, one must often find a small test case that triggers the bug. The existing approach to automated test-case reduction, delta debugging, works by removing substrings of the original input; the result is a concatenation of substrings that delta cannot remove. We have found this approach less than ideal for reducing C programs because it typically yields test cases that are too large or even invalid (relying on undefined behavior). To obtain small and valid test cases consistently, we designed and implemented three new, domain-specific test-case reducers. The best of these is based on a novel framework in which a generic fixpoint computation invokes modular transformations that perform reduction operations. This reducer produces outputs that are, on average, more than 25 times smaller than those produced by our other reducers or by the existing reducer that is most commonly used by compiler developers. We conclude that effective program reduction requires more than straightforward delta debugging.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {335–346},
numpages = {12},
keywords = {test-case minimization, compiler defect, bug reporting, compiler testing, automated testing, random testing}
}

@inproceedings{10.5555/2820704.2820711,
author = {Tonella, Paolo and Tiella, Roberto},
title = {Weekly Round Trips from Norms to Requirements and Tests: An Industrial Experience Report},
year = {2015},
publisher = {IEEE Press},
abstract = {SEAC is a major software provider in Italy in the area of business management, with a focus on norms and human resources. SEAC is re-engineering their huge legacy system to C#/SQL Server. To minimise the risks associated with such reengineering project, SEAC has adopted an incremental and agile process model, which produces small and frequent releases of new, incremental modules that replace a portion of the legacy system at a time.Since the SEAC software handles business activities that are highly dependent on norms, such as the contracts of employees, the taxation of incomes and salaries, the pension contributions, one of the key challenges is to support a smooth transformation of norms into requirements, into code and eventually into test cases used to verify that norms have been implemented as prescribed by the law. The SE research unit at FBK has been involved to introduce a set of practices aimed at supporting such transformation, so as to improve the current process. We report the experience made during the project in this paper.},
booktitle = {Proceedings of the Second International Workshop on Requirements Engineering and Testing},
pages = {20–26},
numpages = {7},
keywords = {requirements and testing in practice, industrial experience, agile process},
location = {Florence, Italy},
series = {RET '15}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: Symbolic Execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {GraalVM, Compiler optimizations, Java, Symbolic execution},
location = {Virtual, UK},
series = {MPLR 2020}
}

@inproceedings{10.1109/ASE.2019.00124,
author = {Fischer, Bernd and La Torre, Salvatore and Parlato, Gennaro},
title = {VERISMART 2.0: Swarm-Based Bug-Finding for Multi-Threaded Programs with Lazy-CSeq},
year = {2019},
isbn = {9781728125084},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ASE.2019.00124},
doi = {10.1109/ASE.2019.00124},
abstract = {Swarm-based verification methods split a verification problem into a large number of independent simpler tasks and so exploit the availability of large numbers of cores to speed up verification. Lazy-CSeq is a BMC-based bug-finding tool for C programs using POSIX threads that is based on sequentialization. Here we present the tool VERISMART 2.0, which extends Lazy-CSeq with a swarm-based bug-finding method. The key idea of this approach is to constrain the interleaving such that context switches can only happen within selected tiles (more specifically, contiguous code segments within the individual threads). This under-approximates the program's behaviours, with the number and size of tiles as additional parameters, which allows us to vary the complexity of the tasks. Overall, this significantly improves peak memory consumption and (wall-clock) analysis time.},
booktitle = {Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering},
pages = {1150–1153},
numpages = {4},
keywords = {sequentialization, swarm verification, concurrency, program analysis},
location = {San Diego, California},
series = {ASE '19}
}

@inproceedings{10.1145/3194733.3194740,
author = {Budnik, Christof and Gario, Marco and Markov, Georgi and Wang, Zhu},
title = {Guided Test Case Generation through AI Enabled Output Space Exploration},
year = {2018},
isbn = {9781450357432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194733.3194740},
doi = {10.1145/3194733.3194740},
abstract = {Black-box software testing is a crucial part of quality assurance for industrial products. To verify the reliable behavior of software intensive systems, testing needs to ensure that the system produces the correct outputs from a variety of inputs. Even more critical, it needs to ensure that unexpected corner cases are tested. Existing approaches attempt to address this problem by the generation of input data to known outputs based on the domain knowledge of an expert. Such input space exploration, however, does not guarantee an adequate coverage of the output space as the test input data generation is done independently of the system output. The paper discusses a novel test case generation approach enabled by neural networks which promises higher probability of exposing system faults by systematically exploring the output space of the system under test. As such, the approach potentially improves the defect detection capability by identifying gaps in the test suite of uncovered system outputs. These gaps are closed by automatically determining inputs that lead to specific outputs by performing backward reasoning on an artificial neural network. The approach is demonstrated on an industrial train control system.},
booktitle = {Proceedings of the 13th International Workshop on Automation of Software Test},
pages = {53–56},
numpages = {4},
keywords = {artificial intelligence, neural networks, test oracle, adversarial examples, test design, black-box testing},
location = {Gothenburg, Sweden},
series = {AST '18}
}

@inbook{10.1145/3368089.3409716,
author = {Zhai, Juan and Shi, Yu and Pan, Minxue and Zhou, Guian and Liu, Yongxiang and Fang, Chunrong and Ma, Shiqing and Tan, Lin and Zhang, Xiangyu},
title = {C2S: Translating Natural Language Comments to Formal Program Specifications},
year = {2020},
isbn = {9781450370431},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3368089.3409716},
abstract = {Formal program specifications are essential for various software engineering tasks, such as program verification, program synthesis, code debugging and software testing. However, manually inferring formal program specifications is not only time-consuming but also error-prone. In addition, it requires substantial expertise. Natural language comments contain rich semantics about behaviors of code, making it feasible to infer program specifications from comments. Inspired by this, we develop a tool, named C2S, to automate the specification synthesis task by translating natural language comments into formal program specifications. Our approach firstly constructs alignments between natural language word and specification tokens from existing comments and their corresponding specifications. Then for a given method comment, our approach assembles tokens that are associated with words in the comment from the alignments into specifications guided by specification syntax and the context of the target method. Our tool successfully synthesizes 1,145 specifications for 511 methods of 64 classes in 5 different projects, substantially outperforming the state-of-the-art. The generated specifications are also used to improve a number of software engineering tasks like static taint analysis, which demonstrates the high quality of the specifications.},
booktitle = {Proceedings of the 28th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {25–37},
numpages = {13}
}

@inproceedings{10.1145/3236024.3236030,
author = {Gao, Yu and Dou, Wensheng and Qin, Feng and Gao, Chushu and Wang, Dong and Wei, Jun and Huang, Ruirui and Zhou, Li and Wu, Yongming},
title = {An Empirical Study on Crash Recovery Bugs in Large-Scale Distributed Systems},
year = {2018},
isbn = {9781450355735},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3236024.3236030},
doi = {10.1145/3236024.3236030},
abstract = {In large-scale distributed systems, node crashes are inevitable, and can happen at any time. As such, distributed systems are usually designed to be resilient to these node crashes via various crash recovery mechanisms, such as write-ahead logging in HBase and hinted handoffs in Cassandra. However, faults in crash recovery mechanisms and their implementations can introduce intricate crash recovery bugs, and lead to severe consequences.  In this paper, we present CREB, the most comprehensive study on 103 Crash REcovery Bugs from four popular open-source distributed systems, including ZooKeeper, Hadoop MapReduce, Cassandra and HBase. For all the studied bugs, we analyze their root causes, triggering conditions, bug impacts and fixing. Through this study, we obtain many interesting findings that can open up new research directions for combating crash recovery bugs.},
booktitle = {Proceedings of the 2018 26th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {539–550},
numpages = {12},
keywords = {empirical study, Distributed systems, crash recovery bugs},
location = {Lake Buena Vista, FL, USA},
series = {ESEC/FSE 2018}
}

@article{10.1145/256175.256180,
author = {Fischer, Olivier and Horn, Richard},
title = {Electronic Performance Support Systems},
year = {1997},
issue_date = {July 1997},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {7},
issn = {0001-0782},
url = {https://doi.org/10.1145/256175.256180},
doi = {10.1145/256175.256180},
journal = {Commun. ACM},
month = {jul},
pages = {31–32},
numpages = {2}
}

@inproceedings{10.1145/2464576.2482746,
author = {Buzdalov, Maxim and Buzdalova, Arina and Petrova, Irina},
title = {Generation of Tests for Programming Challenge Tasks Using Multi-Objective Optimization},
year = {2013},
isbn = {9781450319645},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2464576.2482746},
doi = {10.1145/2464576.2482746},
abstract = {In this paper, an evolutionary approach to generation of test cases for programming challenge tasks is investigated. Multi-objective and single-objective evolutionary algorithms, as well as helper-objective selection strategies, are compared. Particularly, a previously proposed method of choosing between helper-objectives with reinforcement learning is considered. This method is applied to the multi-objective evolutionary algorithm for the first time. Results of the experiment show that the most reasonable approach for the considered problem is using multi-objective evolutionary algorithm with automated helper-objective selection.},
booktitle = {Proceedings of the 15th Annual Conference Companion on Genetic and Evolutionary Computation},
pages = {1655–1658},
numpages = {4},
keywords = {reinforcement learning, programming challenges, testing, genetic algorithms, multi-objective, helper-objectives},
location = {Amsterdam, The Netherlands},
series = {GECCO '13 Companion}
}

@inproceedings{10.5555/2818754.2818851,
author = {Peters, Fayola and Menzies, Tim and Layman, Lucas},
title = {LACE2: Better Privacy-Preserving Data Sharing for Cross Project Defect Prediction},
year = {2015},
isbn = {9781479919345},
publisher = {IEEE Press},
abstract = {Before a community can learn general principles, it must share individual experiences. Data sharing is the fundamental step of cross project defect prediction, i.e. the process of using data from one project to predict for defects in another. Prior work on secure data sharing allowed data owners to share their data on a single-party basis for defect prediction via data minimization and obfuscation. However the studied method did not consider that bigger data required the data owner to share more of their data.In this paper, we extend previous work with LACE2 which reduces the amount of data shared by using multi-party data sharing. Here data owners incrementally add data to a cache passed among them and contribute "interesting" data that are not similar to the current content of the cache. Also, before data owner i passes the cache to data owner j, privacy is preserved by applying obfuscation algorithms to hide project details. The experiments of this paper show that (a) LACE2 is comparatively less expensive than the single-party approach and (b) the multi-party approach of LACE2 yields higher privacy than the prior approach without damaging predictive efficacy (indeed, in some cases, LACE2 leads to better defect predictors).},
booktitle = {Proceedings of the 37th International Conference on Software Engineering - Volume 1},
pages = {801–811},
numpages = {11},
location = {Florence, Italy},
series = {ICSE '15}
}

@inproceedings{10.1145/1966445.1966465,
author = {Fonseca, Pedro and Li, Cheng and Rodrigues, Rodrigo},
title = {Finding Complex Concurrency Bugs in Large Multi-Threaded Applications},
year = {2011},
isbn = {9781450306348},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1966445.1966465},
doi = {10.1145/1966445.1966465},
abstract = {Parallel software is increasingly necessary to take advantage of multi-core architectures, but it is also prone to concurrency bugs which are particularly hard to avoid, find, and fix, since their occurrence depends on specific thread interleavings. In this paper we propose a concurrency bug detector that automatically identifies when an execution of a program triggers a concurrency bug. Unlike previous concurrency bug detectors, we are able to find two particularly hard classes of bugs. The first are bugs that manifest themselves by subtle violation of application semantics, such as returning an incorrect result. The second are latent bugs, which silently corrupt internal data structures, and are especially hard to detect because when these bugs are triggered they do not become immediately visible. Pike detects these concurrency bugs by checking both the output and the internal state of the application for linearizability at the level of user requests. This paper presents this technique for finding concurrency bugs, its application in the context of a testing tool that systematically searches for such problems, and our experience in applying our approach to MySQL, a large-scale complex multi-threaded application. We were able to find several concurrency bugs in a stable version of the application, including subtle violations of application semantics, latent bugs, and incorrect error replies.},
booktitle = {Proceedings of the Sixth Conference on Computer Systems},
pages = {215–228},
numpages = {14},
keywords = {linearizability, latent bugs, semantic bugs, concurrency bugs},
location = {Salzburg, Austria},
series = {EuroSys '11}
}

@inproceedings{10.1145/3439961.3439991,
author = {Rivero, Luis and Diniz, Jo\~{a}o and Silva, Giovanni and Borralho, Gabriel and Braz Junior, Geraldo and Paiva, Anselmo and Alves, Erika and Oliveira, Milton},
title = {Deployment of a Machine Learning System for Predicting Lawsuits Against Power Companies: Lessons Learned from an Agile Testing Experience for Improving Software Quality},
year = {2020},
isbn = {9781450389235},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3439961.3439991},
doi = {10.1145/3439961.3439991},
abstract = { The advances in Machine Learning (ML) require software organizations to evolve their development processes in order to improve the quality of ML systems. Within the software development process, the testing stage of an ML system is more critical, considering that it is necessary to add data validation, trained model quality evaluation, and model validation to traditional unit, integration tests and system tests. In this paper, we focus on reporting the lessons learned of using model testing and exploratory testing within the context of the agile development process of an ML system that predicts lawsuits proneness in energy supply companies. Through the development of the project, the SCRUM agile methodology was applied and activities related to the development of the ML model and the development of the end-user application were defined. After the testing process of the ML model, we managed to achieve 93.89 accuracy; 95.58 specificity; 88.84 sensitivity; and 87.09 precision. Furthermore, we focused on the quality of use of the application embedding the ML model, by carrying out exploratory testing. As a result, through several iterations, different types of defects were identified and corrected. Our lessons learned support software engineers willing to develop ML systems that consider both the ML model and the end-user application.},
booktitle = {19th Brazilian Symposium on Software Quality},
articleno = {30},
numpages = {10},
keywords = {and Testing, and Tools, Software Processes, Validation, Verification, Methods},
location = {S\~{a}o Lu\'{\i}s, Brazil},
series = {SBQS'20}
}

@inproceedings{10.1145/2739482.2768424,
author = {Landsborough, Jason and Harding, Stephen and Fugate, Sunny},
title = {Removing the Kitchen Sink from Software},
year = {2015},
isbn = {9781450334884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2739482.2768424},
doi = {10.1145/2739482.2768424},
abstract = {We would all benefit if software were slimmer, thinner, and generally only did what we needed and nothing more. To this end, our research team has been exploring methods for removing unused and undesirable features from compiled programs. Our primary goal is to improve software security by removing rarely used features in order to decrease a pro- gram's attack surface. We describe two different approaches for "thinning" binary images of compiled programs. The first approach removes specific program features using dynamic tracing as a guide. This approach is safer than many alterna- tives, but is limited to removing code which is reachable in a trace when an undesirable feature is enabled. The second ap- proach uses a genetic algorithm (GA) to mutate a program until a suitable variant is found. Our GA-based approach can potentially remove any code that is not strictly required for proper execution, but may break program semantics in unpredictable ways. We show results of these approaches on a simple program and real-world software and explore some of the implications for software security.},
booktitle = {Proceedings of the Companion Publication of the 2015 Annual Conference on Genetic and Evolutionary Computation},
pages = {833–838},
numpages = {6},
keywords = {feature removal, genetic algorithm, tracing},
location = {Madrid, Spain},
series = {GECCO Companion '15}
}

