@inproceedings{10.1145/3338906.3338953,
author = {Gulzar, Muhammad Ali and Mardani, Shaghayegh and Musuvathi, Madanlal and Kim, Miryung},
title = {White-Box Testing of Big Data Analytics with Complex User-Defined Functions},
year = {2019},
isbn = {9781450355728},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3338906.3338953},
doi = {10.1145/3338906.3338953},
abstract = {Data-intensive scalable computing (DISC) systems such as Google’s MapReduce, Apache Hadoop, and Apache Spark are being leveraged to process massive quantities of data in the cloud. Modern DISC applications pose new challenges in exhaustive, automatic testing because they consist of dataflow operators, and complex user-defined functions (UDF) are prevalent unlike SQL queries. We design a new white-box testing approach, called BigTest to reason about the internal semantics of UDFs in tandem with the equivalence classes created by each dataflow and relational operator. Our evaluation shows that, despite ultra-large scale input data size, real world DISC applications are often significantly skewed and inadequate in terms of test coverage, leaving 34% of Joint Dataflow and UDF (JDU) paths untested. BigTest shows the potential to minimize data size for local testing by 10^5 to 10^8 orders of magnitude while revealing 2X more manually-injected faults than the previous approach. Our experiment shows that only few of the data records (order of tens) are actually required to achieve the same JDU coverage as the entire production data. The reduction in test data also provides CPU time saving of 194X on average, demonstrating that interactive and fast local testing is feasible for big data analytics, obviating the need to test applications on huge production data.},
booktitle = {Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
pages = {290–301},
numpages = {12},
keywords = {symbolic execution, map reduce, test generation, dataflow programs, data intensive scalable computing},
location = {Tallinn, Estonia},
series = {ESEC/FSE 2019}
}

@inproceedings{10.1145/2372251.2372290,
author = {Ali, Nauman bin and Petersen, Kai and M\"{a}ntyl\"{a}, Mika},
title = {Testing Highly Complex System of Systems: An Industrial Case Study},
year = {2012},
isbn = {9781450310567},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2372251.2372290},
doi = {10.1145/2372251.2372290},
abstract = {Context: Systems of systems (SoS) are highly complex and are integrated on multiple levels (unit, component, system, system of systems). Many of the characteristics of SoS (such as operational and managerial independence, integration of system into system of systems, SoS comprised of complex systems) make their development and testing challenging.Contribution: This paper provides an understanding of SoS testing in large-scale industry settings with respect to challenges and how to address them.Method: The research method used is case study research. As data collection methods we used interviews, documentation, and fault slippage data.Results: We identified challenges related to SoS with respect to fault slippage, test turn-around time, and test maintainability. We also classified the testing challenges to general testing challenges, challenges amplified by SoS, and challenges that are SoS specific. Interestingly, the interviewees agreed on the challenges, even though we sampled them with diversity in mind, which meant that the number of interviews conducted was sufficient to answer our research questions. We also identified solution proposals to the challenges that were categorized under four classes of developer quality assurance, function test, testing in all levels, and requirements engineering and communication.Conclusion: We conclude that although over half of the challenges we identified can be categorized as general testing challenges still SoS systems have their unique and amplified challenges stemming from SoS characteristics. Furthermore, it was found that interviews and fault slippage data indicated that different areas in the software process should be improved, which indicates that using only one of these methods would have led to an incomplete picture of the challenges in the case company.},
booktitle = {Proceedings of the ACM-IEEE International Symposium on Empirical Software Engineering and Measurement},
pages = {211–220},
numpages = {10},
keywords = {system of systems, software test, case study},
location = {Lund, Sweden},
series = {ESEM '12}
}

@inproceedings{10.1145/2945408.2945418,
author = {Kro\ss{}, Johannes and Willnecker, Felix and Zwickl, Thomas and Krcmar, Helmut},
title = {PET: Continuous Performance Evaluation Tool},
year = {2016},
isbn = {9781450344111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945408.2945418},
doi = {10.1145/2945408.2945418},
abstract = { Performance measurements and simulations produce large amounts of data in a short period of time. Release cycles are getting shorter due to the DevOps movement and heavily rely on live data from production or test environments. In addition, performance simulations increasingly become accurate and close to exact predictions. Results from these simulations are reliable and can be compared with live data to detect deviations from expected behavior. In this work, we present a comprehensive tool that can process and analyze measurement as well as simulation data quickly utilizing big data technologies. Live measurement data and simulation results can be analyzed for detecting performance problems, deviations from expected behavior or to simply compare a performance model with real world applications. },
booktitle = {Proceedings of the 2nd International Workshop on Quality-Aware DevOps},
pages = {42–43},
numpages = {2},
keywords = {Performance evaluation, performance analysis},
location = {Saarbr\"{u}cken, Germany},
series = {QUDOS 2016}
}

@article{10.1145/2786.2791,
author = {Hanson, Stephen Jos\'{e} and Rosinski, Richard R.},
title = {Programmer Perceptions of Productivity and Programming Tools},
year = {1985},
issue_date = {Feb. 1985},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {28},
number = {2},
issn = {0001-0782},
url = {https://doi.org/10.1145/2786.2791},
doi = {10.1145/2786.2791},
abstract = {Psychometric scaling methods are applied to programmer productivity assessments of 20 tools to recommend a set of minimal, as well as more comprehensive, tools.},
journal = {Commun. ACM},
month = {feb},
pages = {180–189},
numpages = {10}
}

@inproceedings{10.1145/3301293.3302363,
author = {Ma, Yun and Huang, Yangyang and Hu, Ziniu and Xiao, Xusheng and Liu, Xuanzhe},
title = {Paladin: Automated Generation of Reproducible Test Cases for Android Apps},
year = {2019},
isbn = {9781450362733},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3301293.3302363},
doi = {10.1145/3301293.3302363},
abstract = {Automated-test-generation tools generate test cases to enable dynamic analysis of Android apps, such as functional testing. These tools build a GUI model to describe the app states during the app execution, and generate a script that performs actions on UI widgets to form a test case. However, when the test cases are re-executed, the apps under analysis often do not behave consistently. The major reasons for such limited reproducibility are due to (1) backend-service dependencies that cause non-determinism in app behaviors and (2) the severe fragmentation of Android platform (i.e., the alarming number of different Android OS versions in vendor-customized devices). To address these challenges, we design and implement Paladin, a novel system that generates reproducible test cases for Android apps. The key insight of Paladin is to provide a GUI model that leverages the structure of the GUI view tree to identify equivalent app states, since the structure can tolerate the changes on the UI contents for an app behavior performed in different test executions. Based on the model, Paladin can search the view tree to locate the desired UI widgets to trigger events and drive the app exploration to reach the desired app states, making the test cases reproducible. Evaluation results on real apps show that Paladin could reach a much higher reproduction ratio than the state-of-the-art tools when the generated test cases are re-executed across different device configurations. In addition, benefiting from the reproducible capability, Paladin is able to cover more app behaviors compared with the existing tools.},
booktitle = {Proceedings of the 20th International Workshop on Mobile Computing Systems and Applications},
pages = {99–104},
numpages = {6},
keywords = {automated test generation, reproducible, android app},
location = {Santa Cruz, CA, USA},
series = {HotMobile '19}
}

@inproceedings{10.1145/1298275.1298280,
author = {Nezami, Kasra G. and Stephens, Peter W. and Walker, Stuart D.},
title = {Enhanced Communications Firmware Platform for Co-Verification of Complex Algorithms},
year = {2007},
isbn = {9781595938053},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1298275.1298280},
doi = {10.1145/1298275.1298280},
abstract = {A novel, yet simple and generic approach towards hardware/software partitioning, co-implementation and verification of the complex signal-processing algorithms is presented. An Industry standard MATLAB SIMULINK environment is adopted and linked to hardware and software development platforms, which execute individual algorithms. The method has been used in development of the new TETRA Release 2 secure wireless telecommunication standard and its effectiveness is demonstrated.},
booktitle = {Proceedings of the 2nd ACM Workshop on Performance Monitoring and Measurement of Heterogeneous Wireless and Wired Networks},
pages = {26–29},
numpages = {4},
keywords = {hardware/software co-design, co-verification, co-implementation, TETRA release 2},
location = {Chania, Crete Island, Greece},
series = {PM2HW2N '07}
}

@inproceedings{10.1145/55364.55379,
author = {Bruegge, B. and Gross, T.},
title = {An Integrated Environment for Development and Execution of Real-Time Programs},
year = {1988},
isbn = {0897912721},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/55364.55379},
doi = {10.1145/55364.55379},
abstract = {The goal of the Warp Programming Environment (WPE) is to provide easy access to the Warp machine, a parallel supercomputer with a peak performance of 100 MFLOPS that is based on the systolic array architecture. The Warp Programming Environment offers a uniform environment for editing, compiling, debugging and executing Warp programs. It is based on an extensible shell written in Common Lisp and a runtime system written in C. It runs on a network of SUN-3 workstations under UNIX 4.2. This paper describes how the program development environment interacts with the Warp machine to support the development and execution of real-time programs on Warp.},
booktitle = {Proceedings of the 2nd International Conference on Supercomputing},
pages = {153–163},
numpages = {11},
location = {St. Malo, France},
series = {ICS '88}
}

@inproceedings{10.1145/3037697.3037735,
author = {Liu, Haopeng and Li, Guangpu and Lukman, Jeffrey F. and Li, Jiaxin and Lu, Shan and Gunawi, Haryadi S. and Tian, Chen},
title = {DCatch: Automatically Detecting Distributed Concurrency Bugs in Cloud Systems},
year = {2017},
isbn = {9781450344654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3037697.3037735},
doi = {10.1145/3037697.3037735},
abstract = {In big data and cloud computing era, reliability of distributed systems is extremely important. Unfortunately, distributed concurrency bugs, referred to as DCbugs, widely exist. They hide in the large state space of distributed cloud systems and manifest non-deterministically depending on the timing of distributed computation and communication. Effective techniques to detect DCbugs are desired. This paper presents a pilot solution, DCatch, in the world of DCbug detection. DCatch predicts DCbugs by analyzing correct execution of distributed systems. To build DCatch, we design a set of happens-before rules that model a wide variety of communication and concurrency mechanisms in real-world distributed cloud systems. We then build runtime tracing and trace analysis tools to effectively identify concurrent conflicting memory accesses in these systems. Finally, we design tools to help prune false positives and trigger DCbugs. We have evaluated DCatch on four representative open-source distributed cloud systems, Cassandra, Hadoop MapReduce, HBase, and ZooKeeper. By monitoring correct execution of seven workloads on these systems, DCatch reports 32 DCbugs, with 20 of them being truly harmful.},
booktitle = {Proceedings of the Twenty-Second International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {677–691},
numpages = {15},
keywords = {distributed systems, bug detection, concurrency bugs, cloud computing},
location = {Xi'an, China},
series = {ASPLOS '17}
}

@article{10.1145/3093336.3037735,
author = {Liu, Haopeng and Li, Guangpu and Lukman, Jeffrey F. and Li, Jiaxin and Lu, Shan and Gunawi, Haryadi S. and Tian, Chen},
title = {DCatch: Automatically Detecting Distributed Concurrency Bugs in Cloud Systems},
year = {2017},
issue_date = {April 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {4},
issn = {0362-1340},
url = {https://doi.org/10.1145/3093336.3037735},
doi = {10.1145/3093336.3037735},
abstract = {In big data and cloud computing era, reliability of distributed systems is extremely important. Unfortunately, distributed concurrency bugs, referred to as DCbugs, widely exist. They hide in the large state space of distributed cloud systems and manifest non-deterministically depending on the timing of distributed computation and communication. Effective techniques to detect DCbugs are desired. This paper presents a pilot solution, DCatch, in the world of DCbug detection. DCatch predicts DCbugs by analyzing correct execution of distributed systems. To build DCatch, we design a set of happens-before rules that model a wide variety of communication and concurrency mechanisms in real-world distributed cloud systems. We then build runtime tracing and trace analysis tools to effectively identify concurrent conflicting memory accesses in these systems. Finally, we design tools to help prune false positives and trigger DCbugs. We have evaluated DCatch on four representative open-source distributed cloud systems, Cassandra, Hadoop MapReduce, HBase, and ZooKeeper. By monitoring correct execution of seven workloads on these systems, DCatch reports 32 DCbugs, with 20 of them being truly harmful.},
journal = {SIGPLAN Not.},
month = {apr},
pages = {677–691},
numpages = {15},
keywords = {concurrency bugs, bug detection, distributed systems, cloud computing}
}

@article{10.1145/3093337.3037735,
author = {Liu, Haopeng and Li, Guangpu and Lukman, Jeffrey F. and Li, Jiaxin and Lu, Shan and Gunawi, Haryadi S. and Tian, Chen},
title = {DCatch: Automatically Detecting Distributed Concurrency Bugs in Cloud Systems},
year = {2017},
issue_date = {March 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {45},
number = {1},
issn = {0163-5964},
url = {https://doi.org/10.1145/3093337.3037735},
doi = {10.1145/3093337.3037735},
abstract = {In big data and cloud computing era, reliability of distributed systems is extremely important. Unfortunately, distributed concurrency bugs, referred to as DCbugs, widely exist. They hide in the large state space of distributed cloud systems and manifest non-deterministically depending on the timing of distributed computation and communication. Effective techniques to detect DCbugs are desired. This paper presents a pilot solution, DCatch, in the world of DCbug detection. DCatch predicts DCbugs by analyzing correct execution of distributed systems. To build DCatch, we design a set of happens-before rules that model a wide variety of communication and concurrency mechanisms in real-world distributed cloud systems. We then build runtime tracing and trace analysis tools to effectively identify concurrent conflicting memory accesses in these systems. Finally, we design tools to help prune false positives and trigger DCbugs. We have evaluated DCatch on four representative open-source distributed cloud systems, Cassandra, Hadoop MapReduce, HBase, and ZooKeeper. By monitoring correct execution of seven workloads on these systems, DCatch reports 32 DCbugs, with 20 of them being truly harmful.},
journal = {SIGARCH Comput. Archit. News},
month = {apr},
pages = {677–691},
numpages = {15},
keywords = {concurrency bugs, cloud computing, bug detection, distributed systems}
}

@inproceedings{10.1145/3324884.3416532,
author = {Tian, Haoye and Liu, Kui and Kabor\'{e}, Abdoul Kader and Koyuncu, Anil and Li, Li and Klein, Jacques and Bissyand\'{e}, Tegawend\'{e} F.},
title = {Evaluating Representation Learning of Code Changes for Predicting Patch Correctness in Program Repair},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416532},
doi = {10.1145/3324884.3416532},
abstract = {A large body of the literature of automated program repair develops approaches where patches are generated to be validated against an oracle (e.g., a test suite). Because such an oracle can be imperfect, the generated patches, although validated by the oracle, may actually be incorrect. While the state of the art explore research directions that require dynamic information or that rely on manually-crafted heuristics, we study the benefit of learning code representations in order to learn deep features that may encode the properties of patch correctness. Our empirical work mainly investigates different representation learning approaches for code changes to derive embeddings that are amenable to similarity computations. We report on findings based on embeddings produced by pre-trained and re-trained neural networks. Experimental results demonstrate the potential of embeddings to empower learning algorithms in reasoning about patch correctness: a machine learning predictor with BERT transformer-based embeddings associated with logistic regression yielded an AUC value of about 0.8 in the prediction of patch correctness on a deduplicated dataset of 1000 labeled patches. Our investigations show that learned representations can lead to reasonable performance when comparing against the state-of-the-art, PATCH-SIM, which relies on dynamic information. These representations may further be complementary to features that were carefully (manually) engineered in the literature.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {981–992},
numpages = {12},
keywords = {machine learning, patch correctness, embeddings, program repair, distributed representation learning},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.1145/2846661.2846674,
author = {Abadi, Aharon and Flynn, Lori and Gray, Jeff},
title = {Mobile Security: Challenges, Tools, and Techniques (Panel)},
year = {2015},
isbn = {9781450339063},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2846661.2846674},
doi = {10.1145/2846661.2846674},
abstract = { During the MobileDeli’15 workshop held at the SPLASH’15 conference we facilitated a panel, comprised of four distinguished, senior participants from industry. They started by presenting their position with respect to a set of predefined questions, and then we opened the floor to questions from the audience. },
booktitle = {Proceedings of the 3rd International Workshop on Mobile Development Lifecycle},
pages = {51–53},
numpages = {3},
keywords = {App Development, Mobile devices, Smartphone},
location = {Pittsburgh, PA, USA},
series = {MobileDeLi 2015}
}

@article{10.1145/1234741.1234768,
author = {Rosa, Nelson Souto and Cunha, Paulo Roberto Freire},
title = {A Formal Framework for Middleware Behavioural Specification},
year = {2007},
issue_date = {March 2007},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {32},
number = {2},
issn = {0163-5948},
url = {https://doi.org/10.1145/1234741.1234768},
doi = {10.1145/1234741.1234768},
abstract = {The number of open specifications of middleware systems and middleware services is increasing. Despite their complexity, they are traditionally described through APIs (the operation signatures) and informal prose (the behaviour). This fact often leads to ambiguities, whilst making difficult a better understanding of what is actually described. This paper presents a formal framework, specified in LOTOS (Language Of Temporal Ordering Specification), for the specification of middleware systems. The framework consists of a set of basic/common middleware components and some guidelines on how to compose them. The components of the framework facilitate the formal specification of different middleware systems. In order to illustrate how the framework may be used, it is adopted to specify procedural (synchronous) and message-oriented (asynchronous) middleware systems.},
journal = {SIGSOFT Softw. Eng. Notes},
month = {mar},
pages = {1–7},
numpages = {7},
keywords = {middleware, LOTOS, formal specification, framework}
}

@inproceedings{10.1145/1385269.1385284,
author = {Harinath, Sivakumar and Isaza, Gonzalo and Mirchandani, Akshai and Dumitru, Marius},
title = {Testing Microsoft SQL Server Analysis Services},
year = {2008},
isbn = {9781605582337},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1385269.1385284},
doi = {10.1145/1385269.1385284},
abstract = {In this paper, we describe the current techniques used in testing SQL Server Analysis Services, the challenges we face, and techniques we are currently looking into to improve the testing of SQL Server Analysis Services both in functionality as well as performance.},
booktitle = {Proceedings of the 1st International Workshop on Testing Database Systems},
articleno = {12},
numpages = {6},
keywords = {SQL server analysis services, storage engine, MDX, formula engine, calculations},
location = {Vancouver, British Columbia, Canada},
series = {DBTest '08}
}

@article{10.1145/1330017.1330019,
author = {Weimer, Westley and Necula, George C.},
title = {Exceptional Situations and Program Reliability},
year = {2008},
issue_date = {March 2008},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {2},
issn = {0164-0925},
url = {https://doi.org/10.1145/1330017.1330019},
doi = {10.1145/1330017.1330019},
abstract = {It is difficult to write programs that behave correctly in the presence of run-time errors. Proper behavior in the face of exceptional situations is important to the reliability of long-running programs. Existing programming language features often provide poor support for executing clean-up code and for restoring invariants.We present a data-flow analysis for finding a certain class of exception-handling defects: those related to a failure to release resources or to clean up properly along all paths. Many real-world programs violate such resource usage rules because of incorrect exception handling. Our flow-sensitive analysis keeps track of outstanding obligations along program paths and does a precise modeling of control flow in the presence of exceptions. Using it, we have found over 1,300 exception handling defects in over 5 million lines of Java code.Based on those defects we propose a programming language feature, the compensation stack, that keeps track of obligations at run time and ensures that they are discharged. We present a type system for compensation stacks that tracks collections of obligations. Finally, we present case studies to demonstrate that this feature is natural, efficient, and can improve reliability.},
journal = {ACM Trans. Program. Lang. Syst.},
month = {mar},
articleno = {8},
numpages = {51},
keywords = {resource management, Error handling, compensating transactions, linear sagas, linear types}
}

@inbook{10.1145/3373376.3378484,
author = {Yuan, Xinhao and Yang, Junfeng},
title = {Effective Concurrency Testing for Distributed Systems},
year = {2020},
isbn = {9781450371025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3373376.3378484},
abstract = {Despite their wide deployment, distributed systems remain notoriously hard to reason about. Unexpected interleavings of concurrent operations and failures may lead to undefined behaviors and cause serious consequences. We present Morpheus, the first concurrency testing tool leveraging partial order sampling, a randomized testing method formally analyzed and empirically validated to provide strong probabilistic guarantees of error-detection, for real-world distributed systems. Morpheus introduces conflict analysis to further improve randomized testing by predicting and focusing on operations that affect the testing result. Inspired by the recent shift in building distributed systems using higher-level languages and frameworks, Morpheus targets Erlang. Evaluation on four popular distributed systems in Erlang including RabbitMQ, a message broker service, and Mnesia, a distributed database in the Erlang standard libraries, shows that Morpheus is effective: It found previously unknown errors in every system checked, 11 total, all of which are flaws in their core protocols that may cause deadlocks, unexpected crashes, or inconsistent states.},
booktitle = {Proceedings of the Twenty-Fifth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {1141–1156},
numpages = {16}
}

@inproceedings{10.1145/3404397.3404464,
author = {Mururu, Girish and Ravichandran, Kaushik and Gavrilovska, Ada and Pande, Santosh},
title = {Generating Robust Parallel Programs via Model Driven Prediction of Compiler Optimizations for Non-Determinism},
year = {2020},
isbn = {9781450388160},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3404397.3404464},
doi = {10.1145/3404397.3404464},
abstract = {Execution orders in parallel programs are governed by non-determinism and can vary substantially across different executions even on the same input. Thus, a highly non-deterministic program can exhibit rare execution orders never observed during testing. It is desirable to reduce non-determinism to suppress corner case behavior in production cycle (making the execution robust or bug-free) and increase non-determinism for reproducing bugs in the development cycle. Performance-wise different optimization levels (e.g. from O0 to O3) are enabled during development , however, non-determinism-wise, developers have no way to select right compiler optimization level in order to increase non-determinism for debugging or to decrease it for robustness. The major source of non-determinism is the underlying execution model, primarily determined by the processor architecture and the operating system (OS). Architectural artifacts such as cache misses and TLB misses characterize and shape the non-determinism. In this work, we seek to capture such sources of non-determinism through an architectural model based on hardware performance counters and use the model for predicting the appropriate compiler optimization level for generating a robust parallel program, which has minimal non-determinism in production. As a side effect, the generated model also allows maximizing non-determinism for debugging purposes. We demonstrate our technique on the PARSEC benchmark suite, and among other results show that the generated robust program decreases non-deterministic behavior up to 66.48%, and as a practical measure we also show that a known race condition plus randomly injected ones are rendered benign in the robust parallel program generated by our framework. },
booktitle = {49th International Conference on Parallel Processing - ICPP},
articleno = {32},
numpages = {12},
keywords = {Compiler Optimization Prediction, Non-deterministic Execution, Thread-level Non-determinism},
location = {Edmonton, AB, Canada},
series = {ICPP '20}
}

@inproceedings{10.1145/2945408.2945420,
author = {Cito, J\"{u}rgen and Mazlami, Genc and Leitner, Philipp},
title = {TemPerf: Temporal Correlation between Performance Metrics and Source Code},
year = {2016},
isbn = {9781450344111},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2945408.2945420},
doi = {10.1145/2945408.2945420},
abstract = { Today's rapidly evolving software systems continuously introduce new changes that can potentially degrade performance. Large-scale load testing prior to deployment is supposed to avoid performance regressions in production. However, due to the large input space in parameterized load testing, not all performance regressions can be prevented in practice. To support developers in identifying the change sets that had an impact on performance, we present TemPerf, a tool that correlates performance regressions with change sets by exploiting temporal constraints. It is implemented as an Eclipse IDE plugin that allows developers to visualize performance developments over time and display temporally correlated change sets retrieved from version control and continuous integration platforms. },
booktitle = {Proceedings of the 2nd International Workshop on Quality-Aware DevOps},
pages = {46–47},
numpages = {2},
keywords = {Performance, DevOps, Developer Tools},
location = {Saarbr\"{u}cken, Germany},
series = {QUDOS 2016}
}

@article{10.1145/3450758,
author = {Flowers, Lamont A.},
title = {Testing Educational Digital Games},
year = {2021},
issue_date = {September 2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {64},
number = {9},
issn = {0001-0782},
url = {https://doi.org/10.1145/3450758},
doi = {10.1145/3450758},
abstract = {Diversifying usability studies utilizing rapid application development.},
journal = {Commun. ACM},
month = {aug},
pages = {38–40},
numpages = {3}
}

@inproceedings{10.1145/2338966.2336806,
author = {Ashraf, Imran and Ostadzadeh, S. Arash and Meeuws, Roel and Bertels, Koen},
title = {Communication-Aware HW/SW Co-Design for Heterogeneous Multicore Platforms},
year = {2012},
isbn = {9781450314558},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2338966.2336806},
doi = {10.1145/2338966.2336806},
abstract = { QUAD is an open source profiling toolset, which is an integral part of the Q2 profiling framework. In this paper, we extend QUAD to introduce the concept of Unique Data Values regarding the data communication among functions. This feature is important to make a proper partitioning of the application. Mapping a well-known feature tracker application onto the multicore heterogeneous platform at hand is presented as a case study to substantiate the usefulness of the added feature. Experimental results show a speedup of 2.24x by utilizing the new QUAD toolset. },
booktitle = {Proceedings of the Ninth International Workshop on Dynamic Analysis},
pages = {36–41},
numpages = {6},
location = {Minneapolis, MN, USA},
series = {WODA 2012}
}

@inproceedings{10.1145/3117811.3117819,
author = {Xu, Jian and Cao, Qingqing and Prakash, Aditya and Balasubramanian, Aruna and Porter, Donald E.},
title = {UIWear: Easily Adapting User Interfaces for Wearable Devices},
year = {2017},
isbn = {9781450349161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3117811.3117819},
doi = {10.1145/3117811.3117819},
abstract = {Wearable devices such as smartwatches offer exciting new opportunities for users to interact with their applications. However, the current wearable programming model requires the developer to write a custom companion app for each wearable form factor; the companion app extends the smartphone display onto the wearable, relays user interactions from the wearable to the phone, and updates the wearable display as needed. The development effort required to write a companion app is significant and will not scale to an increasing diversity of form factors. This paper argues for a different programming model for wearable devices. The developer writes an application for the smartphone, but only specifies a UI design for the wearable. Our UIWear system abstracts a logical model of the smartphone GUI, re-tailors the GUI for the wearable device based on the specified UI design, and compiles it into a companion app that we call the UICompanion app. We implemented UIWear on Android smartphones, AndroidWear smartwatches, and Sony SmartEyeGlasses. We evaluate 20 developer-written companion apps from the AndroidWear category on Google Play against the UIWear-created UICompanion apps. The lines-of-code required for the developer to specify the UI design in UIWear is an order-of-magnitude smaller compared to the companion app lines-of-code. Further, in most cases, the UICompanion app performed comparably or better than the corresponding companion app both in terms of qualitative metrics, including latency and energy, and quantitative metrics, including look-and-feel.},
booktitle = {Proceedings of the 23rd Annual International Conference on Mobile Computing and Networking},
pages = {369–382},
numpages = {14},
keywords = {smartwatch, wearable, accessibility, smartphone, android},
location = {Snowbird, Utah, USA},
series = {MobiCom '17}
}

@inproceedings{10.1145/3377811.3380384,
author = {Sotiropoulos, Thodoris and Mitropoulos, Dimitris and Spinellis, Diomidis},
title = {Practical Fault Detection in Puppet Programs},
year = {2020},
isbn = {9781450371216},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3377811.3380384},
doi = {10.1145/3377811.3380384},
abstract = {Puppet is a popular computer system configuration management tool. By providing abstractions that model system resources it allows administrators to set up computer systems in a reliable, predictable, and documented fashion. Its use suffers from two potential pitfalls. First, if ordering constraints are not correctly specified whenever a Puppet resource depends on another, the non-deterministic application of resources can lead to race conditions and consequent failures. Second, if a service is not tied to its resources (through the notification construct), the system may operate in a stale state whenever a resource gets modified. Such faults can degrade a computing infrastructure's availability and functionality.We have developed an approach that identifies these issues through the analysis of a Puppet program and its system call trace. Specifically, a formal model for traces allows us to capture the interactions of Puppet resources with the file system. By analyzing these interactions we identify (1) resources that are related to each other (e.g., operate on the same file), and (2) resources that should act as notifiers so that changes are correctly propagated. We then check the relationships from the trace's analysis against the program's dependency graph: a representation containing all the ordering constraints and notifications declared in the program. If a mismatch is detected, our system reports a potential fault.We have evaluated our method on a large set of popular Puppet modules, and discovered 92 previously unknown issues in 33 modules. Performance benchmarking shows that our approach can analyze in seconds real-world configurations with a magnitude measured in thousands of lines and millions of system calls.},
booktitle = {Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering},
pages = {26–37},
numpages = {12},
keywords = {program analysis, ordering relationships, system calls, puppet, notifiers},
location = {Seoul, South Korea},
series = {ICSE '20}
}

