@inproceedings{10.1109/SECSE.2009.5069155,
author = {Hannay, Jo Erskine and MacLeod, Carolyn and Singer, Janice and Langtangen, Hans Petter and Pfahl, Dietmar and Wilson, Greg},
title = {How Do Scientists Develop and Use Scientific Software?},
year = {2009},
isbn = {9781424437375},
publisher = {IEEE Computer Society},
address = {USA},
url = {https://doi.org/10.1109/SECSE.2009.5069155},
doi = {10.1109/SECSE.2009.5069155},
abstract = {New knowledge in science and engineering relies increasingly on results produced by scientific software. Therefore, knowing how scientists develop and use software in their research is critical to assessing the necessity for improving current development practices and to making decisions about the future allocation of resources. To that end, this paper presents the results of a survey conducted online in October-December 2008 which received almost 2000 responses. Our main conclusions are that (1) the knowledge required to develop and use scientific software is primarily acquired from peers and through self-study, rather than from formal education and training; (2) the number of scientists using supercomputers is small compared to the number using desktop or intermediate computers; (3) most scientists rely primarily on software with a large user base; (4) while many scientists believe that software testing is important, a smaller number believe they have sufficient understanding about testing concepts; and (5) that there is a tendency for scientists to rank standard software engineering concepts higher if they work in large software development projects and teams, but that there is no uniform trend of association between rank of importance of software engineering concepts and project/team size.},
booktitle = {Proceedings of the 2009 ICSE Workshop on Software Engineering for Computational Science and Engineering},
pages = {1–8},
numpages = {8},
series = {SECSE '09}
}

@inproceedings{10.5555/2486788.2486812,
author = {Hassan, Mohammad Mahdi and Andrews, James H.},
title = {Comparing Multi-Point Stride Coverage and Dataflow Coverage},
year = {2013},
isbn = {9781467330763},
publisher = {IEEE Press},
abstract = { We introduce a family of coverage criteria, called Multi-Point Stride Coverage (MPSC). MPSC generalizes branch coverage to coverage of tuples of branches taken from the execution sequence of a program. We investigate its potential as a replacement for dataflow coverage, such as def-use coverage. We find that programs can be instrumented for MPSC easily, that the instrumentation usually incurs less overhead than that for def-use coverage, and that MPSC is comparable in usefulness to def-use in predicting test suite effectiveness. We also find that the space required to collect MPSC can be predicted from the number of branches in the program. },
booktitle = {Proceedings of the 2013 International Conference on Software Engineering},
pages = {172–181},
numpages = {10},
location = {San Francisco, CA, USA},
series = {ICSE '13}
}

@inproceedings{10.1145/3324884.3416563,
author = {Usman, Muhammad and Wang, Wenxi and Khurshid, Sarfraz},
title = {TestMC: Testing Model Counters Using Differential and Metamorphic Testing},
year = {2020},
isbn = {9781450367684},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3324884.3416563},
doi = {10.1145/3324884.3416563},
abstract = {Model counting is the problem for finding the number of solutions to a formula over a bounded universe. This is a classic problem in computer science that has seen many recent advances in techniques and tools that tackle it. These advances have led to applications of model counting in many domains, e.g., quantitative program analysis, reliability, and security. Given the sheer complexity of the underlying problem, today's model counters employ sophisticated algorithms and heuristics, which result in complex tools that must be heavily optimized. Therefore, establishing the correctness of implementations of model counters necessitates rigorous testing. This experience paper presents an empirical study on testing industrial strength model counters by applying the principles of differential and metamorphic testing together with bounded exhaustive input generation and input minimization. We embody these principles in the TestMC framework, and apply it to test four model counters, including three state-of-the-art model counters from three different classes. Specifically, we test the exact model counters projMC and dSharp, the probabilistic exact model counter Ganak, and the probabilistic approximate model counter ApproxMC. As subjects, we use three complementary test suites of input formulas. One suite consists of larger formulas that are derived from a wide range of real-world software design problems. The second suite consists of a bounded exhaustive set of small formulas that TestMC generated. The third suite consists of formulas generated using an off-the-shelf CNF fuzzer. TestMC found bugs in three of the four subject model counters. The bugs led to crashes, segmentation faults, incorrect model counts, and resource exhaustion by the solvers. Two of the tools were corrected subsequent to the bug reports we submitted based on our study, whereas the bugs we reported in the third tool were deemed by the tool authors to not require a fix.},
booktitle = {Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering},
pages = {709–721},
numpages = {13},
keywords = {metamorphic testing, delta debugging, differential testing, model counting},
location = {Virtual Event, Australia},
series = {ASE '20}
}

@inproceedings{10.5555/2662413.2662425,
author = {Jia, Changjiang and Chan, W. K.},
title = {Which Compiler Optimization Options Should I Use for Detecting Data Races in Multithreaded Programs?},
year = {2013},
isbn = {9781467361613},
publisher = {IEEE Press},
abstract = {Different compiler optimization options may produce different versions of object code. To the best of our knowledge, existing studies on concurrency bug detection in the public literature have not reported the effects of different compiler optimization options on detection effectiveness. This paper reports a preliminary but the first study in the exploratory nature to investigate this aspect. The study examines the happened-before based predictive data race detection scenarios on four benchmarks from the PARSEC 3.0 suite compiled under six different GNU GCC optimization options. We observe from the data set that the same race detection technique may produce different sets of races or different detection probabilities under different optimization scenarios. Based on the observations, we formulate two hypotheses for future investigations.},
booktitle = {Proceedings of the 8th International Workshop on Automation of Software Test},
pages = {53–56},
numpages = {4},
keywords = {race detection, hypothesis formulation, compiler optimization option, empirical study},
location = {San Francisco, California},
series = {AST '13}
}

@inproceedings{10.1145/174675.175935,
author = {Agrawal, Hiralal},
title = {Dominators, Super Blocks, and Program Coverage},
year = {1994},
isbn = {0897916360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/174675.175935},
doi = {10.1145/174675.175935},
abstract = {In this paper we present techniques to find subsets of nodes of a flowgraph that satisfy the following property: A test set that exercises all nodes in a subset exercises all nodes in the flowgraph. Analogous techniques to find subsets of edges are also proposed. These techniques may be used to significantly reduce the cost of coverage testing of programs. A notion of a super block consisting of one or more basic blocks in that super block must be exercised by the same input. Dominator relationships among super blocks are used to identify a subset of the super blocks whose coverage implies that of all super blocks and, in turn, that of all basic blocks. Experiments with eight systems in the range of 1-75K lines of code show that, on the average, test cases targeted to cover just 29% of the basic blocks and 32% of the branches ensure 100% block and branch coverage, respectively.},
booktitle = {Proceedings of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages},
pages = {25–34},
numpages = {10},
location = {Portland, Oregon, USA},
series = {POPL '94}
}

@article{10.1145/2876441,
author = {Alimadadi, Saba and Sequeira, Sheldon and Mesbah, Ali and Pattabiraman, Karthik},
title = {Understanding JavaScript Event-Based Interactions with Clematis},
year = {2016},
issue_date = {May 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {2},
issn = {1049-331X},
url = {https://doi.org/10.1145/2876441},
doi = {10.1145/2876441},
abstract = {Web applications have become one of the fastest-growing types of software systems today. Despite their popularity, understanding the behavior of modern web applications is still a challenging endeavor for developers during development and maintenance tasks. The challenges mainly stem from the dynamic, event-driven, and asynchronous nature of the JavaScript language. We propose a generic technique for capturing low-level event-based interactions in a web application and mapping those to a higher-level behavioral model. This model is then transformed into an interactive visualization, representing episodes of triggered causal and temporal events, related JavaScript code executions, and their impact on the dynamic DOM state. Our approach, implemented in a tool called Clematis, allows developers to easily understand the complex dynamic behavior of their application at three different semantic levels of granularity. Furthermore, Clematis helps developers bridge the gap between test cases and program code by localizing the fault related to a test assertion. The results of our industrial controlled experiment show that Clematis is capable of improving the comprehension task accuracy by 157% while reducing the task completion time by 47%. A follow-up experiment reveals that Clematis improves the fault localization accuracy of developers by a factor of two.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {may},
articleno = {12},
numpages = {38},
keywords = {JavaScript, event-based interactions, Program comprehension, web applications, fault localization}
}

@inproceedings{10.1145/2594291.2594334,
author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
title = {Compiler Validation via Equivalence modulo Inputs},
year = {2014},
isbn = {9781450327848},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2594291.2594334},
doi = {10.1145/2594291.2594334},
abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
booktitle = {Proceedings of the 35th ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {216–226},
numpages = {11},
keywords = {automated testing, equivalent program variants, compiler testing, miscompilation},
location = {Edinburgh, United Kingdom},
series = {PLDI '14}
}

@article{10.1145/2666356.2594334,
author = {Le, Vu and Afshari, Mehrdad and Su, Zhendong},
title = {Compiler Validation via Equivalence modulo Inputs},
year = {2014},
issue_date = {June 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {49},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/2666356.2594334},
doi = {10.1145/2666356.2594334},
abstract = {We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations.To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed.Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {216–226},
numpages = {11},
keywords = {automated testing, miscompilation, equivalent program variants, compiler testing}
}

@inproceedings{10.1007/978-3-642-12261-3_2,
author = {Schoenboeck, Johannes and Kappel, Gerti and Kusel, Angelika and Retschitzegger, Werner and Schwinger, Wieland and Wimmer, Manuel},
title = {Catch Me If You Can – Debugging Support for Model Transformations},
year = {2009},
isbn = {3642122604},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/978-3-642-12261-3_2},
doi = {10.1007/978-3-642-12261-3_2},
abstract = {Model-Driven Engineering places models as first-class artifacts throughout the software lifecycle requiring the availability of proper transformation languages. Although numerous approaches are available, they lack convenient facilities for supporting debugging and understanding of the transformation logic. This is because execution engines operate on a low level of abstraction, hide the operational semantics of a transformation, scatter metamodels, models, transformation logic, and trace information across different artifacts, and provide limited verification support. To tackle these problems, we propose a Domain-Specific Language (DSL) on top of Colored Petri Nets (CPNs)—called Transformation Nets—for the execution and debugging of model transformations on a high level of abstraction. This formalism makes the afore hidden operational semantics explicit by providing a runtime model in terms of places, transitions and tokens, integrating all artifacts involved into a homogenous view. Moreover, the formal underpinnings of CPNs enable comprehensive verification of model transformations.},
booktitle = {Proceedings of the 2009 International Conference on Models in Software Engineering},
pages = {5–20},
numpages = {16},
keywords = {runtime model, debugging, model transformation, CPN},
location = {Denver, CO},
series = {MODELS'09}
}

@inproceedings{10.1145/3475716.3475786,
author = {Gonzalez, Danielle and Perez, Paola Peralta and Mirakhorli, Mehdi},
title = {Barriers to Shift-Left Security: The Unique Pain Points of Writing Automated Tests Involving Security Controls},
year = {2021},
isbn = {9781450386654},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475716.3475786},
doi = {10.1145/3475716.3475786},
abstract = {Background: Automated unit and integration tests allow software development teams to continuously evaluate their application's behavior and ensure requirements are satisfied. Interest in explicitly testing security at the unit and integration levels has risen as more teams begin to shift security left in their workflows, but there is little insight into any potential pain points developers may experience as they learn to adapt their existing skills to write these tests. Aims: Identify security unit and integration testing pain points that could negatively impact efforts to shift security (testing) left to this level. Method: An mixed-method empirical study was conducted on 525 Stack Overflow and Security Stack Exchange posts related to security unit and integration testing. Latent Dirichlet Allocation (LDA) was applied to identify commonly discussed topics, pain points were learned through qualitative analysis, and links were analyzed to study commonly-shared resources. Results: Nine topics representing security controls, components, and scenarios were identified; Authentication was the most commonly tested control. Developers experienced seven pain points unique to security unit and integration testing, which were all influenced by the complexity of security control designs and implementations. Most linked resources were other Q&amp;A posts, but repositories and documentation for security tools and libraries were also common. Conclusions: Developers may experience several unique pain points when writing tests at this level involving security controls. Additional resources are needed to guide developers through these challenges, which should also influence the creation of strategies and tools to help shift security testing to this level. To accelerate this, actionable recommendations for practitioners and future research directions based on these findings are highlighted.},
booktitle = {Proceedings of the 15th ACM / IEEE International Symposium on Empirical Software Engineering and Measurement (ESEM)},
articleno = {11},
numpages = {12},
keywords = {Pain Points, Unit Testing, Stack Overflow, Shift-Left Security, Latent Dirichlet Allocation, Integration Testing, Security Testing},
location = {Bari, Italy},
series = {ESEM '21}
}

@inproceedings{10.1145/2635868.2635920,
author = {Luo, Qingzhou and Hariri, Farah and Eloussi, Lamyaa and Marinov, Darko},
title = {An Empirical Analysis of Flaky Tests},
year = {2014},
isbn = {9781450330565},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2635868.2635920},
doi = {10.1145/2635868.2635920},
abstract = { Regression testing is a crucial part of software development. It checks that software changes do not break existing functionality. An important assumption of regression testing is that test outcomes are deterministic: an unmodified test is expected to either always pass or always fail for the same code under test. Unfortunately, in practice, some tests often called flaky tests—have non-deterministic outcomes. Such tests undermine the regression testing as they make it difficult to rely on test results. We present the first extensive study of flaky tests. We study in detail a total of 201 commits that likely fix flaky tests in 51 open-source projects. We classify the most common root causes of flaky tests, identify approaches that could manifest flaky behavior, and describe common strategies that developers use to fix flaky tests. We believe that our insights and implications can help guide future research on the important topic of (avoiding) flaky tests. },
booktitle = {Proceedings of the 22nd ACM SIGSOFT International Symposium on Foundations of Software Engineering},
pages = {643–653},
numpages = {11},
keywords = {flaky tests, non-determinism, Empirical study},
location = {Hong Kong, China},
series = {FSE 2014}
}

@article{10.1145/1107541.1107543,
author = {Frenger, Paul},
title = {Ten Years of Forth in ACM Sigplan Notices: Part 1},
year = {2005},
issue_date = {November 2005},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {40},
number = {11},
issn = {0362-1340},
url = {https://doi.org/10.1145/1107541.1107543},
doi = {10.1145/1107541.1107543},
abstract = {So here we are: ten years of the ACM Sigplan Notices Forth Report is behind us. How amazing it is that we have reached this milestone! I hope that this column's longevity is a reflection of its popularity and perceived value to our readers.},
journal = {SIGPLAN Not.},
month = {nov},
pages = {4–16},
numpages = {13}
}

@inproceedings{10.1145/3427796.3427798,
author = {Agape, Andrei-Alexandru and Danceanu, Madalin Claudiu and Hansen, Rene Rydhof and Schmid, Stefan},
title = {P4Fuzz: Compiler Fuzzer ForDependable Programmable Dataplanes},
year = {2021},
isbn = {9781450389334},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3427796.3427798},
doi = {10.1145/3427796.3427798},
abstract = { Emerging software-defined networks and programmable dataplanes promise to render communication networks more dependable, overcoming today’s manual and error-prone approach to operate networks. Indeed, programmable dataplanes such as P4 provide great opportunities for improving network performance and developing innovative security features, by allowing programmers to reconfigure and tailor switches towards their needs. However, extending programmability to the dataplane also introduces new threat models. In this paper, using a systematic security analysis, we identify a particularly worrisome vulnerability: the automated program compilers which lie at the core of programmable dataplanes. The dataplane compilers introduce a risk of persistent threats which are covert and hard to detect, and may be exploited for large-scale attacks, affecting many devices. Our main contribution is P4Fuzz, a compiler fuzzer to find bugs and vulnerabilities in P4 compilers, in an efficient and automated manner. We discuss the challenges involved in designing such a compiler fuzzer for P4, present our fuzzing and taming algorithms, and report on experiments with our prototype implementation, considering the standard compilers of BMv2, eBPF, and NetFPGA. Our experiments confirm that P4Fuzz is able to generate and test the validity of dozens of P4 programs per minute. Using P4Fuzz, we also successfully found several bugs which have been acknowledged and fixed by the community.},
booktitle = {International Conference on Distributed Computing and Networking 2021},
pages = {16–25},
numpages = {10},
keywords = {software defined networking, p4 compiler, fuzzing},
location = {Nara, Japan},
series = {ICDCN '21}
}

@inproceedings{10.1145/2377978.2377983,
author = {Alexandrov, Alexander and Schiefer, Berni and Poelman, John and Ewen, Stephan and Bodner, Thomas O. and Markl, Volker},
title = {Myriad: Parallel Data Generation on Shared-Nothing Architectures},
year = {2011},
isbn = {9781450314398},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2377978.2377983},
doi = {10.1145/2377978.2377983},
abstract = {The need for efficient data generation for the purposes of testing and benchmarking newly developed massively-parallel data processing systems has increased with the emergence of Big Data problems. As synthetic data model specifications evolve over time, the data generator programs implementing these models have to be adapted continuously -- a task that often becomes more tedious as the set of model constraints grows. In this paper we present Myriad - a new parallel data generation toolkit. Data generators created with the toolkit can quickly produce very large datasets in a shared-nothing parallel execution environment, while at the same time preserve with cross-partition dependencies, correlations and distributions in the generated data. In addition, we report on our efforts towards a benchmark suite for large-scale parallel analysis systems that uses Myriad for the generation of OLAP-style relational datasets.},
booktitle = {Proceedings of the 1st Workshop on Architectures and Systems for Big Data},
pages = {30–33},
numpages = {4},
keywords = {testing tools, scalable data generation, software engineering, testing and debugging, scalable data generation myriad parallel data generator toolkit},
location = {Galveston Island, Texas, USA},
series = {ASBD '11}
}

@inproceedings{10.1109/ICSE-SEIP.2019.00024,
author = {Kim, Yunho and Lee, Dongju and Baek, Junki and Kim, Moonzoo},
title = {Concolic Testing for High Test Coverage and Reduced Human Effort in Automotive Industry},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/ICSE-SEIP.2019.00024},
doi = {10.1109/ICSE-SEIP.2019.00024},
abstract = {The importance of automotive software has been rapidly increasing because software now controls many components in motor vehicles such as window controller, smart-key system, and tire pressure monitoring system. Consequently, the automotive industry spends a large amount of human effort testing automotive software and is interested in automated software testing techniques that can ensure high-quality automotive software with reduced human effort.In this paper, we report our industrial experience applying concolic testing to automotive software developed by Hyundai Mobis. We have developed an automated testing framework MAIST that automatically generates the test driver, stubs, and test inputs to a target task by applying concolic testing. As a result, MAIST has achieved 90.5% branch coverage and 77.8% MC/DC coverage on the integrated body unit (IBU) software. Furthermore, it reduced the cost of IBU coverage testing by reducing the manual testing effort for coverage testing by 53.3%.},
booktitle = {Proceedings of the 41st International Conference on Software Engineering: Software Engineering in Practice},
pages = {151–160},
numpages = {10},
keywords = {automated test generation, automotive software, concolic testing, coverage testing},
location = {Montreal, Quebec, Canada},
series = {ICSE-SEIP '19}
}

@article{10.1145/3511887,
author = {Zhang, Huangzhao and Fu, Zhiyi and Li, Ge and Ma, Lei and Zhao, Zhehao and Yang, Hua’an and Sun, Yizhe and Liu, Yang and Jin, Zhi},
title = {Towards Robustness of Deep Program Processing Models—Detection, Estimation, and Enhancement},
year = {2022},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {31},
number = {3},
issn = {1049-331X},
url = {https://doi.org/10.1145/3511887},
doi = {10.1145/3511887},
abstract = {Deep learning (DL) has recently been widely applied to diverse source code processing tasks in the software engineering (SE) community, which achieves competitive performance (e.g., accuracy). However, the robustness, which requires the model to produce consistent decisions given minorly perturbed code inputs, still lacks systematic investigation as an important quality indicator. This article initiates an early step and proposes a framework CARROT for robustness detection, measurement, and enhancement of DL models for source code processing. We first propose an optimization-based attack technique CARROTA to generate valid adversarial source code examples effectively and efficiently. Based on this, we define the robustness metrics and propose robustness measurement toolkit CARROTM, which employs the worst-case performance approximation under the allowable perturbations. We further propose to improve the robustness of the DL models by adversarial training (CARROTT) with our proposed attack techniques. Our in-depth evaluations on three source code processing tasks (i.e., functionality classification, code clone detection, defect prediction) containing more than 3 million lines of code and the classic or SOTA DL models, including GRU, LSTM, ASTNN, LSCNN, TBCNN, CodeBERT, and CDLH, demonstrate the usefulness of our techniques for ❶ effective and efficient adversarial example detection, ❷ tight robustness estimation, and ❸ effective robustness enhancement.},
journal = {ACM Trans. Softw. Eng. Methodol.},
month = {apr},
articleno = {50},
numpages = {40},
keywords = {Source code processing, big code, robustness enhancement, adversarial attack}
}

@inproceedings{10.1145/2025113.2025125,
author = {Jensen, Simon Holm and Madsen, Magnus and M\o{}ller, Anders},
title = {Modeling the HTML DOM and Browser API in Static Analysis of JavaScript Web Applications},
year = {2011},
isbn = {9781450304436},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2025113.2025125},
doi = {10.1145/2025113.2025125},
abstract = {Developers of JavaScript web applications have little tool support for catching errors early in development. In comparison, an abundance of tools exist for statically typed languages, including sophisticated integrated development environments and specialized static analyses. Transferring such technologies to the domain of JavaScript web applications is challenging. In this paper, we discuss the challenges, which include the dynamic aspects of JavaScript and the complex interactions between JavaScript, HTML, and the browser. From this, we present the first static analysis that is capable of reasoning about the flow of control and data in modern JavaScript applications that interact with the HTML DOM and browser API.One application of such a static analysis is to detect type-related and dataflow-related programming errors. We report on experiments with a range of modern web applications, including Chrome Experiments and IE Test Drive applications, to measure the precision and performance of the technique. The experiments indicate that the analysis is able to show absence of errors related to missing object properties and to identify dead and unreachable code. By measuring the precision of the types inferred for object properties, the analysis is precise enough to show that most expressions have unique types. By also producing precise call graphs, the analysis additionally shows that most invocations in the programs are monomorphic. We furthermore study the usefulness of the analysis to detect spelling errors in the code. Despite the encouraging results, not all problems are solved and some of the experiments indicate a potential for improvement, which allows us to identify central remaining challenges and outline directions for future work.},
booktitle = {Proceedings of the 19th ACM SIGSOFT Symposium and the 13th European Conference on Foundations of Software Engineering},
pages = {59–69},
numpages = {11},
keywords = {program analysis, scripting languages},
location = {Szeged, Hungary},
series = {ESEC/FSE '11}
}

@inproceedings{10.1145/1993498.1993551,
author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia},
title = {<i>Kb</i>-Anonymity: A Model for Anonymized Behaviour-Preserving Test and Debugging Data},
year = {2011},
isbn = {9781450306638},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1993498.1993551},
doi = {10.1145/1993498.1993551},
abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.},
booktitle = {Proceedings of the 32nd ACM SIGPLAN Conference on Programming Language Design and Implementation},
pages = {447–457},
numpages = {11},
keywords = {behavior preservation, third-party testing and debugging, k-anonymity, symbolic execution},
location = {San Jose, California, USA},
series = {PLDI '11}
}

@article{10.1145/1993316.1993551,
author = {Budi, Aditya and Lo, David and Jiang, Lingxiao and Lucia},
title = {<i>Kb</i>-Anonymity: A Model for Anonymized Behaviour-Preserving Test and Debugging Data},
year = {2011},
issue_date = {June 2011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {46},
number = {6},
issn = {0362-1340},
url = {https://doi.org/10.1145/1993316.1993551},
doi = {10.1145/1993316.1993551},
abstract = {It is often very expensive and practically infeasible to generate test cases that can exercise all possible program states in a program. This is especially true for a medium or large industrial system. In practice, industrial clients of the system often have a set of input data collected either before the system is built or after the deployment of a previous version of the system. Such data are highly valuable as they represent the operations that matter in a client's daily business and may be used to extensively test the system. However, such data often carries sensitive information and cannot be released to third-party development houses. For example, a healthcare provider may have a set of patient records that are strictly confidential and cannot be used by any third party. Simply masking sensitive values alone may not be sufficient, as the correlation among fields in the data can reveal the masked information. Also, masked data may exhibit different behavior in the system and become less useful than the original data for testing and debugging.For the purpose of releasing private data for testing and debugging, this paper proposes the kb-anonymity model, which combines the k-anonymity model commonly used in the data mining and database areas with the concept of program behavior preservation. Like k-anonymity, kb-anonymity replaces some information in the original data to ensure privacy preservation so that the replaced data can be released to third-party developers. Unlike k-anonymity, kb-anonymity ensures that the replaced data exhibits the same kind of program behavior exhibited by the original data so that the replaced data may still be useful for the purposes of testing and debugging. We also provide a concrete version of the model under three particular configurations and have successfully applied our prototype implementation to three open source programs, demonstrating the utility and scalability of our prototype.},
journal = {SIGPLAN Not.},
month = {jun},
pages = {447–457},
numpages = {11},
keywords = {k-anonymity, behavior preservation, third-party testing and debugging, symbolic execution}
}

@inproceedings{10.1145/2070821.2070823,
author = {Roychowdhury, Shounak and Khurshid, Sarfraz},
title = {Software Fault Localization Using Feature Selection},
year = {2011},
isbn = {9781450310222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2070821.2070823},
doi = {10.1145/2070821.2070823},
abstract = {Manually locating and fixing faults can be tedious and hard. Recent years have seen much progress in automated techniques for fault localization. A particularly promising approach is to analyze passing and failing runs to compute how likely each statement is to be faulty. Techniques based on this approach have so far largely focused on either using statistical analysis or similarity based algorithms, which have a natural application in evaluating such runs. We present a novel approach to fault localization using feature selection techniques from machine learning. Our insight is that each additional failing or passing run can provide significantly diverse amount of information, which can help localize faults in code -- the statements with maximum feature diversity information can point to most suspicious lines of code. Experimental results show that our approach outperforms state-of-the-art approaches for localizing faults in most subject programs of the Siemens suite, which have previously been used to evaluate several fault localization techniques.},
booktitle = {Proceedings of the International Workshop on Machine Learning Technologies in Software Engineering},
pages = {11–18},
numpages = {8},
keywords = {statistical debugging, fault localization, automated debugging, machine learning, RELIEF, feature selection},
location = {Lawrence, Kansas, USA},
series = {MALETS '11}
}

@inproceedings{10.1145/3426182.3426187,
author = {Kloibhofer, Sebastian and Pointhuber, Thomas and Heisinger, Maximilian and M\"{o}ssenb\"{o}ck, Hanspeter and Stadler, Lukas and Leopoldseder, David},
title = {SymJEx: Symbolic Execution on the GraalVM},
year = {2020},
isbn = {9781450388535},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3426182.3426187},
doi = {10.1145/3426182.3426187},
abstract = {Developing software systems is inherently subject to errors that can later cause failures in production. While testing can help to identify critical issues, it is limited to concrete inputs and states. Exhaustive testing is infeasible in practice; hence we can never prove the absence of faults. Symbolic execution, i.e., the process of symbolically reasoning about the program state during execution, can inspect the behavior of a system under all possible concrete inputs at run time. It automatically generates logical constraints that match the program semantics and uses theorem provers to verify the existence of error states within the application. This paper presents a novel symbolic execution engine called SymJEx, implemented on top of the multi-language Java Virtual Machine GraalVM. SymJEx uses the Graal compiler's intermediate representation to derive and evaluate path conditions, allowing GraalVM users to leverage the engine to improve software quality. In this work, we show how SymJEx finds non-trivial faults in existing software systems and compare our approach with established symbolic execution engines.},
booktitle = {Proceedings of the 17th International Conference on Managed Programming Languages and Runtimes},
pages = {63–72},
numpages = {10},
keywords = {Java, GraalVM, Compiler optimizations, Symbolic execution},
location = {Virtual, UK},
series = {MPLR 2020}
}

@inproceedings{10.1145/2642937.2643008,
author = {Harman, Mark and Jia, Yue and Reales Mateo, Pedro and Polo, Macario},
title = {Angels and Monsters: An Empirical Investigation of Potential Test Effectiveness and Efficiency Improvement from Strongly Subsuming Higher Order Mutation},
year = {2014},
isbn = {9781450330138},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2642937.2643008},
doi = {10.1145/2642937.2643008},
abstract = {We study the simultaneous test effectiveness and efficiency improvement achievable by Strongly Subsuming Higher Order Mutants (SSHOMs), constructed from 15,792 first order mutants in four Java programs. Using SSHOMs in place of the first order mutants they subsume yielded a 35%-45% reduction in the number of mutants required, while simultaneously improving test efficiency by 15% and effectiveness by between 5.6% and 12%. Trivial first order faults often combine to form exceptionally non-trivial higher order faults; apparently innocuous angels can combine to breed monsters. Nevertheless, these same monsters can be recruited to improve automated test effectiveness and efficiency.},
booktitle = {Proceedings of the 29th ACM/IEEE International Conference on Automated Software Engineering},
pages = {397–408},
numpages = {12},
keywords = {mutation testing, higher order mutants},
location = {Vasteras, Sweden},
series = {ASE '14}
}

