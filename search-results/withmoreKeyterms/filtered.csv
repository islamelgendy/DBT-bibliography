Document Title,Authors,Publication Year,DOI,PDF Link,Keywords,Abstract,Venue
"""Performance analysis of one-bit transform based motion estimation using sparse search on HEVC""",R. Duvar; A. Küçükmanısa; O. Akbulut; A. T. Çelebı; O. Urhan;,2018,10.1109/SIU.2018.8404776,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8404776,HEVC;Motion Estimation;1BT transform;,"""Video encoders generally employ sparse search-based approaches since the full search-based motion estimation approaches have a significant amount of computational burden. Sparse search-based approaches are often designed by taking software implementations into account performance, but hardware implementations of these methods may suffer important performance losses. In this work, the performance of the Test Zone Search approach which is the fastest motion estimation used in HEVC, is examined when it is integrated with hardware-based 1-bit transform based low bit depth motion estimation approach.""",2018 26th Signal Processing and Communications Applications Conference (SIU)
"""Sparse kernel density construction using orthogonal forward regression with leave-one-out test score and local regularization""",Sheng Chen; Xia Hong; C. J. Harris;,2004,10.1109/TSMCB.2004.828199,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1315754,;,"""This paper presents an efficient construction algorithm for obtaining sparse kernel density estimates based on a regression approach that directly optimizes model generalization capability. Computational efficiency of the density construction is ensured using an orthogonal forward regression, and the algorithm incrementally minimizes the leave-one-out test score. A local regularization method is incorporated naturally into the density construction process to further enforce sparsity. An additional advantage of the proposed algorithm is that it is fully automatic and the user is not required to specify any criterion to terminate the density construction procedure. This is in contrast to an existing state-of-art kernel density estimation method using the support vector machine (SVM), where the user is required to specify some critical algorithm parameter. Several examples are included to demonstrate the ability of the proposed algorithm to effectively construct a very sparse kernel density estimate with comparable accuracy to that of the full sample optimized Parzen window density estimate. Our experimental results also demonstrate that the proposed algorithm compares favorably with the SVM method, in terms of both test accuracy and sparsity, for constructing kernel density estimates.""","IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)"
"""Malware Detection Using Xilinx Software and Adaptive Test Pattern""",S. Meivel; S. K. Nagaharipriya; P. Priyankadevi; S. Sangavi;,2023,10.1109/ICACCS57279.2023.10112911,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10112911,Adaptive Generator;VLSI Circuits;PODEM Algorithm;Unique Sensitization;BIST Architecture;,"""This work introduces novel approaches to accelerate the production of deterministic test patterns for VLSI devices. These methods reduce the number of backtracking while requiring little processing effort, therefore improving the PODEM algorithm. This is accomplished by assigning additional signal lines to the relevant signals, spotting errors earlier, and eliminating extra effort during test generation. These methods have been included into ATOM, a sophisticated ATPG device for combinational circuits. The test generation results for the benchmark circuits' ATOM as well as full scan versions showed how effective these strategies were at improving performance ATOM found all tested problems in a short period of time and proved that all overlapping faults were excessive with minimum backtracking""",2023 9th International Conference on Advanced Computing and Communication Systems (ICACCS)
"""Structural Coverage of LTL Requirements for Learning-based Testing""",H. A. Quddus; M. A. Sindhu;,2022,10.1109/ICIT56493.2022.9989218,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989218,LBT (Learning-based Testing);LTL (Linear Temporal Logic) requirements;RC (Requirement Coverage);AC (Antecedent Coverage);UFCC (Unique First Cause Coverage);,"""Learning-based testing (LBT) is an innovative variant of black-box testing in which test cases are derived by making use of automaton learning and model checking algorithms along with providing Linear Temporal Logic (LTL) requirements of the System Under Test (SUT). There is a scarcity of test coverage metrics for black-box testing in general and LBT in particular. Structural coverage of an LTL requirement is a mechanism that gauges how well a test suite has exercised the structure of the LTL formula. In contrast to the code-driven or model-driven coverage metrics, this coverage provides implementation-independent coverage corresponding to an LTL requirement. This has been defined and used in the literature for black-box testing; however, not for LBT. This paper analyzes and implements the structural coverage criteria for the LTL requirements for evaluating the LBT-generated test suite. We evaluate the structural coverage metrics using the Cruise Control System (CCS) and the ATM systems. The results show that the LBT test suite provides complete structural coverage of the safety LTL requirements in terms of Requirement Coverage (RC), Antecedent Coverage (AC), and Unique First Cause Coverage (UFCC). In the case of liveness LTL requirements, relatively less structural coverage is achieved by the LBT tests, possibly because of the involvement of loops in tests.""",2022 International Conference on IT and Industrial Technologies (ICIT)
"""Reconstructing Surfaces for Sparse Point Clouds with On-Surface Priors""",B. Ma; Y. -S. Liu; Z. Han;,2022,10.1109/CVPR52688.2022.00621,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9879540,3D from multi-view and sensors; Vision + graphics;,"""It is an important task to reconstruct surfaces from 3D point clouds. Current methods are able to reconstruct surfaces by learning Signed Distance Functions (SDFs) from single point clouds without ground truth signed distances or point normals. However, they require the point clouds to be dense, which dramatically limits their performance in real applications. To resolve this issue, we propose to reconstruct highly accurate surfaces from sparse point clouds with an on-surface prior. We train a neural network to learn SDFs via projecting queries onto the surface represented by the sparse point cloud. Our key idea is to infer signed distances by pushing both the query projections to be on the surface and the projection distance to be the minimum. To achieve this, we train a neural network to capture the on-surface prior to determine whether a point is on a sparse point cloud or not, and then leverage it as a differentiable function to learn SDFs from unseen sparse point cloud. Our method can learn SDFs from a single s parse point cloud without ground truth signed distances or point normals. Our numerical evaluation under widely used benchmarks demonstrates that our method achieves state-of-the-art reconstruction accuracy, especially for sparse point clouds. Code and data are available at https://github.com/mabaorui/OnSurfacePrior.""",2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
"""Anomaly Detection Based on Spatio-Temporal and Sparse Features of Network Traffic in VANETs""",L. Nie; H. Wang; S. Gong; Z. Ning; M. S. Obaidat; K. -F. Hsiao;,2019,10.1109/GLOBECOM38437.2019.9013915,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9013915,;,"""Vehicular Ad-Hoc Networks (VANETs) have received a great attention recently due to their potential and various applications. However, the initial phase of the VANET has many research challenges that need to be addressed, such as the issues of security and privacy protection caused by the openness of wireless communication networks among the city-wide applied regions. Specially, anomaly detection for a VANET has become a challenging problem, due to the changes in the scenario of VANETs comparing with traditional wireless networks. Motivated by this issue, we focus on the problem of anomaly detection in VANETs, and propose an effective anomaly detection approach based on the convolutional neural network in this paper. The proposed approach takes into account the spatio-temporal and sparse features of VANET traffic, and it uses a convolutional neural network architecture and a loss function based on Mahalanobis distance for anomaly detection. Furthermore, a comprehensive assessment is provided to validate the proposed approach, which illustrates the effectiveness of this approach.""",2019 IEEE Global Communications Conference (GLOBECOM)
"""On testing physically unclonable functions for uniqueness""",A. Vijayakumar; V. C. Patil; S. Kundu;,2016,10.1109/ISQED.2016.7479229,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7479229,Security;Physically Unclonable Function;Testing;,"""A number of applications from smartcard to ePassport to eID depend on preventing unauthorized access to hardware and software functionality. Physically Unclonable Functions (PUF) rely on manufacturing process variations to create unique identifiers that can be used for various security applications including authentication and secure access. For practical applications, PUFs are required to be unique for each chip. Such property cannot be ensured by design alone. Since PUFs rely on manufacturing process variations, there are no guarantees that two PUFs will never have identical properties. Therefore, testing becomes necessary to screen out PUFs that violate the above property. Despite decades of research on PUFs, there has been scant attention to the problem of testing PUFs for uniqueness. In this paper, we investigate the problem of testing PUFs for uniqueness and we propose techniques for Uniqueness testing. The proposed methods are low-cost and are tailored for testing PUFs using hardware testers.""",2016 17th International Symposium on Quality Electronic Design (ISQED)
"""Locating Software Faults Based on Minimum Debugging Frontier Set""",F. Li; Z. Li; W. Huo; X. Feng;,2017,10.1109/TSE.2016.2632122,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7755837,Fault localization;minimum debugging frontier set;sparse symbolic exploration;dynamic dependence graph;,"""In this article, we propose a novel state-based fault-localization approach. Given an observed failure that is reproducible under the same program input, this new approach uses two main techniques to reduce the state exploration cost. Firstly, the execution trace to be analyzed for the observed failure is successively narrowed by making the set of trace points in each step a cut of the dynamic dependence graph. Such a cut divides the remaining trace into two parts and, based on the sparse symbolic exploration outcome, one part is removed from further exploration. This process continues until reaching where the fault is determined to be. Second, the cut in each step is chosen such that the union of the program states from the members of the cut is of the minimum size among all candidate cuts. The set of statement instances in the chosen cut is called a minimum debugging frontier set (MDFS). To evaluate our approach, we apply it to 16 real bugs from real world programs and compare our fault reports with those generated by state-of-the-art approaches. Results show that the MDFS approach obtains high quality fault reports for these test cases with considerably higher efficiency than previous approaches.""",IEEE Transactions on Software Engineering
"""Ordered Subspace Clustering With Block-Diagonal Priors""",F. Wu; Y. Hu; J. Gao; Y. Sun; B. Yin;,2016,10.1109/TCYB.2015.2500821,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7339439,Low rank representation (LRR);ordered sparse clustering;sparse representation;sparse subspace clustering (SSC);spectral clustering;,"""Many application scenarios involve sequential data, but most existing clustering methods do not well utilize the order information embedded in sequential data. In this paper, we study the subspace clustering problem for sequential data and propose a new clustering method, namely ordered sparse clustering with block-diagonal prior (BD-OSC). Instead of using the sparse normalizer in existing sparse subspace clustering methods, a quadratic normalizer for the data sparse representation is adopted to model the correlation among the data sparse coefficients. Additionally, a block-diagonal prior for the spectral clustering affinity matrix is integrated with the model to improve clustering accuracy. To solve the proposed BD-OSC model, which is a complex optimization problem with quadratic normalizer and block-diagonal prior constraint, an efficient algorithm is proposed. We test the proposed clustering method on several types of databases, such as synthetic subspace data set, human face database, video scene clips, motion tracks, and dynamic 3-D face expression sequences. The experiments show that the proposed method outperforms state-of-the-art subspace clustering methods.""",IEEE Transactions on Cybernetics
"""Effect of Unique Table Implementation in the Performance of BDD Packages""",J. P. Nespolo; R. D. Peralta; P. F. Butzen; A. I. Reis;,2023,10.1109/SBCCI60457.2023.10261960,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10261960,Binary decision diagram;data structures;software performance evaluation;,"""This paper presents an evaluation of the data structures that implement the unique tables used to represent the Binary Decision Diagrams (BDD) nodes in the strong canonical form of BDDs. The unique table is an important element in BDDs, as it avoids re-computations and no duplicated nodes are created. The choice of the data structures used in the unique tables strongly affects the performance of BDD packages. Therefore, is important to identify some aspects that may contribute to the performance. We compare several implementations of unique tables intending to find the more efficient implementations for modern BDD packages using integers (as opposed to pointers) to identify nodes. Among the tested implementations of unique tables, the data structure using a hash table merged with BDD nodes has resulted in reductions of 34.6% in runtime and 43.5% in memory usage compared to the second-best for each criterion.""",2023 36th SBC/SBMicro/IEEE/ACM Symposium on Integrated Circuits and Systems Design (SBCCI)
"""Telugu OCR using Dictionary Learning and Multi-Layer Perceptrons""",G. Madhuri; M. N. L. Kashyap; A. Negi;,2019,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8940599,Dictionary Learning;Sparse Coding;Signal Processing;Telugu Optical Character Recognition (OCR);Deep Learning;,"""Dictionary Learning (DL) methods have been applied successfully in image processing applications like image de-noising, inpainting, mostly using their representation and reconstruction capabilities. From our experimentation and also from literature, it is seen that the performance of DL approaches on classification tasks is not satisfactory. Especially, the performance is seen to degrade with higher dimensionality and increasing number of classes. We propose a hybrid approach to overcome the classification problem encountered by DL approaches. The novel approach uses the strengths of data abstraction and reconstruction from the DL methods while realising a high classification accuracy through a simple Multi-Layer Perceptron (MLP). In the proposed approach, data abstraction is achieved by the DL method and learned sparse codes are used as inputs for training the MLP. The training is relatively fast as the entire dataset need not be trained. The objective is to minimize the computational requirements for classifying complex datasets like Telugu OCR, without compromising the performance. The method has been tested on University of Hyderabad Telugu Printed Connected Components (UHTelPCC) and Modified National Institute of Standards and Technology database (MNIST) datasets with results comparable to the state-of-the-art methods.""","2019 International Conference on Computing, Power and Communication Technologies (GUCON)"
"""SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos""",H. Liu; Y. Teng; T. Lu; H. Wang; L. Wang;,2023,10.1109/ICCV51070.2023.01703,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10377285,;,"""Camera-based 3D object detection in BEV (Bird’s Eye View) space has drawn great attention over the past few years. Dense detectors typically follow a two-stage pipeline by first constructing a dense BEV feature and then performing object detection in BEV space, which suffers from complex view transformations and high computation cost. On the other side, sparse detectors follow a query-based paradigm without explicit dense BEV feature construction, but achieve worse performance than the dense counterparts. In this paper, we find that the key to mitigate this performance gap is the adaptability of the detector in both BEV and image space. To achieve this goal, we propose SparseBEV, a fully sparse 3D object detector that outperforms the dense counterparts. SparseBEV contains three key designs, which are (1) scale-adaptive self attention to aggregate features with adaptive receptive field in BEV space, (2) adaptive spatio-temporal sampling to generate sampling locations under the guidance of queries, and (3) adaptive mixing to decode the sampled features with dynamic weights from the queries. On the test split of nuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. On the val split, SparseBEV achieves 55.8 NDS while maintaining a real-time inference speed of 23.5 FPS. Code is available at https://github.com/MCG-NJU/SparseBEV.""",2023 IEEE/CVF International Conference on Computer Vision (ICCV)
"""Sparse Time–Frequency Analysis of Seismic Data: Sparse Representation to Unrolled Optimization""",N. Liu; Y. Lei; R. Liu; Y. Yang; T. Wei; J. Gao;,2023,10.1109/TGRS.2023.3300578,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10198290,Deep learning (DL);seismic attenuation;sparse time–frequency analysis (STFA);unrolled algorithm;unsupervised learning;,"""Time–frequency analysis (TFA) is widely used to describe local time–frequency (TF) features of seismic data. Among the commonly used TFA tools, sparse TFA (STFA) is an excellent one, which can obtain a TF spectrum with good readability. However, many STFA algorithms suffer from expensive calculation time and unavoidable prior knowledge, such as the iterative shrinkage-thresholding algorithm (ISTA) and the sparse reconstruction by separable approximation (SpaRSA). Inspired by the unrolled algorithm and its successful applications in signal processing, we propose a deep learning (DL)-based ISTA unrolled algorithm, which is named the sparse time–frequency analysis network (STFANet). The STFANet contains two parts, i.e., the sparse TF spectrum generator and the reconstruction module. The former learns how to transform a 1-D seismic signal from a large amount of unlabeled data into a 2-D sparse TF spectrum, which is implemented based on the proposed unrolled iterative dynamic shrinkage-thresholding (UIDST) algorithm. Note that the UIDST algorithm is carried out by using a simplified DL network. The latter serves as a physical constraint of model training to ensure that our generator obtains an accurate TF spectrum, which is actually an inverse TF transform. In this study, the traditional inverse short-time Fourier transform (STFT) is utilized in the reconstruction module. To test the effectiveness of the proposed model, we apply it to 3-D poststack field data. The results show that, compared with the traditional TFA tools, the STFANet can availably compute the TF spectrum with better readability, which benefits seismic attenuation delineation.""",IEEE Transactions on Geoscience and Remote Sensing
"""Architecture for an Ultrasound Advanced Open Platform With an Arbitrary Number of Independent Channels""",D. Mazierli; A. Ramalli; E. Boni; F. Guidi; P. Tortoli;,2021,10.1109/TBCAS.2021.3077664,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9424708,3-D beamforming;high frame rate imaging;matrix array;multi-channel;open scanner;sparse array;synchronization;ultrasound;,"""Ultrasound open platforms are programmable and flexible tools for the development and test of novel methods. In most cases, they embed the electronics for the independent control of (maximum) 256 probe elements. However, a higher number of channels is needed for the control of 2-D array probes. This paper presents a system architecture that, through the hardware and software synchronization of multiple ULA-OP 256 scanners, may implement advanced open platforms with an arbitrary number of channels. The proposed solution needs a single personal computer, maintains real-time features, and preserves portability. A prototype demonstrator, composed of two ULA-OP 256 scanners connected to 512 elements of a matrix array, was implemented and tested according to different channel configurations. Experiments performed under MATLAB control confirmed that by doubling the number of elements (from 256 to 512) the signal-to-noise and contrast ratios improve by 9 dB and 3 dB, respectively. Furthermore, as a full 512-channel scanner, the demonstrator can produce real-time B-mode images at 18 Hz, high enough for probe positioning during acquisitions. Also, the demonstrator permitted the implementation of a new high frame rate, bi-plane, triplex modality. All probe elements are excited to simultaneously produce two planar, perpendicular diverging waves. Each scanner independently processes the echoes received by the 256 connected elements to beamform 1300 frames per second. For each insonified plane, good quality morphological (B-mode), qualitative (color flow-), and quantitative (spectral-) Doppler images are finally shown in real-time by a dedicated interface.""",IEEE Transactions on Biomedical Circuits and Systems
"""Online reconstruction of fast dynamic MR imaging using deep low-rank plus sparse network""",C. Wang; S. Jia; Z. Yan; Y. Zheng; S. Liu; H. Wang; D. Liang; Y. Zhu;,2022,10.1109/CBMS55023.2022.00036,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9867108,L+S-Net;online dynamic MR imaging;Gadgetron;SigPy GPU;,"""In order to test the performance of online reconstruction of deep low-rank pulse sparse network (L+S-Net) for fast dynamic MR imaging. The L+S-Net was implemented on Gadgetron platform for online reconstruction of the scanner. Although L+S-net has a good image reconstruction performance., it takes a long time to estimate the coil sensitivity using ESPIRiT method. In this study, SigPy's signal processing software package was adopted to accelerate the calculation of coil sensitivity to speed up the online reconstruction. The results of experiments showed that compared with the CPU based method., the time of the coil sensitivity estimation could be shortened more than 100 times by using the gridding reconstruction method based on SigPy GPU. The reconstruction performance is stable and can realize online fast dynamic MR imaging reconstruction within 10 seconds.""",2022 IEEE 35th International Symposium on Computer-Based Medical Systems (CBMS)
"""Longitudinal and Multi-modal Data Learning for Parkinson’s Disease Diagnosis via Stacked Sparse Auto-encoder""",S. Li; H. Lei; F. Zhou; J. Gardezi; B. Lei;,2019,10.1109/ISBI.2019.8759385,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8759385,Stacked sparse Auto-encoder;Parkinson’s disease;classification;longitudinal data;,"""Computer aided diagnosis of Parkinson's disease (PD) by using the multi-modality imaging data is a challenging task and a topic of keen interest in the recent years. Feature representation plays a vital role in diagnosis. In previous studies, deep learning (DL) has been a great success in the field of feature representation learning. However, DL methods still need to overcome the issues of overfitting and data insufficiency. In the current study, we propose a stacked sparse auto-encoder (SSAE) based feature encoding algorithm. The proposed SSAE effectively trains on a small-scale data and computes the significant feature representation for PD diagnosis. The feature representation is learned via longitudinal multi-modal neuroimaging data for PD diagnosis. We first train and test the baseline data with the 10-fold cross-validation method, then we exploit the baseline data to train a classification model to complete the classification task in future time points. A number of experiments are carried out to verify the effectiveness of our method on the publicly available Parkinson's Progression Markers Initiative (PPMI) dataset. The experimental results show that the proposed method works efficiently on a small dataset and achieves significant classification results as compared to existing common DL methods.""",2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI 2019)
"""Document Representations to Improve Topic Modelling""",P. V. Poojitha; R. R. K. Menon;,2020,10.1109/ICSSA51305.2020.00022,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9510021,topic modeling;sparse matrix;dictionary representation;Latent Dirichlet Allocation;Orthogonal Matching Pursuit;clustering;TF-IDF;document detection;,"""Each and every day we are collecting lots of information from web applications. So it is difficult to understand or detect what the whole information is all about. To detect, understand and summarise the whole information we need some specific tools and techniques like topic modelling which helps to analyze and identify the crisp of the data. This paper implements the sparsity based document representation to improve Topic Modeling, it organizes the data with meaningful structure by using machine learning algorithms like LDA(Latent Dirichlet Allocation) and OMP(Orthogonal Matching Pursuit) algorithms. It identifies a documents belongs to which topic as well as similarity between documents in an existing dictionary. The OMP(Orthogonal Matching Pursuit) algorithm is the best algorithm for sparse approximation With better accuracy. OMP(Orthogonal Matching Pursuit) algorithm can identify the topics to which the input document[Y] is mostly related to across a large collection of text documents present in a dictionary.""",2020 International Conference on Software Security and Assurance (ICSSA)
"""A new signal analysis method for functional near-infrared spectroscopy""",Z. Zhongpeng; H. Wenxue;,2016,10.1109/ICCSN.2016.7586628,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7586628,functional near-infrared spectroscopy;visualization pattern classification;principle of multivariate graph;partial ordered structure;sparse representation;,"""For research and application of functional near-infrared spectroscopy in neuroscience, appropriate signal analysis method of functional near-infrared spectroscopy is significant. Most of researchers have applied traditional statistical features for feature extraction, and classic machine learning method like support vector machine for further analysis. In this paper, a new feature extraction method based on principles of multivariate graphic representation has been proposed. Then, supervised sparse representation based on partial order structure theory has been suggested for signal pattern classification. Both methods have been tested by signal analysis experiment of functional near-infrared spectroscopy, which is designed to achieve mental workload assessment during n-back work memory experiment. The result indicated, new methods of this work could be applied in functional near-infrared spectroscopy feature extraction and signal analysis.""",2016 8th IEEE International Conference on Communication Software and Networks (ICCSN)
"""Remote Sensing Image Fusion Based on Adaptively Weighted Joint Detail Injection""",Y. Yang; L. Wu; S. Huang; W. Wan; Y. Que;,2018,10.1109/ACCESS.2018.2791574,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8253445,Remote sensing image fusion;detail injection scheme;multiscale decomposition;sparse representation;,"""Remote sensing image fusion based on the detail injection scheme consists of two steps: spatial details extraction and injection. The quality of the extracted spatial details plays an important role in the success of a detail injection scheme. In this paper, a remote sensing image fusion method based on adaptively weighted joint detail injection is presented. In the proposed method, the spatial details are first extracted from the multispectral (MS) and panchromatic (PAN) images through à trous wavelet transform and multiscale guided filter. Different from the traditional detail injection scheme, the extracted details are then sparsely represented to produce the primary joint details by dictionary learning from the subimages themselves. To obtain the refined joint details information, we subsequently design an adaptive weight factor considering the correlation and difference between the previous joint details and PAN image details. Finally, the refined joint details are injected into the MS image using modulation coefficient to achieve the fused image. The proposed method has been tested on QuickBird, IKONOS, and WorldView-2 datasets and compared to several state-of-the-art fusion methods in both subjective and objective evaluations. The experimental results indicate that the proposed method is effective and robust to images from various satellites sensors.""",IEEE Access
"""Instrument Learning and Sparse NMD for Automatic Polyphonic Music Transcription""",A. Rizzi; M. Antonelli; M. Luzi;,2017,10.1109/TMM.2017.2674603,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7862877,Automatic music transcription (AMT);non-negative matrix decomposition (NMD);non-monotone optimization;spectrogram factorization;sparse coding;,"""In this paper, an automatic music transcription (AMT) algorithm based on a supervised non-negative matrix decomposition (NMD) is discussed. In particular, a novel approach for enhancing the sparsity of the solution is proposed. It consists of a two-step processing in which the NMD is solved joining a ℓ2 regularization and a threshold filtering. In the first step, the NMD is performed with the ℓ2 regularization in order to get an overall selection of the notes most likely appearing in the monotimbral musical excerpt. In the second step, a threshold filtering followed by another ℓ2 regularized NMD are repeatedly performed in order to progressively reduce the dictionary matrix and to refine the notes transcription. Furthermore, a user-oriented instrument learning procedure has been conceived and proposed. The proposed AMT system has been tested upon the dataset collected by the LabROSA laboratories considering the transcription of three different pianos. Moreover, it has been validated through a comparison with a regularized NMD and with three open source AMT software. The results prove the effectiveness of the proposed two-step processing in enhancing the sparsity of the solution and in improving the transcription accuracy. Moreover, the proposed system shows promising performance in both multi-F0 and note tracking tasks, obtaining better transcription accuracy than the competing algorithms in most tests.""",IEEE Transactions on Multimedia
"""Generalized Joint Sparse Representation for Multimodal Biometric Fusion of Heterogeneous Features""",R. Primorac; R. Togneri; M. Bennamoun; F. Sohel;,2018,10.1109/WIFS.2018.8630775,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8630775,;,"""In this paper we introduce a novel multimodal biometric recognition system based on generalized sparse representations. In the recently proposed systems, with heterogeneous features (such as audio and video), the joint sparse optimization problem was addressed by bringing the features into the same dynamic range, such as normalizing the features into unitl2 norm. This is however not optimal, and such normalization may decrease the performance of that modality unimodally. We propose to solve the original joint sparse optimization problem by introducing scaling factors for different modalities, such that the modalities interact efficiently at the feature level. The sequence-dependent scaling factors are automatically calculated so that the mismatch between the sparse representations of different modalities is accounted for. In the case of audiovisual recognition system, our experiments on the challenging MOBIO database show that the proposed method outperforms the original joint sparsity-based system (96.8% vs 94.3% recognition rate).""",2018 IEEE International Workshop on Information Forensics and Security (WIFS)
"""Finger Vein Recognition via Sparse Reconstruction Error Constrained Low-Rank Representation""",L. Yang; G. Yang; K. Wang; F. Hao; Y. Yin;,2021,10.1109/TIFS.2021.3118894,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9564038,Finger vein recognition;low rank representation;sparse representation;vein backbone decomposition;reconstruction error;,"""Vein pattern-based methods have powerfully promoted the performance of finger vein recognition. However, it is not easy to precisely extract vein patterns from images, especially from low-quality images, and the non-vein area have been proved to be helpful for recognition. This paper proposes to use low-rank representation to extract as much noiseless discriminative information as possible from finger vein images. However, image deformation and image quality variations weaken the correlation of genuine images, and therefore damage the low-rank linear representation. To further deal with this problem, the class labels of training images and the local geometric structure between testing images and training images, reflected by sparse reconstruction errors of testing images, are used as constraints of low-rank coefficients. In particular, vein backbone decomposition based sparse representation is proposed to fast compute the deformation-robust reconstruction errors of each testing image. The reconstruction error on sub-backbones of one training image are used and modified as the constraint of the low-rank coefficient on this training image. We evaluate the proposed method on three widely used finger vein databases, and experimental results show that the proposed method performs well in finger vein recognition.""",IEEE Transactions on Information Forensics and Security
"""A unique non-intrusive approach to non-ATE Based cul-de-sac SoC debug""",V. Easwaran; V. Bansal; G. Shurtz; R. Gulati; M. Mody; P. Karandikar; P. Shankar;,2014,10.1109/SOCC.2014.6948950,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6948950,Hardware observability;ATE;SoC;IP;non-intrusive debug;dead silicon;MUX;bootstrap;,"""Rapid increasing complexity of chips due to integration of multiple processing elements along with a rich set of peripherals for connectivity needs, poses new set of debug challenges on silicon. The spectrum of issues ranges from minor functional bugs, random timing related bugs to dead on-arrival (DoA) chip. The prior methods of doing silicon test were using ATE (Automatic Test Equipment's) and using trace data under software control, bringing its own set of limitations. This paper proposes a new method to overcome the above limitations by introducing internal state observations at the boundary of chip that can be viewed by probing these pins. The proposed method consists of routing hardware observability signal from every individual IP to pins, which are controlled by means of boot-pins on the chip. The solution accomplishes the same with no additional overhead in terms of pin count and helps to debug DoA silicon. This method is implemented in a multicore SoC (System-on-chip) catering to the automotive market. The actual result from SoC usage has shown to save up to 83% effort in debugging actual issues on silicon by adding about 0.2% more cost in terms of silicon area.""",2014 27th IEEE International System-on-Chip Conference (SOCC)
"""A Comparison Between Three Sparse Unmixing Algorithms Using a Large Library of Shortwave Infrared Mineral Spectra""",M. Berman; L. Bischof; R. Lagerstrom; Y. Guo; J. Huntington; P. Mason; A. A. Green;,2017,10.1109/TGRS.2017.2676816,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7886343,Canonical variates (CVs);cubic spline;linear unmixing;shortwave infrared (SWIR) spectra;sparse unmixing;spectral library;,"""The comparison described in this paper has been motivated by two things: 1) a “spectral library” of shortwave infrared reflectance spectra that we have built, consisting of the spectra of 60 nominally pure materials (mostly minerals, but also water, dry vegetation, and several man-made materials) and 2) the needs of users in the mining industry for the use of fast and accurate unmixing software to analyze tens to hundreds of thousands of spectra measured from drill core or chips using HyLogging instruments, and other commercial reflectance spectrometers. Individual samples are typically a mixture of only one, two, three, or occasionally four minerals. Therefore, in order to avoid overfitting, a sparse unmixing algorithm is required. We compare three such algorithms using some real world test data sets: full subset selection (FSS), sparse demixing (SD), and L1 regularization. To aid the comparison, we introduce two novel aspects: 1) the simultaneous fitting of the low frequency background with mineral identification (which provides greater model flexibility) and 2) the combined fitting being carried out using a suitably defined Mahalanobis distance; this has certain optimality properties under an idealized model. Together, these two innovations significantly improve the accuracy of the results. FSS and L1 regularization (suitably optimized) produce similar levels of accuracy, and are superior to SD. Discussion includes possible improvements to the algorithms, and their possible use in other domains.""",IEEE Transactions on Geoscience and Remote Sensing
"""Sparse Support Vector Machine for pattern recognition""",G. Chen; T. D. Bui; A. Krzyżak;,2013,10.1109/HPCSim.2013.6641476,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6641476,Support vector machines (SVM);pattern recognition;image processing;machine learning;,"""Support Vector Machine (SVM) is one of the most famous classification techniques in the pattern recognition community. However, due to outliers in the training samples, the SVM tend to be over-trained. This means that the generalization ability of the SVM will decrease for further training. In this paper, we borrow the idea of compressive sensing/sparse representation and apply it to the SVM. Our method can achieve higher classification rates than the standard SVM due to the sparser support vectors. Experimental results conducted in this paper show that our proposed technique is feasible in practical pattern recognition applications.""",2013 International Conference on High Performance Computing & Simulation (HPCS)
"""Beyond the Sparsity-Based Target Detector: A Hybrid Sparsity and Statistics-Based Detector for Hyperspectral Images""",B. Du; Y. Zhang; L. Zhang; D. Tao;,2016,10.1109/TIP.2016.2601268,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7547286,Hyperspectral imagery;sparse representation;statistical characteristic;target detection;,"""Hyperspectral images provide great potential for target detection, however, new challenges are also introduced for hyperspectral target detection, resulting that hyperspectral target detection should be treated as a new problem and modeled differently. Many classical detectors are proposed based on the linear mixing model and the sparsity model. However, the former type of model cannot deal well with spectral variability in limited endmembers, and the latter type of model usually treats the target detection as a simple classification problem and pays less attention to the low target probability. In this case, can we find an efficient way to utilize both the high-dimension features behind hyperspectral images and the limited target information to extract small targets? This paper proposes a novel sparsity-based detector named the hybrid sparsity and statistics detector (HSSD) for target detection in hyperspectral imagery, which can effectively deal with the above two problems. The proposed algorithm designs a hypothesis-specific dictionary based on the prior hypotheses for the test pixel, which can avoid the imbalanced number of training samples for a class-specific dictionary. Then, a purification process is employed for the background training samples in order to construct an effective competition between the two hypotheses. Next, a sparse representation-based binary hypothesis model merged with additive Gaussian noise is proposed to represent the image. Finally, a generalized likelihood ratio test is performed to obtain a more robust detection decision than the reconstruction residual-based detection methods. Extensive experimental results with three hyperspectral data sets confirm that the proposed HSSD algorithm clearly outperforms the state-of-the-art target detectors.""",IEEE Transactions on Image Processing
"""Remote Sensing Images Super-resolution Based on Sparse Dictionaries and Residual Dictionaries""",Y. Zhang; W. Wu; Y. Dai; X. Yang; B. Yan; W. Lu;,2013,10.1109/DASC.2013.82,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6844382,Remote Sensing Images;Super-Resolution;Dictionary Learning;Sparse representation;,"""In this paper, a sensing image super-resolution (SR) reconstruction method is proposed. Sparse dictionary dealing with remote sensing image SR problem is introduced in this work. The sparse dictionary is based on a sparsity model where the dictionary atoms have sparse representation over a basic dictionary. The sparse dictionary consists of two parts: basic dictionary and atom representation matrix. The sparse dictionary leads to compact representation and it is both adaptive and efficient. Furthermore, compared with conventional SR methods, two dictionary pairs, i.e. primitive sparse dictionary pair and residual sparse dictionary pair, are proposed. The primitive sparse dictionary pair is learned to reconstruct initial high-resolution (HR) remote sensing image from a single low-resolution (LR) input. However, the initial HR remote sensing image loses some details compare with the corresponding original HR image completely. Therefore, residual sparse dictionary pair is learned to reconstruct residual information. The proposed method is tested on remote sensing images, and the experimental results indicate that the proposed algorithm can provide substantial improvement in resolution of remote sensing images, and the results are superior in quality to the results produced by other methods.""","2013 IEEE 11th International Conference on Dependable, Autonomic and Secure Computing"
"""Adaptive Sparse Self-attention for Object Detection""",M. Xu; Y. Song; K. Xie; P. Guo; J. Mu; W. Liu; Z. Wang;,2022,10.1109/IJCNN55064.2022.9892410,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9892410,;,"""Object detection is a fundamental task for computer vision. The majority of prior methods employ global contextual information to enhance features representation. However, we argue that prior methods suffer from the superfluous features. To address the above mentioned problems, we explore the sparsity on object detection tasks in two dimensions. Specifically, a sparse spatial attention is proposed to capture global sparse long-range relationship into local features adaptively, where a learnable channel-wise mask is obtained to reduce the superfluous channels. Meanwhile, a sparse channel attention module is used to enhance the representation of each channel by introducing sparse global semantics. Experiments demonstrate that our proposed method outperforms comparative methods on the commonly used benchmark dataset, i.e., MS-COCO. The ablative experiments show that the sparsity the effectiveness in feature extraction and bounding boxes selection.""",2022 International Joint Conference on Neural Networks (IJCNN)
"""Multimodal Biometric Verification Using Sparse Representation Based Classification""",K. Meng; Z. Huang; X. Wang; K. Wang;,2018,10.1109/ICIVC.2018.8492886,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8492886,multimodal verification;face and ear;sparse representation;sparsity-based metric;,"""In this paper, we propose a multimodal verification system integrating face and ear based on sparse representation-based classification (SRC). The face and ear query samples are first encoded respectively to derive sparsity-based match scores, and which are then combined with the Sum-rule for verification. Our contribution is two-fold: (1) use two sparsity-based metrics and two biometric traits to demonstrate the superior performance of SRC-based verification to the well-known multimodal fusion methods using cosine similarity like likelihood ratio (LLR), support vector machine (SVM), and the Sum-rule; (2) empirically analyze the weakness and feasibility of SRC-based verification and validate the correlation between its performance and the inter-class separability of samples in dictionary. The SRC-based one-to-many comparison is not preferable to the biometrics or scenarios where inferior inter-class separability is available. This could be a selection guideline of SRC for verification applications.""","2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)"
"""Skin beautification detection using sparse coding""",T. Sun; X. Hui; Z. Wang; S. Zhang;,2017,10.23919/MVA.2017.7986916,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7986916,;,"""In the past years, skin beautifying softwares have been widely used in portable devices for social activities, which have the functionalities of turning one's skin into flawless complexion. With a huge number of photos uploaded to social media, it is useful for users to distinguish whether a photo is beautified or not. To address this problem, in this paper, we propose a skin beautification detection method by mining and distinguishing the intrinsic features of original photos and the corresponding beautified photos. To this aim, we propose to use sparse coding to learn two sets of basis functions using densely sampled patches from the original photos and the beautified photos, respectively. To detect whether a test photo is beautified, we represent the sampled patches from the photo using the learned basis functions and then see which set of basis functions produces more sparse coefficients. To our knowledge, our effort is the first one to detect skin beautification. To validate the effectiveness of the proposed method, we collected about 1000 photos including both the original photos and the photos beautified by a software. Our experimental results indicate the proposed method achieved a desired detection accuracy of over 80%.""",2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)
"""A software tool for compressive sensing based time-frequency analysis""",A. Draganie; M. Brajović; I. Orović; S. Stanković;,2015,10.1109/ELMAR.2015.7334492,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7334492,Compressive Sensing;Complex-time Distribution;Ambiguity Domain;Instantaneous Frequency estimation;Sparse Representation;,"""A software tool that implements Compressive sensing based time-frequency analysis and performs instantaneous frequency estimation, is proposed and described in the paper. A focus is made on the signals with fast varying instantaneous frequency (IF), which can be accurately estimated using complex-time distribution. Therefore, the proposed tool offers different possibilities to adjust parameters of complex-lag distributions in order to comply with fast-varying IF laws. Moreover, beside the standard implementation based on the full set of samples, a compressive sensing based time-frequency approach is included in order to obtain sparse time-frequency representation. Sparse time-frequency representation is reconstructed from very few ambiguity domain observations. The tool performance is tested on real and synthetic signals.""",2015 57th International Symposium ELMAR (ELMAR)
"""On methods for ordering sparse matrices in circuit simulation""",G. Reissig;,2001,10.1109/ISCAS.2001.922048,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=922048,;,"""Recently proposed methods for ordering sparse symmetric matrices are discussed and their performance is compared with that of the Minimum Degree and the Minimum Local Fill algorithms. It is shown that these methods applied to symmetrized modified nodal analysis matrices yield orderings significantly better than those obtained from the Minimum Degree and Minimum Local Fill algorithms, in some cases at virtually no extra computational cost.""",ISCAS 2001. The 2001 IEEE International Symposium on Circuits and Systems (Cat. No.01CH37196)
"""Sparse tableau relaxation for the optimal power flow problem""",B. Park; C. L. DeMarco;,2017,10.1109/ALLERTON.2017.8262754,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8262754,;,"""Optimal power flow (OPF) approaches employing such methods as semi-definite programming have garnered considerable interest in the literature of the last decade. The OPF formulations for these approaches have almost universally relied on Ybus admittance matrix representations, which derive from nodal analysis, and restrict allowable network elements to be voltage-controlled only. Limitations of nodal analysis long been recognized, and to overcome these, commercial power system software often employs modified nodal analysis (MNA). However, with OPF requiring many monitored links with constrained flows, the work here argues for a formulation even more versatile than MNA: Sparse Tableau Formulation (STF) of network constraints, with multi-port representation of individual components. The vast majority of transmission network components (e.g., transmission lines, transformers) have multi-port voltage-current behavior that is well-modelled as linear. Therefore, nonlinearities appear only in equations associated with generation and load at each bus. Utilizing STF, these sources of nonconvexity are confined to constraints local to each bus. This opens the door to simple, engineering-based convex relaxations. We introduce a constant current source at each bus, with these sources' values bounded in a manner that contains any feasible current produced by the actual generator or load model. The tightness of the relaxation is shown to improve when angle constraints are provided, and a DCOPF-based heuristic method is proposed to obtain such angle bounds.""","2017 55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)"
"""Consistent iterative hard thresholding for signal declipping""",S. Kitic; L. Jacques; N. Madhu; M. P. Hopwood; A. Spriet; C. De Vleeschouwer;,2013,10.1109/ICASSP.2013.6638804,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6638804,Signal Clipping;Sparse Recovery;Inverse Problems;Greedy Methods;Audio Processing;,"""Clipping or saturation in audio signals is a very common problem in signal processing, for which, in the severe case, there is still no satisfactory solution. In such case, there is a tremendous loss of information, and traditional methods fail to appropriately recover the signal. We propose a novel approach for this signal restoration problem based on the framework of Iterative Hard Thresholding. This approach, which enforces the consistency of the reconstructed signal with the clipped observations, shows superior performance in comparison to the state-of-the-art declipping algorithms. This is confirmed on synthetic and on actual high-dimensional audio data processing, both on SNR and on subjective user listening evaluations.""","2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
"""Scaling Software Security Analysis to Satellites: Automated Fuzz Testing and Its Unique Challenges""",J. Willbold; M. Schloegel; F. Göhler; T. Scharnowski; N. Bars; S. Wörner; N. Schiller; T. Holz;,2024,10.1109/AERO58975.2024.10521316,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10521316,;,"""The security of space assets is becoming an increasingly important concern, as the number of satellite services offered from space grows at an accelerating rate. In recent years, the functionalities of satellites have become increasingly sophisticated, allowing them to seamlessly provide complex services such as space-based Internet and high-resolution Earth observation. A significant contribution to these advancements was made by the software systems that control spacecraft in the harsh space environment. However, the development of satellite software poses a significant challenge due to the absence of physical access to the spacecraft during its mission. Recent research conducted by Willbold et al. has highlighted software security concerns, revealing an alarming absence of modern security measures among many satellites. Their analysis uncovered various security vulnerabilities in satellite software that could potentially allow attackers to gain full control over the spacecraft. Despite these results, their analysis is limited by the fact that software is analyzed manually, making the approach hard to scale.In this paper, we propose to use an automated vulnerability analysis technique, fuzz testing (fuzzing for short), to scale the analysis without the need of a human expert. Fuzzing is a dynamic program analysis technique that has proven highly successful at locating bugs in application software, such as browsers, or the Linux kernel. Its effectiveness has seen widespread adoption among the industry, such as Google or Meta, and launched multiple research efforts to make it even more effective. In essence, fuzzing creates a large number of inputs for the system under test and executes them while monitoring the system behavior, i.e., execution paths and crashes. Advanced approaches use lightweight instrumentation to gain introspection capabilities, allowing them to track the program path executed by a specific input and thus to guide the exploration to unseen program behavior. Despite its success, applying fuzzing to spacecraft presents unique challenges that we introduce and thoroughly discuss in this paper. First, obtaining feedback from the target program proves challenging, necessitating the exploration of firmware rehosting techniques where the target firmware is executed in an emulated environment without a precise representation of all peripherals. Second, satellites often employ complex boot processes that ensure memory integrity, perform device checks and configurations, and execute various time-intensive tasks, thereby posing challenges to approaches like fuzzing that aim to execute a program as frequently as possible, i.e., thousands of times per second. Finally, fuzzers rely primarily on crashes to identify bugs in the software under test, which fails to account for unrecoverable configuration issues. Beyond discussing these issues, we analyze their practical impact on the software of three satellites, ESTCube-1, OPS-Sat, and Flying Laptop. By discussing the challenges associated with applying fuzzing to spacecrafts and exploring potential solutions, we aim to contribute to the advancement of security practices in the aerospace industry.""",2024 IEEE Aerospace Conference
"""Feature Selection with Structural Sparse Mode for Text Categorization""",W. Zheng; D. Tang; H. Zhang; H. Tang;,2017,10.1109/IHMSC.2017.88,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8047647,Text Categorization;Feature Selection;Structure;Group;Sparse;,"""The grouped structure has successfully been embedded in sparse models for feature selection; however, some groups generated by clustering method might be difficult to interpret their semantic information if the number of words in the group is very large. This paper proposes a novel approach in which a group structure is constructed and its corresponding sparse model is used to select features for text categorization. After variable preselection, an algorithm is developed to generate groups, such that each group only contains two or three closely related words, which can reflect more essential semantic meaningful. Finally, structural sparse mode is used to select feature in wrapper way. The experimental results demonstrate that the proposed method achieves comparable precision and improves the sparsity considerably, which means that the model has better interpretability.""",2017 9th International Conference on Intelligent Human-Machine Systems and Cybernetics (IHMSC)
"""Verification and validation of F-15 and S/MTD unique software""",F. L. Tuttle; R. L. Kisslinger;,1991,10.1109/NAECON.1991.165827,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=165827,;,"""Four separate operational flight programs (OFPs) were developed by five vendors, using seven computer languages, and were integrated for use in the STOL (short takeoff and landing)/Maneuver Technology Demonstrator (S/MTD). The verification and validation (V&V) process used was very successful, as evidenced by the fact that very few problems have been encountered during the test program, and no flights were delayed as a result of software anomalies. The authors provide an overview of the V&V approach used by MCAIR and its suppliers. The approach is not a management breakthrough; rather, it is the application of well-known, but often neglected, principles of engineering management, particularly attention to detail. Emphasis is placed on the V&V of the flight critical software for the flight controllers.<>""",Proceedings of the IEEE 1991 National Aerospace and Electronics Conference NAECON 1991
"""Asynchronous Hybrid Parallel SpMV in an Industrial Application""",C. Thiele; M. Araya-Polo; D. Stoyanov; F. Frank; F. O. Alpak;,2016,10.1109/CSCI.2016.0226,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7881519,Linear systems;Sparse matrices;Fluid dynamics;Numerical simulation;Hydrocarbons;,"""Pore-scale simulation is both computationally challenging and a key element of hydrocarbon exploration and production ([1], [2], [3]). The subject of this study is the efficient solution of the linear systems arising from the discretization of the Cahn-Hilliard equation ([4], [5]), which governs the separation of a two-component fluid mixture. We investigate synchronous and asynchronous parallel iterative solvers for our simulation. We focus our efforts on two main aspects: first, software infrastructure that supports this kind of computations, mainly Trilinos and GaspiLS. Second, scalability and performance analysis. The asynchronous solver performs well and scales better than the synchronous ones for most test problems, especially for large systems distributed over many computational nodes (up to 23% faster and 10% higher parallel efficiency in these cases).""",2016 International Conference on Computational Science and Computational Intelligence (CSCI)
"""Super-Resolution of Face Images Using Weighted Elastic Net Constrained Sparse Representation""",X. Pei; T. Dong; Y. Guan;,2019,10.1109/ACCESS.2019.2913008,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8698238,Sparse representation;super-resolution;weighted elastic net;,"""High-resolution (HR) face images are usually preferred in many computer vision tasks. However, low-resolution (LR) face images, which are often obtained in real scenarios, can be converted to a high resolution with the super-resolution techniques. In this paper, we propose the weighted elastic net constrained sparse representation (WENSR) super resolution method for face images. The method considers image gradient and weighted elastic net penalties. Due to the high similarity between human faces, it is not very suitable to only use ℓ1-norm in the sparse representation model. The elastic net has a grouping effect and is more suitable for real-world data. The gradient is very important information in the image, we also use image gradient to enhance the final output. The tests of our method on both synthetic data and real-world data, such as FEI, CAS-PEAL-R1, and CMU+MIT face image dataset suggest a competitive performance gain in terms of peak signal to noise ratio (PSNR) and structural similarity (SSIM).""",IEEE Access
"""Sparse matrix methods for chemical process separation calculations on supercomputers""",S. E. Zitney;,1992,10.1109/SUPERC.1992.236662,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=236662,;,"""The author considers using the frontal method on supercomputers to solve the large, sparse linear equation systems arising in process separation calculations. The motivation is that the frontal method takes advantage of vector computers by treating parts of the sparse matrix as full submatrices. This allows arithmetic operations to be performed with full-matrix code and circumvents the difficulties inherent in indirect addressing on vector processors. Separation problems from the commercial simulators ASPEN PLUS and SPEEDUP are used as test cases. Results on a CRAY Y-MP supercomputer show that the frontal method significantly reduces simulation time, by more than an order of magnitude in many cases, compared to traditional sparse matrix methods.<>""",Supercomputing '92:Proceedings of the 1992 ACM/IEEE Conference on Supercomputing
"""Time- and Space-Efficient Evaluation of Sparse Boolean Functions in Embedded Software""",V. Dvorak;,2007,10.1109/ECBS.2007.72,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4148932,;,"""The paper addresses software implementation of large sparse systems of Boolean functions. Fast evaluation of such functions with the smallest memory consumption is often required in embedded systems. A new heuristic method of obtaining compact representation of sparse Boolean functions in a form of linked tables is described that can be used for BDD minimization as well. Evaluation of Boolean functions reduces to multiple indirect memory accesses. The method is compared to other techniques like a walk through a BDD or a list search and is illustrated on examples. The presented method is flexible in making trade-offs between performance and memory consumption and may be thus useful for embedded microprocessor or microcontroller software.""",14th Annual IEEE International Conference and Workshops on the Engineering of Computer-Based Systems (ECBS'07)
"""Situational control algorithm for ensuring parameter stability in unique software and hardware""",E. V. Yurkevich; L. N. Kryukova; V. D. Babishin;,2017,10.1109/MLSD.2017.8109710,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8109710,spacecraft;unique onboard system device;situational control;operational control;external influences;ground test number;parameter stability control;,"""In order to ensure the stability of unique software and hardware parameters in a spacecraft's onboard system, a situational control algorithm is being offered. The proposed approach allows to minimize errors when processing external influence test results. A mechanism of minimization of ground test numbers for a single copy device is formed.""",2017 Tenth International Conference Management of Large-Scale System Development (MLSD)
"""Unique Raspberry Pi-based Honeypot""",A. Bhardwaj; L. Sapra; V. Sapra;,2023,10.1109/ICRASET59632.2023.10420242,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10420242,Raspberry Pi;Honeypot;Network;Alerts;Port scan;,"""Honeypots run services to lure attackers and prospective scans on Telnet, SSH, and HTTP. The system detects and sends email alerts for any unauthorized access or potential denial of service (DoS) attacks. This is normally performed by the Honeypot application service. The authors propose converting the Honeypot app to as daemon service or corn, which checks status & restarts if required. The authors designed and developed a unique Honeypot framework, named PIPOT using a Python socket program with low-cost Raspberry Pi for monitoring and alerting, instead of using expensive server systems in small organizations running few services with little or no security alerting systems.""",2023 International Conference on Recent Advances in Science and Engineering Technology (ICRASET)
"""Detecting and Mapping Snow Induced Damage to Commercial Forest Plantations Using Sparse Partial Least Squares – Discriminant Analysis""",A. Sharkey; R. Lottering; K. Peerbhay;,2022,10.1109/ICECET55527.2022.9873015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9873015,Snow induced damage;commercial forestry;SPLS-DA;,"""Snow can bring about devastating consequences to commercial forestry in the KwaZulu-Natal midlands. These consequences may result in serious ecological and economic risk for commercial forestry in the region. With the use of remote sensing methods, the detection and mapping of snow induced damage to vegetation used for commercial forestry in the region can potentially provide useful information on both the spatial extent and severity of damage, which can feed into better informed and sustainable forest management. To our knowledge, no study has explored the ability of remote sensing in detecting and mapping snow damaged vegetation in South Africa. Therefore, this study integrates vegetation indices derived from multi-temporal Landsat 8 imagery with Sparse Partial Least Squares - Discriminant Analysis (SPLS-DA) to detect and map snow induced damage to vegetation used for commercial forestry. The results indicated the ability of SPLS-DA to detect and map snow damage with overall accuracies of 55%, 78%, and 77% achieved for the 01/09/2018, 10/09/2018 and 17/09/2018 dated images, respectively. The results of this study validate that the SPLS-DA model can be successfully used to detect and map snow induced damage. In addition, the use of multi-temporal imagery allowed for the successful validation of snow damage and illustrated the progression of damage overtime.""","2022 International Conference on Electrical, Computer and Energy Technologies (ICECET)"
"""Adaptive Multitrace Seismic Deconvolution via Structural L1-2 Minimization""",H. Chen; L. Wang; H. He; H. Zhou;,2024,10.1109/LGRS.2024.3413686,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10556726,Seismic data;seismic deconvolution;sparse reconstruction;spatial continuity;,"""Deconvolution technology, as an effective means to enhance the resolution of seismic data, has emerged as a prominent research area in the field of seismic exploration. However, due to its inherent ill-posed nature, seismic deconvolution poses significant challenges. The conventional approach for deconvolution employs a sparse inversion strategy to reconstruct underground reflection coefficients but overlooks the spatial relationship between adjacent seismic traces, resulting in inadequate spatial continuity of the deconvolved results. In this letter, we propose an adaptive seismic deconvolution strategy that incorporates spatial continuity regularization based on local seismic similarity. The adaptive multitrace deconvolution process consists of three parts: first, we consider spatial continuity by introducing a spatial regularization term derived from local seismic similarity; second, we formulate an objective function by combining  $L1$ -2 norm for sparse regularization with terms accounting for misfit and spatial regularization to reconstruct reflectivity; finally, we solve the objective function using alternative direction method of multipliers (ADMMs) and L-BFGS algorithms. Our proposed method effectively preserves weak effective signals while providing a clearer depiction of geological body distribution and ensuring superior spatial continuity in complex geological structures. Synthetic and field data tests demonstrate that our proposed method yields high-resolution deconvolved results with strong spatial continuity.""",IEEE Geoscience and Remote Sensing Letters
"""Transfer Sparse Coding for Robust Image Representation""",M. Long; G. Ding; J. Wang; J. Sun; Y. Guo; P. S. Yu;,2013,10.1109/CVPR.2013.59,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6618903,sparse coding;transfer learning;image representation;,"""Sparse coding learns a set of basis functions such that each input signal can be well approximated by a linear combination of just a few of the bases. It has attracted increasing interest due to its state-of-the-art performance in BoW based image representation. However, when labeled and unlabeled images are sampled from different distributions, they may be quantized into different visual words of the codebook and encoded with different representations, which may severely degrade classification performance. In this paper, we propose a Transfer Sparse Coding (TSC) approach to construct robust sparse representations for classifying cross-distribution images accurately. Specifically, we aim to minimize the distribution divergence between the labeled and unlabeled images, and incorporate this criterion into the objective function of sparse coding to make the new representations robust to the distribution difference. Experiments show that TSC can significantly outperform state-of-the-art methods on three types of computer vision datasets.""",2013 IEEE Conference on Computer Vision and Pattern Recognition
"""Automated Generation of Robot Trajectories for Assembly Processes Requiring Only Sparse Manual Input""",S. Madsen; M. Jami; H. G. Petersen;,2021,10.1109/ICRA48506.2021.9561465,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561465,;,"""In this paper, a new method for offline programming part assembly operations with tight fittings is presented. More specifically, an assembly process trajectory generator with self programming capabilities is developed where the user needs to provide only very sparse and intuitive input. The presented system is added to the existing skill based robot software package VEROSIM. In VEROSIM, the trajectory generator is applied to an industrial test platform for assembling insulin injection devices at the Danish pharmaceutical company Novo Nordisk, where it is shown that the trajectories as expected are executable. Hence, the method is a strong alternative to online approaches such as programming by demonstration.""",2021 IEEE International Conference on Robotics and Automation (ICRA)
"""Credit Risk Analysis Using Sparse Non-negative Matrix Factorizations""",H. Sun; Z. Chen; J. Chen;,2015,10.1109/ICISCE.2015.47,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7120587,feature extraction;machine learning;non-negative matrix factorization;sparsity;credit risk analysis;SVM;,"""Credit risk analysis is to determine if a customer is likely to default on the financial obligation. In this paper, we will introduce sparse non-negative matrix factorization method to discovery the lower dimensional space for reducing the data dimensionality, which will contribute to good performance and fast computation in the credit risk classification performed by support vector machine. We test the sparse NMF in a real-world credit risk prediction task, and the empirical results demonstrate the advantage of sparse NMF by comparing with other state of art methods.""",2015 2nd International Conference on Information Science and Control Engineering
"""Sparse cost-sensitive classifier with application to face recognition""",J. Man; X. Jing; D. Zhang; C. Lan;,2011,10.1109/ICIP.2011.6115804,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6115804,Cost-sensitive learning;sparse representation;sparse cost-sensitive classifier;face recognition;,"""Sparse representation technique has been successfully employed to solve face recognition task. Though current sparse representation based classifier proves to achieve high classification accuracy, it implicitly assumes that the losses of all misclassifications are the same. However, in many real-world applications, different misclassifications could lead to different losses. Driven by this concern, we propose in this paper a sparse cost-sensitive classifier for face recognition. Our approach uses probabilistic model of sparse representation to estimate the posterior probabilities of a testing sample, calculates all the misclassification losses via the posterior probabilities and then predicts the class label by minimizing the losses. Experimental results on the public AR and FRGC face databases validate the efficacy of the proposed approach.""",2011 18th IEEE International Conference on Image Processing
"""Sparse Representation and Low-Rank Approximation for Robust Face Recognition""",K. G. Quach; C. N. Duong; T. D. Bui;,2014,10.1109/ICPR.2014.238,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6976948,low-rank approximation;occlusion dictionary;sparse representation;,"""Face recognition under various conditions such as illumination, poses, expression, and occlusion has been one of the most challenging problems in computer vision. Over the last few years there has been significant attention paid to the low-rank approximation (LRA) and sparse representation (SR) techniques. The applications of these techniques have appeared in many different areas ranging from handwritten character recognition to multi-factor face recognition. In this paper, we will review some of the most recent works using LRA and SR in the multi-factor face recognition problem, and present a novel framework to improve their performance in the recognition of faces under various affecting conditions. Our results are comparable to or better than the state-of-the-art in this area.""",2014 22nd International Conference on Pattern Recognition
"""Cognitive Fusion of Graph Neural Network and Convolutional Neural Network for Enhanced Hyperspectral Target Detection""",S. Xu; S. Geng; P. Xu; Z. Chen; H. Gao;,2024,10.1109/TGRS.2024.3392188,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10506566,Attention mechanism;deep learning (DL);graph neural network (GNN);hyperspectral target detection (HTD);sparse subspace clustering (SSC);,"""In recent years, deep learning has emerged as a prominent technique in hyperspectral target detection (HTD). Extensive research has highlighted the potential of graph neural network (GNN) as a promising framework for exploring non-Euclidean dependencies within hyperspectral imagery (HSI). However, GNN has not been introduced to HTD. Additionally, achieving a balanced training set while effectively suppressing background remains a challenge. Therefore, we propose the cognitive fusion of GNN and convolutional neural network (CNN) for enhanced HTD (named as CFGC), which marks the first integration of GNN and CNN in HTD. Initially, using sparse subspace clustering (SSC) and a similarity measurement strategy, we select the most representative background samples for HTD. Subsequently, linear interpolation combines the prior target with the Laplacian-weighted prior target, yielding abundant targets with meaningful transformations. Finally, a fused network of CNN and GNN is utilized for training both the prior target and the constructed training set. Significantly, the incorporation of attention mechanism in both the CNN and GNN branches stands out as a noteworthy advantage, augmenting the models’ ability to selectively prioritize crucial information. Four benchmark hyperspectral images have been used in extensive experiments, and the results demonstrate that CFGC exhibits superior performance in HTD.""",IEEE Transactions on Geoscience and Remote Sensing
"""Sparse Matrix Factorization Using Diagonal Pivoting for Power Distribution Network Applications""",A. Husagic-Selman;,2017,10.1109/IEEEGCC.2017.8448230,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8448230,Sparse Systems;Pivoting;Power Distribution Network Applications;Minimum Degree Ordering;Matrix Classification;Definite Matrices;indefinite Matrices;,"""Power distribution network (PDN) applications are real-time applications with complex data represented by sparse matrices that, depending on the application, may be positive definite or indefinite and irregular. Modifying existing algorithms to solve all types of PDN matrices would ease PDN solver design and reduce code maintenance cost, but that is very challenging task. Algorithms that deal with PDN matrices should provide stability to the system, yet process data fast and efficiently, utilizing the existing hardware architecture to the maximum. The algorithms that deal with PDN data are based on factorization of sparse matrices, where pivoting strategy may destabilize the system. The algorithms used in commercial software ensure the system stability, but are complex and hard to implement. In this paper, we propose the usage of partial diagonal pivoting, with minimum degree ordering for matrix factorization in the domain of PDN applications. Results show that this pivoting technique ensures the stability over the tested set of matrices, and outperforms the commercial software in terms of time efficiency.""",2017 9th IEEE-GCC Conference and Exhibition (GCCCE)
"""Deep Sparse Auto-Encoder Features Learning for Arabic Text Recognition""",N. Rahal; M. Tounsi; A. Hussain; A. M. Alimi;,2021,10.1109/ACCESS.2021.3053618,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9333558,Arabic text recognition;feature learning;bag of features;sparse auto-encoder;hidden Markov models;,"""One of the most recent challenging issues of pattern recognition and artificial intelligence is Arabic text recognition. This research topic is still a pervasive and unaddressed research field, because of several factors. Complications arise due to the cursive nature of the Arabic writing, character similarities, unlimited vocabulary, use of multi-size and mixed-fonts, etc. To handle these challenges, an automatic Arabic text recognition requires building a robust system by computing discriminative features and applying a rigorous classifier together to achieve an improved performance. In this work, we introduce a new deep learning based system that recognizes Arabic text contained in images. We propose a novel hybrid network, combining a Bag-of-Feature (BoF) framework for feature extraction based on a deep Sparse Auto-Encoder (SAE), and Hidden Markov Models (HMMs), for sequence recognition. Our proposed system, termed BoF-deep SAE-HMM, is tested on four datasets, namely the printed Arabic line images Printed KHATT (P-KHATT), the benchmark printed word images Arabic Printed Text Image (APTI), the benchmark handwritten Arabic word images IFN/ENIT, and the benchmark handwritten digits images Modified National Institute of Standards and Technology (MNIST).""",IEEE Access
"""Network-Regularized Sparse Logistic Regression Models for Clinical Risk Prediction and Biomarker Discovery""",W. Min; J. Liu; S. Zhang;,2018,10.1109/TCBB.2016.2640303,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7784786,Sparse logistic regression;network-regularized penalty;survival risk prediction;feature selection;,"""Molecular profiling data (e.g., gene expression) has been used for clinical risk prediction and biomarker discovery. However, it is necessary to integrate other prior knowledge like biological pathways or gene interaction networks to improve the predictive ability and biological interpretability of biomarkers. Here, we first introduce a general regularized Logistic Regression (LR) framework with regularized term λ||w||1 + ηwT Mw, which can reduce to different penalties, including Lasso, elastic net, and network-regularized terms with different M. This framework can be easily solved in a unified manner by a cyclic coordinate descent algorithm which can avoid inverse matrix operation and accelerate the computing speed. However, if those estimated wi and wj have opposite signs, then the traditional network-regularized penalty may not perform well. To address it, we introduce a novel network-regularized sparse LR model with a new penalty λ||w||1 + η|w|T M|w| to consider the difference between the absolute values of the coefficients. We develop two efficient algorithms to solve it. Finally, we test our methods and compare them with the related ones using simulated and real data to show their efficiency.""",IEEE/ACM Transactions on Computational Biology and Bioinformatics
"""Hyperspectral image classification using Fisher criterion-based Gabor cube selection and multi-task joint sparse representation""",S. Jia; Y. Xie; L. Shen; L. Deng;,2015,10.1109/WHISPERS.2015.8075364,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8075364,Hyperspectral imagery;Gabor wavelet;multi-task joint sparse representation;,"""Recently, Gabor wavelet transformation has been introduced for feature extraction of hyperspectral imagery. Due to the discriminative power of obtained Gabor features, high classification performance has been achieved. However, thousands of Gabor features cause too much burden for onboard computation, limiting the efficiency of the method. In fact, not all features have a positive effect on classification. In this paper, we have proposed a Gabor cube selection-based Multi-task Joint Sparse Representation framework, abbreviated as MT-SG, for hyperspectral imagery classification. Firstly, based on the Fisher discrimination criterion, the most representative Gabor cubes for each class have been picked out. Next, under multi-task joint sparse representation framework, a coefficient vector can be obtained for each test sample with the selected cube features, which can be applied for the following residual-based classification. Experimental results on real hyperspectral data have demonstrated the feasibility and efficiency of the proposed method.""",2015 7th Workshop on Hyperspectral Image and Signal Processing: Evolution in Remote Sensing (WHISPERS)
"""Automatic Microaneurysm Detection Using the Sparse Principal Component Analysis-Based Unsupervised Classification Method""",W. Zhou; C. Wu; D. Chen; Y. Yi; W. Du;,2017,10.1109/ACCESS.2017.2671918,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7859305,Diabetic retinopathy;microaneurysm detection;sparse PCA;unsupervised classification;,"""Since microaneurysms (MAs) can be seen as the earliest lesions in diabetic retinopathy, its detection plays a critical role in the diabetic retinopathy diagnosis. In recent years, many machine-learning methods have been developed for MA detection. Generally, MA candidates are first identified and then a set of features for these candidates are extracted. Finally, machine-learning methods are applied for candidate classification. In this paper, we present a novel unsupervised classification method based on sparse posterior cerebral artery (PCA) for MA detection. Since it does not have to consider a non-MA training set, the class imbalance problem can be avoided. Furthermore, effective features can be selected due to the characteristic of sparse PCA, which combines the elastic net penalty with the PCA. Meanwhile, a single T2 statistic is introduced, and the control limit can be determined for distinguishing true MAs from spurious candidates automatically. Experiment results on the retinopathy online challenge competition database show the effectiveness of our proposed method.""",IEEE Access
"""Heterogeneous dataflow architectures for FPGA-based sparse LU factorization""",Siddhartha; N. Kapre;,2014,10.1109/FPL.2014.6927401,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6927401,;,"""FPGA-based token dataflow architectures with heterogeneous computation and communication subsystems can accelerate hard-to-parallelize, irregular computations in sparse LU factorization. We combine software pre-processing and architecture customization to fully expose and exploit the underlying heterogeneity in the factorization algorithm. We perform a one-time pre-processing of the sparse matrices in software to generate dataflow graphs that capture raw parallelism in the computation through substitution and reassociation transformations. We customize the dataflow architecture by picking the right mixture of addition and multiplication processing elements to match the observed balance in the dataflow graphs. Additionally, we modify the network-onchip to route certain critical dependencies on a separate, faster communication channel while relegating less-critical traffic to the existing channels. Using our techniques, we show how to achieve speedups of up to 37% over existing state-of-the-art FPGA-based sparse LU factorization systems that can already run 3–4× faster than CPU-based sparse LU solvers using the same hardware constraints.""",2014 24th International Conference on Field Programmable Logic and Applications (FPL)
"""Natural Scene Character Recognition Using Robust PCA and Sparse Representation""",Z. Zhang; Y. Xu; C. -L. Liu;,2016,10.1109/DAS.2016.32,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7490141,Scene character recognition;Robust principal component analysis;Sparse representation;HOG;,"""Natural scene character recognition is challenging due to the cluttered background, which is hard to separate from text. In this paper, we propose a novel method for robust scene character recognition. Specifically, we first use robust principal component analysis (PCA) to denoise character image by recovering the missing low-rank component and filtering out the sparse noise term, and then use a simple Histogram of oriented Gradient (HOG) to perform image feature extraction, and finally, use a sparse representation based classifier for recognition. In experiments on four public datasets, namely the Char74K dataset, ICADAR 2003 robust reading dataset, Street View Text (SVT) dataset and IIIT5K-word dataset, our method was demonstrated to be competitive with the state-of-the-art methods.""",2016 12th IAPR Workshop on Document Analysis Systems (DAS)
"""Exploring Permanence and Uniqueness of EEG Brain Signals as a Biometric Signature, Part-II: Statistical Techniques""",M. E. Oztemel; Ö. M. Soysal;,2023,10.1109/CSCI62032.2023.00239,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590082,Electroencephalography;Biometric;Brain;Permanence;T-test;Mann Whitney U-test;,"""In this second part of our study, we continue our exploration of EEG-based biometric authentication by employing both parametric (T-test) and non-parametric (Mann-Whitney U-test) statistical techniques to measure the similarity of time-series signals. After filtering stimuli response signals, we employed two distinct normalization techniques-Forward-Backward-Slope-Normalization and Median-Referenced-Normalization. Additionally, two time-domain filters- Adjusted Laplacian of Mean and Laplacian of Gaussian - are applied for extraction of specific spectral neural activity. Through this multifaceted approach, we aim to enhance our understanding of the robustness and effectiveness of EEG-derived biometric signatures, offering valuable insights for future applications in this promising field. Our preliminary findings indicate that the proposed statistical-based pipeline can achieve an 86% accuracy rate in identifying individuals over the course of a week. This suggests that the pipeline shows promise in its ability to reliably distinguish between individuals based on the data it processes. However, further validation and testing may be necessary to confirm and refine these findings.""",2023 International Conference on Computational Science and Computational Intelligence (CSCI)
"""Surface Design of Automobile AB Column and Ceiling Based on Alias""",W. Xiaohui; S. Yanhong;,2017,10.1109/ICRIS.2017.87,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8101410,Alias;AB column;ceiling;curvature sparse;zebra line;,"""This paper introduces the testing method of surface design and quality of Alias software for automobile AB column and roof. Based on the first analysis of the concept car surface, the basic conditions of the automobile a surface design, and then introduces the surface modeling method of automobile AB column and roof, modeling method according to the three view cast graphs or real photos, Bezier curve. Bezier surface, select the appropriate modeling method, finally tested according to the quality standard of surface design.""",2017 International Conference on Robots & Intelligent System (ICRIS)
"""Unique applications of reverberation techniques""",K. A. Brezinski; D. R. Kempf;,2001,10.1109/ISEMC.2001.950464,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=950464,;,"""Box level radiated susceptibility was required on a large, highly interconnected avionics system. Traditional laboratory testing compliance needed to be demonstrated, however, the complexity of the system and necessary support equipment prohibited normal approaches in a laboratory environment. This paper discusses the testing performed using reverberation techniques inside the aircraft and also outside the aircraft in a shielded hangar. The paper also discusses the innovative testing techniques and departures from traditional rules and paradigms of the trade.""",2001 IEEE EMC International Symposium. Symposium Record. International Symposium on Electromagnetic Compatibility (Cat. No.01CH37161)
"""An Accurate Model for Fast Calculating the Resonant Frequency of an Irregular Solenoid""",W. Zhou; S. Huang;,2019,10.1109/TMTT.2019.2915514,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8721687,Calculation of inductance;irregular solenoid;solenoid;sparse solenoid;,"""A fast and accurate calculation of the resonant frequency of a solenoid coil is needed in many applications, e.g., wireless power transfer and magnetic resonance imaging. It helps to accelerate the design and optimization of a coil. However, the existing calculation models for the key equivalent parameters, the inductance ( $L$ ) and the capacitance ( $C$ ), of the solenoid coils can only provide estimations for standard tightly wound cylindrical coils, and the accuracy is not high. There are no accurate models available for calculating  $L$ ,  $C$ , and the resonant frequency when a solenoid becomes irregular, e.g., when it is sparse (with a large pitch), when it has nonuniform pitches, or when it is noncylindrical. In this paper, we propose a fast and accurate model for such an irregular solenoid coil to calculate its inductance, capacitance, and, thus, the resonant frequency. The accuracy of the proposed model is tested on the solenoid coils (with large pitches, nonuniform pitches, or noncylindrical shape) by comparing the calculated results to those using commercial simulation software and the measurement results. For a sparsely wound solenoid, the proposed model provides an accurate calculation of the inductance with a maximal pitch-to-wire-diameter ratio of 40 (an error rate of 8.33% at this ratio). For a varied-pitch solenoid and a noncylindrical one, an error rate of less than 5% can be achieved for a calculation of the inductance.""",IEEE Transactions on Microwave Theory and Techniques
"""Effective Feature Extraction Framework to Improve Network Intrusion Detection System""",H. V. MV; J. T; K. T. Raja;,2023,10.1109/WIECON-ECE60392.2023.10456531,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10456531,intrusion detection system;deep learning;sparse autoencoder;NSL-KDD;,"""A network intrusion detection system (NIDS) is a crucial component of a robust cybersecurity strategy. Its primary purpose is to continuously monitor network traffic and detect suspicious or malicious activity that could be indicative of a cyberattack or unauthorized network access. The effectiveness of NIDS depends heavily on the techniques we use to boost the classification accuracy of intrusion and minimize the computational difficulty while performing training and testing. The high volume of network traffic combined with its large number of features will increase the classification time. With the recent emergence of deep learning techniques, scientists have shown interest in learning dataset features, followed by the classification of intrusions. This study offers a novel method for extracting high-dimensional features from input data by employing a stacked sparse autoencoder. Simple machine learning models are then built using the remaining low-dimensionality features. Simulations were conducted, and the efficacy of binary and multiclass classifications was verified. The proposed method exceeds most of the other existing approaches in terms of performance.""",2023 IEEE 9th International Women in Engineering (WIE) Conference on Electrical and Computer Engineering (WIECON-ECE)
"""Face hallucination via Cauchy regularized sparse representation""",S. Qu; R. Hu; S. Chen; Z. Wang; J. Jiang; C. Yang;,2015,10.1109/ICASSP.2015.7178163,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7178163,Super-resolution;face hallucination;sparse representation;Cauchy regularization;,"""In dictionary-learning-based face hallucination, the testing image is represented as a linear combination of the training samples, and how to obtain the optimal coefficients is the primary issue. Sparse representation (SR) has ever been widely used in face hallucination, however, due to the fact that SR overemphasizes the sparsity, the obtained linear combination coefficients turn out far aggressively sparse, then leading to unsatisfactory hallucinated results. In this paper, we present a moderately sparse prior model for face hallucination problem with the L1 norm penalty in classic SR replaced by a Cauchy penalty term. An iterative optimization is further presented to solve the minimization of Cauchy regularized objective function. The experimental results on public face database demonstrate that our method is much more effective than state-of-the-art methods.""","2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
"""A unique system for automated, generic testing of hydraulic UUT's""",J. E. Sage;,1991,10.1109/AUTEST.1991.197558,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=197558,;,"""The IMAGE system is used to test a variety of hydraulic components in many commercial and military aircraft. The test system executes under control of the run time system and permits the prescribed testing sequence to occur. The IMAGE system operates in three modes and can be executed if a valid UUT unit under test definition and appropriate transducers have been specified. The modes of operation are outlined.<>""",Conference Record AUTOTESTCON '91 IEEE Systems Readiness Technology Conference Improving Systems Effectiveness in the Changing Environment of the '90s
"""RF Front-End Concept and Implementation for Direct Sampling of Multiband Signals""",J. -M. Muñoz-Ferreras; R. Gómez-García; F. Pérez-Martínez;,2011,10.1109/TCSII.2011.2110391,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5720287,Aliasing;analog-to-digital converters (ADCs);bandpass sampling;frequency-sparse signals;multiband filters;multichannel systems;software-defined radios (SDRs);sub-Nyquist sampling;ultrawideband (UWB) technology;,"""The placement of the analog-to-digital converter as near the antenna as possible is a key issue in the software-defined radio receiver design. Direct sampling of the incoming filtered signal is a compact solution enabling channel simultaneity. In this brief, in the context of evenly spaced equal-bandwidth multiband systems, sufficient conditions for the channel allocation assuring that the minimum sub-Nyquist sampling frequency does not imply aliasing are provided. Subsequently, as a validation example, the design of a minimum-sampling-frequency acquisition system for quad-band applications within a ultrawideband frequency range is shown. Moreover, an innovative solution for its radio-frequency front end, basically consisting of a signal-interference multiband bandpass filter, is reported. Experimental results of the built microstrip-filter prototype for the proposed 1-3-GHz-range quad-band system are also given.""",IEEE Transactions on Circuits and Systems II: Express Briefs
"""Unique measurement to monitor the gate oxide lifetime indicator, case studies""",X. Gagnard; O. Bonnaud;,2001,10.1109/IPFA.2001.941476,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=941476,;,"""Previous works (Gagnard and Bonnaud, Microelectron. Reliability vol. 39, pp. 75-763, 1999, and Proc. SPIE vol. 4182, pp. 142-50, 2000) demonstrated the possibility of realization of the gate oxide lifetime by a unique measurement based on leakage current. This indicator, easy to implement and able to decrease the test time, can be included in the routine of parametric tests. This work confirms the validity of this indicator and presents case studies related to BCD technology.""",Proceedings of the 2001 8th International Symposium on the Physical and Failure Analysis of Integrated Circuits. IPFA 2001 (Cat. No.01TH8548)
"""An Optimal Ordering Algorithm for Sparse Matrix Applications""",G. Irisarri; S. F. Hodges; A. M. Sasson;,1978,10.1109/TPAS.1978.354729,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4181678,;,"""This paper presents a new, optimal (according to a criterion defined later), sparsity-oriented, ordering algorithm for application in sparse matrix calculations. A dynamic programming algorithm which determines an ordered elimination such that the total number of fill-in terms is minimum is developed. The ordering algorithm is shown to be better, i.e. less fill-in, than the clustering method of reference [4]. The algorithm, is valid for diagonally dominant matrices which are symmetric in pattern of nonzero elements. The method has been found practical for ordering matrices appearing in a wide variety of engineering applications. Presently, it can be efficiently applied to matrices of the order of 50 rows.""",IEEE Transactions on Power Apparatus and Systems
"""Assessing the Test Suite of a Large System Based on Code Coverage, Efficiency and Uniqueness""",L. Vidács; F. Horváth; D. Tengeri; Á. Beszédes;,2016,10.1109/SANER.2016.69,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7476754,code coverage;regression testing;test suite evaluation;test suite quality;test efficiency;test metrics;,"""Regression test suites of evolving software systems play a key role in maintaining software quality throughout continuous changes. They need to be effective (in terms of detecting faults and helping their localization) and efficient (optimally sized and without redundancy) at the same time. However, test suite quality attributes are usually difficult to formalize and measure. In this paper, we rely on a recent approach for test suite assessment and improvement that utilizes code coverage information, but at a more detailed level, hence it adds further evaluation aspects derived from the coverage. The basic idea of the method is to decompose the test suite and the program code into coherent logical groups which are easier to analyze and understand. Several metrics are then computed from code coverage information to characterize the test suite and its constituents. We extend our previous study and employ derived coverage metrics (which express efficiency and uniqueness) to analyze the test suite of a large scale industrial open source system containing 27 000 test cases.""","2016 IEEE 23rd International Conference on Software Analysis, Evolution, and Reengineering (SANER)"
"""Domain Siamese CNNs for Sparse Multispectral Disparity Estimation""",D. -A. Beaupre; G. -A. Bilodeau;,2021,10.1109/ICPR48806.2021.9412723,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9412723,;,"""Multispectral disparity estimation is a difficult task for many reasons: it has all the same challenges as traditional visible-visible disparity estimation (occlusions, repetitive patterns, textureless surfaces), in addition of having very few common visual information between images (e.g. colour information vs. thermal information). In this paper, we propose a new CNN architecture able to do disparity estimation between images from different spectra, namely thermal and visible in our case. Our proposed model takes two patches as input and proceeds to do domain feature extraction for each of them. Features from both domains are then merged with two fusion operations, namely correlation and concatenation. These merged vectors are then forwarded to their respective classification heads, which are responsible for classifying the inputs as being same or not. Using two merging operations gives more robustness to our feature extraction process, which leads to more precise disparity estimation. Our method was tested using the publicly available LITIV 2014 and LITIV 2018 datasets, and showed best results when compared to other state-of-the-art methods.""",2020 25th International Conference on Pattern Recognition (ICPR)
"""Fast fault location in power transmission networks using transient signatures from sparsely-placed synchrophasors""",J. Valdez; X. Zhang; J. A. Torres; S. Roy;,2014,10.1109/NAPS.2014.6965412,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6965412,Synchrophasor measurements;Power System Fault location;Maximum A-Posteriori Probability (MAP);,"""This paper explores real-time fault location in a power transmission network using measurements of transients from sparsely-placed synchrophasors. The fault-location problem is abstracted to a statistical hypothesis-testing or detection problem, wherein the linearized dynamical models corresponding to different fault conditions must be distinguished in the face of fault-clearing and measurement uncertainty. A maximum a posteriori probability (MAP) detector is constructed. A strategy for real-time implementation of the fault-locator is discussed, which is based on pre-computation of detector parameters using state-estimator and contingency-analysis data, along with on-line collection of synchrophrasor data and implementation of the hypothesis test. Numerical case studies of the 11-Bus two area power system verify that the proposed fault-location algorithm can locate a faulted line accurately and quickly.""",2014 North American Power Symposium (NAPS)
"""Learning Motion Predictors for Smart Wheelchair Using Autoregressive Sparse Gaussian Process""",Z. Fan; L. Meng; T. Q. Chen; J. Li; I. M. Mitchell;,2018,10.1109/ICRA.2018.8460502,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8460502,;,"""Constructing a smart wheelchair on a commercially available powered wheelchair (PWC) platform avoids a host of seating, mechanical design and reliability issues but requires methods of predicting and controlling the motion of a device never intended for robotics. Analog joystick inputs are subject to black-box transformations which may produce intuitive and adaptable motion control for human operators, but complicate robotic control approaches; furthermore, installation of standard axle mounted odometers on a commercial PWC is difficult. In this work, we present an integrated hardware and software system for predicting the motion of a commercial PWC platform that does not require any physical or electronic modification of the chair beyond plugging into an industry standard auxiliary input port. This system uses an RGB-D camera and an Arduino interface board to capture motion data, including visual odometry and joystick signals, via ROS communication. Future motion is predicted using an autoregressive sparse Gaussian process model. We evaluate the proposed system on real-world short-term path prediction experiments. Experimental results demonstrate the system's efficacy when compared to a baseline neural network model.""",2018 IEEE International Conference on Robotics and Automation (ICRA)
"""Iterative Methods for Sparse Linear Systems on Graphics Processing Unit""",A. -K. Cheik Ahamed; F. Magoulès;,2012,10.1109/HPCC.2012.118,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332256,Krylov methods;linear algebra;sparse matrix-vector multiplication;graphics processing unit;CUDA;CUBLAS;CUSPARSE;Cusp;,"""Many engineering and science problems require a computational effort to solve large sparse linear systems. Krylov subspace based iterative solvers have been widely used in that direction. Iterative Krylov methods involve linear algebra operations such as summation of vectors, dot product, norm, and matrix-vector multiplication. Since these operations could be very costly in computation time on Central Processing Unit (CPU), we propose in this paper to focus on the design of iterative solvers to take advantage of massive parallelism of Graphics Processing Unit (GPU). We consider Stabilized BiConjugate Gradient (BiCGStab), Stabilized BiConjugate Gradient (L) (BiCGStabl), Generalized Conjugate Residual (P-GCR), Bi-Conjugate Gradient Conjugate Residual (P-BiCGCR), transpose-free Quasi Minimal Residual (P-tfQMR) for the solution of sparse linear systems with non symmetric matrices and Conjugate Gradient (CG) for symmetric positive definite matrices. We discuss data format and data structure for sparse matrices, and how to efficiently implement these solvers on the Nvidia's CUDA platform. The scalability and performance of the methods are tested on several engineering problems, together with numerous numerical experiments which clearly illustrate the robustness, competitiveness and efficiency of our own proper implementation compared to the existing libraries.""",2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems
"""On the Uniqueness and Sensitivity of Nanoindentation Testing for Determining Elastic and Plastic Material Properties of Electroplating Copper Filled in Through-Silicon-Via (TSV)""",M. El Barbary; L. Chen; Y. Liu; F. Qin; X. Fan;,2018,10.1109/ECTC.2018.00157,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8429670,TSV-Cu;nanoindentation;elastic and plastic properties;uniqueness;error sensitivty;,"""Through-Silicon-Vias (TSV) for 3D integration are susceptible to mechanical failures such as TSV extrusion, chip cracking, and carrier mobility change, mainly due to significant thermal mismatch between electroplating copper and silicon wafer. Assessing TSV reliability requires accurate mechanical properties of electroplating Cu, which may not be achieved by traditional testing methods for large-size samples. Alternatively, nanoindentation can be used as a versatile and non-destructive technique to obtain material properties at micro-scale or nan-scale. The technique, however, is sensitive to experimental data variation and may yield non-unique properties, which is not yet fully understood for TSV-Cu and other packaging materials. This paper reports the results and analysis of in-situ nanoindentation for TSV-Cu specimens. Each specimen has a diameter of 20 um and a depth of 180 um, fabricated on a 200-mm silicon wafer. The relationship of power-law hardening was used to characterize the mechanical behaviors of TSV-Cu with Young's modulus (E), yielding strength (Y), and strain hardening exponent (n). To extract these properties, two methods of so-called reverse analysis were applied to experimental test data. Finite element models were also built to reproduce the displacement-loading curve based on the extracted material properties. The following results were observed: 1) with very different yield strength and hardening coefficient, identical load-displacement curves were produced. This indicates that single indentation testing cannot provide unique material properties for TSV-Cu; 2) the loading-displacement curves vary with the locations of the specimens, possibly due to the effect of grain size on nanoindentation, or experimental ""errors""; 3) the plastic properties can vary dramatically with a slight change in the loading-displacement curve. This implies the sensitivity of extracted properties on experimental data variations. Generally, the non-uniqueness and sensitivity due to experimental data variations of nanoindentation testing are not independent. This study concludes that single nanoindentation testing itself may not be able to adequately and uniquely determine the material properties of TSV-Cu. To make the best use of the nanoindentation technique, a robust methodology is needed to minimize the sensitivity to experimental errors and analysis.""",2018 IEEE 68th Electronic Components and Technology Conference (ECTC)
"""Self-Supervised Learning-Based Time Series Classification via Hierarchical Sparse Convolutional Masked-Autoencoder""",T. Yu; K. Xu; X. Wang; B. Ding; D. Feng;,2024,10.1109/OJSP.2024.3435673,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10614789,Time series classification;self-supervised learning;time series pre-training;,"""In recent years, the use of time series analysis has become widespread, prompting researchers to explore methods to improve classification. Time series self-supervised learning has emerged as a significant area of study, aiming to uncover patterns in unlabeled data for richer information. Contrastive self-supervised learning, particularly, has gained attention for time series classification. However, it introduces inductive bias by generating positive and negative samples. Another approach involves Masked Autoencoders (MAE), which are effective for various data types. However, due to their reliance on the Transformer architecture, they demand significant computational resources during the pre-training phase. Recently, inspired by the remarkable advancements achieved by convolutional networks in the domain of time series forecasting, we aspire to employ convolutional networks utilizing a strategy of mask recovery for pre-training time series models. This study introduces a novel model termed Hierarchical Sparse Convolutional Masked-Autoencoder, “HSC-MAE”, which seamlessly integrates convolutional operations with the MAE architecture to adeptly capture time series features across varying scales. Furthermore, the HSC-MAE model incorporates dedicated decoders that amalgamate global and local information, enhancing its capacity to comprehend intricate temporal patterns. To gauge the effectiveness of the proposed approach, an extensive array of experiments was conducted across nine distinct datasets. The experimental outcomes stand as a testament to the efficacy of HSC-MAE in effectively mitigating the aforementioned challenges.""",IEEE Open Journal of Signal Processing
"""Face Hallucination Via Weighted Adaptive Sparse Regularization""",Z. Wang; R. Hu; S. Wang; J. Jiang;,2014,10.1109/TCSVT.2013.2290574,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6662396,Super-resolution;face hallucination;adaptive sparse regularization;weighted penalty;ℓq norm;$ell_{q}$ norm;adaptive sparse regularization;face hallucination;super-resolution;weighted penalty;,"""Sparse representation-based face hallucination approaches proposed so far use fixed ℓ1 norm penalty to capture the sparse nature of face images, and thus hardly adapt readily to the statistical variability of underlying images. Additionally, they ignore the influence of spatial distances between the test image and training basis images on optimal reconstruction coefficients. Consequently, they cannot offer a satisfactory performance in practical face hallucination applications. In this paper, we propose a weighted adaptive sparse regularization (WASR) method to promote accuracy, stability and robustness for face hallucination reconstruction, in which a distance-inducing weighted ℓq norm penalty is imposed on the solution. With the adjustment to shrinkage parameter q , the weighted ℓq penalty function enables elastic description ability in the sparse domain, leading to more conservative sparsity in an ascending order of q . In particular, WASR with an optimal q > 1 can reasonably represent the less sparse nature of noisy images and thus remarkably boosts noise robust performance in face hallucination. Various experimental results on standard face database as well as real-world images show that our proposed method outperforms state-of-the-art methods in terms of both objective metrics and visual quality.""",IEEE Transactions on Circuits and Systems for Video Technology
"""Effect of a sparse architecture on generalization behaviour of connectionist networks: a comparative study""",B. Chakraborty;,1999,10.1109/ICSMC.1999.823241,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=823241,;,"""Generalization, the ability of achieving equal performance with respect to the training patterns for the design of the system as well as the unknown test patterns outside the training set, is considered to be the most desirable aspect of a cognitive learning system. For connectionist classifiers, generalization depends on several factors like the network architecture and size, learning algorithm, complexity of the problem and the quality and quantity of the training samples. Various studies on the generalization behaviour of a neural classifier suggest that the architecture and the size of the network should match the size and complexity of the training sample set of the particular problem. The popular ideas for improving generalization is network pruning by removing redundant nodes or growing by adding nodes to reach the optimum size for matching. In this work a feedforward multilayer connectionist model proposed earlier by Chakraborty et al. (1997) with a sparse fractal connection structure between the neurons of adjacent layers has been studied for its suitability compared to other architectures for achieving good generalization. A comparative study with another feedforward multilayer model with network pruning algorithms for pattern classification problem revealed that it is easier to achieve good generalization with the proposed sparse fractal architecture.""","IEEE SMC'99 Conference Proceedings. 1999 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.99CH37028)"
"""A Two-Stage Method to Identify Joint Modules From Matched MicroRNA and mRNA Expression Data""",W. Min; J. Liu; F. Luo; S. Zhang;,2016,10.1109/TNB.2016.2556744,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7458909,$L_{0}$ -penalized singular value decomposition ( $L_{0}$ -SVD);miRNA cluster;miRNA-gene joint module;multiple-output sparse group lasso;,"""MicroRNAs (miRNAs) are a class of small non-coding RNAs, which play key roles in gene regulation. Previous studies have revealed that they are likely to work together to regulate their common target genes. Discovery of the underlying combinatorial regulation between miRNAs and genes can provide useful information for understanding their functions as well as their close relevance to cancer. In this paper, we propose a two-stage method for identifying miRNA-gene regulatory modules by integrating miRNA and mRNA expression profiles and miRNA genomic cluster data. We first develop a multiple-output sparse group lasso (MSGL) regression model to predict a miRNA-gene association matrix (i.e., a miRNA-gene regulatory network). Further, we propose a L0-regularized singular value decomposition (L0-SVD) to identify miRNA-gene joint modules from the predicted regulatory network. We test our method on the matched miRNA and mRNA expression profiles in breast cancer from TCGA project and identify ten miRNA-gene regulatory modules. We find that 1) the modules are significantly associated in the predicted miRNA-gene regulatory network; 2) the modules are significantly enriched in GO biological processes and KEGG pathways, respectively; 3) many miRNAs and genes in the modules are related with breast cancer. On average, 51% of the miRNAs and 30% of the genes are related with breast cancer. The results demonstrate that miRNA-gene regulatory modules provide insights into the mechanisms of the combinatorial regulation between miRNAs and genes.""",IEEE Transactions on NanoBioscience
"""The unique reference NCSLOC metric""",S. D. Crouch; D. B. Simmons;,1999,10.1109/CMPSAC.1999.812726,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=812726,;,"""We propose that by utilizing certain metrics and gathering operating system information it can be determined if a file is a version or duplicate of another file on the system. In a software development environment, knowing what files are versions of other files would be beneficial in many areas. It would help a manager estimate the amount of code that needs to be modified in a reengineering project. When comparing files to a software reuse library, the amount of reuse can be measured. If a survey is being done on the entire contents of the computer's file system, then all code for a variety of programming languages can be located and the amount of new and reused code can be quantified. The authors present a description of the metrics, how they are collected, and the results of processing a large number of files using the metrics.""",Proceedings. Twenty-Third Annual International Computer Software and Applications Conference (Cat. No.99CB37032)
"""Data-Driven Discovery of Social Network Dynamics""",A. Bakhtiarnia; A. Fahim; E. M. Miandoab;,2020,10.1109/SCIOT50840.2020.9250195,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9250195,data-driven methods;social networks;sparse identification of nonlinear dynamical systems;machine learning;,"""In recent years, rigorous mathematical frameworks have been developed for modelling complex networks such as social networks, which can be used to determine several of their properties such as the resilience of the network to external perturbation and the propagation time of signals within them. Several modern algorithms have been proposed in order to identify models of dynamical systems from big data, such as the well-known “sparse identification of nonlinear dynamics (SINDy)” algorithm. We modify this algorithm such that given data regarding the dynamics of a social network, the differential equation that best describes the underlying dynamics of the social network is identified in accordance with the aforementioned mathematical frameworks. Due to the massive growth of the activity within social networks, the efficiency and speed of such algorithms are becoming increasingly crucial. Testing the proposed algorithm on empirical data verifies the accuracy and efficiency of our approach.""","2020 4th International Conference on Smart City, Internet of Things and Applications (SCIOT)"
"""Weight-Based Convolutional Neural Network with Sparse Optimization""",D. N. Endrawati; I. Syafalni; N. Sutisna; T. Adiono;,2022,10.1109/ISESD56103.2022.9980660,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9980660,convolution;accelerator;sparse data;indexing;,"""The Convolutional Neural Network (CNN) algorithm is widely used in modern AI systems and has been applied to various technologies. CNN is widely used for image processing, such as detecting and classifying objects from an image. However, the large size of the network presents challenges to the throughput and energy efficiency of the hardware it uses. The YoloV3-Tiny model is one of the architectures for real-time object detection, and it will create significant data movements. In the inference process, most of the processes used are convolution. While the results using pruning process applied on YoloV3-Tiny have a sparsity level of up to 75% in several layers, it means that the convolution process for the inference process will do many multiplication processes with zero values. In this accelerator, all weight data with a value of zero will be removed after the training process by pruning, then the input weight that enters the accelerator and carries out the convolution process without delay using “weight stream” dataflow. The results of the design test show that the results of the accelerator give the output of the convolution process faster when applied using trimmed sparse data. The calculation results of all YoloV3-Tiny layers when carrying out the convolution process using an accelerator designed have been estimated reduced 56% of the convolution process. The amount of data weight required for this accelerator’s convolution process is 4.2 MB. This data size is smaller than the weight data from the original weight YoloV3-Tiny model, which is 33 MB.""",2022 International Symposium on Electronics and Smart Devices (ISESD)
"""A high performance algorithm using pre-processing for the sparse matrix-vector multiplication""",R. C. Agarwal; F. G. Gustavson; M. Zubair;,1992,10.1109/SUPERC.1992.236712,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=236712,;,"""The authors propose a feature-extraction-based algorithm (FEBA) for sparse matrix-vector multiplication. The key idea of FEBA is to exploit any regular structure present in the sparse matrix by extracting it and processing it separately. The order in which these structures are extracted is determined by the relative efficiency with which they can be processed. The authors have tested FEBA on IBM 3000 VF for matrices from the Harwell Boeing and OSL collection. The results obtained were on average five times faster than the ESSL routine which is based on the ITPACK storage structure.<>""",Supercomputing '92:Proceedings of the 1992 ACM/IEEE Conference on Supercomputing
"""Sparse Modeling of Mandibular Reconstruction Procedures Using Statistical Geometric Features""",M. Nakao; T. Matsuda;,2018,10.1109/EMBC.2018.8512986,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8512986,;,"""This paper introduces a sparse modeling method that uses statistical geometric features for automated pre-operative planning. It further shows the application of this method to mandibular reconstruction with fibular segments. With this method, instead of using all the training data, only a small number of data that have similar features to the test data are selected and appropriately synthesized to reconstruct patient-specific plans. We compared the performance of three automated planning models using 120 patterns of mandibular reconstruction data manually planned by oral surgeons. The sparseness of the data selection and the efficacy of the automated planning framework were quantitatively confirmed.""",2018 40th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)
"""A Unique Vertex Deleting Algorithm for Graph Isomorphism""",B. Zhang; Y. Tang; J. Wu; L. Huang;,2011,10.1109/ISIDF.2011.6024200,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6024200,;,"""NA""",2011 International Symposium on Image and Data Fusion
"""Unique IRS (Inertial reference System), for safety critical applications, ITAR-FREE and based on high performance Fiber Optic Gyroscope (FOG)""",G. Biancucci; R. Senatore; F. D’Angelo; A. Pizzarulli; M. Verola; E. Quatraro; D. Grifi; M. Perlmutter;,2019,10.1109/ISS46986.2019.8943594,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8943594,;,"""ARGO 4000 is the outcome of three years of design and development effort carried out with the objective to realize the world smallest IRS (Inertial Reference System) certified by EASA with pending ETSO-c201, capable to provide high accuracy inertial navigation data in the most challenging environmental conditions applicable to fixed and rotary wing aircrafts and verified by means of the DO-160 standard qualification procedures. Such work has been partially financed by European Union H2020 SME innovation funds. ARGO 4000 architecture integrates a sensor block based on FOG's (Fiber Optic Gyros) completely design and manufactured in-house using a proprietary technology, and on best-in-class Quartz accelerometers. The sensors are controlled by an extremely compact electronics compliant with DO-254, on which a DO-178C software executes the navigation algorithms to provide better than 1 NM/hr CEP (Circular Error Probable) and 0.05° heading accuracies. External GNSS and Air Data sensors can be interfaced through serial or ARINC-429 buses for aiding purposes. The architecture embeds a provision for extending the interface capabilities to further devices and communication standards (such as MIL-STD-1553) by means of a connecting bridge to an add-on-module, that can be customized with respect the specific platform-dependent integration requirements. ARGO 4000 development went through and extensive phase of simulation and experimental verification of many system aspects, fine tuning them to reach targeted performance and reliability.ARGO 4000 includes a sophisticated motion detection algorithm to improve the accuracy when aircraft is on ground and an IFA (In Flight Alignment) capability to manage emergency power switch off/switch on conditions during flight. ARGO 4000 is a dual-use, ITAR-free product, highly compact (4.1 kg, 3.1 liter) characterized by the best accuracy over weight and size ratio in the worldwide scenario.""",2019 DGON Inertial Sensors and Systems (ISS)
"""Optimal Synchronizable Test Sequence from Test Segments""",J. Chen; L. Duan;,2006,10.1109/QSIC.2006.46,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4032294,conformance testing;finite state machine;test sequence;distributed testing;unique input/output sequence.;,"""Finite-state-machine-based conformance testing has been extensively studied in the literature in the context of centralized test architecture. With a distributed test architecture which involves multiple remote testers, the application of a test sequence may encounter controllability problems. One of the possible solutions to overcome this problem is to carefully select a suitable test sequence so that its application to the implementation under test will not involve controllability problems. A common problem in doing so is to generate a test sequence, avoiding controllability problems during its application in the test procedure, so that all given test segments have been contained in the test sequence. In this paper, we present an optimal solution to this problem, taking into account possible overlaps among the test segments""",2006 Sixth International Conference on Quality Software (QSIC'06)
"""Exact k-way sparse matrix partitioning""",E. L. Jenneskens; R. H. Bisseling;,2022,10.1109/IPDPSW55747.2022.00129,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9835459,branch-and-bound;integer linear programming;exact algorithm;sparse matrix-vector multiplication;hypergraph;parallel computing;,"""To minimize the communication in parallel sparse matrix-vector multiplication while maintaining load balance, we need to partition the sparse matrix optimally into k disjoint parts, which is an NP-complete problem. We present an exact algorithm based on the branch and bound (BB) method which partitions a matrix for any k, and we explore exact sparse matrix partitioning beyond bipartitioning. The algorithm has been implemented in a software package General Matrix Partitioner (GMP). We also present an integer linear programming (ILP) model for the same problem, based on a hypergraph formulation. We used both methods to determine optimal 2,3,4-way partitionings for a subset of small matrices from the SuiteSparse Matrix Collection. For k=2, BB outperforms ILP, whereas for larger k, ILP is superior. We used the results found by these exact methods for k=4 to analyse the performance of recursive bipartitioning (RB) with exact bipartitioning. For 46 matrices of the 89 matrices in our test set of matrices with less than 250 nonzeros, the communication volume determined by RB was optimal. For the other matrices, RB is able to find 4-way partitionings with communication volume close to the optimal volume.""",2022 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
"""Unknown Attack Detection Based on Zero-Shot Learning""",Z. Zhang; Q. Liu; S. Qiu; S. Zhou; C. Zhang;,2020,10.1109/ACCESS.2020.3033494,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9239385,Zero-shot learning;network intrusion;unknown attack detection;sparse semantic autoencoder;,"""In recent years, due to the frequent occurrence of network intrusions, more and more researchers have begun to focus on network intrusion detection. However, it is still a challenge to detect unknown attacks. Currently, there are two main methods of unknown attack detection: clustering and honeypot. But they still have unsolved problems such as difficulty in collecting unknown attack samples and failure to detect on time. Zero-Shot learning is proposed to deal with the problem in this article, which can recognize unknown attacks by learning the mapping relations between feature space and semantic space (such as attribute space). When the semantic descriptions of all attacks (including known and unknown attacks) are provided, the classifier built by Zero-Shot learning can extract common semantic information among all attacks and construct connections between known and unknown attacks. The classifier then utilizes the connections to classify unknown attacks although there are no samples for unknown attacks. In this article, we first propose to use Zero-Shot learning to overcome the challenge of unknown attack detection and illustrate the feasibility of this method. Secondly, we then propose a novel method of Zero-Shot learning based on sparse autoencoder for unknown attack detection. This method maps the feature of known attacks to the semantic space, and restores the semantic space to the feature space by constrains of reconstruction error, and establishes the feature to semantic mapping, which is used to detect unknown attacks. Verification tests have been carried out by using the public dataset NSL_KDD. From the experiments conducted in this work, the results show that the average accuracy reaches 88.3%, which performs better than other methods.""",IEEE Access
"""An Optimized GP-GPU Warp Scheduling Algorithm for Sparse Matrix-Vector Multiplication""",L. Liu; M. Liu; C. -J. Wang;,2013,10.1109/NAS.2013.35,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6665367,GPU;scheduling;SpMV;data locality;multicore;manycore;,"""GP-GPUs have been used as the platform for many applications due to their powerful computation ability and massively parallel features. In this paper, we first investigate the CSR sparse matrix format, the performance of existing optimized SpMV (Sparse matrix-vector multiplication) algorithms, and analyze the memory access patterns of the SpMV algorithms. Based on the analysis of the memory access patterns, we propose a new thread scheduling technique that can take advantage of inter-warp locality and intra-warp locality simultaneously, and also can achieve memory coalescing automatically. This proposed new scheduling technique will change the memory access pattern of SpMVs significantly. The simulation results show that the performance of the SpMV using the new proposed thread scheduling technique achieves much better performance than the implementation of the SpMV optimized by other techniques.""","2013 IEEE Eighth International Conference on Networking, Architecture and Storage"
"""Landslide Susceptibility Prediction Using Sparse Feature Extraction and Machine Learning Models Based on GIS and Remote Sensing""",L. Zhu; G. Wang; F. Huang; Y. Li; W. Chen; H. Hong;,2022,10.1109/LGRS.2021.3054029,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350108,Geographic information system (GIS);landslide susceptibility prediction (LSP);neural network;remote sensing (RS);sparse feature extraction (SFE);,"""Landslide susceptibility prediction (LSP) is a useful technology for landslide prevention. Due to the complex nonlinear correlations among environmental factors, traditional machine learning (ML) models have unsatisfactory LSP accuracies. In this letter, a sparse feature extraction network (SFE+) is proposed for LSP. First, the landslides and environmental factors are collected, and frequency ratios of environmental factors are calculated as the model inputs. Second, the input data are passed through the input layer with the dropout, and then, the features are passed through the hidden layers, that is, the k% lifetime sparsity layers. The hidden layers are employed to further sparse these factors to obtain the independent and redundant prediction features as much as possible. Finally, certain classifiers are used to realize the LSP in the study area. SFE-support vector machine (SVM), SFE-logistic regression (LR), and SFE-stochastic gradient descent (SGD) models are built. For comparison, principal component analysis (PCA)-SVM, PCA-LR, PCA-SGD, SVM, LR, and SGD models are also built for LSP in Shicheng County, China. Results show that the SFE-based ML models, especially the SFE-SVM, can effectively extract the sparse nonlinear features of environmental factors to improve LSP accuracies and have promising prospects for LSP.""",IEEE Geoscience and Remote Sensing Letters
"""Recognize human activities from multi-part missing videos""",K. Xu; Z. Qin; G. Wang;,2016,10.1109/ICME.2016.7552941,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7552941,Activity recognition;sparse representation;combinatorial dictionary;multi-part missing;,"""Recognizing human activities from multi-part missing videos is a challenge problem. When the multiple missing parts are continuous, the problem is reduced to activity recognition in videos with single part missing at any position which is focused on by many researches. However, in many practical applications, some temporal gaps always appear in captured videos due to random frame loss(e.g. noise interfere). To solve this problem, we propose a novel framework: 1) dividing each video into multiple equal-length segments, where the local spatio-temporal features extracted; 2) concatenating combinatorial sparse activity dictionaries, formed by over-complete dictionary of each segment; 3) computing combinatorial sparse coefficients of each segment, based on activity dictionaries above; 4) formulating probability of each activity to estimate the correct class. Our experiments achieve superior performance not only in videos with single part missing at any position, but also in videos with multiple parts missing.""",2016 IEEE International Conference on Multimedia and Expo (ICME)
"""UAV as a Data Ferry for a Sparse Adaptive WSN""",P. A. Karegar; A. Al-Anbuky;,2022,10.1109/APCC55198.2022.9943645,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9943645,Unmanned Aerial Vehicle (UAV);Wireless Sensor Networks (WSN);Fuzzy UAV route;UAV path Planning;,"""Using UAVs as mobile data ferries can amplify ground network performance by mitigating the impact of horizontal radio pollution on the environment. Designing an energy-efficient UAV communication with a randomly distributed ground sensors via enhancing the ground network structure dependent to the UAV path has been one of the challenges for the UAV-assisted WSN data gathering efforts. This paper aims at developing an approach called ‘UAV Fuzzy Travel Path’ that supports UAV smooth trajectory planning in gathering data of distributed wireless sensors over a large space. The dynamic orchestration of ground wireless sensors grouping has been considered in this paper for improving the network performance in data collection. This offers a more dynamic and flexible network that interact with the UAV planned path. Preliminary results of testbed simulations reflect that the proposed software define ground network system has presented promising outcomes in terms of network performances such as network latency and packet delivery rate. Further work is required for testing the scheme on a wider range of software defined WSN and UAV flight formations.""",2022 27th Asia Pacific Conference on Communications (APCC)
"""Phaseless Planar Near-far Field Transformation Based on Low-rank Matrix Completion Method""",G. Sun; T. Ni; K. Xue; C. Zhang; Z. Weng; D. Hou;,2023,10.1109/EMCSIPI50001.2023.10241726,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10241726,Compressive sensing;Low-rank matrix completion;Phaseless near-far field transformation;Sparse sampling;Outdoor near-field measurement;,"""With the diversified development of antenna shapes, outdoor near-field measurement technology for large antennas and electromagnetic equipment has received extensive attention. However, the required near-field scanning plane is large and time-consuming for such antennas. In addition, the positioning accuracy of sampling points decreased due to the use of mobile devices instead of near-field scanning frames. In order to solve the problems mentioned above, this paper proposes a phaseless near-far field transformation method based on low-rank matrix completion for outdoor near-field measurement. In this paper, the Square Kilometre Array (SKA) element is taken as the antenna under tested (AUT), the Ansoft HFSS and MATLAB software are used for simulation. It is found that for 50% sparse matrix and 60% sparse matrix, the relative error of electric field amplitude completion is less than 0.0334 and 0.0118, respectively. The far-field pattern of the AUT is calculated by substituting a 50% sparse matrix into the near-far field transformation method proposed in this paper. The average error between the calculated value and the simulated value of the pattern is less than 0.5684dB. It can be considered that the proposed method can realize phaseless planar near-far field transformation under sparse sampling.""",2023 IEEE Symposium on Electromagnetic Compatibility & Signal/Power Integrity (EMC+SIPI)
"""Research on Determination of Overloaded Function Uniqueness for Regression Testing Oriented""",S. Qi; Y. -m. Mu; Z. -h. Zhang; K. -q. Peng;,2010,10.1109/ICIECS.2010.5678400,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5678400,;,"""Function call path testing is based on static analysis of source code. The relationship between the definitions of the overloaded function and the call points is uncertain in the static analysis, which will produce a great deal of redundant paths and increase the cost of the testing. In order to determine the paths of the actual process and get function call relation, firstly, we need to determine the uniqueness of the overloaded function. We analyze the form of the overloaded function call from several aspects, design the algorithm of DUOF(determining the uniqueness of the overloaded function), ultimately determine its unique, by analyzing the realization mechanism of overloaded functions and applying the theory of compile and object-oriented. The experiment's result shows that the result of its implementation is not only accurate but also can improve the efficiency of regression testing, efficiency of the algorithm is more significant especially when the amount of code is larger.""",2010 2nd International Conference on Information Engineering and Computer Science
"""Application of a Simple, Spiking, Locally Competitive Algorithm to Radionuclide Identification""",M. Carson; W. Woods; S. Reynolds; M. Wetzel; A. J. Morton; A. A. Hecht; M. Osiński; C. Teuscher;,2021,10.1109/TNS.2021.3054608,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9335987,Compton scatter;gamma-ray;identification;locally competitive algorithm (LCA);memristor;neuromorphic;power efficient;radionuclide;simple spiking LCA (SSLCA);sparse coding;spectrum;spiking architecture;,"""Many radionuclide identification algorithms use statistical inference to collect a variety of features from gamma-ray spectra to deduce the presence of particular radionuclides. More modern algorithms require large amounts of data to learn and use latent features from spectra for classification. Both approaches are computationally expensive, which is reflected in their power consumption, and require large amounts of user intervention to prepare. In this article, we introduce a low-power, neuromorphic algorithm for the real-time identification of radionuclides which simultaneously considers the entire shape of a gamma-ray spectrum. Utilizing the output of a traditional gamma-ray detector, our spiking, locally competitive algorithm uses sparse coding optimization to compare global patterns in a gamma-ray spectrum with a dictionary of radionuclide templates. This approach allows us to model informative global features resulting from both photoelectric absorption and Compton scattering. For the purpose of radiation threat reduction, the dictionary consists of data from the Nuclear Wallet Cards, a list of radionuclides and their properties compiled by the National Nuclear Data Center. To test our algorithm, we use a variety of gamma-ray spectra created using radionuclides measured under laboratory conditions with varying durations, distances, activity levels, and backgrounds, resulting in a wide range of signal-to-noise ratios. We have created test sets for three different gamma-ray detector types, with 57Co, 137Cs, 152Eu, 60Co, 239Pu, and 235U sources, to quantify the effect of resolution, efficiency, and background on the accuracy of the algorithm. We demonstrate a true positive accuracy of 91% with a high-resolution detector and 89% with a low-resolution detector on the corresponding test sets. Experimenting with the same radionuclides included in the test sets in a variety of special nuclear material (SNM) masking configurations, we show that our algorithm is capable of correctly identifying both SNM and mask even when the activity level of the mask is several times higher than that of the SNM. We also determine that our algorithm achieves over a 99% reduction in power consumption over other radionuclide identification software applications, which is critical for long-term, independent monitoring and is the goal of this research.""",IEEE Transactions on Nuclear Science
"""Semi-Supervised Autoencoder: A Joint Approach of Representation and Classification""",W. Haiyan; Y. Haomin; L. Xueming; R. Haijun;,2015,10.1109/CICN.2015.275,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7546333,semi-supervised Autoencoder;deep learning;logistic regression;Sparse Autoencoder;,"""Recent years have witnessed the significant success of representation learning and deep learning in various prediction and recognition applications. Most of these previous studies adopt the two-phase procedures, namely the first step of representation learning and then the second step of supervised learning. In this process, to fit the training data the initial model weights, which inherits the good properties from the representation learning in the first step, will be changed in the second step. In other words, the second step leans better classification models at the cost of the possible deterioration of the effectiveness of representation learning. Motivated by this observation we propose a joint framework of representation and supervised learning. It aims to learn a model, which not only guarantees the ""semantics"" of the original data from representation learning but also fit the training data well via supervised learning. Along this line we develop the model of semi-supervised Auto encoder under the spirit of the joint learning framework. The experiments on various data sets for classification show the significant effectiveness of the proposed model.""",2015 International Conference on Computational Intelligence and Communication Networks (CICN)
"""Fuzzy KNN Method With Adaptive Nearest Neighbors""",Z. Bian; C. M. Vong; P. K. Wong; S. Wang;,2022,10.1109/TCYB.2020.3031610,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9269360,Decision tree;fuzzy k-nearest-neighbor method (FKNN);nearest neighbors;sparse representation/reconstruction;,"""Due to its strong performance in handling uncertain and ambiguous data, the fuzzy  ${k}$ -nearest-neighbor method (FKNN) has realized substantial success in a wide variety of applications. However, its classification performance would be heavily deteriorated if the number k of nearest neighbors was unsuitably fixed for each testing sample. This study examines the feasibility of using only one fixed k value for FKNN on each testing sample. A novel FKNN-based classification method, namely, fuzzy KNN method with adaptive nearest neighbors (A-FKNN), is devised for learning a distinct optimal k value for each testing sample. In the training stage, after applying a sparse representation method on all training samples for reconstruction, A-FKNN learns the optimal  ${k}$  value for each training sample and builds a decision tree (namely, A-FKNN tree) from all training samples with new labels (the learned optimal  ${k}$  values instead of the original labels), in which each leaf node stores the corresponding optimal  ${k}$  value. In the testing stage, A-FKNN identifies the optimal  ${k}$  value for each testing sample by searching the A-FKNN tree and runs FKNN with the optimal  ${k}$  value for each testing sample. Moreover, a fast version of A-FKNN, namely, FA-FKNN, is designed by building the FA-FKNN decision tree, which stores the optimal k value with only a subset of training samples in each leaf node. Experimental results on 32 UCI datasets demonstrate that both A-FKNN and FA-FKNN outperform the compared methods in terms of classification accuracy, and FA-FKNN has a shorter running time.""",IEEE Transactions on Cybernetics
"""Study on Image recognition based on computer visual angle point detection""",S. Zhou;,2021,10.1109/ICMSSE53595.2021.00014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9609252,Harris algorithm;corner detection;convolution neural network;sparse learning;image recognition;,"""Considering the objective change of human beings and the effect of temporal parallax, it is difficult to extract the required feature points accurately and recognize the recognition algorithm. In order to obtain effective special feature points, the depth perception mechanism of HVS is simulated, and the robustness of the recognition system is improved by corner detection of facial images. Three-dimensional imaging effect is simulated by two-dimensional image processing method, and image matching is carried out by sparse learning, clustering and other algorithms to reduce error samples and complete identification test.""",2021 International Conference on Management Science and Software Engineering (ICMSSE)
"""Maximally Sparse, Steerable, and Nonsuperdirective Array Antennas via Convex Optimizations""",M. D’Urso; G. Prisco; R. M. Tumolo;,2016,10.1109/TAP.2016.2586490,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7502084,$ell _{0}$ -norm;active electronically scanned array (AESA) antennas;cardinality constraints;compressed sensing;convex optimization;sparse array;,"""Achieving the minimum number of radiating elements in the active electronically scanned array synthesis represents a key problem in those applications where the power consumption, the weight, and the hardware/software complexity of the radiating system have a strong impact on the system cost. In this paper, a new method for the synthesis of planar, nonsuperdirective, maximally sparse, and steerable arrays is proposed and tested on several benchmarks available in the open literature. The method optimizes simultaneously the weight coefficients and sensor positions of a planar array without using global optimization schemes, properly exploiting convex optimization based algorithms. The resulting arrays are able to radiate a steerable beam pattern, satisfying a prescribed power mask and avoid to constraint the fitting of any a priori assigned reference field pattern. Several tests are presented to show the high efficiency in achieving the desired steerable radiation pattern with the minimum number of antenna elements.""",IEEE Transactions on Antennas and Propagation
"""Adding user unique capabilities to 'LtoM'""",J. Singleton; J. Mikolaj;,2000,10.1109/AUTEST.2000.885595,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=885595,;,"""The introduction of the M9-Series test instrument and the Re-Configurable Transportable Consolidated Automated Support System (RTCASS) required the development of a process to migrate CASS L-Series Digital Test Program code to the RTCASS M-Series Instrument. The RTCASS DTI development team utilized Teradyne LtoM Translation Software, supporting CShell dynamic linked library and user written support software to produce an L-Series like ANSI-C Test Program. This paper presents the process used by the RTCASS DTI development team to meet L-Series software and hardware requirements on WinNT and M-Series instrument.""",2000 IEEE Autotestcon Proceedings. IEEE Systems Readiness Technology Conference. Future Sustainment for Military Aerospace (Cat. No.00CH37057)
"""A constrained sparse representation approach for video anomaly detection""",Yugen Yi; Xiaohui Li; Rui Zhao; Chao Bi; Jianzhong Wang; Hui Sun;,2016,10.1109/IMCEC.2016.7867110,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7867110,local geometrical structure;sparse representation;abnormal event detection;,"""Sparse representation based anomaly detection algorithms have received a widely interest in recent years. However, most of the existing approaches fail to pay attention to the manifold structure of the video data, which has been pointed to be important for data representation. To overcome this limitation, we develop a new sparse coding algorithm named constrained sparse representation (CSR) for video anomaly detection, which explicitly takes the manifold structure of data into account. CSR assumes that each sample's sparse coding coefficient can be linearly reconstructed by the coding coefficients of its neighbors. Therefore, CSR algorithm can obtain relatively smoothly sparse representations along the manifold of data. We apply the proposed CSR model to video abnormal event detection task and conduct extensive experiments on the UMN database. The experimental results demonstrate that our proposed CSR model performed better than some related algorithms.""","2016 IEEE Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)"
"""Adaptive Gaussian Regularization Constrained Sparse Subspace Clustering for Image Segmentation""",S. Song; D. Ren; Z. Jia; F. Shi;,2024,10.1109/ICASSP48485.2024.10446289,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10446289,SSC;Adaptive;l1-norm;Gaussian Regularization;Image Segmentation;,"""Sparse Subspace Clustering (SSC) is integral to image processing, drawing from spectral clustering foundations. However, prevalent methods, relying on an l1-norm constraint, fail to capture nuanced inter-region correlations, affecting segmentation efficacy. To remedy this, we introduce an Adaptive Gaussian Regularization Constrained SSC for enhanced image segmentation. This method begins with superpixel preprocessing to enrich local information. Given the Gaussian nature of the SSC’s sparse coefficient matrix, a Gaussian probability density function is infused as a regularization term, reinforcing regional image ties and facilitating similarity matrix creation. Using spectral clustering, we then define superpixel clusters leading to the final segmentation. When tested against the BSDS500 and SBD datasets and other leading algorithms, our model showcases marked improvements in natural image segmentation.""","ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
"""3D ear identification using LC-KSVD and local histograms of surface types""",Lida Li; L. Zhang; H. Li;,2015,10.1109/ICME.2015.7177475,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7177475,3D ear;surface type;local histogram;LC-KSVD;sparse representation;,"""In this paper, we propose a novel 3D ear classification scheme, making use of the label consistent K-SVD (LC-KSVD) framework. As an effective supervised dictionary learning algorithm, LC-KSVD learns a compact discriminative dictionary for sparse coding and a multi-class linear classifier simultaneously. To use LC-KSVD, one key issue is how to extract feature vectors from 3D ear scans. To this end, we propose a block-wise statistics based scheme. Specifically, we divide a 3D ear ROI into blocks and extract a histogram of surface types from each block; histograms from all blocks are concatenated to form the desired feature vector. Feature vectors extracted in this way are highly discriminative and are robust to mere misalignment. Experimental results demonstrate that the proposed approach can achieve much better recognition accuracy than the other state-of-the-art methods. More importantly, its computational complexity is extremely low at the classification stage.""",2015 IEEE International Conference on Multimedia and Expo (ICME)
"""Sparse Bayesian Learning-Based Seismic Denoise by Using Physical Wavelet as Basis Functions""",L. Deng; S. Yuan; S. Wang;,2017,10.1109/LGRS.2017.2745564,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8036194,Denoising;physical wavelet;seismic data;sparse Bayesian learning (SBL);sparse representations;,"""Attenuating random noise is a fundamental yet necessary step for subsequent seismic image processing and interpretation. We introduce a sparse Bayesian learning (SBL)-based seismic denoise method by using the physical wavelet as the basis function. The physical wavelet estimated from seismic and well logging data can appropriately describe the characteristics of the seismic data. Thus, it is an appropriate choice of basis function. Moreover, the tradeoff regularization parameter for determining denoise quality can be adaptively estimated according to the updated data misfit and sparseness degree during the iterative process of the SBL algorithm. The motivation behind the denoise method using sparse representations is that seismic signals can be sparsely represented by using several physical wavelets, whereas noise cannot. Both synthetic and real seismic data examples are adopted to demonstrate the effectiveness of the method.""",IEEE Geoscience and Remote Sensing Letters
"""Research on Least Squares Support Vector Machine Combinatorial Optimization Algorithm""",L. Taian; W. Yunjia; L. Wentong;,2009,10.1109/IFCSTA.2009.116,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5385037,Least squares support vector machine;Sparse method;Combinatorial optimization algorithm;Linear equations least squares support vector machine;,"""LS-SVM(least squares support vector machine) has been widely used in engineering practice. However, the solving of LS-SVM still remains difficult under the condition of large sample. Based on algorithm of combinatorial optimization, this paper put forward the combinatorial optimization least squares support vector machine algorithm. On several different data aggregation of dimensions, the numerical value experiment and comparison are carried out on traditional LS-SVM algorithm, COLS-SVM algorithm and its improvement algorithm. The numerical value test has shown that COLS-SVM algorithm and its improvement algorithm are effective and have certain advantages on time and regression accuracy, compared with traditional LS-SVM algorithm.""",2009 International Forum on Computer Science-Technology and Applications
"""Parallel Sparse LU Factorization With Machine-Learning Method on Multi-core Processors""",J. Zhou; W. Yang; M. Dai; Q. Cai; H. Wang; K. Li;,2021,10.1109/ICSAI53574.2021.9664163,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9664163,Multi-core;NUMA;LU factorization;Sparse matrix;Multifrontal method;Logistic regression;,"""Since the emergence of multi-core systems, many efforts must be taken to make existing software take advantage of these new architectures. We exploit dense matrix kernels and node parallelism in the sparse LU factorization, at the same time, also relying on third-party optimized multithreaded BLAS libraries. We introduce multi-threaded unified management mode and Task parallel optimization, targeting multi-core architectures. Our approach avoids a deep redesign and fully benefits from the numerical kernels and features of the original code. In this context, we propose simple approaches to take advantage of NUMA architectures. The performance gains are analyzed in detail on test problems, compared with MKL libraries.""",2021 7th International Conference on Systems and Informatics (ICSAI)
"""Signal Processing on Static and Dynamic 3D Meshes: Sparse Representations and Applications""",A. S. Lalos; E. Vlachos; G. Arvanitis; K. Moustakas; K. Berberidis;,2019,10.1109/ACCESS.2019.2894533,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8624249,Signal processing on static and dynamic meshes;sparse representation theory & algorithms;3D geometry acquisition and processing;,"""Nowadays, real-time 3D scanning and reconstruction becomes a requirement for a variety of interactive applications in various fields, including heritage science, gaming, engineering, landscape topography, and medicine. From the introduction of 3D scanning, which allowed the representation of real world or synthetic objects into the virtual world, hardware and software advances have seen tremendous progress. However, despite the continuous improvement of the new generation image sensors and acquisition techniques, the acquired data are often corrupted by the low-frequency noise, outliers, misalignment, missing data, and variations in point density. These effects are amplified if the low-cost sensors and hardware are being used (e.g., mobile devices); thus, the acquisition and communication cost per datum is driven to a minimum. This paper provides a comprehensive review of the ongoing efforts in geometry and signal processing, describing several models from a wide range of signal processing relevant tasks, such as robust principal component analysis, compressive sampling, and matrix completion. Various scalable architectures and optimization algorithms are analyzed and reviewed, revealing significant insights into the fundamental processing operations and the involved implementation tradeoffs. Moreover, the impact of sparse modeling and optimization tools to several 3D mesh processing tasks, such as completion of missing data, feature preserving noise removal, and rejection of outliers, is illustrated via test cases with several constraints posed by the arbitrarily complex animated scenarios. Finally, the identified limitations together with the potential open research directions are also presented for future research efforts toward modeling and optimization for static and dynamic 3D models.""",IEEE Access
"""Application of Sparse Identification of Nonlinear Dynamics (SINDy) Towards Modeling and Prediction of Potential Induced Degradation (PID) Susceptibility of Photovoltaic Modules""",F. F. Batayola; J. C. Jueco; X. J. P. Fernandez; M. A. Bariquit; F. M. M. Nacorda; J. R. H. Villamor; M. A. Radaza; W. M. O. Narvios;,2024,10.1109/ICCMSO61761.2024.00082,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10638362,Photovoltaic;Potential Induced Degradation (PID);Sparse Identification of Nonlinear Dynamics (SINDy);Computational Modelling;Dynamical System;,"""The process of Potential Induced Degradation (PID) occurs when a potential difference exists between the photovoltaic cells and the outer part of the modules, resulting in leakage currents and rapid temperature rise, eventually resulting in system failure and power losses. Using Sparse Identification of Nonlinear Dynamical Systems (SINDy), the paper developed a PID Susceptibility Prediction Model for photovoltaic (PV) modules. Our goal is to elucidate the nonlinear dynamics and interactions among key parameters contributing to PID susceptibility. PID susceptibility was determined to be significantly affected by temperature, relative humidity, voltage bias, and leakage current based on a thorough literature review. The model captures the nonlinear dynamics and interactions among the factors contributing to PID susceptibility with an MAE ≈ 0.35 and RMSE = 0.40. Temperature, relative humidity, voltage bias, and leakage current all significantly affected PID susceptibility. Temperature and relative humidity have the greatest influence, but their interactions are equally important. Voltage bias has a relatively small effect on PID degradation susceptibility, but it cannot be ignored. Experiment results demonstrate substantial proof to support the effectiveness of Sparse Identification of Nonlinear Dynamics (SINDy) to capture system dynamics and provide valuable tools for accurate predictions.""","2024 3rd International Conference on Computational Modelling, Simulation and Optimization (ICCMSO)"
"""WPU: A FPGA-based Scalable, Efficient and Software/Hardware Co-design Deep Neural Network Inference Acceleration Processor""",X. Xie; C. Wu;,2021,10.1109/HPBDIS53214.2021.9658451,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9658451,FPGA;Convolutional Neural Network;Sparse;Processor;,"""Convolutional neural network (CNN) based algorithms are becoming dominant in a broad range of applications. Due to the high computation requirement, accelerator design for CNN has drawn many research interests in recent years. Besides GPU and ASIC, FPGA is also a major resource for CNN computation due to its re-programmability and flexibility for architecture designs [9].In this paper, we propose a software-hardware combined CNN accelerator design for FPGAs. Our focus is on architecture design for computation efficiency per DSP and the adaptability for different sizes of FPGAs. We designed a scheme of network pruning algorithm which enables simpler sparse data selection logic in hardware. Our software-hardware combined solution achieves 4x computation speedup with the same number of DSPs. Full pipeline of data transferring and computing reduces the requirement of BRAM usage significantly. Our test on Yolo-v2 [8] network shows a DSP efficiency enhancement of 3x over RTX 2070 GPU and 58% over an existing FPGA design in [10], respectively.""",2021 International Conference on High Performance Big Data and Intelligent Systems (HPBD&IS)
"""Improving the locality of the sparse matrix-vector product on shared memory multiprocessors""",J. C. Pichel; D. B. Heras; J. C. Cabaleiro; F. F. Rivera;,2004,10.1109/EMPDP.2004.1271429,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1271429,;,"""We extend a model of locality and the subsequent process of locality improvement previously developed for the case of sparse algebra codes in monoprocessors to the case of NUMA shared memory multiprocessors (SMPs). In particular the product of a sparse matrix by a dense vector (SpM/spl times/V) is studied. In the model, locality is established at run-time considering parameters that describe the structure of the sparse matrix involved in the computations. The problem of increasing the locality is formulated as a graph problem, whose solution indicates some appropriate reordering of rows and columns of the sparse matrix. The reordering algorithms were tested for a broad set of matrices. We have also performed a comparison with other reordering algorithms. The results lead to general conclusions about improving SMP performance for other sparse algebra codes.""","12th Euromicro Conference on Parallel, Distributed and Network-Based Processing, 2004. Proceedings."
"""Cloud-Based Actor Identification With Batch-Orthogonal Local-Sensitive Hashing and Sparse Representation""",G. Gao; C. H. Liu; M. Chen; S. Guo; K. K. Leung;,2016,10.1109/TMM.2016.2579305,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7488194,Actor identification;cloud computing;locality-sensitive hashing;shot boundary detection;sparse representation;,"""Recognizing and retrieving multimedia content with movie/TV series actors, especially querying actor-specific videos in large scale video datasets, has attracted much attention in both the video processing and computer vision research field. However, many existing methods have low efficiency both in training and testing processes and also a less than satisfactory performance. Considering these challenges, in this paper, we propose an efficient cloud-based actor identification approach with batch-orthogonal local-sensitive hashing (BOLSH) and multi-task joint sparse representation classification. Our approach is featured by the following: 1) videos from movie/TV series are segmented into shots with the cloud-based shot boundary detection; 2) while faces in each shot are detected and tracked, the cloud-based BOLSH is then implemented on these faces for feature description; 3) the sparse representation is then adopted for actor identification in each shot; and 4) finally, a simple application, actor-specific shots retrieval is realized to verify our approach. We conduct extensive experiments and empirical evaluations on a large scale dataset, to demonstrate the satisfying performance of our approach considering both accuracy and efficiency.""",IEEE Transactions on Multimedia
"""Low Sidelobe Optimization Design of Conformal Sparse Array Antenna using Active Pattern""",L. Tang; X. Wang; Y. Lu; J. Ren; J. Qu;,2023,10.1109/CSRSWTC60855.2023.10426817,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10426817,reconfigurable;low sidelobe;conformal sparse array;weighted random sampling;,"""Aiming at the optimization problem of the sidelobe level of conformal sparse array antenna, a design method based on weighted random sampling and multi-population genetic algorithm is proposed in this paper. Firstly, under the condition of full array of conformal array antenna, the elements that have little influence on the radiation characteristics of the array is sparsified. Then, beam scanning and sidelobe level optimization are carried out on the sparse conformal array antenna. Due to the conformal and sparse requirements of the array, the radiation characteristics of each array element in the array antenna are different. The Active Element Pattern (AEP) of each array antenna unit is obtained by using ALTAIR FEKO software. In order to improve the optimization accuracy and efficiency, the genetic algorithm and the multi-population cooperative mechanism is used to optimize the array antenna unit. The radiation characteristics of the conformal sparse array antenna calculated by this method are in good agreement with the test results and have strong engineering application value.""",2023 Cross Strait Radio Science and Wireless Technology Conference (CSRSWTC)
"""SoC-FPGA implementation of the sparse fast fourier transform algorithm""",A. López-Parrado; J. Velasco-Medina;,2017,10.1109/MWSCAS.2017.8052875,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8052875,Sparse Fast Fourier Transform;SoC-FPGA;RTOS;DMA;,"""This brief presents the SoC-FPGA implementation of the modified Nearly Optimal Sparse Fast Fourier Transform (sFFT) algorithm. The implementation was carried out by using hardware/software co-design based on software profiling that helped to find out that pseudo-random Spectral Permutation, Windowing, and Sub-Sampling (SPWS) are the signal processing operations that require most processing time in the modified sFFT algorithm. Then, by considering the software profiling results, a SPWS hardware accelerator was designed by using structural and generic VHDL. The SPWS hardware accelerator is composed of one Random Sampling Direct Memory Access Controller (RS-DMAC) and one Windowing and Sampling (WS) circuit. Later, the SPWS is integrated into the FPGA fabric of the SoC-FPGA to accelerate the whole modified sFFT algorithm. In this case, the software sub-system is managed by the Real Time Operating System (RTOS) QNX Neutrino. Finally, the verification results showed that 4.6 times acceleration is achieved for the SPWS, and 3.1 times acceleration is achieved for the whole modified sFFT algorithm when it is compared with the fully software implementation.""",2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)
"""Gradient-Guided Test Case Generation for Image Classification Software""",S. Ji; Y. Li; H. Dong; M. Xiao; P. Zhang; M. Zhang;,2024,10.1109/COMPSAC61105.2024.00164,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10633408,Image Classification Software;Software Testing;Test Case Generation;Sparse Perturbation;,"""The widespread use of deep neural networks (DNNs) in image classification sofwares underlines the importance of the robustness. Researchers have proposed sparse adversarial attack methods for generating test cases, which add pixel-level perturbations to construct the test case to mislead the target model. However, the existing methods have certain limitations, such as high time cost, poor flexibility, and poor quality of the test cases. To address these issues, we propose a gradient-guided test case generation method (GGTM) to evaluate the robustness of image classification software. The method firstly identifies the key region in the image based on the gradient-weighted class activation mapping (Grad-CAM) and the prediction confidence of the target model on the input image. In the key region, it selects a set of pixels as candidate perturbation pixels according to the gradient value and the change of loss function. Then perturbations are added to the candidate perturbation pixels after applying a random dropout strategy to reduce some candidate perturbation pixels which is used to avoid local optimum. For the initially constructed test case which can mislead the target model, after removing redundant and unimportant perturbations, perturbations are re-added to optimize the test case. Experiments show the effectiveness of GGTM, which achieves 100% attack success rate. And the test cases generated by GGTM have the best perturbation sparsity. Furthermore, compared with the baseline method SparseAG which achieves optimal perturbation sparsity among the baseline methods, GGTM significantly improves the efficiency.""","2024 IEEE 48th Annual Computers, Software, and Applications Conference (COMPSAC)"
"""Efficient Multi-interval Attention Retractable Network for Image Compressed Sensing""",J. Wu; X. Wu;,2024,10.1109/NNICE61279.2024.10498907,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10498907,compressed sensing;Efficient Multi-interval Attention Retractable Networks (EMARNet);Dense Attention Block(DAB);Group-wise Multi-interval Sparse Attention Block(GM-SAB);,"""Recently, many algorithms employing deep neural networks have been developed for image compressed sensing (CS) and have made significant strides in reconstructing image details. However, the performance of DNN-based reconstruction methods is limited by the fact that the inherent inductive bias of convolutional operations prevents them from capturing long-range dependencies of images. In this work, we propose an Efficient Multi-interval Attention Retractable Network (EMARNet) for CS image reconstruction. EMARNet employs a network architecture composed of multiple residual Transformer blocks as the core of the reconstruction process. Specifically, each Transformer block comprises a sequence of interconnected Dense Attention Block (DAB) and Group-wise Multi-interval Sparse Attention Block (GM-SAB), alongside a residual connection. The cascaded DABs and GM-SABs can effectively capture the global information and achieve better reconstruction quality. Experimental results on diverse benchmark datasets confirm the superior performance of our proposed approach compare to state-of-the-art CS methods.""","2024 4th International Conference on Neural Networks, Information and Communication Engineering (NNICE)"
"""Cuper: Customized Dataflow and Perceptual Decoding for Sparse Matrix-Vector Multiplication on HBM-Equipped FPGAs""",E. Yi; Y. Duan; Y. Bai; K. Zhao; Z. Jin; W. Liu;,2024,10.23919/DATE58400.2024.10546672,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546672,Sparse Matrix- Vector Multiplication;FPGAs;HBM;Accelerator;,"""Sparse matrix-vector multiplication ($S$pMV) is pivotal in many scientific computing and engineering applications. Considering the memory-intensive nature and irregular data access patterns inherent in SpMV, its acceleration is typically bounded by the limited bandwidth. Multiple memory channels of the emerging high bandwidth memory (HBM) provide exceptional bandwidth, offering a great opportunity to boost the performance of SpMV. However, ensuring high bandwidth utilization with low memory access conflicts is still non-trivial. In this paper, we present Cuper, a high-performance SpMV accelerator on HBM-equipped FPGAs. Through customizing the dataflow to be HBM-compatible with the proposed sparse storage format, the bandwidth utilization can be sufficiently enhanced. Furthermore, a two-step reordering algorithm and perceptual decoder-centric hardware architecture are designed to greatly mitigate read-after-write (RAW) conflicts, enhance the vector reusability and on-chip memory utilization. The evaluation of 12 large matrices shows that Cuper's geomean throughput outperforms the four latest SpMV accelerators HiSparse, GraphLily, Sextans, and Serpens, by 3.28×, 1.99×, 1.75×, and 1.44×, respectively. Furthermore, the geomean bandwidth efficiency shows 3.28×, 2.20×, 2.82×, and 1.31x improvements, while the geomean energy efficiency has 3.59×, 2.08×, 2.21×, and 1.44× optimizations, respectively. Cuper also demonstrates 2.51× throughput and 7.97× energy efficiency of improvement over the K80 GPU on 2,757 SuiteSparse matrices.""","2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
"""A Deep Learning Model via Long Short Term Memory for Voltage Sag Location in Sparsely Monitored System""",Y. Deng; H. Jia; S. Lin; X. Tong; X. Zhang; L. Wang;,2022,10.1109/CAC57257.2022.10055497,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10055497,voltage sag;source location;sparsely monitored system;deep learning;,"""Voltage sag has already been recognized as a critical power quality issue in power system. In fact, not only economic loss but also social impact has been produced due to voltage sag. And hence, voltage sag location is of great importance to taking effective measures, evaluating power quality level, dividing responsibility and constructing harmonious power supply and consumption environment. And hence, a deep learning method via Long Short Term Memory for voltage sag location in power system, which is sparsely monitored is presented. In detail, for the presented model, the input is measured voltage through limited sensors in a sparsely monitored power system, and meanwhile, the output is the detailed line in the whole network. In this study, the data is collected via Matlab software and the algorithm is conducted through TensorFlow tool. The test results through IEEE 30-bus system illustrate that the accuracy of voltage sag location can be achieved with high accuracy.""",2022 China Automation Congress (CAC)
"""Data-driven Symbolic Regression for Identification of Nonlinear Dynamics in Power Systems""",A. M. Stanković; A. A. Sarić; A. T. Sarić; M. K. Transtrum;,2020,10.1109/PESGM41954.2020.9281935,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9281935,Power system;Dynamic model;System identification;Nonlinear dynamics;Sparse model;,"""The paper describes a data-driven system identification method tailored to power systems and demonstrated on models of synchronous generators (SGs). In this work, we extend the recent sparse identification of nonlinear dynamics (SINDy) modeling procedure to include the effects of exogenous signals and nonlinear trigonometric terms in the library of elements. We show that the resulting framework requires fairly little in terms of data, and is computationally efficient and robust to noise, making it a viable candidate for online identification in response to rapid system changes. The proposed method also shows improved performance over linear data-driven modeling. While the proposed procedure is illustrated on a SG example in a multi-machine benchmark, it is directly applicable to the identification of other system components (e.g., dynamic loads) in large power systems.""",2020 IEEE Power & Energy Society General Meeting (PESGM)
"""Enhanced Sparse based Discriminative Topic Representation for Text Categorization""",W. Zheng; W. Zhang; X. Yue; Y. Gao; H. Tang;,2019,10.1109/ICSAI48974.2019.9010289,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010289,Text Categorization;Enhanced Sparse;Topic;Representation;,"""This paper presents an enhanced sparse based discriminative topic representation method for text categorization. By constructing category center vectors and combining with the latent Dirichlet allocation model, a more discriminative dictionary is obtained, which can describe the relationship between topic and word well. Furthermore, an enhanced sparse representation of documents can be generated with a L1/2 regularization in order to achive a good relationship between document and topic. The experimental results show that our proposed approach achieves more stable classification performance and obtains more high sparse degree.""",2019 6th International Conference on Systems and Informatics (ICSAI)
"""SparseMEM: Energy-efficient Design for In-memory Sparse-based Graph Processing""",M. Zahedi; G. Custers; T. Shahroodi; G. Gaydadjiev; S. Wong; S. Hamdioui;,2023,10.23919/DATE56975.2023.10137303,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10137303,in-memory;memristor;graph;sparsity;,"""Performing analysis on large graph datasets in an energy-efficient manner has posed a significant challenge; not only due to excessive data movements and poor locality, but also due to the non-optimal use of high sparsity of such datasets. The latter leads to a waste of resources as the computation is also performed on zero's operands which do not contribute to the final result. This paper designs a novel graph processing accelerator, SparseMEM, targeting sparse datasets by leveraging the computing-in-memory (CIM) concept; CIM is a promising solution to alleviate the overhead of data movement and the inherent poor locality of graph processing. The proposed solution stores the graph information in a compressed hierarchical format inside the memory and adjusts the workflow based on this new mapping. This vastly improves resource utilization, leading to higher energy and permanence efficiency. The experimental results demonstrate that SparseMEM outperforms a GPU-based platform and two state-of-the-art in-memory accelerators on speedup and energy efficiency by one and three orders of magnitude, respectively.""","2023 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
"""A novel 3D ear identification approach based on sparse representation""",Z. Ding; L. Zhang; H. Li;,2013,10.1109/ICIP.2013.6738858,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6738858,Biometrics;3D ear recognition;sparse representation;Iterative Closest Point;,"""Recently, ear shape has attracted tremendous interests in biometric research due to its richness of feature and ease of acquisition. In this paper, we present a novel 3D ear identification approach based on the sparse representation framework. To this end, at first, we propose a template-based ear detection method. By utilizing such a method, the extracted ear regions are represented in a common standard coordinate system determined by the template, which facilitates the following feature extraction and classification. For each 3D ear, a feature vector can be generated as its representation. With respect to the ear identification, we resort to the l1-minimization based sparse representation. Experiments conducted on a benchmark dataset corroborate the effectiveness and efficacy of the proposed approach. The associated Matlab source code and the evaluation results have been made online available at http://sse.tongji.edu.cn/linzhang/ear/srcear/srcear.htm.""",2013 IEEE International Conference on Image Processing
"""Performance of a Fault Location Method in the Context of Modern Electric Power Distribution Systems""",R. F. P. Paternost; T. S. D. Ferreira; F. C. L. Trindade;,2019,10.1109/ISGT-LA.2019.8894923,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8894923,Distributed generation;fault location;OpenDSS;power distribution systems;sparse voltage measurements;,"""This work performs tests in a fault location method based on sparse voltage measurements considering modern scenarios of electric power distribution systems, in which a large penetration of distributed generation is expected. These kinds of technologies also could provide data that is currently not available for power distribution utilities and can support the fault location process. The technique used in this paper had a slight modification compared to the original one to enable the use of OpenDSS software for power flow calculation. This software facilitates the use of the fault location technique in practical application scenarios because it is a very robust software and largely used by power distribution utilities.""",2019 IEEE PES Innovative Smart Grid Technologies Conference - Latin America (ISGT Latin America)
"""Online Informative Path Planning Using Sparse Gaussian Processes""",R. Mishra; M. Chitre; S. Swarup;,2018,10.1109/OCEANSKOBE.2018.8559183,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8559183,;,"""Estimating the environmental fields for large survey areas is a difficult task, primarily because of the field's spatio-temporal nature. A good approach in performing this task is to do adaptive sampling using robots. In such a scenario, robots have limited time to collect data before the field varies significantly. In this paper, we suggest an algorithm, AdaPP, to perform this task of data collection within a constraint on sampling time and provide an approximation of the environmental field. We test our performance against conventional sampling paths and show that we are able to obtain a good approximation of the field within the stipulated time.""",2018 OCEANS - MTS/IEEE Kobe Techno-Oceans (OTO)
"""Bayesian Hypothesis Testing for Block Sparse Signal Recovery""",M. Korki; H. Zayyani; J. Zhang;,2016,10.1109/LCOMM.2016.2518169,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7383259,Block-sparse;Bayesian hypothesis testing;Bernoulli-Gaussian hidden Markov model;Block-sparse;Bayesian hypothesis testing;Bernoulli–Gaussian hidden Markov model;,"""A novel block Bayesian hypothesis testing algorithm (BBHTA) is presented for reconstructing block-sparse signals with unknown block structures. The BBHTA detects and recovers the supports and then estimates the amplitudes of block sparse signal. The support detection and recovery are performed by a Bayesian hypothesis testing. Using the detected and reconstructed supports, the nonzero amplitudes are then estimated by linear minimum mean-square error estimation. Numerical experiments demonstrate the effectiveness of BBHTA.""",IEEE Communications Letters
"""Rejection of Smooth GPS Time Synchronization Attacks via Sparse Techniques""",E. Schmidt; J. Lee; N. Gatsis; D. Akopian;,2021,10.1109/JSEN.2020.3014239,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9157959,Global Positioning System (GPS);higher-order derivative;sparsity;spoofing detection;time synchronization attacks (TSAs);,"""This article presents a novel time synchronization attack (TSA) model for the Global Positioning System (GPS) based on clock data behavior changes in a higher-order derivative domain. Further, TSA rejection and mitigation based on sparse domain (TSARM-S) is presented. TSAs affect stationary GPS receivers in applications where precise timing is required, such as cellular communications, financial transactions, and monitoring of the electric power grid. In the present work, higher-order derivatives of the clock bias and clock drift are monitored to reveal TSAs that show up as sparse spike-like events. The smoothness of the attack relates to the derivative order where the sparsity is observed. The proposed method jointly estimates a dynamic solution for GPS timing and rejects clock behavior changes based on such sparse events. An evaluation procedure is presented for two testbeds, namely a commercial receiver and a software-defined radio (SDR). Further, the proposed method is evaluated against real spoofing scenarios available online in the Texas Spoofing Test Battery (TEXBAT). Combined synthetic and real-data results show an average RMS clock bias error of 12.08 m for the SDR platform, and 45.74 m for the commercial device. Furthermore, the technique is evaluated against state-of-the-art mitigation techniques and in a spoofing-plus-multipath scenario for robustness. Finally, TSARM-S can be potentially optimized and implemented in commercial devices via a firmware upgrade.""",IEEE Sensors Journal
"""Evaluating Open-Universe Face Identification on the Web""",B. C. Becker; E. G. Ortiz;,2013,10.1109/CVPRW.2013.133,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6595978,large-scale;open-universe;open set;face identification;sparse representation;,"""Face recognition is becoming a widely used technique to organize and tag photos. Whether searching, viewing, or organizing photos on the web or in personal photo albums, there is a growing demand to index real-world photos by the subjects in them. Even consumer platforms such as Google Picasa, Microsoft Photo Gallery, and social network sites such as Facebook have integrated forms of automated face tagging and recognition, furthermore, a number of libraries and cloud-based APIs that perform face recognition have become available. With such a plethora of choices, comparisons of recent advances become more important to gauge the state of progress in the field. This paper evaluates face identification in the context of not only research algorithms, but also considers consumer photo products, client-side libraries, and cloud-based APIs on a new, large-scale dataset derived from PubFig83 and LFW in a realistic open-universe scenario.""",2013 IEEE Conference on Computer Vision and Pattern Recognition Workshops
"""Hardware Development for Joint Sparse Decentralized Heterogeneous Data Fusion for Target Estimation""",D. Shen; M. Guo; G. Chen; R. Niu; P. Zulch; E. Blasch;,2022,10.1109/AERO53065.2022.9843789,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9843789,;,"""For surveillance applications, users are typically interested in detecting and tracking targets of interest. In many surveillance systems, heterogeneous sensor data are collected by sensors of varying sensing modalities with different dimensionalities. However, individual sensing system collects only one kind of signal, leading to a lack of accuracy to characterize the target. One of the most popular approaches to overcome single sensor assessment is data fusion. In our recent work, a new joint-sparse data-level fusion (JSDLF) approach to integrate heterogeneous sensor data for target discovery is developed in the matter of using a decentralized architecture algorithm. Several decentralized implementations of the data-level fusion approach based on the JSDLF approach were developed and investigated in the software aspect. In this paper, the hardware development of this algorithm will be displayed. Several unmanned aerial vehicles (UAVs) were utilized as nodes for image or RF SIGINT data processing. Meanwhile, a computer with RF SIGINT data processing communicated with these nodes as decentralized architecture as the hardware development of this algorithm. The performance of the decentralized hardware development is represented in this paper and the results are comparable with only software development in the algorithm.""",2022 IEEE Aerospace Conference (AERO)
"""Towards A Clustering Guided Rule Interpolation for ANFIS Construction""",J. Yang; T. Chen; L. Chen; J. Zhao;,2024,10.1109/FUZZ-IEEE60900.2024.10612196,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10612196,Fuzzy rule interpolation;Clustering;ANFIS construction;Sparse training data;,"""How to construct an effective ANFIS (Adaptive Network-based Fuzzy Inference System) with insufficient (sparse) training data is a challenging problem, as the rule base of such an ANFIS model will be sparse. Fuzzy rule interpolation technique enables fuzzy inference to be performed over a sparse rule base, so it is natural to introduce FRI to support the ANFIS construction. In this work, a new clustering guided rule interpolation approach is proposed for the ANFIS construction problem. Different with most existing FRI based ANFIS construction methods that commonly conduct rule interpolation at individual rule level, the proposed method makes the interpolation to be performed on a cluster level. It adopts a clustering strategy to guide the rule selection and rule weights calculation processes, ensuring the rule similarity and diversity at the same time. Particularly, the proposed approach firstly generates a rule dictionary and divides it into several clusters. Following that a cluster guided method is designed for automated selection of relevant rules from each cluster to be included in subsequent interpolation process. Then the weight for each selected rule is calculated by considering both the cluster size and the cluster distance. Experimental results against benchmark regression datasets indicate the effectiveness of the proposed approach.""",2024 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE)
"""Hardness-Aware Dictionary Learning: Boosting Dictionary for Recognition""",L. Wang; S. Li; S. Wang; D. Kong; B. Yin;,2021,10.1109/TMM.2020.3017916,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9173647,Sparse representation;classification;AdaBoost;dictionary learning;,"""Sparse representation is a powerful tool in many visual applications since images can be represented effectively and efficiently with a dictionary. Conventional dictionary learning methods usually treat each training sample equally, which would lead to the degradation of recognition performance when the samples from same category distribute dispersedly. This is because the dictionary focuses more on easy samples (known as highly clustered samples), and those hard samples (known as widely distributed samples) are easily ignored. As a result, the test samples which exhibit high dissimilarities to most of intra-category samples tend to be misclassified. To circumvent this issue, this paper proposes a simple and effective hardness-aware dictionary learning (HADL) method, which considers training samples discriminatively based on the AdaBoost mechanism. Different from learning one optimal dictionary, HADL learns a set of dictionaries and corresponding sub-classifiers jointly in an iterative fashion. In each iteration, HADL learns a dictionary and a sub-classifier, and updates the weights based on the classification errors given by current sub-classifier. Those correctly classified samples are assigned with small weights while those incorrectly classified samples are assigned with large weights. Through the iterated learning procedure, the hard samples are associated with different dictionaries. Finally, HADL combines the learned sub-classifiers linearly to form a strong classifier, which improves the overall recognition accuracy effectively. Experiments on well-known benchmarks show that HADL achieves promising classification results.""",IEEE Transactions on Multimedia
"""Human action recognition based on sparse representation induced by L1/L2 regulations""",Z. Gao; A. -a. Liu; H. Zhang; G. -p. Xu; Y. -b. Xue;,2012,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6460518,;,"""Sparse representation based classification (SRC) has been widely used for face recognition (FR). Although SRC algorithm is also adopted in human action recognition, the evaluations of different regular terms have not been given. In this paper, we will discuss and evaluate the role of different regular terms of SRC in human action recognition, after that, we propose human action recognition algorithm based on sparse representation induced by L1 and L2 regulations - called SR-L12. Experiments on well known KTH action dataset show that SR-L12 is much better than that of nearest neighbor (NN), nearest subspace (NS), full-space (NF), SRC and collaborative representation classification (CRC). Moreover, the proposed method is comparable to most of state-of-the-art algorithms for human action recognition.""",Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)
"""HTM Spatial Pooler With Memristor Crossbar Circuits for Sparse Biometric Recognition""",A. P. James; I. Fedorova; T. Ibrayev; D. Kudithipudi;,2017,10.1109/TBCAS.2016.2641983,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7865953,Crossbar architecture;face recognition;hierarchical temporal memory;memristor;neuromorphic design;spatial pooler;speech recognition;,"""Hierarchical Temporal Memory (HTM) is an online machine learning algorithm that emulates the neo-cortex. The development of a scalable on-chip HTM architecture is an open research area. The two core substructures of HTM are spatial pooler and temporal memory. In this work, we propose a new Spatial Pooler circuit design with parallel memristive crossbar arrays for the 2D columns. The proposed design was validated on two different benchmark datasets, face recognition, and speech recognition. The circuits are simulated and analyzed using a practical memristor device model and 0.18 μm IBM CMOS technology model. The databases AR, YALE, ORL, and UFI, are used to test the performance of the design in face recognition. TIMIT dataset is used for the speech recognition.""",IEEE Transactions on Biomedical Circuits and Systems
"""A Fine-Grained Parallel Power Flow Method for Large Scale Grid Based on Lightweight GPU Threads""",H. Jiang; D. Chen; Y. Li; R. Zheng;,2016,10.1109/ICPADS.2016.0107,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7823822,power flow study;parallel computing;GPU;sparse linear system;path tree;,"""This paper proposes a parallel Newton-Raphson Power Flow (PNPF) method which is suitable for GPU (Graphics Processing Unit). Aiming at the most time-consuming part of power flow-solving the sparse linear equations, an efficient hierarchy parallel solver is presented, in which LU decomposition and forward/back substitution were implemented in parallel level by level under the direction of path tree. To improve the efficiency of the method, a short-type path tree was formed, an optimization on data transfer and a self-adapted task-allocation algorithm was designed. In addition, the sparse Jacobian matrix and the right-hand side of the linear equations were also generated in parallel in a fine-grained pattern. At last, the developed GPU-based PNPF program has been tested on large-scale power systems of up to 23215 buses. The method provides a speedup of 3.91 times compared to mainstream commercial software, which proves the effectiveness and practicality of the algorithm proposed in this paper.""",2016 IEEE 22nd International Conference on Parallel and Distributed Systems (ICPADS)
"""Sparse Kernels for Bayes Optimal Discriminant Analysis""",O. C. Hamsici; A. M. Martinez;,2007,10.1109/CVPR.2007.383409,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4270407,;,"""Discriminant Analysis (DA) methods have demonstrated their utility in countless applications in computer vision and other areas of research - especially in the C class classification problem. The most popular approach is linear DA (LDA), which provides the C - 1-dimensional Bayes optimal solution, but only when all the class covariance matrices are identical. This is rarely the case in practice. To alleviate this restriction, Kernel LDA (KLDA) has been proposed. In this approach, we first (intrinsically) map the original nonlinear problem to a linear one and then use LDA to find the C - 1-dimensional Bayes optimal subspace. However, the use of KLDA is hampered by its computational cost, given by the number of training samples available and by the limitedness of LDA in providing a C - 1-dimensional solution space. In this paper, we first extend the definition of LDA to provide subspace of q < C - 1 dimensions where the Bayes error is minimized. Then, to reduce the computational burden of the derived solution, we define a sparse kernel representation, which is able to automatically select the most appropriate sample feature vectors that represent the kernel. We demonstrate the superiority of the proposed approach on several standard datasets. Comparisons are drawn with a large number of known DA algorithms.""",2007 IEEE Conference on Computer Vision and Pattern Recognition
"""Cox-PASNet: Pathway-based Sparse Deep Neural Network for Survival Analysis""",J. Hao; Y. Kim; T. Mallavarapu; J. H. Oh; M. Kang;,2018,10.1109/BIBM.2018.8621345,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8621345,Cox-PASNet;Deep neural network;Survival analysis;Glioblastoma multiforme;,"""An in-depth understanding of complex biological processes associated to patients’ survival time at the cellular and molecular level is critical not only for developing new treatments for patients but also for accurate survival prediction. However, highly nonlinear and high-dimension, low-sample size (HDLSS) data cause computational challenges in survival analysis. We developed a novel pathway-based, sparse deep neural network, called Cox-PASNet, for survival analysis by integrating highdimensional gene expression data and clinical data. Cox-PASNet is a biologically interpretable neural network model where nodes in the network correspond to specific genes and pathways, while capturing nonlinear and hierarchical effects of biological pathways to a patient’s survival. We also provide a solution to train the deep neural network model with HDLSS data. Cox-PASNet was evaluated by comparing the performance of different cutting-edge survival methods such as Cox-nnet, SurvivalNet, and Cox elastic net (Cox-EN). Cox-PASNet significantly outperformed the benchmarking methods, and the outstanding performance was statistically assessed. We provide an open-source software implemented in PyTorch (https://github.com/DataX-JieHao/Cox-PASNet) that enables automatic training, evaluation, and interpretation of Cox-PASNet.""",2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)
"""Fault diagnosis for sparsely interconnected multiprocessor systems""",D. M. Blough; G. F. Sullivan; G. M. Masson;,1989,10.1109/FTCS.1989.105544,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=105544,;,"""The authors present a general approach to fault diagnosis that is widely applicable and requires only a limited number of connections among units. Each unit in the system forms a private opinion on the status of each of its neighboring units based on duplication of jobs and comparison of job results over time. A diagnosis algorithm that consists of simply taking a majority vote among the neighbors of a unit to determine the status of that unit is then executed. The performance of this simple majority-vote diagnosis algorithm is analyzed using a probabilistic model for the faults in the system. It is shown that with high probability, for systems composed of n units, the algorithm will correctly identify the status of all units when each unit is connected to O(log n) other units. It is also shown that the algorithm works with high probability in a class of systems in which the average number of neighbors of a unit is constant. The results indicate that fault diagnosis can in fact be achieved quite simply in multiprocessor systems containing a low to moderate number of testing conditions.<>""",[1989] The Nineteenth International Symposium on Fault-Tolerant Computing. Digest of Papers
"""OSBulk: Optimal Sparse Bulk Transfer""",J. Cheng; A. Tang;,2022,10.1109/ICC45855.2022.9838579,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9838579,Enterprise Networks;Network Virtualization;Software-Defined Networking;Transport layer;Wide Area Networks;,"""Motivated by the need to accelerate bulk transfers (i.e., flows which transfer files with large sizes) over WAN enterprise networks, we present OSBulk, a system which optimizes sparse bulk transfers over a given network. OSBulk allocates bandwidth to bulk transfers according to an optimal solution to any given multi-commodity flow (MCF) formulation. The key components of OSBulk include logical network provision which sets flow rates and Multiple Path Protocol (MPP) which specifies flow paths. OSBulk can dynamically react to varying network conditions by logical network reprovision and MPP reconfiguration. Evaluation demonstrates that OSBulk is able to achieve near optimal throughput, which improves over the performance of TCP and MPTCP by 1.1 - 2.1× in cloud testing.""",ICC 2022 - IEEE International Conference on Communications
"""Broadband Wide Beam Dual Polarization Antenna Unit and Its Sparse Array""",X. Jin; S. Wang; Y. Liu;,2018,10.1109/ISAPE.2018.8634324,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634324,broadband;wide beam;dual polarization;sparse array;scanning angle;,"""This paper presents a new type of wide-bandwidth beam antenna, which uses a relatively new type of cross-type microstrip dipole. This form has a wider -3dB beamwidth than ordinary microstrip antennas. The radiation profile uses a gradation profile and widens the bandwidth of the antenna by adding parasitic structures and increasing the air frame height. Antenna structure parameters are optimized using ANSYS HFSS. What the test results display is that the 1 port and 2 port standing wave's relative bandwidth <;-7.5dB of designed antenna unit reach 35.9% and 22.8%, the central frequency E-plane and H-plane beam width are 110 ° and 91 °, respectively, which facilitates large-angle scanning. This antenna is an orthogonal two-line polarized antenna with a zenith-directional line polarization cross-polarization level of -19.55 dB. With the help of software Matlab, we use sparse Focuss algorithm obtain the antenna array composed of 64 units. The gain reaches 19.61dB when the scanning angle is 60°. It was verified that this antenna element and sparse array have good characteristics.""","2018 12th International Symposium on Antennas, Propagation and EM Theory (ISAPE)"
"""Towards Building A Robust Large-Scale Bangla Text Recognition Solution Using A Unique Multiple-Domain Character-Based Document Recognition Approach""",A. S. A. Rabby; M. M. Islam; Z. Islam; N. Hasan; F. Rahman;,2021,10.1109/ICMLA52953.2021.00225,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680139,OCR;Document Processing;Handwriting;Segmentation;Recognition;,"""Bangla is one of the world’s top ten popular languages in terms of the number of speakers. It also happens to have a complex script primarily because of complex characters, e.g. graphemes, composed of multiple single characters, and the characteristic short-hands, e.g. vowel diacritics, and consonant diacritics making the number of classes of this script recognition quite large, varied, and challenging. In this paper, we present a unique large-scale Bangla document OCR solution based on character-level recognition modules. We have tested our approach on two independent domains: printed and handwritten documents. We also applied our solution to three subdomains within the printed domain: computer-composed documents, letterpress documents, and typewritten documents. Our extensive experiments show that our approach achieves state-of-the-art performance on handwritten and printed documents.""",2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)
"""Neural Enhanced Variational Bayesian Inference on Graphs for Localized Statistical Channel Modeling""",Y. Wang; S. Zhang; Y. Xue; T. Yu; Q. Shi; T. -H. Chang;,2024,10.1109/ICC51166.2024.10622313,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10622313,Channel modeling;graph neural network;sparse recovery;variational Bayesian inference;,"""This paper proposes an innovative graph neural network (GNN)-based approach to address the challenge of recovering ill-conditioned sparse signals within the task of multi-grid localized statistical channel modeling (LSCM). Our proposed GNN architecture captures the structural sparsity inherent in the channel angular power spectrum (APS) by leveraging reference signal receiving power (RSRP) measured from multiple grids. It can effectively mitigate the severe coherence in the measurement matrix. Furthermore, we present a novel online unsupervised training scheme that enables real-time adaptability for multi-grid LSCM applications. Through extensive simulations, we demonstrate the superior performance of our GNN-based method in the context of multi-grid LSCM, showcasing its advantages over existing sparse recovery techniques.""",ICC 2024 - IEEE International Conference on Communications
"""Validation of PIM DM and PIM SM protocols in the NS2 network simulator""",T. Bartczak; P. Zwierzykowski;,2009,10.1109/AFRCON.2009.5308360,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5308360,multicast;multicast protocols;Protocol Independent Multicast Dense Mode;Protocol Independent Multicast Sparse Mode;network simulator NS2;software validation;,"""Multicast transmission offers efficient network resource utilization, but, at the same time, it is also a demanding and complex technology. Multicast protocols are far more sophisticated than their unicast counterparts. As a result, building one's own simulation environment is a difficult, time-consuming and error-prone endeavor. Hence, there is a need for ready-made network simulation tools supporting these protocols. One of such tools is the open source NS2 simulator. However, the results obtained from this simulator are not reliable without prior testing and a validation of the application. This work concentrates on validation of the PIM SM and the PIM DM protocols implementation in NS2. The NS2 implementation was tested using a wide range of techniques commonly used in software engineering filed.""",AFRICON 2009
"""Leveraging geometric correlation for input-adaptive facial landmark regression""",Y. Feng; R. Liu; X. Fan; K. Huyan; Z. Luo;,2017,10.1109/ICME.2017.8019469,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8019469,Facial landmark;Regression;Face transfer;Sparse coding;,"""Facial analysis plays very important role in many vision applications, such as authentication and entertainments. The very early works in the 1990s mostly focus on estimating geometric deformations of facial landmarks to address this task. While in the past several years, more and more efforts have been made to directly learn an appearance regression for facial analysis. Though training regressions on controlled facial images can successfully capture the appearance variations, the performance of these appearance-based models are tightly related to the quantity and quality of the training data. In this paper, we develop a novel framework, named geometric correlated landmark regression (GCLR), to inherit the advantages but overcome limitations of these two categories of methods. Specifically, we first establish a landmark-to-landmark regression to estimate the geometry of facial images. By further incorporating a sparse coding term into the regression framework, we can successfully leverage the geometric correlations between the test image and the shape dictionary, thus significantly enhance the geometry regression performance. Experimental results on various challenging facial data sets verify the effectiveness and efficiency of GCLR.""",2017 IEEE International Conference on Multimedia and Expo (ICME)
"""A Novel Parallel Scan for Multicore Processors and Its Application in Sparse Matrix-Vector Multiplication""",N. Zhang;,2012,10.1109/TPDS.2011.174,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5887318,Parallel algorithms;parallel scan;prefix sum;multicore computing;sparse matrix-vector multiplication.;,"""We present a novel parallel algorithm for computing the scan operations on x86 multicore processors. The existing best known parallel scan for the same platform requires the number of processors to be a power of two. But this constraint is removed from our proposed method. In the design of the algorithm architectural considerations for x86 multicore processors are given so that the rate of cache misses is reduced and the cost of thread synchronization and management is minimized. Results from tests made on a machine with dual-socket times quad-core Intel Xeon E5405 showed that the proposed solution outperformed the best known parallel reference. A novel approach to sparse matrix-vector multiplication (SpMV) based on the proposed scan is then explained. The approach, unlike the existing ones that make use of backward segmented operations, uses forward ones for more efficient caching. An implementation of the proposed SpMV was tested against the SpMV in Intel's Math Kernel Library (MKL) and merits were found in the proposed approach.""",IEEE Transactions on Parallel and Distributed Systems
"""Incomplete-Data Oriented Multiview Dimension Reduction via Sparse Low-Rank Representation""",W. Yang; Y. Shi; Y. Gao; L. Wang; M. Yang;,2018,10.1109/TNNLS.2018.2828699,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8360747,Incomplete multiview data;multiview dimension reduction;multiview subspace learning;sparse low-rank representation;sparse multiview feature selection (FS);,"""For dimension reduction on multiview data, most of the previous studies implicitly take an assumption that all samples are completed in all views. Nevertheless, this assumption could often be violated in real applications due to the presence of noise, limited access to data, equipment malfunction, and so on. Most of the previous methods will cease to work when missing values in one or multiple views occur, thus an incomplete-data oriented dimension reduction becomes an important issue. To this end, we mathematically formulate the above-mentioned issue as sparse low-rank representation through multiview subspace (SRRS) learning to impute missing values, by jointly measuring intraview relations (via sparse low-rank representation) and interview relations (through common subspace representation). Moreover, by exploiting various subspace priors in the proposed SRRS formulation, we develop three novel dimension reduction methods for incomplete multiview data: 1) multiview subspace learning via graph embedding; 2) multiview subspace learning via structured sparsity; and 3) sparse multiview feature selection via rank minimization. For each of them, the objective function and the algorithm to solve the resulting optimization problem are elaborated, respectively. We perform extensive experiments to investigate their performance on three types of tasks including data recovery, clustering, and classification. Both two toy examples (i.e., Swiss roll and S-curve) and four real-world data sets (i.e., face images, multisource news, multicamera activity, and multimodality neuroimaging data) are systematically tested. As demonstrated, our methods achieve the performance superior to that of the state-of-the-art comparable methods. Also, the results clearly show the advantage of integrating the sparsity and low-rankness over using each of them separately.""",IEEE Transactions on Neural Networks and Learning Systems
"""$mathcal k$-branching uio sequences for partially specified observable non-deterministic fsms""",K. El-Fakih; R. M. Hierons; U. C. Türker;,2021,10.1109/TSE.2019.2911076,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8691615,Software engineering/software/program verification;software engineering/testing and debugging;software engineering/test design;finite state machine;unique input output sequence generation;general purpose graphics processing units;,"""In black-box testing, test sequences may be constructed from systems modelled as deterministic finite-state machines (DFSMs) or, more generally, observable non-deterministic finite state machines (ONFSMs). Test sequences usually contain state identification sequences, with unique input output sequences (UIOs) often being used with DFSMs. This paper extends the notion of UIOs to ONFSMs. One challenge is that, as a result of non-determinism, the application of an input sequence can lead to exponentially many expected output sequences. To address this scalability problem, we introduce ${mathcal K}$K-UIOs: UIOs that lead to at most ${mathcal K}$K output sequences from states of $M$M. We show that checking ${mathcal K}$K-UIO existence is PSPACE-Complete if the problem is suitably bounded; otherwise it is in EXPSPACE and PSPACE-Hard. We provide a massively parallel algorithm for constructing ${mathcal K}$K-UIOs and the results of experiments on randomly generated and real FSM specifications. The proposed algorithm was able to construct UIOs in cases where the existing UIO generation algorithm could not and was able to construct UIOs from FSMs with 38K states and 400K transitions.""",IEEE Transactions on Software Engineering
"""Unique challenges testing SDRs for space""",D. Chelmins; J. Downey; S. K. Johnson; J. Nappier;,2013,10.1109/AERO.2013.6497339,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6497339,;,"""This paper describes the approach used by the Space Communication and Navigation (SCaN) Testbed team to qualify three Software Defined Radios (SDR) for operation in space and the characterization of the platform to enable upgrades on-orbit. The three SDRs represent a significant portion of the new technologies being studied on board the SCAN Testbed, which is operating on an external truss on the International Space Station (ISS). The SCaN Testbed provides experimenters an opportunity to develop and demonstrate experimental waveforms and applications for communication, networking, and navigation concepts and advance the understanding of developing and operating SDRs in space.""",2013 IEEE Aerospace Conference
"""Strength Check of Aircraft Parts Based on Multi-GPU Clusters for Fast Calculation of Sparse Linear Equations""",Y. Zhang; B. Hu;,2020,10.1109/ACCESS.2020.2991099,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9079791,Sparse matrix;CUDA;RCM;PCG;architecture;,"""In order to improve the cost-effectiveness ratio, the next-generation vehicle needs to meet the requirements of reuse, while adopting a lighter structural weight, so it is necessary to realize the strength calculation and condition monitoring of key components in the digital twin. Most of the current monitoring methods are based on the characteristics of various data acquisition systems, but they need the support of a large number of flight data. The disadvantages of the above strategy can be avoided by reducing the structure of aircraft components to a finite element model and quickly checking the key components in the health management system. In order to solve the problem of fast calculation of the finite element model of the key components of the aircraft, a parallel algorithm and framework of large-scale sparse matrix preprocessing conjugate gradient method based on CUDA(Compute Unified Device Architecture) technology is proposed in the multi GPU(Graphics Processing Unit) workstation cluster environment. Once the sparse matrix is too large to be processed in a single workstation, this paper discusses how to realize the optimized data segmentation in the distributed multi-GPU computing environment. For the problem of iterative solution of matrix preprocessing, two preprocessing strategies of matrix bandwidth reduction parallelization and incomplete Cholesky decomposition are proposed, and asynchronous task concurrency and load balancing strategies are designed on the architecture. The calculation of some examples in the standard sparse matrix database shows that the algorithm and architecture proposed in this paper have the ability to solve large-scale sparse matrix quickly and efficiently, and can complete the fast strength verification of vehicle components.""",IEEE Access
"""Using Handle system to provide persistent identifiers for diploma""",M. A. Hisseine; D. Chen; X. Yang; X. Wang;,2022,10.1109/ICCECE54139.2022.9712843,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9712843,Handle system;MPAs;GHR;LHS;diploma;persistent identifiers;uniqueness;blockchain;,"""One of the essential factors for recruiters in a selection process is study background. It is, therefore, crucial to verify the authenticity of diplomas. Currently, it's easy to make a fake diploma with some software. Some websites can also generate one in a few minutes. You just need to give your information and pay some money. Between lies on the resume, fake documents, buying diplomas, producing real fake certificates, recruiters have a hard time dealing with candidates who come forward. This paper proposes to combine the handle system with blockchain technology to avoid this scourge. The handle system will provide to each diploma delivered by an institution a persistent identifier and will use blockchain technology to store and secure the data. The system will give all graduated students a unique identifier to check quickly and indefinitely their diplomas and grades.""",2022 2nd International Conference on Consumer Electronics and Computer Engineering (ICCECE)
"""Design and Evaluation of a Unique Social Perception System for Human–Robot Interaction""",A. Zaraki; M. Pieroni; D. De Rossi; D. Mazzei; R. Garofalo; L. Cominelli; M. B. Dehkordi;,2017,10.1109/TCDS.2016.2598423,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7534850,Context-aware social perception;humanoid social robots;human–robot interaction (HRI);meta-scene;platform-independent system;scene analysis;,"""Robot's perception is essential for performing high-level tasks such as understanding, learning, and in general, human-robot interaction (HRI). For this reason, different perception systems have been proposed for different robotic platforms in order to detect high-level features such as facial expressions and body gestures. However, due to the variety of robotics software architectures and hardware platforms, these highly customized solutions are hardly interchangeable and adaptable to different HRI contexts. In addition, most of the developed systems have one issue in common: they detect features without awareness of the real-world contexts (e.g., detection of environmental sound assuming that it belongs to a person who is speaking, or treating a face printed on a sheet of paper as belonging to a real subject). This paper presents a novel social perception system (SPS) that has been designed to address the previous issues. SPS is an out-of-the-box system that can be integrated into different robotic platforms irrespective of hardware and software specifications. SPS detects, tracks, and delivers in real-time to robots, a wide range of human- and environment- relevant features with the awareness of their real-world contexts. We tested SPS in a typical scenario of HRI for the following purposes: to demonstrate the system capability in detecting several high-level perceptual features as well as to test the system capability to be integrated into different robotics platforms. Results show the promising capability of the system in perceiving real world in different social robotics platforms, as tested in two humanoid robots, i.e., FACE and ZENO.""",IEEE Transactions on Cognitive and Developmental Systems
"""Learning Joint and Specific Patterns: A Unified Sparse Representation for Off-the-Person ECG Biometric Recognition""",Y. Huang; G. Yang; K. Wang; H. Liu; Y. Yin;,2021,10.1109/TIFS.2020.3006384,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9130771,Off-the-person ECG biometric recognition;joint and specific pattern;unified sparse representation;pairwise constraints;,"""Devices such as smartphones and tablets have spurred interest in off-the-person electrocardiogram (ECG) biometric recognition. While the advantage of using multi-feature information for establishing identities has been widely recognized, computational sparse representation models for multi-feature biometric recognition have only recently received more attention. We propose a unified sparse representation framework which collaboratively exploits joint and specific patterns for ECG biometric recognition. In particular, unlike joint sparse representation, which only considers the consistency among sparsity patterns of multiple features, we combine the consistent and pairwise constraints, which not only learn latent discriminant representations for all features but capture the interactions between them. In addition, our framework is universal and easily adapts to other multi-feature sparse representation models by just tuning the regularization parameters. The optimization problem is solved by an efficient alternating direction method of multipliers (ADMM). Extensive experiments on two publicly available off-the-person datasets demonstrate that our method can achieve competitive or even superior performance compared to state-of-the-art ECG biometric recognition methods.""",IEEE Transactions on Information Forensics and Security
"""Sparse Coding of Intra Prediction Residuals for Screen Content Coding""",M. G. Schimpf; N. Ling; Y. Shi; Y. Liu;,2021,10.1109/ICCE50685.2021.9427722,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9427722,video coding;sparse coding;sparse representation;orthogonal matching pursuit;screen content coding;HEVC;residual coding;intra prediction;KSVD;,"""High Efficiency Video Coding - Screen Content Coding (HEVC-SCC) is an extension to HEVC which adds sophisticated compression methods for computer generated content. A video frame is usually split into blocks that are predicted and subtracted from the original, which leaves a residual. These blocks are transformed by integer discrete sine transform (IntDST) or integer discrete cosine transform (IntDCT), quantized, and entropy coded into a bitstream. In contrast to camera captured content, screen content contains a lot of similar and repeated blocks. The HEVC-SCC tools utilize these similarities in various ways. After these tools are executed, the remaining signals are handled by IntDST/IntDCT which is designed to code camera-captured content. Fortunately, in sparse coding, the dictionary learning process which uses these residuals adapts much better and the outcome is significantly sparser than for camera captured content. This paper proposes a sparse coding scheme which takes advantage of the similar and repeated intra prediction residuals and targets low to mid frequency/energy blocks with a low sparsity setup. We also applied an approach which splits the common test conditions (CTC) sequences into categories for training and testing purposes. It is integrated as an alternate transform where the selection between traditional transform and our proposed method is based on a rate-distortion optimization (RDO) decision. It is integrated in HEVC-SCC test model (HM) HM- 16.18+SCM-8.7. Experimental results show that the proposed method achieves a Bjontegaard rate difference (BD-rate) of up to 4.6% in an extreme computationally demanding setup for the ""all intra"" configuration compared with HM-16.18+SCM-8.7.""",2021 IEEE International Conference on Consumer Electronics (ICCE)
"""Unique Power Electronics and Drives Experimental Bench (PEDEB) to Facilitate Learning and Research""",S. Anand; R. S. Farswan; B. G. Fernandes;,2012,10.1109/TE.2012.2200681,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6213161,Do-it-yourself;electric drives;laboratory course;power conversion;power electronics;reconfigurable;teaching;,"""Experimentation is important for learning and research in the field of power electronics and drives. However, a great deal of equipment is required to study the various topologies, controllers, and functionalities. Thus, the cost of establishing good laboratories and research centers is high. To address this problem, the authors have developed a “Power Electronics and Drives Experimental Bench” (PEDEB), whose details are given in this paper. This unique kit includes reconfigurable hardware modules, which can be interconnected to achieve more than 14 different circuit topologies. Moreover, the software (controller) is accessible to users, thereby facilitating quick verification and testing of new ideas. A 2-kVA prototype of the PEDEB was developed and tested for various possible modes of operation. The kit is being used for a first-semester post-graduate laboratory course on “Power Electronics and Drives.” This paper includes observations and learning from experiments on dc-dc buck converter, an induction motor drive, and a grid feeding inverter conducted using the PEDEB.""",IEEE Transactions on Education
"""Sparse decomposition of stereo signals with Matching Pursuit and application to blind separation of more than two sources from a stereo mixture""",R. Gribonval;,2002,10.1109/ICASSP.2002.5745294,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5745294,;,"""We develop a method of sparse decomposition of stereo audio signals, and test its application to blind separation of more than two sources from only two linear mixtures. The decomposition is done in a stereo dictionary which we can define based on any standard time-frequency or time-scale dictionary, such as the multiscale Gabor dictionary. A decomposition of a stereo mixture in the dictionary is computed with a Matching Pursuit type algorithm called Stereo Matching Pursuit. We experiment an application to blind source separation with three (mono) sources mixed on two channels. We cluster the parameters of the stereo atoms of the decomposition to estimate the mixing parameters, and recover estimates. of the sources by a partial reconstruction using only the appropriate atoms of the decomposition. The method outperforms the best achievable linear demixing by 3 dB to more than 7 dB on our preliminary experiments, and its performance should increase as we let the number of iterations of the pursuit increase. Sample sound files can be found here: http://www.irisa.fr/metiss/gribonva/""","2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
"""Gabor Cube Selection Based Multitask Joint Sparse Representation for Hyperspectral Image Classification""",S. Jia; J. Hu; Y. Xie; L. Shen; X. Jia; Q. Li;,2016,10.1109/TGRS.2015.2513082,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7389391,Fisher discrimination criterion;Gabor features;hyperspectral image classification;multitask sparse representation;Fisher discrimination criterion;Gabor features;hyperspectral image classification;multitask sparse representation;,"""The large amount of spectral and spatial information contained in hyperspectral imagery has provided great opportunity to effectively characterize and identify the surface materials of interest. As a novel feature extraction technique, a series of Gabor wavelet filters with different scales and frequencies was applied on hyperspectral data to extract spectral-spatial-combined features, which produced impressive performance on pixel-oriented classification. However, the incredibly large number of Gabor features could cause too much burden for onboard computation, limiting the efficiency of the method. To make matters worse, due to the nonhomogeneous spatial distribution of materials as well as the different characteristics of the constructed Gabor filters, some Gabor features could have a smaller or even negative impact on material representation, deteriorating the classification accuracy eventually. In this paper, a Gabor cube selection based multitask joint sparse representation approach, abbreviated as GS-MTJSRC, was proposed for hyperspectral image classification. First, based on the Fisher discrimination criterion, the most representative Gabor cubes for each class were picked out. Next, under multitask joint sparse representation framework, a coefficient vector could be obtained for each test sample with the selected Gabor cube features, which could be directly used for the following residual-based classification. Experimental results on three real hyperspectral data sets with different characteristics and spatial resolutions demonstrated the feasibility and efficiency of the proposed method.""",IEEE Transactions on Geoscience and Remote Sensing
"""A Momentum-Accelerated Hessian-Vector-Based Latent Factor Analysis Model""",W. Li; X. Luo; H. Yuan; M. Zhou;,2023,10.1109/TSC.2022.3177316,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9785520,Services computing;service application;recommendation service;high-dimensional and sparse data;machine learning;representation learning;latent factor analysis;hessian-vector;second-order optimization;generalized momentum method;,"""Service-oriented applications commonly involve high-dimensional and sparse (HiDS) interactions among users and service-related entities, e.g., user-item interactions from a personalized recommendation services system. How to perform precise and efficient representation learning on such HiDS interactions data is a hot yet thorny issue. An efficient approach to it is latent factor analysis (LFA), which commonly depends on large-scale non-convex optimization. Hence, it is vital to implement an LFA model able to approximate second-order stationary points efficiently for enhancing its representation learning ability. However, existing second-order LFA models suffer from high computational cost, which significantly reduces its practicability. To address this issue, this paper presents a Momentum-accelerated Hessian-vector algorithm (MH) for precise and efficient LFA on HiDS data. Its main ideas are two-fold: a) adopting the principle of a Hessian-vector-product-based method to utilize the second-order information without manipulating a Hessian matrix directly, and b) incorporating a generalized momentum method into its parameter learning scheme for accelerating its convergence rate to a stationary point. Experimental results on nine industrial datasets demonstrate that compared with state-of-the-art LFA models, an MH-based LFA model achieves gains in both accuracy and convergence rate. These positive outcomes also indicate that a generalized momentum method is compatible with the algorithms, e.g., a second-order algorithm, which implicitly rely on gradients.""",IEEE Transactions on Services Computing
"""Spline curve matching with sparse knot sets: applications to deformable shape detection and recognition""",Sang-Mook Lee; A. L. Abbott; N. A. Clark; P. A. Araman;,2003,10.1109/IECON.2003.1280334,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1280334,;,"""Splines can be used to approximate noisy data with a few control points. This paper presents a new curve matching method for deformable shapes using two-dimensional splines. In contrast to the residual error criterion [F.S. Cohen et al., 1992], which is based on relative locations of corresponding knot points such that is reliable primarily for dense point sets, we use deformation energy of thin-plate-spline mapping between sparse knot points and normalized local curvature information. This method has been tested successfully for the detection and recognition of deformable shapes.""",IECON'03. 29th Annual Conference of the IEEE Industrial Electronics Society (IEEE Cat. No.03CH37468)
"""Weighted Collaborative Sparse and L1/2 Low-Rank Regularizations With Superpixel Segmentation for Hyperspectral Unmixing""",L. Sun; F. Wu; C. He; T. Zhan; W. Liu; D. Zhang;,2022,10.1109/LGRS.2020.3019427,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9186305,Sparse unmixing;superpixel;weighted collaborative sparse;L₁̷₂ low-rank regularization;,"""In this letter, using the sparse unmixing framework, a weighted collaborative sparse and  $L_{1/2}$  low-rank regularization with superpixel segmentation method is proposed for hyperspectral unmixing. The method outlined here first uses superpixel segmentation to obtain local homogeneous regions. The reason for this approach is that the shape and size of superpixels are adaptive, which are better for obtaining homogeneous regions than square patches. Next, the weighted collaborative sparse term and  $L_{1/2}$  low-rank regularization were utilized to exploit the spatial and spectral correlation of each superpixel. In addition, the smoothness between adjacent pixels is enforced by total variation regularization. Finally, the proposed method and several state-of-the-art methods were tested on two simulated data sets and two real data sets. The results demonstrate the superiority of the method proposed here.""",IEEE Geoscience and Remote Sensing Letters
"""TileSpMV: A Tiled Algorithm for Sparse Matrix-Vector Multiplication on GPUs""",Y. Niu; Z. Lu; M. Dong; Z. Jin; W. Liu; G. Tan;,2021,10.1109/IPDPS49936.2021.00016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460505,sparse matrix-vector multiplication;tiling;GPU;,"""With the extensive use of GPUs in modern supercomputers, accelerating sparse matrix-vector multiplication (SpMV) on GPUs received much attention in the last couple of decades. A number of techniques, such as increasing utilization of wide vector units, reducing load imbalance and selecting the best formats, have been developed. However, the 2D spatial sparsity structure has not been well exploited in the existing work for SpMV on GPUs. In this paper, we propose an efficient tiled algorithm called TileSpMV for optimizing SpMV on GPUs through exploiting 2D spatial structure of sparse matrices. We first implement seven warp-level SpMV methods for calculating sparse tiles stored in a variety of formats, and then design a selection method to find the best format and SpMV implementation for each tile. We also adaptively extract nonzeros in the very sparse tiles into a separate matrix to maximize the overall performance. The experimental results show that our method is faster than state-of-the-art SpMV methods such as Merge-SpMV, CSR5 and BSR in most matrices of the full SuiteSparse Matrix Collection and delivers up to 2.61x, 3.96x and 426.59x speedups, respectively.""",2021 IEEE International Parallel and Distributed Processing Symposium (IPDPS)
"""Enhanced Weighted Method for Test Case Prioritization in Regression Testing Using Unique Priority Value""",A. Ammar; S. Baharom; A. A. A. Ghani; J. Din;,2016,10.1109/ICISSEC.2016.7885851,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7885851,;,"""Regression testing is an integral and expensive part in software testing. To reduce its effort, test case prioritization approaches were proposed. The problem with most of the existing approaches is the random ranking of test cases with equal weight. In this paper, an enhanced weighted method to prioritize the full test suite without using random ranking is presented. In addition, a controlled experiment was executed to evaluate the effectiveness of the proposed method. The results show an improved performance in terms of prioritizing test cases and recording higher APFD values over the original weighted method. In future, a larger experiment would be executed to generalize the results.""",2016 International Conference on Information Science and Security (ICISS)
"""A fast Pascal program for the product of large sparse matrices on personal computers""",A. Agnifili; P. Di Felice;,1990,10.1109/SOAC.1990.82192,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=82192,;,"""An implementation of a slight variation of the near optimal Gustavson's algorithm (1978) for the product of sparse matrices is given. Special care is devoted to the selection of data structures. The result is an elegant and very fast code specifically tailored for running on personal computers. An analysis is conducted of the X=X*Y problem in order to design the best compact representation for matrices X and Y. The code is briefly described, and test cases are presented.<>""",Proceedings of the 1990 Symposium on Applied Computing
"""Large-Scale Multiobjective Static Test Generation for Web-Based Testing with Integer Programming""",M. Luan Nguyen; S. Cheung Hui; A. C.M. Fong;,2013,10.1109/TLT.2012.22,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6365621,Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;Web-based testing;Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;static test generation;Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;multiobjective optimization;Testing;Linear programming;Web and internet services;Runtime;Sparse matrices;Integer linear programming;Computer aided instruction;Online services;integer programming;,"""Web-based testing has become a ubiquitous self-assessment method for online learning. One useful feature that is missing from today's web-based testing systems is the reliable capability to fulfill different assessment requirements of students based on a large-scale question data set. A promising approach for supporting large-scale web-based testing is static test generation (STG), which generates a test paper automatically according to user specification based on multiple assessment criteria. And the generated test paper can then be attempted over the web by users for assessment purpose. Generating high-quality test papers under multiobjective constraints is a challenging task. It is a 0-1 integer linear programming (ILP) that is not only NP-hard but also need to be solved efficiently. Current popular optimization software and heuristic-based intelligent techniques are ineffective for STG, as they generally do not have guarantee for high-quality solutions of solving the large-scale 0-1 ILP of STG. To that end, we propose an efficient ILP approach for STG, called branch-and-cut for static test generation (BAC-STG). Our experimental study on various data sets and a user evaluation on generated test paper quality have shown that the BAC-STG approach is more effective and efficient than the current STG techniques.""",IEEE Transactions on Learning Technologies
"""Tracking Sparse Linear Classifiers""",T. Zhai; F. Koriche; H. Wang; Y. Gao;,2019,10.1109/TNNLS.2018.2877433,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8533616,Concept drift;curse of dimensionality;online linear classification;shifting regret;sparse online learning;,"""In this paper, we investigate the problem of sparse online linear classification in changing environments. We first analyze the tracking performance of standard online linear classifiers, which use gradient descent for minimizing the regularized hinge loss. The derived shifting bounds highlight the importance of choosing appropriate step sizes in the presence of concept drifts. Notably, we show that a better adaptability to concept drifts can be achieved using constant step sizes rather than the state-of-the-art decreasing step sizes. Based on these observations, we then propose a novel sparse approximated linear classifier, called sparse approximated linear classification (SALC), which uses a constant step size. In essence, SALC simply rounds small weights to zero for achieving sparsity and controls the truncation error in a principled way for achieving a low tracking regret. The degree of sparsity obtained by SALC is continuous and can be controlled by a parameter which captures the tradeoff between the sparsity of the model and the regret performance of the algorithm. Experiments on nine stationary data sets show that SALC is superior to the state-of-the-art sparse online learning algorithms, especially when the solution is required to be sparse; on seven groups of nonstationary data sets with various total shifting amounts, SALC also presents a good ability to track drifts. When wrapped with a drift detector, SALC achieves a remarkable tracking performance regardless of the total shifting amount.""",IEEE Transactions on Neural Networks and Learning Systems
"""A Leak in PRNU Based Source Identification—Questioning Fingerprint Uniqueness""",M. Iuliani; M. Fontani; A. Piva;,2021,10.1109/ACCESS.2021.3070478,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9393356,Image processing;image classification;image forensics;source identification;,"""Photo Response Non-Uniformity (PRNU) is considered the most effective trace for the image source attribution task. Its uniqueness ensures that the sensor pattern noises extracted from different cameras are strongly uncorrelated, even when they belong to the same camera model. However, with the advent of computational photography, most recent devices heavily process the acquired pixels, possibly introducing non-unique artifacts that may reduce PRNU noise's distinctiveness, especially when several exemplars of the same device model are involved in the analysis. Considering that PRNU is an image forensic technology that finds actual and wide use by law enforcement agencies worldwide, it is essential to keep validating such technology on recent devices as they appear. In this paper, we perform an extensive testing campaign on over 33.000 Flickr images belonging to 45 smartphone and 25 DSLR camera models released recently to determine how widespread the issue is and which is the plausible cause. Experiments highlight that most brands, like Samsung, Huawei, Canon, Nikon, Fujifilm, Sigma, and Leica, are strongly affected by this issue. We show that the primary cause of high false alarm rates cannot be directly related to specific camera models, firmware, nor image contents. It is evident that the effectiveness of PRNU based source identification on the most recent devices must be reconsidered in light of these results. Therefore, this paper is intended as a call to action for the scientific community rather than a complete treatment of the subject. Moreover, we believe publishing these data is important to raise awareness about a possible issue with PRNU reliability in the law enforcement world.""",IEEE Access
"""State-of-the-Art of Voice Assistance Technology, Mitigating Replay Attacks: A Comprehensive Discussion""",S. Sandosh; R. Saxena; S. Shah; S. S. Rachiraju;,2024,10.1109/ICICV62344.2024.00100,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10511148,Smart home assistants;Replay attacks;Security concerns;Privacy Connectivity;Artificial intelligence;Voice recognition technology;Nonce numbers;Timestamps;Communication protocol;Authentication;Authorization;Freshness;Uniqueness;Attack detection;Unauthorized access;System integrity;Vulnerabilities;Feasibility;Practicality;Implementation;Testing;,"""In recent years, smart home assistants have become increasingly popular among consumers worldwide. These intelligent devices, powered by artificial intelligence and voice recognition technology, Smart home assistants bring convenience and effectiveness to handling daily activities and overseeing different aspects of our residences. Nevertheless, the expanding use of these assistants raises apprehensions regarding security and privacy due to heightened connectivity and data gathering capabilities. The intensity of security threats associated with these devices is alarming. Specifically, security concerns regarding replay attacks have emerged as a critical challenge. While various strategies have been proposed by researchers in this domain, there is a need for more robust mechanisms. This research paper proposes a novel approach to mitigate replay attacks by combining nonce numbers and timestamps. The proposed use of timestamp and nonce number methods in preventing replay attacks in smart home assistants is highly promising. The proposed solution enhances the security of voice commands transmitted between the user and the smart home assistant. By introducing unique nonce numbers and timestamps in the communication protocol, the system effectively detects and prevents the replaying of previously recorded commands.""",2024 5th International Conference on Intelligent Communication Technologies and Virtual Mobile Networks (ICICV)
"""Sparse Matrix-vector Multiplication on GPGPU Clusters: A New Storage Format and a Scalable Implementation""",M. Kreutzer; G. Hager; G. Wellein; H. Fehske; A. Basermann; A. R. Bishop;,2012,10.1109/IPDPSW.2012.211,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6270844,GPGPU;Sparse matrices;CUDA;,"""Sparse matrix-vector multiplication (spMVM) is the dominant operation in many sparse solvers. We investigate performance properties of spMVM with matrices of various sparsity patterns on the nVidia ""Fermi"" class of GPGPUs. A new ""padded jagged diagonals storage"" (pJDS) format is proposed which may substantially reduce the memory overhead intrinsic to the widespread ELLPACK-R scheme while making no assumptions about the matrix structure. In our test scenarios the pJDS format cuts the overall spMVM memory footprint on the GPGPU by up to 70%, and achieves 91% to 130% of the ELLPACK-R performance. Using a suitable performance model we identify performance bottlenecks on the node level that invalidate some types of matrix structures for efficient multi-GPGPU parallelization. For appropriate sparsity patterns we extend previous work on distributed-memory parallel spMVM to demonstrate a scalable hybrid MPI-GPGPU code, achieving efficient overlap of communication and computation.""",2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops & PhD Forum
"""Demography-based facial retouching detection using subclass supervised sparse autoencoder""",A. Bharati; M. Vatsa; R. Singh; K. W. Bowyer; X. Tong;,2017,10.1109/BTAS.2017.8272732,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8272732,;,"""Digital retouching of face images is becoming more widespread due to the introduction of software packages that automate the task. Several researchers have introduced algorithms to detect whether a face image is original or retouched. However, previous work on this topic has not considered whether or how accuracy of retouching detection varies with the demography of face images. In this paper, we introduce a new Multi-Demographic Retouched Faces (MDRF) dataset, which contains images belonging to two genders, male and female, and three ethnicities, Indian, Chinese, and Caucasian. Further, retouched images are created using two different retouching software packages. The second major contribution of this research is a novel semi-supervised autoencoder incorporating “sub-class” information to improve classification. The proposed approach outperforms existing state-of-the-art detection algorithms for the task of generalized retouching detection. Experiments conducted with multiple combinations of ethnicities show that accuracy of retouching detection can vary greatly based on the demographics of the training and testing images.""",2017 IEEE International Joint Conference on Biometrics (IJCB)
"""A 16-channel Readout Chip - A new sparse data readout architecture""",P. Murray; M. Lovell;,1992,10.1109/ESSCIRC.1992.5468222,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5468222,;,"""This paper describes a new architecture for performing data compression on a large matrix of parallel digital inputs, only a small proportion of which carries a logical 'I'. The system assigns a unique address to each of the active inputs (suppressing the inactive ones) and feeds them serially onto a data bus. The architecture is implemented using an array of CMOS digital signal processing ASICs, which have been designed at R.A.L, England and tested at C.E.R.N. in Geneva.""",ESSCIRC '92: Eighteenth European Solid-State Circuits conference
"""Processor Hardware Security Vulnerabilities and their Detection by Unique Program Execution Checking""",M. R. Fadiheh; D. Stoffel; C. Barrett; S. Mitra; W. Kunz;,2019,10.23919/DATE.2019.8715004,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715004,;,"""Recent discovery of security attacks in advanced processors, known as Spectre and Meltdown, has resulted in high public alertness about security of hardware. The root cause of these attacks is information leakage across covert channels that reveal secret data without any explicit information flow between the secret and the attacker. Many sources believe that such covert channels are intrinsic to highly advanced processor architectures based on speculation and out-of-order execution, suggesting that such security risks can be avoided by staying away from high-end processors. This paper, however, shows that the problem is of wider scope: we present new classes of covert channel attacks which are possible in average-complexity processors with in-order pipelining, as they are mainstream in applications ranging from Internet-of-Things to Autonomous Systems. We present a new approach as a foundation for remedy against covert channels: while all previous attacks were found by clever thinking of human attackers, this paper presents a formal method called Unique Program Execution Checking which detects and locates vulnerabilities to covert channels systematically, including those to covert channels unknown so far.""","2019 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
"""Continual Multiview Task Learning via Deep Matrix Factorization""",G. Sun; Y. Cong; Y. Zhang; G. Zhao; Y. Fu;,2021,10.1109/TNNLS.2020.2977497,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9037204,Deep matrix factorization;lifelong machine learning;multiview learning;sparse subspace learning;,"""The state-of-the-art multitask multiview (MTMV) learning tackles a scenario where multiple tasks are related to each other via multiple shared feature views. However, in many real-world scenarios where a sequence of the multiview task comes, the higher storage requirement and computational cost of retraining previous tasks with MTMV models have presented a formidable challenge for this lifelong learning scenario. To address this challenge, in this article, we propose a new continual multiview task learning model that integrates deep matrix factorization and sparse subspace learning in a unified framework, which is termed deep continual multiview task learning (DCMvTL). More specifically, as a new multiview task arrives, DCMvTL first adopts a deep matrix factorization technique to capture hidden and hierarchical representations for this new coming multiview task while accumulating the fresh multiview knowledge in a layerwise manner. Then, a sparse subspace learning model is employed for the extracted factors at each layer and further reveals cross-view correlations via a self-expressive constraint. For model optimization, we derive a general multiview learning formulation when a new multiview task comes and apply an alternating minimization strategy to achieve lifelong learning. Extensive experiments on benchmark data sets demonstrate the effectiveness of our proposed DCMvTL model compared with the existing state-of-the-art MTMV and lifelong multiview task learning models.""",IEEE Transactions on Neural Networks and Learning Systems
"""WaSP: Hierarchical Warping, Merging, and Sparse Prediction for Light Field Image Compression""",P. Astola; I. Tabus;,2018,10.1109/EUVIP.2018.8611756,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8611756,;,"""We propose a versatile light field compression scheme that is organized on hierarchical levels, where all views belonging to a particular level are encoded using several views already encoded in the previous hierarchical levels. The new scheme builds on an earlier version of our codec, and provides a more generalized functionality with improved view merging. The operations needed when one view is encoded conditional on its reference views are: first warping its reference views to the location of the current view and partitioning the pixels according to their state of occlusion in various warped versions; then merging the warped references using one optimal LS merger for each class of occluded pixels; finally, adjustment of the overall merged image to the original view by using a sparse predictor. The new scheme is applied to both plenoptic camera images and high density camera array data, and is evaluated in accordance with the JPEG Pleno test conditions. We compare the performance of the proposed codec to that of the HEVC anchors defined in the JPEG Pleno test conditions. We also make comparisons to the performance achieved by our earlier scheme. The proposed codec is publicly available on GitHub and it was accepted as the Verification Model (VM) 1.0 software for JPEG Pleno Light Field coding standard.""",2018 7th European Workshop on Visual Information Processing (EUVIP)
"""Efficient Solving of Markov Decision Processes on GPUs Using Parallelized Sparse Matrices""",A. Sapio; S. S. Bhattacharyya; M. Wolf;,2018,10.1109/DASIP.2018.8596969,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8596969,Markov decision processes;MDP;GPU;CUDA;Value iteration;Sparsity;,"""Markov Decision Processes (MDPs) provide important capabilities for facilitating the dynamic adaptation of hardware and software configurations to the environments in which they operate. However, the use of MDPs in embedded signal processing systems is limited because of the large computational demands for solving this class of system models. This paper presents Sparse Parallel Value Iteration (SPVI), a new algorithm for solving large MDPs on resource-constrained embedded systems that are equipped with mobile GPUs. SPVI leverages recent advances in parallel solving of MDPs and adds sparse linear algebra techniques to significantly outperform the state-of-the-art. The method and its application are described in detail, and demonstrated with case studies that are implemented on an NVIDIA Tegra K1 System On Chip (SoC). The experimental results show execution time improvements in the range of 65 % -78% for several applications. SPVI also lifts restrictions required by other MDP solver approaches, making it more widely compatible with large classes of optimization problems.""",2018 Conference on Design and Architectures for Signal and Image Processing (DASIP)
"""Synthesis of a specific wavefront using 2D full and sparse arrays""",D. Cassereau; N. Chakrovun; F. Wu; M. Fink; F. Datchi;,1992,10.1109/ULTSYM.1992.275944,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=275944,;,"""A new application of the time-reversal approach is proposed, i.e., the synthesis of a specific wavefront (plane and short wavefront) in the near-field domain of a transducer array. The theoretical background for computing the temporal excitation signals that must be emitted on the different channels of the array is presented. The efficiency of this approach is illustrated with two kinds of transducer arrays: a full and periodic array containing 1024 elements and a sparse array made of 128 elements. These results are obtained through numerical software; they prove that it is theoretically possible to create any desired wavefront shape if the array contains enough independent elements. Experimental results obtained with a real sparse array are presented.<>""",IEEE 1992 Ultrasonics Symposium Proceedings
"""3D Ear Identification Using Block-Wise Statistics-Based Features and LC-KSVD""",L. Zhang; L. Li; H. Li; M. Yang;,2016,10.1109/TMM.2016.2566578,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7467559,3D ear;dictionary learning;label consistent K-SVD (LC-KSVD);sparse coding;surface types;,"""Biometrics authentication has been corroborated to be an effective method for recognizing a person's identity with high confidence. In this field, the use of three-dimensional (3D) ear shape is a recent trend. As a biometric identifier, the ear has several inherent merits. However, although a great deal of efforts have been devoted, there is still large room for improvement in developing a highly effective and efficient 3D ear identification approach. In this paper, we attempt to fill this gap to some extent by proposing a novel 3D ear classification scheme that makes use of the label consistent K-SVD (LC-KSVD) framework. As an effective supervised dictionary learning algorithm, LC-KSVD learns a single compact discriminative dictionary for sparse coding and a multi-class linear classifier simultaneously. To use the LC-KSVD framework, one key issue is how to extract feature vectors from 3D ear scans. To this end, we propose a blockwise statistics-based feature extraction scheme. Specifically, we divide a 3D ear region of interest into uniform blocks and extract a histogram of surface types from each block; histograms from all blocks are then concatenated to form the desired feature vector. Feature vectors extracted in this way are highly discriminative and are robust to mere misalignment between samples. Experiments demonstrate that our approach can achieve better recognition accuracy than the other state-of-the-art methods. More importantly, its computational complexity is extremely low, making it quite suitable for the large-scale identification applications. MATLAB source codes are publicly online available at http://sse.tongji.edu.cn/linzhang/LCKSVDEar/LCKSVDEar. htm.""",IEEE Transactions on Multimedia
"""Hardware accelerator for analytics of sparse data""",E. Nurvitadhi; A. Mishra; Y. Wang; G. Venkatesh; D. Marr;,2016,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7459571,Hardware accelerator;analytics;machine learning;,"""Rapid growth of Internet led to web applications that produce large unstructured sparse datasets (e.g., texts, ratings). Machine learning (ML) algorithms are the basis for many important analytics workloads that extract knowledge from these datasets. This paper characterizes such workloads on a high-end server for real-world datasets and shows that a set of sparse matrix operations dominates runtime. Further, they run inefficiently due to low compute-per-byte and challenging thread scaling behavior. As such, we propose a hardware accelerator to perform these operations with extreme efficiency. Simulations and RTL synthesis to 14nm ASIC demonstrate significant performance and performance/Watt improvements over conventional processors, with only a small area overhead.""","2016 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
"""Sparse Measurement Algorithm Execution Time Prediction on Heterogeneous Edge Devices for Early Stage Software-Hardware Matching""",B. Rupprecht; B. Vogel-Heuser; J. Möhrle; D. Hujo; Y. Wang;,2024,10.1109/ICPS59941.2024.10640034,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10640034,Execution time modeling;execution time prediction;performance benchmarking;software-hardware matching;edge devices;,"""The design and implementation of edge computing solutions needs elaborate knowledge about the inter-dependencies of software and hardware, which are also often developed parallel to meet performance requirements like time constraints. However, the suitability of hardware to execute a given algorithm and vice versa is often assessed by time-consuming trial-and-error approaches. For algorithm assessment, the execution time is crucial. However, existing execution time estimation approaches usually rely on either thorough timing models for the underlying hardware or vast amounts of measurements. Consequently, these approaches are not feasible for an early design stage, where an assessment with limited effort is crucial. Thus, this paper tries to overcome those limitations by comparing a parametric and a non-parametric execution time prediction approach suitable for an early design stage. The evaluation with four edge devices out of heterogeneous categories using measured data is a first attempt at generalizability. A selection of four different algorithms applicable in smart manufacturing and benchmarking ensures the approaches’ broad applicability. The parametric and non-parametric model comparison shows the trade-off between source code analysis and performing measurements.""",2024 IEEE 7th International Conference on Industrial Cyber-Physical Systems (ICPS)
"""Sparse Support Vector Machines with L_{p} Penalty for Biomarker Identification""",Z. Liu; S. Lin; M. Tan;,2010,10.1109/TCBB.2008.17,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4459304,Embedded method;feature selection;L_{p} regularization;SVM;SNP data analysis;protease data analysis.;J.3.a Biology and genetics;I.5.2.b Feature evaluation and selection;I.5.2.c Pattern analysis;I.5.4.h Medicine;,"""The development of high-throughput technology has generated a massive amount of high-dimensional data, and many of them are of discrete type. Robust and efficient learning algorithms such as LASSO [1] are required for feature selection and overfitting control. However, most feature selection algorithms are only applicable to the continuous data type. In this paper, we propose a novel method for sparse support vector machines (SVMs) with L_{p} (p ≪ 1) regularization. Efficient algorithms (LpSVM) are developed for learning the classifier that is applicable to high-dimensional data sets with both discrete and continuous data types. The regularization parameters are estimated through maximizing the area under the ROC curve (AUC) of the cross-validation data. Experimental results on protein sequence and SNP data attest to the accuracy, sparsity, and efficiency of the proposed algorithm. Biomarkers identified with our methods are compared with those from other methods in the literature. The software package in Matlab is available upon request.""",IEEE/ACM Transactions on Computational Biology and Bioinformatics
"""Sparse/DCT (S/DCT) Two-Layered Representation of Prediction Residuals for Video Coding""",J. -W. Kang; M. Gabbouj; C. . -C. J. Kuo;,2013,10.1109/TIP.2013.2256917,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6494295,$rho$ domain rate model;discrete cosine transform (DCT);high efficiency video coding (HEVC);multilayered coding;overcomplete dictionary based video coding;residual coding;sparse representation;,"""In this paper, we propose a cascaded sparse/DCT (S/DCT) two-layer representation of prediction residuals, and implement this idea on top of the state-of-the-art high efficiency video coding (HEVC) standard. First, a dictionary is adaptively trained to contain featured patterns of residual signals so that a high portion of energy in a structured residual can be efficiently coded via sparse coding. It is observed that the sparse representation alone is less effective in the R-D performance due to the side information overhead at higher bit rates. To overcome this problem, the DCT representation is cascaded at the second stage. It is applied to the remaining signal to improve coding efficiency. The two representations successfully complement each other. It is demonstrated by experimental results that the proposed algorithm outperforms the HEVC reference codec HM5.0 in the Common Test Condition.""",IEEE Transactions on Image Processing
"""Application of Sparse Attention Mechanism and Bidirectional GRU Neural Networks in Knowledge Tracing""",J. Wu; G. Wang; Y. Wang;,2023,10.1109/CSECS60003.2023.10428652,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428652,Knowledge Tracing;Deep learning;Sparse attention. bidirectional Gated Recurrent Units. RMSNorm;,"""To overcome the limitations of traditional deep knowledge tracing models, particularly in capturing complex exercise relationships and handling long data sequences, we introduce SKT-BiGRU. This optimized model combines bidirectional Gated Recurrent Units (BiGRU), sparse attention, and RMSNorm. It first translates a learner's historical interactions into real-valued vectors for more accurate trajectory modeling. The sparse attention mechanism boosts computational efficiency and interpretability, allowing focus on critical data points for prediction. RMSNorm normalizes the neural network's outputs, improving training stability and convergence speed. The model effectively captures intricate exercise dependencies through self-attention and sparse attention. These hidden states and attention weights are combined with the bidirectional GRU output to predict a learner's performance on upcoming questions.Experimental tests carried out on two public datasets have confirmed that SKT-BiGRU, in comparison to the baseline model, exhibits an improvement in its AUC (Area Under the Curve) metrics ranging from 3.5% to 6.1%, and an increase in ACC (Accuracy) from 0.4% to 3.2%. These results demonstrate its effectiveness in augmenting deep knowledge tracing.""",2023 6th International Conference on Software Engineering and Computer Science (CSECS)
"""A Nonvolatile Associative Memory-Based Context-Driven Search Engine Using 90 nm CMOS/MTJ-Hybrid Logic-in-Memory Architecture""",H. Jarollahi; N. Onizawa; V. Gripon; N. Sakimura; T. Sugibayashi; T. Endoh; H. Ohno; T. Hanyu; W. J. Gross;,2014,10.1109/JETCAS.2014.2361061,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6932498,Associative memory;context-driven search;logic-in-memory;magnetic tunnel junction (MTJ);sparse clustered networks;,"""This paper presents algorithm, architecture, and fabrication results of a nonvolatile context-driven search engine that reduces energy consumption as well as computational delay compared to classical hardware and software-based approaches. The proposed architecture stores only associations between items from multiple search fields in the form of binary links, and merges repeated field items to reduce the memory requirements and accesses. The fabricated chip achieves $13.6times$  memory reduction and 89% energy saving compared to a classical field-based approach in hardware, based on content-addressable memory (CAM). Furthermore, it achieves $8.6times$  reduced number of clock cycles in performing search operations compared to the CAM, and five orders of magnitude reduced number of clock cycles compared to a fabricated and measured ultra low-power CPU-based counterpart running a classical search algorithm in software. The energy consumption of the proposed architecture is on average three orders of magnitude smaller than that of a software-based approach. A magnetic tunnel junction (MTJ)-based logic-in-memory architecture is presented that allows simple routing and eliminates leakage current in standby using 90 nm CMOS/MTJ-hybrid technologies.""",IEEE Journal on Emerging and Selected Topics in Circuits and Systems
"""Finite Element Study of Bearing Estimation in the Presence of Underwater Multipath Scenario Using Non-Uniform Linear Array""",P. Gupta; B. Kumar; M. Agrawal; F. Fauziya;,2019,10.1109/OCEANSE.2019.8867182,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8867182,DoA estimation;Noncircular Signals;Sparse array;Virtual Array;Cumulants;Pseudo cumulants;NULA;ULA Higher Order Statistics;Underdetermined System;,"""In this paper we have analyzed the performance of ULA, NULA and virtual array in multipath scenario using sparse Bayesian learning (SBL) and beamforming techniques. Using COMSOL we have generated the impulse response of the environment at the sensors locations. Direction of arrival (DoA) estimation has been done for ULA, NULA and Virtual array for underwater environment. Array performance has been tested in computing software MATLAB. For creating real environment we have used COMSOL, a finite element analysis software. Underwater multipath environment has been created in COMSOL. SBL provide better result as compared to beamforming technique and also NULA and virtual provide better result as compared to ULA for same number of sensors.""",OCEANS 2019 - Marseille
"""A Sparse Coding Neural Network ASIC With On-Chip Learning for Feature Extraction and Encoding""",P. Knag; J. K. Kim; T. Chen; Z. Zhang;,2015,10.1109/JSSC.2014.2386892,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7015626,Feature extraction;hardware acceleration;neural network architecture;sparse coding;sparse and independent local network;,"""Hardware-based computer vision accelerators will be an essential part of future mobile devices to meet the low power and real-time processing requirement. To realize a high energy efficiency and high throughput, the accelerator architecture can be massively parallelized and tailored to vision processing, which is an advantage over software-based solutions and general-purpose hardware. In this work, we present an ASIC that is designed to learn and extract features from images and videos. The ASIC contains 256 leaky integrate-and-fire neurons connected in a scalable two-layer network of 8 × 8 grids linked in a 4-stage ring. Sparse neuron activation and the relatively small grid keep the spike collision probability low to save access arbitration. The weight memory is divided into core memory and auxiliary memory, such that the auxiliary memory is only powered on for learning to save inference power. High-throughput inference is accomplished by the parallel operation of neurons. Efficient learning is implemented by passing parameter update messages, which is further simplified by an approximation technique. A 3.06 mm2 65 nm CMOS ASIC test chip is designed to achieve a maximum inference throughput of 1.24 Gpixel/s at 1.0 V and 310 MHz, and on-chip learning can be completed in seconds. To improve the power consumption and energy efficiency, core memory supply voltage can be reduced to 440 mV to take advantage of the error resilience of the algorithm, reducing the inference power to 6.67 mW for a 140 Mpixel/s throughput at 35 MHz.""",IEEE Journal of Solid-State Circuits
"""A Stacked Sparse Autoencoder-Based Detector for Automatic Identification of Neuromagnetic High Frequency Oscillations in Epilepsy""",J. Guo; K. Yang; H. Liu; C. Yin; J. Xiang; H. Li; R. Ji; Y. Gao;,2018,10.1109/TMI.2018.2836965,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8359295,High-frequency oscillations;MEG;SSAE;brain;deep learning model;detector;,"""High-frequency oscillations (HFOs) are spontaneous magnetoencephalography (MEG) patterns that have been acknowledged as a putative biomarker to identify epileptic foci. Correct detection of HFOs in the MEG signals is crucial for the accurate and timely clinical evaluation. Since the visual examination of HFOs is time-consuming, error-prone, and with poor inter-reviewer reliability, an automatic HFOs detector is highly desirable in clinical practice. However, the existing approaches for HFOs detection may not be applicable for MEG signals with noisy background activity. Therefore, we employ the stacked sparse autoencoder (SSAE) and propose an SSAE-based MEG HFOs (SMO) detector to facilitate the clinical detection of HFOs. To the best of our knowledge, this is the first attempt to conduct HFOs detection in MEG using deep learning methods. After configuration optimization, our proposed SMO detector is outperformed other classic peer models by achieving 89.9% in accuracy, 88.2% in sensitivity, and 91.6% in specificity. Furthermore, we have tested the performance consistency of our model using various validation schemes. The distribution of performance metrics demonstrates that our model can achieve steady performance.""",IEEE Transactions on Medical Imaging
"""Deblurring of Document Images Based on Sparse Representations Enhanced by Non-local Means""",N. Nayef; P. Gomez-Krämer; J. -M. Ogier;,2014,10.1109/ICPR.2014.760,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6977473,;,"""Blur is one of the most difficult distortions in camera captured documents. It degrades the visual quality of an image, and makes it difficult to read whether by a human or OCR systems. This paper presents a novel non-blind deblurring method that combines the well known effective techniques of sparse representations and non-local image similarity. The presented problem formulation enables the use of standard sparse coding methods for solving sparse coding-based deblurring when enhanced by a non-local means prior. The method has been tested on both synthetic and real document images degraded with a variety of blur kernels. The resulting deblurred images have high quality in terms of both signal-to-noise ratio and OCR accuracy.""",2014 22nd International Conference on Pattern Recognition
"""Context independent unique sequences generation for protocol testing""",T. Ramalingom; K. Thulasiraman; A. Das;,1996,10.1109/INFCOM.1996.493058,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=493058,;,"""A number of test sequence generation methods proposed for protocols represented as extended finite state machines (EFSMs) use state identification sequences for checking the states. However, neither a formal definition nor a method of computation of these sequences for an EFSM state is known. We define a new type of state identification sequence, called context independent unique sequence (CIUS) and present an algorithm for computing it. A unified method based on CIUSes is developed for automatically generating executable test cases for both control flow and data flow aspects of an EFSM. In control flow testing, CIUSes are very useful in confirming the tail state of the transitions. In data flow testing, CIUSes improve the observability of the test cases for the def-use associations of different variables used in the EFSM. Unlike general state identification sequences, the use of CIUSes does not increase the complexity of the already intractable feasibility problem in the test case generation.""",Proceedings of IEEE INFOCOM '96. Conference on Computer Communications
"""Sparse Representation-Based Image Quality Index With Adaptive Sub-Dictionaries""",L. Li; H. Cai; Y. Zhang; W. Lin; A. C. Kot; X. Sun;,2016,10.1109/TIP.2016.2577891,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7486021,Quality evaluation;overcomplete synthesis dictionary;sparse coding;adaptive sub-dictionary;Quality evaluation;overcomplete synthesis dictionary;sparse coding;adaptive sub-dictionary;,"""Distortions cause structural changes in digital images, leading to degraded visual quality. Dictionary-based sparse representation has been widely studied recently due to its ability to extract inherent image structures. Meantime, it can extract image features with slightly higher level semantics. Intuitively, sparse representation can be used for image quality assessment, because visible distortions can cause significant changes to the sparse features. In this paper, a new sparse representation-based image quality assessment model is proposed based on the construction of adaptive sub-dictionaries. An over-complete dictionary trained from natural images is employed to capture the structure changes between the reference and distorted images by sparse feature extraction via adaptive sub-dictionary selection. Based on the observation that image sparse features are invariant to weak degradations and the perceived image quality is generally influenced by diverse issues, three auxiliary quality features are added, including gradient, color, and luminance information. The proposed method is not sensitive to training images, so a universal dictionary can be adopted for quality evaluation. Extensive experiments on five public image quality databases demonstrate that the proposed method produces the state-of-the-art results, and it delivers consistently well performances when tested in different image quality databases.""",IEEE Transactions on Image Processing
"""SASDL and RBATQ: Sparse Autoencoder With Swarm Based Deep Learning and Reinforcement Based Q-Learning for EEG Classification""",S. K. Prabhakar; S. -W. Lee;,2022,10.1109/OJEMB.2022.3161837,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9740532,Deep learning;EEG;PSO;Q-learning;reinforcement learning;,"""The most vital information about the electrical activities of the brain can be obtained with the help of Electroencephalography (EEG) signals. It is quite a powerful tool to analyze the neural activities of the brain and various neurological disorders like epilepsy, schizophrenia, sleep related disorders, parkinson disease etc. can be investigated well with the help of EEG signals. Goal: In this paper, two versatile deep learning methods are proposed for the efficient classification of epilepsy and schizophrenia from EEG datasets. Methods: The main advantage of using deep learning when compared to other machine learning algorithms is that it has the capability to accomplish feature engineering on its own. Swarm intelligence is also a highly useful technique to solve a wide range of real-world, complex, and non-linear problems. Therefore, taking advantage of these factors, the first method proposed is a Sparse Autoencoder (SAE) with swarm based deep learning method and it is named as (SASDL) using Particle Swarm Optimization (PSO) technique, Cuckoo Search Optimization (CSO) technique and Bat Algorithm (BA) technique; and the second technique proposed is the Reinforcement Learning based on Bidirectional Long-Short Term Memory (BiLSTM), Attention Mechanism, Tree LSTM and Q learning, and it is named as (RBATQ) technique. Results and Conclusions: Both these two novel deep learning techniques are tested on epilepsy and schizophrenia EEG datasets and the results are analyzed comprehensively, and a good classification accuracy of more than 93% is obtained for all the datasets.""",IEEE Open Journal of Engineering in Medicine and Biology
"""A Sparse Manifold Learning Approach to Robust Indoor Positioning Based on Wi-Fi RSS Fingerprinting""",G. Shen; D. Han; P. Liu;,2019,10.1109/ACCESS.2019.2940629,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8830475,Expectation-maximization;indoor positioning;manifold learning;received signal strength;,"""The emerging location-based applications depend on the fast and accurate positioning of mobile targets. Wi-Fi received signal strength (RSS) fingerprinting provides a promising solution to localize an object in indoor environments. Among the factors challenging the RSS fingerprinting based algorithms are the site survey cost and the time-varying environment, given the unreliable signal qualities. Here we present a novel approach to indoor object positioning using the manifold assumption on the radio map in RSS-location space. Thinking the measured RSS from one access point (AP) in different locations are randomly drawn from a nonlinear manifold (ground truth), we propose an expectation-maximization (EM) style algorithm to reconstruct the sparse representation of this manifold from the noisy RSS observations. Motivated by the observation that the radio map has a strong local correlation in the RSS-location space, we introduce a multi-scale constrained quadratic programming to approximate the manifold. Within limited iterations, we can estimate the ground truth RSS values and parameters simultaneously. As a result, the learned manifold is exploited to predict the object's position: we develop a positioning algorithm by minimizing the manifold distortion effort which integrates both measurement error and manifold shape preservation. We conducted extensive simulations and experiments in different settings, testing the datasets collected in a building in the last 8 months. The results showed that the proposed approach was adaptive to the varying environmental noise levels, presenting robust positioning performance.""",IEEE Access
"""Algorithm Analysis of Sparse Matrix Multiplication""",H. Ren; H. Ma; J. Kang; Y. Liu; L. Wang; X. Zheng;,2021,10.1109/QRS-C55045.2021.00138,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9741870,sparse;matrix;multiplication;parallel;,"""Matrix is widely used in telecommunication, cryptography, computer science and other field. Especially in wireless sensor network data processing, it is important and necessary to keep data transmitting reliable and resilient. In channel coding and secure communication, matrix is used to realize the coding of transmission information and source information in the channel, which not only reduces the bit error rate of wireless communication, but also realizes the confidentiality of communication. The development of effective algorithms for matrix calculation has been an interesting subject for several centuries and an expanding research field. For some widely used and special matrices, such as sparse matrix and quasi diagonal matrix, there are specific fast algorithms. This paper briefly describes and explains our design of serial algorithms which implementing the sparse matrix multiplication by parallel programming, and also to provide benchmark results to justify the correctness and performance of our design.""","2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)"
"""Handwritten Bangla digit recognition using Sparse Representation Classifier""",H. A. Khan; A. A. Helal; K. I. Ahmed;,2014,10.1109/ICIEV.2014.6850817,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6850817,Sparse Representation Classifier;Bangla Optical Character Recognition;Handwritten character recognition;Digit recognition;,"""We present a framework for handwritten Bangla digit recognition using Sparse Representation Classifier. The classifier assumes that a test sample can be represented as a linear combination of the train samples from its native class. Hence, a test sample can be represented using a dictionary constructed from the train samples. The most sparse linear representation of the test sample in terms of this dictionary can be efficiently computed through ℓ1-minimization, and can be exploited to classify the test sample. We applied Sparse Representation Classifier on the image zone density, an image domain statistical feature extracted from the character image, to classify the Bangla numerals. This is a novel approach for Bangla Optical Character Recognition, and demonstrates an excellent accuracy of 94% on the off-line handwritten Bangla numeral database CMATERdb 3.1.1. This result is promising, and should be investigated further.""","2014 International Conference on Informatics, Electronics & Vision (ICIEV)"
"""Bit-Flip Attack Detection for Secure Sparse Matrix Computations on FPGA""",N. G; N. S; K. S;,2023,10.1109/APCCAS60141.2023.00026,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10509980,Bit-Flip Attack;Neural Network;Hardware Security;Row Hammer Attack;Sparse Matrix;FPGA;,"""Bit-Flip Attacks (BFA) pose a significant threat to the security of sparse Deep Neural Network (DNN) architectures. This paper introduces a novel hardware implementation to identify these attacks in large-scale DNN architectures. By leveraging the benefits of Compressed Sparse Row (CSR) representations, the proposed architecture effectively safeguards the sparse connections of DNNs from bit-flip attacks. This paper highlights the importance of protecting sparse connections in DNN architectures, considering the security implications. While software-based prevention methods have limitations and are vulnerable to attacks, a hardware implementation offers enhanced security measures. Experimental evaluations are conducted using various benchmark sparse matrices from the SuiteSparse matrix collection. The results show an average accuracy rate of 96%, proving the effectiveness of the hardware-based approach in detecting and preventing bit-flip attacks in sparse DNN architectures. Proposed architecture has been implemented on a Xilinx Kintex-7 FPGA device and has observed significant reduction in resource utilization when compared to state-of-the-art techniques.""",2023 IEEE Asia Pacific Conference on Circuits and Systems (APCCAS)
"""A Hybrid Deep Learning-Based (HYDRA) Framework for Multifault Diagnosis Using Sparse MDT Reports""",M. S. Riaz; H. N. Qureshi; U. Masood; A. Rizwan; A. Abu-Dayya; A. Imran;,2022,10.1109/ACCESS.2022.3185639,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9804486,Root cause analysis;cellular data sparsity;data enrichment;multi-fault diagnosis;minimization of drive tests;hybrid deep learning;radio environment maps;image inpainting;self healing;network automation;,"""Diminishing viability of manual fault diagnosis in the increasingly complex emerging cellular network has motivated research towards artificial intelligence (AI)-based fault diagnosis using the minimization of drive test (MDT) reports. However, existing AI solutions in the literature remain limited to either diagnosis of faults in a single base station only or the diagnosis of a single fault in a multiple BS scenario. Moreover, lack of robustness to MDT reports spatial sparsity renders these solutions unsuitable for practical deployment. To address this problem, in this paper we present a novel framework named Hybrid Deep Learning-based Root Cause Analysis (HYDRA) that uses a hybrid of convolutional neural networks, extreme gradient boosting, and the MDT data enrichment techniques to diagnose multiple faults in a multiple base station network. Performance evaluation under realistic and extreme settings shows that HYDRA yields an accuracy of 93% and compared to the state-of-the-art fault diagnosis solutions, HYDRA is far more robust to MDT report sparsity.""",IEEE Access
"""Power distribution management system software deployment""",A. Husagić-Selman; T. Namas;,2016,10.1109/MEDO.2016.7746548,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7746548,Multithreading in sparse systems;power distribution management system;single thread and multithread performance;,"""Due to the parallel features of hardware devices multi-threading became a trend in applications that require intensive computations. However, it is not always the best [1]. In this ongoing work an overview of real-life Power Distribution Management System (PDMS) is given and its performance in single-threaded and multi-threaded environment is tested. PDMS implements parallelism through sub-division of networks based on their natural features. Each sub-network is run in parallel on separate processor cores using a single thread processing, and with such setup it outperforms the multi-threaded BLAS by the factor of 20. With multi-threading implemented, the performance dreadfully goes down, and processing time increases. Reasons for this are the structure of power distribution network matrices (indefinite and very sparse) and synchronization overhead involved in multi-thread operations.""",2016 International Conference Multidisciplinary Engineering Design Optimization (MEDO)
"""A Sparse Minimal-Order Dynamic Model of Power Networks Based on dq0 Signals""",J. Belikov; Y. Levron;,2018,10.1109/TPWRS.2017.2702746,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7922595,Power systems;dynamics;stability;transients;modeling;dq0 transformation;renewable energy;,"""Today the dq0 reference frame is mainly used for modeling and control of traditional electric machines and small power sources. A current challenge is to merge various dq0-based models appearing in recent literature to obtain a complete model of a large power system. To this end, in this paper we propose a model describing the dynamics of large transmission networks based on dq0 quantities. The proposed model is based on a standard network topology, uses sparse system matrices, and is of minimal order. We also demonstrate how this model may be used to construct a small-signal description of a complete system that includes the transmission network, generators, and loads. Results are illustrated on the basis of a long transmission line, and using the 118-bus test case network. This paper is accompanied by a free software package.""",IEEE Transactions on Power Systems
"""Accelerating Sparse LU Factorization with Density-Aware Adaptive Matrix Multiplication for Circuit Simulation""",T. Wang; W. Li; H. Pei; Y. Sun; Z. Jin; W. Liu;,2023,10.1109/DAC56929.2023.10247767,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10247767,sparse LU factorization;circuit simulation;matrix multiplication;supernodal LU factorization;machine learning;random forest;,"""Sparse LU factorization is considered to be one of the most time-consuming components in circuit simulation, particularly when dealing with circuits of considerable size in the advanced process era. Sparse LU factorization can be expedited by utilizing the supernode structure, which partitions the matrix into dense sub-matrices, thereby improving computational performance by utilizing level-3 Basic Linear Algebra Subprograms (BLAS) General Matrix Multiplication (GEMM) operations. The sparse and irregular structure of circuit matrices often impedes the formation of supernodes or results in the formation of supernodes with many zero elements, which in turn poses challenges for exploiting GEMM operations. In this paper, by fully utilizing the density in sub-matrices and combining GEMM with the Dense-Sparse Matrix Multiplication (SpMM), we propose a density-aware adaptive matrix multiplication equipped with machine learning techniques to optimize performance of the most-time consuming matrix multiplication operator so as to accelerate the sparse LU factorization. Numerical experiment results show that among the 6 circuit matrices tested, the average performance of matrix multiplication in our algorithm can be improved by 5.35x (up to 9.35x) compared to the performance of using GEMM directly in Schur-complement updates. Compared with state-of-the-art solver SuperLU_DIST, our method shows a substantial performance improvement.""",2023 60th ACM/IEEE Design Automation Conference (DAC)
"""3D face recognition via discriminative keypoint selection""",J. Kim; D. Han; W. Hwang; J. Kim;,2017,10.1109/URAI.2017.7992781,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7992781,Face recognition;feature selection;sparse representation;,"""In this paper, we propose a discriminative keypoint selection-based 3D face recognition method that is superior to prevalent techniques in terms of both computational complexity and performance. We use the average face model (AFM) for face registration to efficiently locate the axis of symmetry in the rotated face mesh and recover a full frontal face from a 3D face model commonly corrupted due to pose variances. Instead of using the keypoint detection method, we use the feature selection algorithm to find the most discriminant keypoints for face identification and reduce computational time for not only feature extraction but also keypoint matching. The results of the experiments conducted on the Bosphorus database and the UMBDB show that our algorithm can improve rank-1 identification accuracy, thus confirming its robustness against pose variances, expressions, and occlusions.""",2017 14th International Conference on Ubiquitous Robots and Ambient Intelligence (URAI)
"""A Novel Sparse Subspace Correlation Analysis-Based Domain Adaptation Method for Sensor Drift Suppression in E-nose""",Z. Liang; L. Yang; T. Guo; J. Li;,2021,10.1109/ICCSN52437.2021.9463598,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9463598,electronic nose;sensor drift;sparse reconstruction;domain adaptation;,"""Sensor drift caused by the sensor aging and environmental factors is an urgent problem that seriously affects the detection performance and service life of electronic nose (E-nose). It is necessary to research the sensor drift suppression methods to realize the long-term and stable detection of E-nose. In this paper, a highly efficient sparse subspace correlation analysis-based domain adaptation(SSCA-DA) method is proposed to suppress the sensor drift. This method is to find the optimal subspace for each dataset, and the transformed data after transforming to the optimal subspace is sparsely reconstructed, which can realize the knowledge transfer in the data domains with and without drift information. From the experiment results, it can be found that the sensor drift can be satisfactorily solved by the proposed method.""",2021 13th International Conference on Communication Software and Networks (ICCSN)
"""Speeding up 3D SAFT for ultrasonic NDT by sparse deconvolution""",J. Kirchhof; F. Krieg; F. Römer; A. Ihlow; A. Osman; G. Del Galdo;,2016,10.1109/ULTSYM.2016.7728434,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7728434,;,"""In this paper we propose to pre-process ultrasonic measurements (A-scans) in Non-Destructive Testing (NDT) by sparse deconvolution before post-processing the data with the Synthetic Aperture Focusing Technique (SAFT). Compared to state-of-the-art SAFT post-processing of raw A-scan measurements, pre-processing by sparse deconvolution can improve NDT in the following ways: First, the temporal resolution of signal reflections is increased. Second, because the A-scans appear as a sparse signal of spikes, it is possible to formulate the time-domain SAFT algorithm in a new fashion that is both faster compared to conventional SAFT and the deconvolved input data can be focussed better leading to a higher resolution. Since sparse deconvolution could be implemented directly into the ultrasonic probe hardware/software measurement setup, this approach can significantly speed up measurements in time-critical environments. We test the proposed scheme on CIVA simulation data as well as measurements and show B- and C-images of raw SAFT vs. Orthogonal Matching Pursuit (OMP) + SAFT and Basis Pursuit Denoising (BPDN) + SAFT.""",2016 IEEE International Ultrasonics Symposium (IUS)
"""Estimating the Uniqueness of Test Scenarios derived from Recorded Real-World-Driving-Data using Autoencoders""",J. Langner; J. Bach; L. Ries; S. Otten; M. Holzäpfel; E. Sax;,2018,10.1109/IVS.2018.8500464,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8500464,;,"""Advanced Driver Assistant Systems (ADAS) use a multitude of input signals for tasks like trajectory planning and control of vehicle dynamics provided by a large variety of information sources such as sensors and digital maps. To assure the feature's valid behavior all realistically possible environmental situations have to be tested. The test scenarios used for simulation can be derived from real-worlddriving-data. However, the significance of derived scenarios is weakened by repetitive similar situations within the driving data, which increase the test efforts without providing new insights regarding the test of the ADAS. In this contribution, an automated selection algorithm for test scenarios based on relevant environmental parameters is presented. Starting with a randomly selected initial testset, the machine-learning concept of autoencoders is utilized to recognize novel scenarios within the data pool, which are iteratively added to the initial testset. Furthermore, the key parameters for the autoencoder's performance are shown in depths. The approach is fully automated, so that the identified novel scenarios within an entire testset are automatically combined to a reduced testset of unique relevant scenarios. The achieved testset reduction and thereby the saving potential in simulation time is demonstrated on a dataset including several thousand test kilometers.""",2018 IEEE Intelligent Vehicles Symposium (IV)
"""Reducing Annotation Times: Semantic Segmentation of Coral Reef Survey Images""",J. P. Pierce; Y. Rzhanov; K. Lowell; J. A. Dijkstra;,2020,10.1109/IEEECONF38699.2020.9389163,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9389163,Coral Reefs;Marine Images;Semantic Segmentation;Convolutional Neural Networks (CNNs);Fully Convolutional Networks (FCNs);Dense;Sparse;Annotations;,"""Benthic quadrat studies requiring time-intensive manual image annotation are currently a critical component of assessing the health of coral reefs. Patch-based image classification using convolutional neural networks (CNNs) can automate this task by providing sparse labels, but remain computationally inefficient. This work extends the idea of automatic image annotation by using fully convolutional networks (FCNs) to provide dense labels through semantic segmentation. We present an improved version the Multilevel Superpixel Segmentation (MSS) algorithm, which repurposes existing sparse labels for images by converting them into the dense labels necessary for training a FCN automatically. Our implementation-Fast-MSS-is demonstrated to perform considerably faster than the original without sacrificing accuracy. To showcase the applicability to benthic ecologists, we validate this method using the Moorea Labeled Coral (MLC) dataset as a benchmark. FCNs are evaluated by comparing their predictions on test images with the corresponding ground-truth sparse labels. Our results indicate that FCNs' perform with accuracies that are suitable for many ecological applications, and can increase even further when trained on dense labels augmented with additional sparse labels provided by a patch-based image classifier. The contributions of this study help move the field of benthic ecology towards more efficient monitoring of coral reefs through entirely automated processes.""",Global Oceans 2020: Singapore – U.S. Gulf Coast
"""The Application of Bayesian Penalty Regression in Sparse Regularization""",W. CHEN;,2020,10.1109/ICBASE51474.2020.00042,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9403778,regularization;shrinkage priors;Gibbs sampler;hierarchical models;convex penalty regression;non-convex penalty regression;,"""High-dimensional sparse data is prone to overfitting problems when building regression models, and regularization is a classic and effective method, which includes ridge regression, Lasso regression, and elastic net. In the Bayesian framework, the penalty term is derived from specific shrinkage priors, and hierarchical models and the Gibbs sampler are used in the simulation. In this paper, we use the concrete slump test dataset to establish the regression model and apply the Bayesian convex penalty regression and non-convex penalty regression to screen the variables in the model. The result is compared to the classic regularization method by evaluating the model based on the prediction results, and finally, we explain the advantages of the Bayesian models over the ordinary regularization method.""",2020 International Conference on Big Data & Artificial Intelligence & Software Engineering (ICBASE)
"""The Nature-Inspired BASIS Feature Descriptor for UAV Imagery and Its Hardware Implementation""",S. G. Fowers; D. -J. Lee; D. A. Ventura; J. K. Archibald;,2013,10.1109/TCSVT.2012.2223631,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6327642,Computer vision;feature description;feature descriptor;feature detection;feature detector;sparse coding;,"""This paper presents a feature descriptor well suited for limited-resource applications such as an unmanned aerial vehicle embedded systems, small microprocessors, and small low-power field programmable gate array (FPGA) fabric. The basis sparse-coding inspired similarity (BASIS) descriptor utilizes sparse coding to create dictionary images that model the regions in the human visual cortex. Due to the reduced amount of computation required for computing BASIS descriptors, reduced descriptor size, and the ability to create the descriptors without the use of a floating point, this approach is an excellent candidate for FPGA hardware implementation. The bit-level-accurate BASIS descriptor was tested on a dataset of real aerial images with the task of calculating a frame-to-frame homography and compared to software versions of scale-invariant feature transform (SIFT) and speeded-up robust features (SURF). Experimental results show that the BASIS descriptor outperforms SIFT and performs comparably to SURF on frame-to-frame aerial feature point matching. BASIS descriptors require less memory storage than other descriptors and can be computed entirely in hardware, allowing the descriptor to operate at real-time frame rates on a low-power embedded platform such as an FPGA.""",IEEE Transactions on Circuits and Systems for Video Technology
"""Sparse matrix computations on clusters with GPGPUs""",V. Cardellini; A. Fanfarillo; S. Filippone;,2014,10.1109/HPCSim.2014.6903665,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6903665,Sparse matrices;GPGPU computing;Message Passing Interface (MPI);,"""Hybrid nodes containing GPUs are rapidly becoming the norm in parallel machines. We have conducted some experiments regarding how to plug GPU-enabled computational kernels into PSBLAS, a MPI-based library specifically geared towards sparse matrix computations. In this paper, we present our findings on which strategies are more promising in the quest for the optimal compromise among raw performance, speedup, software maintainability, and extensibility. We consider several solutions to implement the data exchange with the GPU focusing on the data access and transfer, and present an experimental evaluation for a cluster system with up to two GPUs per node. In particular, we compare the pinned memory and the Open-MPI approaches, which are the two most used alternatives for multi-GPU communication in a cluster environment. We find that OpenMPI turns out to be the best solution for large data transfers, while the pinned memory approach is still a good solution for small transfers between GPUs.""",2014 International Conference on High Performance Computing & Simulation (HPCS)
"""Recognition of pests based on compressive sensing theory""",A. Han; H. Peng; J. Li; J. Han; X. Guo;,2011,10.1109/ICCSN.2011.6014437,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6014437,pests;recognition;compressive sensing;feature parameters;sparse decomposition;recognition precision;,"""In order to improve the performance of the existing recognition methods of pests, the limitations of these methods are analyzed in this paper. Based on the analysis, the novel recognition method of pests by using compressive sensing theory is presented in this paper. In the proposed method, a large number of representative training samples of pests are used to construct the training samples matrix, then the sparse decomposition representation of the testing samples of pests is obtained by solving the L1-norm optimization problem, which contains distinct class information and could be used for the different species of pests recognition directly. The 12 species of stored-grain pests and the 110 species of common pests are separately recognized by the proposed method. The experimental results prove that the application of compressive sensing theory in the recognition of pests is practical and feasible.""",2011 IEEE 3rd International Conference on Communication Software and Networks
"""Organ Location Determination and Contour Sparse Representation for Multiorgan Segmentation""",S. Li; H. Jiang; Y. -d. Yao; B. Yang;,2018,10.1109/JBHI.2017.2705037,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7930394,Energy function;extreme learning machine;image segmentation;location determination;sparse optimization;,"""Organ segmentation on computed tomography (CT) images is of great importance in medical diagnoses and treatment. This paper proposes organ location determination and contour sparse representation methods (OLD-CSR) for multiorgan segmentation (liver, kidney, and spleen) on abdomen CT images using an extreme learning machine classifier. First, a location determination method is designed to obtain location information of each organ, which is used for coarse segmentation. Second, for coarse-to-fine segmentation, a contour gradient and rate change based feature point extraction method is proposed. A sparse optimization model is developed for refining the contour feature points. Experimentations with 153 CT images demonstrate the performance advantages of OLD-CSR as compared with related work.""",IEEE Journal of Biomedical and Health Informatics
"""Multi-Level Cascade Sparse Representation Learning for Small Data Classification""",W. Zhong; H. Li; Q. Hu; Y. Gao; C. Chen;,2023,10.1109/TCSVT.2022.3222226,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9950524,Deep cascade;sparse representation;face recognition;small data;,"""Deep learning (DL) methods have recently captured much attention for image classification. However, such methods may lead to a suboptimal solution for small-scale data since the lack of training samples. Sparse representation stands out with its efficiency and interpretability, but its precision is not so competitive. We develop a Multi-Level Cascade Sparse Representation (ML-CSR) learning method to combine both advantages when processing small-scale data. ML-CSR is proposed using a pyramid structure to expand the training data size. It adopts two core modules, the Error-To-Feature (ETF) module, and the Generate-Adaptive-Weight (GAW) module, to further improve the precision. ML-CSR calculates the inter-layer differences by the ETF module to increase the diversity of samples and obtains adaptive weights based on the layer accuracy in the GAW module. This helps ML-CSR learn more discriminative features. State-of-the-art results on the benchmark face databases validate the effectiveness of the proposed ML-CSR. Ablation experiments demonstrate that the proposed pyramid structure, ETF, and GAW module can improve the performance of ML-CSR. The code is available at https://github.com/Zhongwenyuan98/ML-CSR.""",IEEE Transactions on Circuits and Systems for Video Technology
"""Joint Reflectivity and Structural Interval-Q Estimation by Using Nonstationary Sparse Inversion""",L. Cheng; S. Yuan; S. Wang;,2021,10.1109/LGRS.2020.2987395,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9080576,Interpreted horizon(s);inverted reflectivity;nonstationary sparse inversion;sparsity;structural interval-Q;,"""Reliable estimate of the anelastic attenuation factor- Q from seismic records is highly desirable for improving seismic resolution. However, the conventional equivalent- Q or horizontal interval- Q estimation ignores that Q-distribution should hold the same ability for the subsurface structure characterization as seismic data. To pursue an accurate Q-model, we propose a technique for joint reflectivity and structural interval- Q estimation by using nonstationary sparse inversion. We designed a structural interval- Q model by dividing the seismic data into several structural layers with the interpreted horizon(s). Attenuations in each layer are close to each other and can be described by an equivalent- Q or gradient- Q. Based on the attenuation theory, the nonstationary sparse inversion is solved iteratively, where, at each iteration, the equivalent- Q of only one layer is optimized by searching for the corresponding optimum inverted reflectivity, leading to a structural interval- Q model. The main advantages of our method are its objectivity and accuracy because of the integration of the prior structural information from interpreted horizons into joint reflectivity-estimation and Q-estimation. The test of synthetic and field data clearly illustrates that the proposed method enables high-precision structural interval- Q estimation and sufficiently compensates for Q-related attenuation.""",IEEE Geoscience and Remote Sensing Letters
"""A non-speculative parallelization of reverse cuthill-McKee algorithm for sparse matrices reordering""",T. N. Rodrigues; M. C. Silva Boeres; L. Catabriga;,2017,10.15439/2017F179,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8104594,;,"""This work presents a new parallel non-speculative implementation of the Unordered Reverse Cuthill-McKee algorithm. Reordering quality (bandwidth reduction) and reordering performance (CPU time) are evaluated in comparison with a serial implementation of the algorithm made available by the state-of-the-art mathematical software library HSL. The bandwidth reductions reached by our parallel RCM were more than 90% for several large matrices out of the ones tested, and the time reordering improvement was up to 57.82%. Speedups higher than 3.0X were achieved with the parallel RCM. The underlying parallelism was supported by the OpenMP framework and three strategies for reducing idle threads were incorporated into the algorithm.""",2017 Federated Conference on Computer Science and Information Systems (FedCSIS)
"""Power system modelling and sparse matrix operations using object-oriented programming""",B. Hakavik; A. T. Holen;,1994,10.1109/59.317627,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=317627,;,"""This paper reports on power system modelling and sparse matrix operations using object-oriented programming (OOP). It has been claimed that OOP leads to more flexible, modular and reusable code, and that programs can be written more generally. The main focus of the paper is OOP design principles and practical implementations for power systems. Specific examples included demonstration of a power system model design, particularly focusing on OOP mechanisms and object-oriented style of programming. Also demonstrated, using OOP, are tailor-made sparse matrix storage schemes and operations. Numerical tests indicate that the proposed design is efficient compared to standard numerical library routines, and that the particular OOP features that are used to obtain flexibility etc. do not significantly increase computation time.<>""",IEEE Transactions on Power Systems
"""Two Sparsification Strategies for Accelerating Demand-Driven Pointer Analysis""",K. Karakaya; E. Bodden;,2023,10.1109/ICST57152.2023.00036,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10132184,sparse pointer analysis;demand-driven analysis;data-flow analysis;,"""To resolve aliasing, precise program analyses rely on pointer analyses. Demand-driven pointer analysis seeks to be efficient by computing information only for variables on which a demand is raised, through a points-to or alias query. Yet, research has shown that when applied to large-scale programs even demand-driven analyses can become expensive in terms of memory and runtime. This paper thus investigates to what extent demand-driven pointer analysis can be accelerated further if being executed over a sparse control-flow graph (CFG), specialized to those queries. We investigate two designs: First, typeaware sparsification, in which the resulting CFG only consists of statements containing variables that are type compatible with the query variable. Second, alias-aware sparsification, where the resulting CFG consists of the def-use chains of the query variable and all its intra-procedural aliases.We implement both designs in SparseBoomerang by extending Boomerang, a pointer analysis framework based on push-down systems. We evaluate SparseBoomerang by comparing it to Boomerang in terms of precision and performance. On the Pointerbench micro-benchmark suite for alias analysis, SparseBoomerang maintains the precision of Boomerang, in both designs. We evaluate the runtime and memory performance of SparseBoomerang by using Flowdroid as a taint analysis client on real-world apps. Compared to the baseline Boomerang, on average SparseBoomerang solves alias queries 2.4x faster when using the type-aware sparsification strategy, and 2.8x faster when using the alias-aware variant with negligible memory overhead.""","2023 IEEE Conference on Software Testing, Verification and Validation (ICST)"
"""Classification of Power Quality Disturbances using the Unique Combination of Hilbert Transform, Image Processing and K-Nearest Neighbor""",R. S. Kankale; S. R. Paraskar; S. S. Jadhao;,2022,10.1109/ICETEMS56252.2022.10093403,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10093403,Hilbert Transform. Image Processing;K-Nearest Neighbor;Power Quality Disturbances;,"""This paper introduces the unique combination of Hilbert Transform (HT), Image Processing, and K-Nearest Neighbor (KNN) for classifying the Power Quality Disturbances (PQDs). Power Quality (PQ) is a term that is frequently used these days. Everyone is cautious of the power supply they are purchasing from the utility because the end-user sensitive equipment may malfunction or trip as a result ofPQDs. In order to get a clean and disturbance free power supply, the utility needs to identify the type of disturbance, the cause of the disturbance, and mitigate it. This paper presents a novel approach for classifying the commonly occurring PQDs like sag, swell, and interruption. The proposed algorithm is realized by generating voltage signals pertaining to the PQDs using integral mathematical models, Simulink models, and experimentation. The voltage signals related to different PQDs are processed using HT and the processed signals having elliptical shapes are plotted and converted into images. These images are further processed using the image processing technique in order to turn the RGB image into a grayscale image. The statistical parameters namely mean and standard deviation are calculated from the grayscale image input to the algorithm for feature extraction. The KNN classifier is trained and tested using these extracted features. In the KNN classifier, the minimum Euclidean distance is calculated to identify the class of PQDs with high accuracy.""",2022 International Conference on Emerging Trends in Engineering and Medical Sciences (ICETEMS)
"""Sparse representation super-resolution method for enhancement analysis in video forensics""",N. A. Zamani; A. D. M. Zahamdin; S. N. H. S. Abdullah; M. J. Nordin;,2012,10.1109/ISDA.2012.6416661,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6416661,Super-resolution;object hallucination;sparse coding;Non-Negative Matrix Factorization;video forensics;,"""The enhancement analysis in video forensics is used to enhance the clarity of video frames of a video exhibit. The enhanced version of these video frames is important as to assist law enforcement agency for investigation or to be tended as evidence in court. The most significant problem observed in the analysis is the enhancement of objects under probe in video. In many cases, the probes appeared to be in low-resolution and degraded with noise, lens blur and compression artifacts. The enhancement of these low quality probes via conventional method of denoising and resizing has proven to further degrade the quality of the prober The objective of this paper is to propose an enhancement analysis algorithm based on super-resolution. Hence, we present an solution which is a single-frame solution for super-resolution. For that purpose, our proposed method incorporates sparse coding with Non-Negative Matrix Factorization in order to improve hallucination of probes in video. Sparse coding is employed in learning a localized part-based subspace which synthesizes higher resolution with respect to overcomplete patch dictionaries. We test our proposed method and compare with state-of-the-art methods namely resampling and super-resolution method, by enhancing probes in exhibit videos. We measure the image quality using peak-signal-to-noise-ratio. The result shows that our proposed method outperforms state-of the-art methods after enhancing probes in exhibit videos.""",2012 12th International Conference on Intelligent Systems Design and Applications (ISDA)
"""Unique optical networking facilities and cross-layer networking""",I. Baldine;,2009,10.1109/LEOSST.2009.5226210,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5226210,;,"""The future global network will have a number of characteristics, enabled by novel optical technologies, that make it distinct from today's network: optical domain will likely become part of the convergence layer and service provisioning will involve explicit cross-layer interactions and flexible optical spectrum management. These will be tied together by intelligent cross-layer protocol software. In order to enable this transition, we need a new networking protocol architecture and network facilities that allow experimentation across all networking layers at scale.""",2009 IEEE/LEOS Summer Topical Meeting
"""Electromagnetic Simulations with 3D FEM and Intel Optane Persistent Memory""",M. Jakubowski; P. Sypek;,2022,10.23919/MIKON54314.2022.9924749,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9924749,Intel Optane PMem;finite element method;sparse matrix factorization;Intel PARDISO;,"""Intel Optane persistent memory has the potential to induce a change in how high-performance calculations requiring a large system memory capacity are conducted. This article presents what this change may look like in the case of factorization of large sparse matrices describing electromagnetic problems arising in the 3D FEM analysis of passive high-frequency components. In numerical tests, the Intel oneAPI MKL PARDISO was used to solve relatively large electromagnetic problems defined using the finite element method.""",2022 24th International Microwave and Radar Conference (MIKON)
"""Design and realisation of a unique MV converter implemented in a new power electronic equipment test laboratory for emerging MV applications""",E. de Jong; E. de Meulemeester; P. Heskes;,2008,10.1109/PES.2008.4596544,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4596544,Power quality;programmable grid;power electronic converter;research facility;,"""This paper describes the design philosophy, commissioning and validation of a 4 Q, 3.3 kV, 1 MVA power converter intended for a power electronics test laboratory. The focus lies on the sophisticated control algorithms responsible for creating a long lasting programmed ""bad grid"" (DC and AC), as well as the modelling effort required to identify the safe operating area within the vast possibilities. The paper will highlight the technological advancement achieved, in hardware and control software and the experience gained during commissioning.""",2008 IEEE Power and Energy Society General Meeting - Conversion and Delivery of Electrical Energy in the 21st Century
"""A Deep Learning Method for Microaneurysm Detection in Fundus Images""",J. Shan; L. Li;,2016,10.1109/CHASE.2016.12,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7545864,deep learning;stacked sparse autoencoder;feature representation;automated microaneurysm detection;diabetic retinopathy;,"""Diabetic Retinopathy (DR) is the leading cause of blindness in the working-age population. Microaneurysms (MAs), due to leakage from retina blood vessels, are the early signs of DR. However, automated MA detection is complicated because of the small size of MA lesions and the low contrast between the lesion and its retinal background. Recently deep learning (DL) strategies have been used for automatic feature extraction and classification problems, especially for image analysis. In this paper, a Stacked Sparse Autoencoder (SSAE), an instance of a DL strategy, is presented for MA detection in fundus images. Small image patches are generated from the original fundus images. The SSAE learns high-level features from pixel intensities alone in order to identify distinguishing features of MA. The high-level features learned by SSAE are fed into a classifier to categorize each image patch as MA or non-MA. The public benchmark DIARETDB is utilized to provide the training/testing data and ground truth. Among the 89 images, totally 2182 image patches with MA lesions, serve as positive data, and another 6230 image patches without MA lesions are generated by a randomly sliding window operation, to serve as negative data. Without any blood vessel removal or complicated preprocessing operations, SSAE learned directly from the raw image patches, and automatically extracted the distinguishing features to classify the patches using Softmax Classifier. By employing the fine-tuning operation, an improved F-measure 91.3% and an average area under the ROC curve (AUC) 96.2% were achieved using 10-fold cross-validation.""","2016 IEEE First International Conference on Connected Health: Applications, Systems and Engineering Technologies (CHASE)"
"""Hierarchical sparse representation with adaptive dictionaries for image super-resolution""",X. Wu; D. Deng; J. Li; X. Luo; K. Zeng;,2013,10.1109/CISP.2013.6744001,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6744001,feature clustering;super-resolution;Adaptive dictionary;,"""This paper presents an image hierarchical super-resolution (SR) method with adaptive dictionaries, based on signal sparse representation. It can not only improve image detail quality but also reduce computational cost. Research on the human visual system suggests that our eyes are mainly sensitive to high-frequency contents. Inspired by this observation, we implemented a hierarchical process where an image was decomposed into a detail layer and a base layer. The detail layer is reconstructed through an over-complete dictionary while the base layer is interpolated by bi-cubic. Through these, we can keep the HR details better. Next is how to accelerate while keeping good quality. In our method, adaptive dictionaries are trained by feature clustering. Firstly, we train low dimension sub-dictionaries to reduce time complexity. Secondly, then we apply overlapping feature clustering to the training. Thus dictionaries can be adaptive and more complete. All these can also prevent sub-dictionaries with over strong independence but less compatibility. Besides, initializing the sparse coefficients also plays an important role in our acceleration. Experimental results validate that ours are competitive or even superior in quality than those produced by other methods and our test data indicates substantial reduction in processing time over other similar SR methods.""",2013 6th International Congress on Image and Signal Processing (CISP)
"""A Unique Portable Signal Acquisition/Processing Station""",R. D. Garron; S. G. Azevedo;,1983,10.1109/TNS.1983.4333035,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4333035,;,"""At Lawrence Livermore National Laboratory, there are experimental applications requiring digital signal acquisition as well as data reduction and analysis. A prototype Signal Acquisition/Processing Station (SAPS) has been constructed and is currently undergoing tests. The system employs an LSI-11/23 computer with Data Translation analog-to-digital hardware. SAPS is housed in a roll-around cart which has been designed to withstand most subtle EMI/RFI environments. A user-friendly menu allows a user to access powerful data acquisition packages with a minimum of training. The software architecture of SAPS involves two operating systems, each being transparent to the user. Since this is a general purpose workstation with several units being utilized, an emphasis on low cost, reliability, and maintenance was stressed during conception and design. The system is targeted fur mid-range frequency data acquisition; between a data logger and a transient digitizer.""",IEEE Transactions on Nuclear Science
"""Sparseland model for speckle suppression of B-mode ultrasound images""",M. Srinivas; R. Bharath; P. Rajalakshmi; C. Krishna Mohan;,2015,10.1109/NCC.2015.7084842,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7084842,Dictionary learning;K-SVD;multiplicative noise;speckle;sparse representations;,"""Speckle is a multiplicative noise which is inherent in medical ultrasound images. Speckles contributes high variance between neighboring pixels reducing the visual quality of an image. Suppression of speckle noise significantly improves the diagnostic content present in the image. In this paper, we propose how sparseland model can be used for speckle suppression. The performance of the model is evaluated based on variance to mean ratio of a patch in the filtered image. The algorithm is tested on both software generated images and real time ultrasound images. The proposed algorithm has performed similar to past adaptive speckle suppression filters and seems promising in improving diagnostic content.""",2015 Twenty First National Conference on Communications (NCC)
"""Efficient computation of unique input/output sequences in finite state machines""",K. Naik;,1997,10.1109/90.649519,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=649519,;,"""This paper makes two contributions toward computing unique input/output (UIO) sequences in finite-state machines. Our first contribution is to compute all UIO sequences of minimal lengths in a finite-state machine. Our second contribution is to present a generally efficient algorithm to compute a UIO sequence for each state, if it exists. We begin by defining a path vector, vector perturbation, and UIO tree. The perturbation process allows us to construct the complete UIO tree for a machine. Each sequence of input/output from the initial vector of a UIO tree to a singleton vector represents a UIO sequence. Next, we define the idea of an inference rule that allows us to infer UIO sequences of a number of states from the UIO sequence of some state. That is, for a large class of machines, it is possible to compute UIO sequences for all possible states from a small set of initial UIOs. We give a modified depth-first algorithm, called the hybrid approach, that computes a partial UIO tree, called an essential subtree, from which UIO sequences of all possible states can be inferred. Using the concept of projection machines, we show that sometimes it is unnecessary to construct even a partial subtree. We prove that if a machine remains strongly connected after deleting all the converging transitions, then all of the states have UIO sequences. To demonstrate the effectiveness of our approach, we develop a tool to perform experiments using both small and large machines.""",IEEE/ACM Transactions on Networking
"""Group Sparse Radiomics Representation Network for Diagnosis of Alzheimer's Disease Using Triple Graph Convolutional Neural Network""",C. Zhu; Z. Song; Y. Wang; M. Jiang; L. Song; Q. Zheng;,2023,10.1109/CSECS60003.2023.10428290,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10428290,Brain networks;Sparse representation;Alzheimer's disease;Graph neural network;Magnetic resonance imaging;,"""Brain network have been widely used in the study of brain abnormalities in Alzheimer's disease (AD). The construction of network is important for brain characterization and graph neural networks (GNNs)-based classification tasks. Currently, structural magnetic resonance imaging (sMRI)-based structural covariance networks usually only consider interactions between pairs of brain regions at the individual level, and do not take into account higher-order relationships between multiple brain regions or subjects. To overcome these limitations, we proposed the group sparse radiomics representation network (GSR2N) which utilizes group sparse representation (GSR) instead of traditional Pearson correlation (PC) construction method. This reduces inter-individual differences in network topology and accounts for mixing effects among multiple brain regions. We conducted extensive experiments using the ADNI and AIBL dataset on triple graph convolutional network (TGCN) to compare the classification performance of different network construction methods. The results demonstrate that GSR method has the highest classification performance, with intra and inter-dataset test accuracies of 0.894 and 0.856, respectively. This new approach is expected to contribute to future sMRI-based brain network construction and brain disease diagnosis.""",2023 6th International Conference on Software Engineering and Computer Science (CSECS)
"""SparseJSR: A Fast Algorithm to Compute Joint Spectral Radius via Sparse SOS Decompositions""",J. Wang; M. Maggio; V. Magron;,2021,10.23919/ACC50511.2021.9483347,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9483347,;,"""This paper focuses on the computation of joint spectral radii (JSR), when the involved matrices are sparse. We provide a sparse variant of the procedure proposed by Parrilo and Jadbabaie in [24], to compute upper bounds of the JSR by means of sum-of-squares (SOS) relaxations. Our resulting iterative algorithm, called SparseJSR, is based on the term sparsity SOS (TSSOS) framework, developed by Wang, Magron and Lasserre in [37], yielding SOS decompositions of polynomials with arbitrary sparse support. SparseJSR exploits the sparsity of the input matrices to significantly reduce the computational burden associated with the JSR computation. Our algorithmic framework is then successfully applied to compute upper bounds for JSR, on randomly generated benchmarks as well as on problems arising from stability proofs of controllers, in relation with possible hardware and software faults.""",2021 American Control Conference (ACC)
"""PSyGS Gen A Generator of Domain-Specific Architectures to Accelerate Sparse Linear System Resolution""",N. Nicolosi; F. R. Negri; F. Pesce; F. Peverelli; D. Conficconi; M. D. Santambrogio;,2024,10.1109/IPDPSW63119.2024.00015,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10596450,Gauss-Seidel;sparse linear systems;graph coloring;RISC;hardware acceleration;SoC;Chipyard;FPGA;,"""Sparse linear system resolution is a crucial task in scientific computing. The symmetric Gauss-Seidel algorithm (GS) is an efficient iterative resolution method. However, its inherently sequential nature makes it challenging to parallelize. The state-of-the-art parallelization techniques for the GS iteration are based on the concept of graph coloring, and they have been exploited to speed up the execution of each iteration of the algorithm on CPUs, e.g., in the context of the HPCG benchmark. Nevertheless, existing hardware accelerators for GS execute the serial algorithm, often tailoring it to a specific application scenario, limiting its applicability. We propose a hardware-software codesign methodology that leverages state-of-the-art software graph coloring and a generator of domain specific architectures (DSAs) that computes GS using coloring-based hardware parallelization. PSyGS Gen (Parallel Symmetric Gauss-Seidel Generator) is the first architecture to parallelize the Gauss-Seidel iteration via graph coloring. By comparing the performance of the DSAs generated by PSyGS Gen to a parallelized version of the algorithm implemented using coloring and OpenMP and run on a CPU, we achieve from 1.23x to 34.84x higher parallel efficiency than the best CPU run. The DSAs generated using PSyGS Gen are programmable, therefore, they support any present and future coloring techniques. Moreover, they also support the execution of the algorithm for a fixed number of iterations, allowing its usage as a smoother in more complex iterative methods. These programmability features, with the solution being open-source and integrated with the Chipyard SoC generator, allow it to be integrated into more complex flows of computation such as the HPCG benchmark.""",2024 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)
"""Sparse Codes Auto-Extractor for Classification: A Joint Embedding and Dictionary Learning Framework for Representation""",Z. Zhang; F. Li; T. W. S. Chow; L. Zhang; S. Yan;,2016,10.1109/TSP.2016.2550016,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7446342,sparse codes auto-extractor;embedding learning;dictionary learning;feature representation;joint classification;Sparse codes auto-extractor;embedding learning;dictionary learning;feature representation;joint classification;,"""In this paper, we discuss the sparse codes auto-extractor based classification. A joint label consistent embedding and dictionary learning approach is proposed for delivering a linear sparse codes auto-extractor and a multi-class classifier by simultaneously minimizing the sparse reconstruction, discriminative sparse-code, code approximation and classification errors. The auto-extractor is characterized with a projection that bridges signals with sparse codes by learning special features from input signals for characterizing sparse codes. The classifier is trained based on extracted sparse codes directly. In our setting, the performance of the classifier depends on the discriminability of sparse codes, and the representation power of the extractor depends on the discriminability of input sparse codes, so we incorporate label information into the dictionary learning to enhance the discriminability of sparse codes. So, for inductive classification, our model forms an integration process from test signals to sparse codes and finally to assigned labels, which is essentially different from existing sparse coding based approaches that involve an extra sparse reconstruction with the trained dictionary for each test signal. Remarkable results are obtained by our model compared with other state-of-the-arts.""",IEEE Transactions on Signal Processing
"""Unmanned Aerial Vehicles unique cost estimating requirements""",P. Malone; H. Apgar; S. Stukes; S. Sterk;,2013,10.1109/AERO.2013.6496852,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6496852,;,"""Unmanned Aerial Vehicles (UAVs), also referred to as drones, are aerial platforms that fly without a human pilot onboard. UAVs are controlled autonomously by a computer in the vehicle or under the remote control of a pilot stationed at a fixed ground location. There are a wide variety of drone shapes, sizes, configurations, complexities, and characteristics. Use of these devices by the Department of Defense (DoD), NASA, civil and commercial organizations continues to grow. UAVs are commonly used for intelligence, surveillance, reconnaissance (ISR). They are also use for combat operations, and civil applications, such as firefighting, non-military security work, surveillance of infrastructure (e.g. pipelines, power lines and country borders). UAVs are often preferred for missions that require sustained persistence (over 4 hours in duration), or are “too dangerous, dull or dirty” for manned aircraft. Moreover, they can offer significant acquisition and operations cost savings over traditional manned aircraft. Because of these unique characteristics and missions, UAV estimates require some unique estimating methods. This paper describes a framework for estimating UAV systems total ownership cost including hardware components, software design, and operations. The challenge of collecting data, testing the sensitivities of cost drivers, and creating cost estimating relationships (CERs) for each key work breakdown structure (WBS) element is discussed. The autonomous operation of UAVs is especially challenging from a software perspective.""",2013 IEEE Aerospace Conference
"""On Gleaning Knowledge From Cross Domains by Sparse Subspace Correlation Analysis for Hyperspectral Image Classification""",X. Li; L. Zhang; B. Du; L. Zhang;,2019,10.1109/TGRS.2018.2882420,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8574055,Canonical correlation analysis (CCA);classification;domain adaptation;hyperspectral data;latent sparse representation;remote sensing;transfer learning;,"""Despite the availability of an increasing amount of remote sensing images, problems still arise in that the knowledge from existing images is underutilized and the collection of reference knowledge for each newly obtained image is expensive. Recently, an attractive solution called “transfer learning” has received increasing attention in the remote sensing field, by transferring knowledge from source domains to help improve the learning procedure in the target domain. In this paper, we propose a sparse subspace correlation analysis-based supervised classification (SSCA-SC) method for transfer learning in hyperspectral remote sensing image classification, which is not restricted by the data dimensionality or the data acquisition sensors. Specifically, we first propose a sparse subspace correlation analysis (SSCA) method to simultaneously learn the optimal projection matrices for heterogeneous domains into a common subspace and obtain sparse reconstruction coefficients over a shared self-expressive dictionary in the derived subspace. In order to fully utilize the label information to improve the class separability, the SSCA-SC framework learns more discriminative representations for the input data by training a corresponding SSCA model for each class. As a result, the projected data belonging to the same class are maximally correlated and represented well, while those from different classes will have a low correlation. Another advantage of the SSCA-SC framework lies in the fact that it not only learns new representations for the data from different domains but it also designs a discriminative and robust classifier that properly adapts to the new representation. The proposed method was tested with three hyperspectral remote sensing data sets, and the experimental results confirm the effectiveness and reliability of the proposed SSCA-SC method.""",IEEE Transactions on Geoscience and Remote Sensing
"""Road Enforcement Monitoring System based on Vehicle Type Recognition using Sparse Filtering Convolutional Neural Network with Layer Skipping Strategy (SFCNNLS)""",S. Awang; N. M. A. N. Azmi; N. A. Ghani;,2019,10.1109/IEA.2019.8715122,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8715122,vehicle type classification;convolutional neural network;deep learning;computational intelligence;,"""Road Enforcement Monitoring System (REMS) is one of the traffic monitoring systems to monitor the enforcement of a specific route for public transportation in cities. The aim of this system is to automatically and efficiently monitor the enforcement to ensure it is adhered by the traffic users. This aim is difficult to be achieved in current practice that relied on human observation by the authorities. Due to that, we proposed to combine REMS with vehicle type recognition (VTR) method known as Sparse-Filtered Convolutional Neural Network with Layer Skipping-strategy (SF-CNNLS). The purpose of using this method is to recognize and classify the vehicles that use the specific route. It is to prevent any vehicle other than public transportations use that route. The output from VTR will be used by REMS to trigger an immediate message to the authorities for further action. The major challenge of our method is to differentiate taxi and bus as public transportations with car and truck. This is because these vehicles have almost similar features. We tested our method with a self-obtained video that captured from a mounted-camera to observe if the challenge is able to be overcome. For the initial stage, the test is deployed on 4 major vehicle classes; car, taxi, truck and bus. The highest accuracy is obtained from car class with 92.5% and an average accuracy is 81.76%. Based on the test, we proved that our method is able to recognize and classify the vehicle classes although the vehicles are sharing almost similar features.""",2019 IEEE 6th International Conference on Industrial Engineering and Applications (ICIEA)
"""Novel Randomized & Biased Placement for FPGA Based Robust Random Number Generator with Enhanced Uniqueness""",A. S. Chauhan; V. Sahula; A. S. Mandal;,2019,10.1109/VLSID.2019.00079,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8710956,Hardware Security;PUF;Ring Oscillator;FPGA;Random Number Generator;Biased Placement;NIST Statistical Test;,"""The physical unclonable functions (PUF) have widely been used to provide software as well as hardware security for the cyber-physical systems. They are used for performing significant cryptography tasks such as generating keys, device authentication and securing against IP piracy. They have also been used to produce the root of trust. However, they lack in reliability metric. We present a novel approach for improving the uniqueness as well as the reliability of field programmable gated arrays (FPGAs) based ring oscillator PUF and derive a random number, consuming a very small area concerning look up tables (LUTs). We use profiling method for observing frequency variations in ring oscillators (RO), spatially placed across the FPGA floor, and are able to spot the suitable locations for RO mapping, which leads to enhanced ROPUF reliability. We have implemented proposed methodology on Xilinx -7 series FPGAs and tested the robustness against environmental variations e.g. temperature and supply voltage variations. The proposed approach achieves 6% higher uniqueness of 49.83% along with the reliability of 99.35%, which as a group of PUF characteristics, is a significant improvement as compared to characteristics provided by existing ROPUF methods. The random number generator so realized passes all applicable nine tests of NIST uniformity statistical test suite.""",2019 32nd International Conference on VLSI Design and 2019 18th International Conference on Embedded Systems (VLSID)
"""Array Bounds Check Elimination for Java Based on Sparse Representation""",K. Yang; Z. Huang; M. Yang;,2009,10.1109/SERA.2009.11,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5381765,Java;array bounds check elimination;optimization;performance;,"""As a type-safe program language, Java requires bounds checks of array accesses. Whenever an array element is accessed, a cmp (compare) instruction is executed to check whether the index value is within the valid bounds. Array bounds checks may prevent many useful optimizations because of precise exception. We present a new ABCE (Array Bounds Check Elimination) algorithm to eliminate redundant checks based on sparse representation for a Java static compiler. In contrast to other approaches performing in JVMs, we adhere to the design principle of the static compiler to optimize scientific Java applications. The algorithm is a light-weight algorithm working on an intermediate representation in Static Single Assignment form. It fully removes bounds checks if it can be proven that they never fail. Whenever possible, it moves bounds checks out of loops to reduce the total number of executed checks. If such a check fails, the executing program branches into the unmodified loop to preserve the exception semantics of Java. For the scientific SciMark 2.0 benchmark suite, this algorithm removes on average 76% of bounds check instructions. The evaluation shows a speedup near to the theoretical maximum for LU test case.""","2009 Seventh ACIS International Conference on Software Engineering Research, Management and Applications"
"""A Multi-Objective Learning Method for Building Sparse Defect Prediction Models""",X. Li; X. Yang; J. Su; W. Wen;,2020,10.1109/QRS51102.2020.00037,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9282752,software defect prediction;multi-objective learning methods;ranking task;cross-version defect prediction;,"""Software defect prediction constructs a model from the previous version of a software project to predict defects in the current version, which can help software testers to focus on software modules with more defects in the current version. Most existing methods construct defect prediction models through minimizing the defect prediction error measures. Some researchers proposed model construction approaches that directly optimized the ranking performance in order to achieve an accurate order. In some situations, the model complexity is also considered. Therefore, defect prediction can be seen as a multi-objective optimization problem and should be solved by multi-objective approaches. And hence, in this paper, we employ an existing multi-objective evolutionary algorithm and propose a new multi-objective learning method based on it, to construct defect prediction models by simultaneously optimizing more than one goal. Experimental results over 30 sets of cross-version data show the effectiveness of the proposed multi-objective approaches.""","2020 IEEE 20th International Conference on Software Quality, Reliability and Security (QRS)"
"""Uniqueness of tomography with unknown view angles""",S. Basu; Y. Bresler;,2000,10.1109/83.846251,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=846251,;,"""In the standard two-dimensional (2-D) parallel beam tomographic formulation, it is assumed that the angles at which the projections were acquired are known. In certain situations, however, these angles are known only approximately (as in the case of magnetic resonance imaging (MRI) of a moving patient), or are completely unknown. The latter occurs in a three-dimensional (3-D) version of the problem in the electron microscopy-based imaging of viral particles. We address the problem of determining the view angles directly from the projection data itself in the 2-D parallel beam case. We prove the surprising result that under some fairly mild conditions, the view angles are uniquely determined by the projection data. We present conditions for the unique recovery of these view angles based on the Helgasson-Ludwig consistency conditions for the Radon transform, we also show that when the projections are shifted by some random amount which must be jointly estimated with the view angles, unique recovery of both the shifts and view angles is possible.""",IEEE Transactions on Image Processing
"""High-Dimensional Sparse Graph Estimation by Integrating DTW-D Into Bayesian Gaussian Graphical Models""",Y. Li; X. Xu; J. Li;,2018,10.1109/ACCESS.2018.2849213,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8396215,Bayesian model selection;birth-death process;dynamic time warping;graphical structure learning;G-Wishart;Markov chain Monte Carlo;,"""Graphical models provide an effective way to reveal complicated associations in data and especially to learn the structures among large numbers of variables with respect to few observations in a high-dimensional space. In this paper, a novel graphical algorithm that integrates the dynamic time warping (DTW)-D measure into the birth-death Markov Chain Monte Carlo (BDMCMC) methodology (DTWD-BDMCMC) is proposed for modeling the intrinsic correlations buried in data. The DTW-D, which is the ratio of DTW over the Euclidean distance (ED), is targeted to calibrate the warping observation sequences. The approach of the BDMCMC is a Bayesian framework used for structure learning in sparse graphical models. In detail, a modified DTW-D distance matrix is first developed to construct a weighted covariance instead of the traditional covariance calculated with the ED. We then build on Bayesian Gaussian models with the weighted covariance with the aim to be robust against problems of sequence distortion. Moreover, the weighted covariance is used as limited prior information to facilitate an initial graphical structure, on which we finally employ the BDMCMC for the determination of the reconstructed Gaussian graphical model. This initialization is beneficial to improve the convergence of the BDMCMC sampling. We implement our method on broad simulated data to test its ability to deal with different kinds of graphical structures. This paper demonstrates the effectiveness of the proposed method in comparison with its rivals, as it is competitively applied to Gaussian graphical models and copula Gaussian graphical models. In addition, we apply our method to explore real-network attacks and genetic expression data.""",IEEE Access
"""The Uniqueness of Changes: Characteristics and Applications""",B. Ray; M. Nagappan; C. Bird; N. Nagappan; T. Zimmermann;,2015,10.1109/MSR.2015.11,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180065,;,"""Changes in software development come in many forms. Some changes are frequent, idiomatic, or repetitive (e.g. Adding checks for nulls or logging important values) while others are unique. We hypothesize that unique changes are different from the more common similar (or non-unique) changes in important ways, they may require more expertise or represent code that is more complex or prone to mistakes. As such, these unique changes are worthy of study. In this paper, we present a definition of unique changes and provide a method for identifying them in software project history. Based on the results of applying our technique on the Linux kernel and two large projects at Microsoft, we present an empirical study of unique changes. We explore how prevalent unique changes are and investigate where they occur along the architecture of the project. We further investigate developers' contribution towards uniqueness of changes. We also describe potential applications of leveraging the uniqueness of change and implement two of those applications, evaluating the risk of changes based on uniqueness and providing change recommendations for non-unique changes.""",2015 IEEE/ACM 12th Working Conference on Mining Software Repositories
"""Sparse Weighted Naive Bayes Classifier for Efficient Classification of Categorical Data""",Z. Zheng; Y. Cai; Y. Yang; Y. Li;,2018,10.1109/DSC.2018.00110,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8411931,weighted naive Bayes; feature selection; sparse regression; L1 regularized learning;,"""Feature selection has become a key challenge in machine learning with the rapid growth of data size in real-world applications. However, existing feature selection methods mainly focus on numeric data, which will lead to quality loss when handling classification problems involving categorical variables. In this paper, we proposed an improvement of Bayesian Classifier with the sparse regression technology. To the best of our knowledge, this is the first attempt to extend sparse regression for directly process of categorical variables. We implemented the idea for the case of weighted naive Bayes classifier. The introduction of L1 regularized learning ensures the algorithm to retain only a minimal subset of variables for model building, while at the same time achieves a near-optimal decision hyper-plane, which leads to excellent performance in case of high dimensional or small sample size situations. We carried out benchmark test on five UCI benchmark categorical data sets, which proved that the proposed algorithm have competitive performances over the original weighted naive bayes classifier and several state-of-the-art feature selection methods including L1 logistic regression and SVM-RFE.""",2018 IEEE Third International Conference on Data Science in Cyberspace (DSC)
"""Sparse Multiview Task-Centralized Ensemble Learning for ASD Diagnosis Based on Age- and Sex-Related Functional Connectivity Patterns""",J. Wang; Q. Wang; H. Zhang; J. Chen; S. Wang; D. Shen;,2019,10.1109/TCYB.2018.2839693,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8388295,ABIDE;autism spectrum disorder (ASD);diagnosis;high-order functional connectivity (FC);machine learning;multiview multitask (MVMT) learning;sparse multiview task-centralized (Sparse-MVTC) learning;,"""Autism spectrum disorder (ASD) is an ageand sex-related neurodevelopmental disorder that alters the brain's functional connectivity (FC). The changes caused by ASD are associated with different age and sex-related patterns in neuroimaging data. However, most contemporary computer-assisted ASD diagnosis methods ignore the aforementioned age-/sexrelated patterns. In this paper, we propose a novel sparse multiview task-centralized (Sparse-MVTC) ensemble classification method for image-based ASD diagnosis. Specifically, with the age and sex information of each subject, we formulate the classification as a multitask learning problem, where each task corresponds to learning upon a specific age/sex group. We also extract multiview features per subject to better reveal the FC changes. Then, in Sparse-MVTC learning, we select a certain central task and treat the rest as auxiliary tasks. By considering both task-task and view-view relationships between the central task and each auxiliary task, we can learn better upon the entire dataset. Finally, by selecting the central task, in turn, we are able to derive multiple classifiers for each task/group. An ensemble strategy is further adopted, such that the final diagnosis can be integrated for each subject. Our comprehensive experiments on the ABIDE database demonstrate that our proposed Sparse-MVTC ensemble learning can significantly outperform the state-of-the-art classification methods for ASD diagnosis.""",IEEE Transactions on Cybernetics
"""A Theoretical Model to Link Uniqueness and Min-Entropy for PUF Evaluations""",C. Gu; W. Liu; N. Hanley; R. Hesselbarth; M. O'Neill;,2019,10.1109/TC.2018.2866241,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8444682,Entropy;physical unclonable functions;uniqueness;,"""Physical unclonable functions (PUFs) are security primitives that enable the extraction of digital identifiers from electronic devices, based on the inherent silicon process variations between devices which occur during the manufacturing process. Due to the intrinsic and lightweight nature of a PUF, they have been proposed to provide security at a low cost for many applications, in particular for the internet of things (IoT). Many metrics have been proposed to evaluate the security and performance of PUF architectures, two of which are uniqueness and min-entropy. The uniqueness of a PUF response evaluates its ability to differentiate between different physical devices, while the min-entropy estimation is a measure of how much uncertainty the PUF response contains. The min-entropy is a lower-bound of real entropy. When the uniqueness of a PUF design is close to the optimal, it is unclear if this also implies that the design has a significantly high entropy; hence it would be useful to ascertain the minimum uniqueness required to achieve a given entropy. To date, a thorough investigation of the relationship between uniqueness and entropy for PUF designs has not been conducted. In this paper, this relationship between the uniqueness and entropy is explored, and for the first time, to the authors' knowledge, the relationship between them is modeled. To verify this model, both simulated and hardware-based experimental results are performed, with a test-bed containing 184 Xilinx Artix-7 FPGA based Basys3 boards providing a large data set for granular results. The experimental results demonstrate that the proposed model accurately estimates the relationship between uniqueness and min-entropy, with both the theoretical analysis and software simulations closely matching the experimental results.""",IEEE Transactions on Computers
"""Half-broken rotor bar detection on IM by using sparse representation under different load conditions""",C. Morales-Perez; J. Rangel-Magdaleno; H. Peregrina-Barreto; J. Ramirez-Cortes;,2017,10.1109/ROPEC.2017.8261594,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8261594,Broken rotor bar;induction motor;raw signal;signal classification;sparse representation;time domain;,"""Currently, the Induction Motor is widely used in industry, due to its easy installation and operation. Induction motors require a more reliable monitoring due to constant operation increases the possibility of faults, for example, a broken rotor bar fault. Early stage, broken bar is not easy to detect, and its evolves is slow and quiet. In the most of cases, it is detected when the fault is critical and other faults have appeared. Many techniques have been proposed in the literature, but majority of these performs analysis in frequency domain, applying additional transformation or preprocessing methods. In this paper, a novel methodology to detect a half-broken bar fault is proposed, making use of the vibration signal from induction motor under two fault conditions: healthy and half-broken bar; and three load conditions: unloaded, half-loaded and three-fourths loaded. The detection is possible due to the sparse representation of the raw signal which is obtained and then evaluated by minimal decomposition error criterion. In this way, preprocessing methods are not needed, and the fault is detected early and directly. These tests were developed in Matlab software, with vibration signals from induction motors in steady state.""","2017 IEEE International Autumn Meeting on Power, Electronics and Computing (ROPEC)"
"""Sparse-DySta: Sparsity-Aware Dynamic and Static Scheduling for Sparse Multi-DNN Workloads""",H. Fan; S. I. Venieris; A. Kouris; N. D. Lane;,2023,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10411414,Sparse Multi-DNN Scheduling;Dynamic and Static Approach;Algorithm and Hardware Co-Design;,"""Running multiple deep neural networks (DNNs) in parallel has become an emerging workload in both edge devices, such as mobile phones where multiple tasks serve a single user for daily activities, and data centers, where various requests are raised from millions of users, as seen with large language models. To reduce the costly computational and memory requirements of these workloads, various efficient sparsification approaches have been introduced, resulting in widespread sparsity across different types of DNN models. In this context, there is an emerging need for scheduling sparse multi-DNN workloads, a problem that is largely unexplored in previous literature. This paper systematically analyses the use-cases of multiple sparse DNNs and investigates the opportunities for optimizations. Based on these findings, we propose Dysta, a novel bi-level dynamic and static scheduler that utilizes both static sparsity patterns and dynamic sparsity information for the sparse multi-DNN scheduling. Both static and dynamic components of Dysta are jointly designed at the software and hardware levels, respectively, to improve and refine the scheduling approach. To facilitate future progress in the study of this class of workloads, we construct a public benchmark that contains sparse multi-DNN workloads across different deployment scenarios, spanning from mobile phones and AR/VR wearables to data centers. A comprehensive evaluation on the sparse multi-DNN benchmark demonstrates that our proposed approach outperforms the state-of-the-art methods with up to 10% decrease in latency constraint violation rate and nearly 4 × reduction in average normalized turnaround time. Our artifacts and code are publicly available at: https://github.com/SamsungLabs/Sparse-Multi-DNN-Scheduling.CCS CONCEPTS• Computing methodologies → Neural networks; • Theory of computation → Scheduling algorithms; • Computer systems organization → Real-time system architecture.""",2023 56th IEEE/ACM International Symposium on Microarchitecture (MICRO)
"""Design of generic direct sparse linear system solver in C++ for power system analysis""",S. Pandit; S. A. Soman; S. A. Khaparde;,2001,10.1109/59.962409,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=962409,;,"""This paper presents design of generic linear system solver (LSS) for a class of large sparse symmetric matrices over real and complex numbers. These matrices correspond to either of the following: (1) symmetric positive definite (SPD) matrices, (2) complex Hermitian matrices, (3) complex matrices with SPD real and imaginary matrices. Such matrices arise in various power system analysis applications like load flow analysis and short circuit analysis. Template facility of C++ is used to write a generic program on float, double and complex data types. Design of algorithm guarantees numerical stability and efficient sparsity implementation. A reusable class SET is defined to cater to graph theoretic computations. LSS problems with matrices up to 20000 nodes have been tested. Another feature of the proposed LSS is implementation of associative array, which allows subscripting an array with character strings, such as bus names. This helps in making the power system analysis software user friendly. The proposed LSS reflects an important development toward a truly object oriented power system analysis software.""",IEEE Transactions on Power Systems
"""The unique aspects of simulation verification and validation""",D. Thomas; A. Joiner; W. Lin; M. Lowry; T. Pressburger;,2010,10.1109/AERO.2010.5446785,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5446785,;,"""Models and simulations (M&S) will be employed to support important design decisions and verification of system requirements in the development of NASA's Orion Crew Exploration Vehicle. Most simulations are implemented in software. For developed software, NASA's software engineering procedural guideline NPR 7150.2 and safety standard NASA-STD-8719.13B apply. Recognizing the need for critical M&S to be validated to be credible for their intended uses, NASA developed a Modeling and Simulation Standard, NASA-STD-7009. This paper analyzes the requirements specified by these standards and their role in test, validation and certification of modeling and simulation software. It discusses simulation validation as a distinct instance of software validation with corresponding unique requirements. Simulation-specific validation concerns include fit to intended use, validation against experimental data, uncertainty quantification, and sensitivity analysis. The paper also describes the Orion M&S verification, validation, and accreditation (VV&A) process.""",2010 IEEE Aerospace Conference
"""A Multi-GPU Aggregation-Based AMG Preconditioner for Iterative Linear Solvers""",M. Bernaschi; A. Celestini; F. Vella; P. D'Ambra;,2023,10.1109/TPDS.2023.3287238,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10155151,GPU accelerators;heterogeneous computing;iterative sparse linear solvers;parallel numerical algorithms;scalability;,"""We present and release in open source format a sparse linear solver which efficiently exploits heterogeneous parallel computers. The solver can be easily integrated into scientific applications that need to solve large and sparse linear systems on modern parallel computers made of hybrid nodes hosting Nvidia Graphics Processing Unit (GPU) accelerators. The work extends previous efforts of some of the authors in the exploitation of a single GPU accelerator and proposes an implementation, based on the hybrid MPI-CUDA software environment, of a Krylov-type linear solver relying on an efficient Algebraic MultiGrid (AMG) preconditioner already available in the BootCMatchG library. Our design for the hybrid implementation has been driven by the best practices for minimizing data communication overhead when multiple GPUs are employed, yet preserving the efficiency of the GPU kernels. Strong and weak scalability results of the new version of the library on well-known benchmark test cases are discussed. Comparisons with the Nvidia AmgX solution show a speedup, in the solve phase, up to 2.0x.""",IEEE Transactions on Parallel and Distributed Systems
"""Sparse Representation for Face Verification in Social Insurance System""",R. Xue; M. -Y. You; G. -Z. Li;,2010,10.1109/CCPR.2010.5659256,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5659256,;,"""Face verification system has been applied in the process of social insurance payment to prevent the pension impostors. However, the previous methods of face verification compare the test face with the corresponding training sample in the database, and then simply calculate the similarity of both, ignoring global similarity distribution. In this paper, we put the sparse representation method into the face verification system and propose a new method called SRS. Experiments are performed on six face data sets, such as Yale A, Extended Yale B, AR etc., combing the feature extraction methods of Randomfaces, Eigenfaces, Fisherfaces and Laplacianfaces. Cosine method is used for comparison. Experimental results show that the new method achieves higher accuracy with large numbers of classes.""",2010 Chinese Conference on Pattern Recognition (CCPR)
"""SFLU: Synchronization-Free Sparse LU Factorization for Fast Circuit Simulation on GPUs""",J. Zhao; Y. Wen; Y. Luo; Z. Jin; W. Liu; Z. Zhou;,2021,10.1109/DAC18074.2021.9586141,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9586141,sparse LU factorization;circuit simulation;GPU;,"""Sparse LU factorization is one of the key building blocks of sparse direct solvers and often dominates the computing time of circuit simulation programs. Existing GPU-accelerated sparse LU factorization methods either offload relatively small dense matrix-matrix multiplications to GPU cores, or extract level-set information to parallelize elimination operations in each level. However, because of the insufficient parallelism, neither of the methods can saturate a large amount of compute units on modern GPUs.We in this paper propose a synchronization-free sparse LU factorization algorithm called SFLU. To saturate GPU cores, our method lets each thread block eliminate a column and runs all the thread blocks at the same time. Through communicating dependency information stored on global memory, all the thread blocks either busy wait to run or get updated by their previous columns. Because elimination of all the columns work concurrently, our method avoids any barrier synchronization and saturates GPU resources. By benchmarking over 1000 sparse matrices on an NVIDIA Titan RTX GPU, our SFLU outperforms SuperLU and GLU by a factor of on average 155.71 and 8.21 (up to 3585.62 and 252.66), respectively.""",2021 58th ACM/IEEE Design Automation Conference (DAC)
"""Incremental sparse linear regression algorithms for face recognition""",X. Liu; J. Liu; X. Liu; Z. Kong; X. Yang;,2018,10.1109/ICACI.2018.8377583,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377583,Linear Regression Classification;Sparse Representation;Incremental Learning;Face Recognition;Tensor;,"""Recently, a face identification algorithm naming Linear Regression Classification(LRC) is proposed, which uses a basic idea that data from the same class lie on a linear subspace and achieves better performance compared to the benchmark algorithms. However, solution of LRC involves computing the inverse matrix which is a time consuming thing for larger datasets. To overcome this limitation, in this study, considering that sparsity has good discriminative nature for classifying, we first propose an incremental sparse linear regression classification algorithm (ISLRC) for face recognition. When dealing with larger datasets by incremental learning, with the new training samples coming, ISLRC has a simple analytical solution only involving matrix multiplication and leads to a high running efficiency. Then, motivated by the facts that face images are naturally tensors and a compact and meaningful representations of tensor data can be obtained by multilinear principle component analysis (MPCA), we extend ISLRC to tensor patterns and propose an MPCA based ISLRC algorithm (MPCA-ISLRC). Experimental results on face recognition show that in most cases, ISLRC and MPCA-ISLRC are superior to the benchmark methods in test accuracy and training time for larger face image datasets.""",2018 Tenth International Conference on Advanced Computational Intelligence (ICACI)
"""PASGCN: An ReRAM-Based PIM Design for GCN With Adaptively Sparsified Graphs""",T. Yang; D. Li; F. Ma; Z. Song; Y. Zhao; J. Zhang; F. Liu; L. Jiang;,2023,10.1109/TCAD.2022.3175031,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9774869,Acceleration;graph convolutional network (GCN);processing in memory (PIM);resistive random access memory (ReRAM);sparse;,"""Graph convolutional network (GCN) is a promising but computing- and memory-intensive learning model. Processing-in-memory (PIM) architecture based on the resistive random access memory-based crossbar (ReRAM crossbar) is a natural fit for GCN inference. It can reduce the data movements and compute the vector-matrix multiplication (VMM) in analog. However, it requires an unbearable crossbar cost to leverage the massive parallelism exhibited in GCNs. First, this article explores the design space for GCN inference on ReRAM crossbars and presents the first PIM-based GCN accelerator named PIMGCN, PIMGCN employs dense data mapping and a search-execute architecture to take full advantage of the intravertex parallelisms with acceptable crossbars cost. Two scheduling strategies for PIMGCN to maximize the intervertex parallelisms and optimize the pipeline are proposed. The optimal scheduling is reduced to a maximum independent set problem, which is solved by a novel node-grouping algorithm. Second, this article explores the task-irrelevant information in the graphs and proposes an adaptively sparsified GCN network targeted for PIMGCN, which is named as ASparGCN. ASparGCN exploits a multilayer perceptron (MLP)-based edge predictor to get edge selection strategies for each GCN layer separately and adaptively in the training stage, and only inferences with the selected edges in the test stage. We design two regularization terms to guide the selection strategies to achieve architecture-friendly sparse graphs for PIMGCN. The overall algorithm-architecture co-design is named as PASGCN. Compared to the state-of-the-art software framework running on Intel Xeon CPU and NVIDIA RTX8000 GPU, PASGCN achieves an average of  $16455times $  and  $110.7times $  speedup and 8.0E $+ 06times $  and 6.67E $+ 03times $  energy reduction, respectively. Compared with the ASIC accelerator HyGCN (Yan et al., 2020), PASGCN achieves  $326.31times $  speedup and  $124.8times $  energy reduction.""",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems
"""Improvements in sparse matrix/vector technique applications for online load flow calculation""",P. Ristanovic; M. Bjelogrlic; B. S. Babic;,1989,10.1109/59.32477,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=32477,;,"""Several analytical and computational improvements in sparsity applications are discussed. A novel partial matrix refactorization method and ordering algorithm are presented. The method is very efficient when applied to programs such as online load flow, optimal power flow, and steady-state security analysis. It is applied in a fast decoupled load flow program which includes the treatment of tap violations on underload tap changing transformers and reactive power generation on PV buses. The effects of proposed improvements are tested and documented on a 118-bus IEEE test network and two utility networks with 209 and 519 buses, respectively.<>""",IEEE Transactions on Power Systems
"""rpcPRF: Generalizable MPI Neural Radiance Field for Satellite Camera With Single and Sparse Views""",T. Zhang; Y. Li; X. Wei;,2024,10.1109/TGRS.2024.3382537,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10480403,Multiplane image (MPI);neural radiance field (NeRF);novel view synthesis;rational polynomial camera (RPC);,"""Most advances in neural radiance fields (NeRFs) assume sufficient input views from pinhole cameras. This article proposes rpcPRF, a multiplane image (MPI)-based planar neural radiance field for rational polynomial camera (RPC), exhibits robust performance even with single or sparse inputs. Unlike coordinate-based NeRFs that require sufficient views of one scene, our model can be applied to new scenes and has shown positive results for single or sparse test images. This allows for generalization across various scenes. To achieve generalization across scenes, we use reprojection supervision to ensure that the predicted MPI accurately captures the geometry between the 3-D coordinates and the images. In addition, we have obviated the requisite for dense depth supervision in multiview-stereo-based methods by introducing rendering techniques of radiance fields. rpcPRF combines the superiority of implicit representations and the advantages of the RPC model, to capture the continuous altitude space while learning the 3-D structure. On the DFC2019 dataset with sparse input of the same scene, rpcPRF achieves the best results. On the TLC dataset and the SatMVS3D dataset with changing scenes in every batch, rpcPRF outperforms state-of-the-art NeRF-based methods by a significant margin in terms of image fidelity, reconstruction accuracy, and efficiency, for both single-view and multiview tasks.""",IEEE Transactions on Geoscience and Remote Sensing
"""A multi-platform linear algebra toolbox for finite element solvers on heterogeneous clusters""",V. Heuveline; C. Subramanian; D. Lukarski; J. -P. Weiss;,2010,10.1109/CLUSTERWKSP.2010.5613084,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5613084,Heterogeneous clusters;multi-GPU;sparse linear algebra;conjugate gradient;performance analysis;speedup;scalability;,"""Heterogeneous clusters with multiple sockets and multicore-processors accelerated by dedicated coprocessors like GPUs, Cell BE, FPGAs or others nowadays provide unrivaled computing power in terms of floating point operations. Specific capabilities of additional processor technologies enable dedicated exploitation with respect to particular application and data characteristics. However, resource utilization, programmability, and scalability of applications across heterogeneous platforms is a major concern. In the framework of the HiFlow finite element software package we have developed a portable software approach that implements efficient parallel solvers for partial differential equations by means of unified and modular user interfaces across a variety of heterogeneous platforms - in particular on GPU accelerated clusters. We detail our concept and provide performance analysis for various test scenarios that prove performance capabilities, scalability, viability, and user friendliness.""",2010 IEEE International Conference On Cluster Computing Workshops and Posters (CLUSTER WORKSHOPS)
"""Double Detector for Sparse Signal Detection From One-Bit Compressed Sensing Measurements""",H. Zayyani; F. Haddadi; M. Korki;,2016,10.1109/LSP.2016.2613898,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7577796,Compressed sensing (CS);generalized likelihood ratio test (GLRT) detector;one-bit measurements;signal detection;,"""This letter presents the sparse vector signal detection from one bit compressed sensing measurements, in contrast to the previous works that deal with scalar signal detection. Available results are extended to the vector case and the generalized likelihood ratio test (GLRT) detector and the optimal quantizer design are obtained. A double-detector scheme is introduced, in which a sensor level threshold detector is integrated into network level GLRT to improve the performance. The detection criteria of oracle and clairvoyant detectors are also derived. Simulation results show that with careful design of the threshold detector, the overall detection performance of double-detector scheme would be better than the sign-GLRT proposed in [J. Fang et al., “One-bit quantizer design for multisensor GLRT fusion,” IEEE Signal Process. Lett., vol. 20, no. 3, pp. 257-260, Mar. 2013] and close to oracle and clairvoyant detectors. The proposed detector is applied to spectrum sensing and the results are near the well-known energy detector, which uses the real valued data, while the proposed detector only uses the sign of the data.""",IEEE Signal Processing Letters
"""Space-efficient Sparse Matrix Storage Formats for Massively Parallel Systems""",I. imecek; D. Langr; P. Tvrdík;,2012,10.1109/HPCC.2012.18,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6332159,sparse matrix storage formats;space-efficient formats;parallel I/O;basic hierarchical format;advanced hierarchical format;,"""In this paper, we propose and evaluate new storage formats for sparse matrices that minimize the space complexity of information about matrix structure. The motivation of our work are applications with very large sparse matrices that due to their size must be processed on massively parallel computer systems consisting of tens or hundreds of thousands of processor cores and that must be stored in a distributed file system using parallel I/O. The parallel I/O is typically the main performance bottleneck and reading or writing such matrices from/to distributed file system can take significant amount of time. We try to reduce this time by reducing the amount of data to be processed.""",2012 IEEE 14th International Conference on High Performance Computing and Communication & 2012 IEEE 9th International Conference on Embedded Software and Systems
"""Unique Degradation of Flash Memory as an Identifier of ICT Device""",S. S. Vladimirov; R. Pirmagomedov; R. Kirichek; A. Koucheryavy;,2019,10.1109/ACCESS.2019.2932804,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8786232,Internet of things;counterfeiting;system identification;flash memory cells;physical identification;communication system security;network security;,"""The counterfeit and stolen information and communication technologies (ICT) devices are an essential and growing problem. Reliable technology for the identification of ICT devices is required to enable blocking of these devices in the network worldwide. Motivated by this challenge, we elaborate on the idea of using the unique degradation image of flash memory chip (DFMC) as the identifier of the device. This idea is based on the assumption that the distribution of degraded segments in the memory chip is unique enough to provide reliable identification of the device. In this paper, we provide a proof of concept through a hardware experiment. For this experiment, we developed a custom test bed and special software enabling the forced degradation of NOR-flash memory chips. We, then, consider the uniqueness of such identifiers using combination theory and consider practical issues of DFMC implementation, including the initial identification procedure, light dynamic identifiers, and identification using a cross-correlation function and options of dynamic identification. We conclude that using DFMC addresses relevant challenges of ICT devices identification.""",IEEE Access
"""Modified sparse distributed memory as transient episodic memory for cognitive software agents""",U. Ramamaurthy; S. K. D'Mello; S. Franklin;,2004,10.1109/ICSMC.2004.1401130,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1401130,;,"""This paper presents a modified sparse distributed memory architecture for use in software agents with natural language processing capabilities. We have modified Kanerva's sparse distributed memory (SDM) into an architecture with a ternary memory space. This enables the memory to be used in IDA, the intelligent distribution agent built for the U.S. Navy. IDA implements Boars' global workspace theory, a psychological theory of consciousness. As a result, it can react to novel and problematic situations in a more flexible, more human-like way than traditional AI systems. IDA performs a function, namely billet assignment, therefore reserved for humans. We argue that such flexibility requires advanced memory systems such as transient episodic memory and autobiographical memory. Here, we present the architecture, tests and results of this modified SDM system which can be used as a transient episodic memory in suitable software agents.""","2004 IEEE International Conference on Systems, Man and Cybernetics (IEEE Cat. No.04CH37583)"
"""A Model of Front-End Pre-Change Corrective Testing""",M. Kajko-Mattsson; S. Britts;,2006,10.1109/ICSEA.2006.261280,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4031809,Support Line 1 and 2;global problem and solution repository;problem uniqueness;process model.;,"""Most of the testing process models deal with testing within development. To our knowledge, there are no process models exclusively dedicated to the testing of corrective changes. For this reason, we have outlined a process model covering the testing activities at the front-end support level and evaluated them within 15 software organizations.""",2006 International Conference on Software Engineering Advances (ICSEA'06)
"""Design of Real-time Message Synchronization cross Smart Grid Dispatching System""",C. Xu; T. Liu; B. Lei; H. Peng;,2020,10.1109/EI250167.2020.9347175,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9347175,cross smart grid dispatching and control system;message synchronization;high-reliable transmission;message uniqueness;,"""In the integrated operation mode of the smart grid dispatching and control system, there is a large amount of real-time data such as important alarms and manual operations that need to be shared between systems. How to synchronize these real-time data across systems with high reliability and performance becomes a problem to be solved. This paper studies the application scenarios of cross-system message synchronization and proposes a scheme based on redundant dual networks, including hardware and software architecture. The key technologies and important functions were implemented, reliability and throughput verifications were performed, and the expected results were achieved.""",2020 IEEE 4th Conference on Energy Internet and Energy System Integration (EI2)
"""Runtime analysis of (1+l) EA on computing unique input output sequences""",P. Kristian Lehre; Xin Yao;,2007,10.1109/CEC.2007.4424703,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4424703,;,"""Computing unique input output (UIO) sequences is a fundamental and hard problem in conformance testing of finite state machines (FSM). Previous experimental research has shown that evolutionary algorithms (EAs) can be applied successfully to find UIOs on some instances. However, before EAs can be recommended as a practical technique for computing UIOs, it is necessary to better understand the potential and limitations of these algorithms on this problem. In particular, more research is needed in determining for what instances of the problem EAs are feasible. This paper presents a rigorous runtime analysis of the (1+1) EA on three classes of instances of this problem. First, it is shown that there are instances where the EA is efficient, while random testing fails completely. Secondly, an instance class that is difficult for both random testing and the EA is presented. Finally, a parametrised instance class with tunable difficulty is presented. Together, these results provide a first theoretical characterisation of the potential and limitations of the (1+1) EA on the problem of computing UIOs.""",2007 IEEE Congress on Evolutionary Computation
"""BSGenerator: a Tool to Design Unique Online Examinations using Question Variations""",M. Nicolau;,2022,10.1109/IIAIAAI55812.2022.00045,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9894646,online assessment;online quizzes;virtual learning environment;question variations;,"""The sudden arrival of the COVID-19 pandemic caused chaos in a multitude of industries and sectors. In many universities around the world, the resulting restrictions forced the closure of campuses, and with face-to-face examinations cancelled, a rapid adaptation of assessment strategies was needed.To adapt to this scenario, we developed a software tool to assist in the design of unique examinations for each student, when using online quizzes. Results from its application to a large cohort of students validate the approach taken, in terms of fairness of assessment, grade distribution, and foul play deterrence.This paper describes the developed approach, its advantages and limitations, and plans for future refinement.""",2022 12th International Congress on Advanced Applied Informatics (IIAI-AAI)
"""Permanence and Uniqueness of EEG Brain Signals as a Biometric Signature, Part-I: Template-based Techniques""",Ö. M. Soysal; M. E. Oztemel;,2023,10.1109/CSCI62032.2023.00232,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10590607,Electroencephalography;Biometric;Brain;Permanence;Dynamic Time Warping;Cross-Correlation;,"""The utilization of electroencephalography as a potential biometric tool has been garnering significant attention, leveraging the distinct signatures embedded in human brain activity. The effectiveness of such characteristic patterns, however, hinges on their stability over time. In this study, we aim to explore the permanence and the uniqueness of neural patterns. A comprehensive comparison is conducted on a spatio-temporal space across various stimuli protocols. Summarizing our findings regarding the agreement of different time-series techniques for measuring intra- and inter-subject similarity, we present our approach and outcomes in two parts. In this first part, we employed two template-based time series similarity measures, namely normalized Dynamic Time Warping, and normalized Cross-Correlation. Additionally, we enhance our analysis by applying two time-domain filters- Adjusted Laplacian of Mean and Laplacian of Gaussian- for the extraction of specific spectral neural activity, followed by similarity measures. Our preliminary findings indicate that the proposed template-based pipeline can achieve an 86% accuracy rate in identifying individuals over the course of a week. This suggests that the pipeline shows promise in its ability to reliably distinguish between individuals based on the data it processes. However, further validation and testing may be necessary to confirm and refine these findings.""",2023 International Conference on Computational Science and Computational Intelligence (CSCI)
"""Discriminative Bayesian Dictionary Learning for Classification""",N. Akhtar; F. Shafait; A. Mian;,2016,10.1109/TPAMI.2016.2527652,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7404063,Bayesian sparse representation;discriminative dictionary learning;supervised learning;classification;,"""We propose a Bayesian approach to learn discriminative dictionaries for sparse representation of data. The proposed approach infers probability distributions over the atoms of a discriminative dictionary using a finite approximation of Beta Process. It also computes sets of Bernoulli distributions that associate class labels to the learned dictionary atoms. This association signifies the selection probabilities of the dictionary atoms in the expansion of class-specific data. Furthermore, the non-parametric character of the proposed approach allows it to infer the correct size of the dictionary. We exploit the aforementioned Bernoulli distributions in separately learning a linear classifier. The classifier uses the same hierarchical Bayesian model as the dictionary, which we present along the analytical inference solution for Gibbs sampling. For classification, a test instance is first sparsely encoded over the learned dictionary and the codes are fed to the classifier. We performed experiments for face and action recognition; and object and scene-category classification using five public datasets and compared the results with state-of-the-art discriminative sparse representation approaches. Experiments show that the proposed Bayesian approach consistently outperforms the existing approaches.""",IEEE Transactions on Pattern Analysis and Machine Intelligence
"""Retracted: Instant Kirlian diagnostic system (IKiDs). A unique form of diagnosis""",V. Soultanov; J. Charalambous; E. White; D. Sapunar;,1998,10.1109/ICBEM.1998.666438,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=666438,;,"""Medicine has long been challenged to detect disease before it can do irreversible damage, usually by means of invasive tests after the presentation of symptoms. This paper introduces a unique diagnostic tool, the Instant Kirlian Diagnostic System (IKiDS) which derives from the work of the Kirlians of Russia in 1939. It delineates both the disease process and its energetic precursors in the human energy field, allowing for treatment and lifestyle changes before real damage occurs. IKiDS utilises a sensor, high voltage electronics and computer software to ""capture"" an energy discharge from the fingertips which contains valuable diagnostic information. The research summary is based on analysis of over 1000 sets of prints. Whilst some diagnostic puzzles remain, it is clear that IKiDS has impressive diagnostic and evaluative capabilities. Furthermore, it is accurate, user friendly and offers a complete picture of the physical, mental and emotional condition in ""real time"". Future research directions are indicated.""",Proceedings of the 2nd International Conference on Bioelectromagnetism (Cat. No.98TH8269)
"""Doppler Radar Techniques for Accurate Respiration Characterization and Subject Identification""",A. Rahman; V. M. Lubecke; O. Boric–Lubecke; J. H. Prins; T. Sakamoto;,2018,10.1109/JETCAS.2018.2818181,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8322143,Doppler radar;Internet of Things;dynamic range (DR);SNR;unique identification;,"""A low distortion dc coupled CW radar system with high signal to noise ratio is capable of accurate representation of respiration in human subjects. We propose to test the hypothesis that a non-contact physiological radar monitoring system which measures and characterizes subtle body kinematics, can be made to resolve patterns accurately enough to recognize an individual's identity. This paper investigates a technique to attain the requisite signal to noise ratio by dc offset management. Detailed exploration of the unique features in respiration signals using noncontact CW Doppler radar are presented. A proposed dynamic segmentation technique allowed detection of various unique features and patterns. KMN nearest neighbor and majority vote algorithms were implemented in software for this radar-based unique identification system. The system was tested and validated for six test subjects with 95% success rate. Fractal analysis of minor components of linearly demodulated radar signal was also presented for additional improvement in accuracy. This paper is believed to be significant as radar unique identification of human subjects has many potential applications, including security, health monitoring, IoT applications, and virtual reality.""",IEEE Journal on Emerging and Selected Topics in Circuits and Systems
"""Concise Fuzzy System Modeling Integrating Soft Subspace Clustering and Sparse Learning""",P. Xu; Z. Deng; C. Cui; T. Zhang; K. -S. Choi; S. Gu; J. Wang; S. Wang;,2019,10.1109/TFUZZ.2019.2895572,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8626516,Enhanced soft subspace clustering;high-dimensional data;interpretability;sparse learning;Takagi–Sugeno–Kang (TSK) fuzzy system;,"""The superior interpretability and uncertainty modeling ability of Takagi-Sugeno-Kang fuzzy system (TSK FS) make it possible to describe complex nonlinear systems intuitively and efficiently. However, classical TSK FS usually adopts the whole feature space of the data for model construction, which can result in lengthy rules for high-dimensional data and lead to degeneration in interpretability. Furthermore, for highly nonlinear modeling task, it is usually necessary to use a large number of rules which further weaken the clarity and interpretability of TSK FS. To address these issues, an enhanced soft subspace clustering (ESSC) and sparse learning (SL) based concise zero-order TSK FS construction method, called ESSC-SL-CTSK-FS, is proposed in this paper by integrating the techniques of ESSC and SL. In this method, ESSC is used to generate the antecedents and various sparse subspaces for different fuzzy rules, whereas SL is used to optimize the consequent parameters of the fuzzy rules based on which the number of fuzzy rules can be effectively reduced. Finally, the proposed ESSC-SL-CTSK-FS method is used to construct concise zero-order TSK FS that can explain the scenes in high-dimensional data modeling more clearly and easily. Experiments are conducted on various real-world datasets to confirm the advantages.""",IEEE Transactions on Fuzzy Systems
"""An $L_0$ Regularization Method for Imaging Genetics and Whole Genome Association Analysis on Alzheimer's Disease""",X. Li; Y. Lin; X. Meng; Y. Qiu; B. Hu;,2021,10.1109/JBHI.2021.3093027,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9466442,Image genetics;multiple regression tasks model;sparse learning model;Alzheimer's disease;whole genome association study;,"""Although the neuroimaging measures build a bridge between genetic variants and disease phenotypes, an assessment of single nucleotide variants changes in brain structure and their clinically influence on the progression of Alzheimer's disease remain largely preliminary. Note that each variant has very weak correlation signal to neuroimaging measures or Alzheimer's disease phenotypes. Therefore, traditional sparse regression-based image genetics approaches confront with unresolvable features, relative high regression error or inapplicability of high-dimensional data. Adopting an $text{L}_0$ regularization method, we significantly elevate the regression accuracy of imaging genetics compared with group-sparse multitask regression method. With further analysis on the simulation results, we conclude that multiple regression tasks model may be unsuitable for image genetics. In addition, we carried out a whole genome association analysis between genetic variants (about 388 million loci) and phenotypes (cognition normal, mild cognitive impairment and Alzheimer's disease) with using the $text{L}_0$ regularization method. After annotating the effect of all variants by Ensembl Variant Effect Predictor (VEP), our method locates 33 missense variants which can explain 40% phenotype variance. Then, we mapped each missense variant to the nearest gene and carried out pathway enrichment analysis. The Notch signaling pathway and Apoptosis pathway have been reported to be related to the formation of Alzheimer's disease.""",IEEE Journal of Biomedical and Health Informatics
"""Unique features of electronic device testing using NI-technologies""",V. Butin; A. Butina; F. Chubrukov;,2015,10.1109/SIBCON.2015.7147314,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7147314,PXI;testing algorithm;electronic device;multiplex channel;,"""The paper describes methodological problems of functional check and parametric testing of electronic devices by using modular NI PXI-devices in various environmental conditions.The operation algorithm of developed automated test system can analyze the time measurement of various parameters; it synchronizes time between measurements, creates experimental data back-ups and minimizes measurement time. The log-file allows detecting the sequence of errors during the functional check of electronic device. The interface module controls the data exchange by multiplex channel and it is integrated in automated test system. The application-dependent software provides the remote parameters measurement and research control. The control program realizes the switching of the device power supply to external power source in case of the control parameters exceeding the limits. The developed test system has been approbated at modeling and simulation facilities of ROSATOM companies such as FSUE VNIIA (Moscow, Russia), FSUE RISI (Lytkarino, Russia) and SSC RF - IPPE (Obninsk, Russia).""",2015 International Siberian Conference on Control and Communications (SIBCON)
"""Gesture Recognition Based on Sparse Reconstruction""",A. Aitimov; C. Turan; Z. Duisebekov;,2018,10.1109/ICECCO.2018.8634687,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8634687,Gesture recognition;sparse representation classifier;recognition rate;,"""Due to the variety of feature extractions and classifiers, many different algorithms have been proposed for gesture recognition. In this paper, we work to increase the recognition performance in terms of recognition rate and execution time by using recently proposed modified sparse representation classifier based on intensity of images. Sparsity based classifier is compared with two conventional ones as K-nearest neighbor and random forest classifiers on gesture recognition. Simulation results showed that, our recognition algorithm based on sparsity has a higher performance than that of the others for both recognition rate and execution time.""",2018 14th International Conference on Electronics Computer and Computation (ICECCO)
"""Dynamic Texture Comparison Using Derivative Sparse Representation: Application to Video-Based Face Recognition""",F. Hajati; M. Tavakolian; S. Gheisari; Y. Gao; A. S. Mian;,2017,10.1109/THMS.2017.2681425,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7898817,Directional derivative;dynamic texture;Grassmann manifold;high-order pattern;sparse representation;spatiotemporal;,"""Video-based face, expression, and scene recognition are fundamental problems in human-machine interaction, especially when there is a short-length video. In this paper, we present a new derivative sparse representation approach for face and texture recognition using short-length videos. First, it builds local linear subspaces of dynamic texture segments by computing spatiotemporal directional derivatives in a cylinder neighborhood within dynamic textures. Unlike traditional methods, a nonbinary texture coding technique is proposed to extract high-order derivatives using continuous circular and cylinder regions to avoid aliasing effects. Then, these local linear subspaces of texture segments are mapped onto a Grassmann manifold via sparse representation. A new joint sparse representation algorithm is developed to establish the correspondences of subspace points on the manifold for measuring the similarity between two dynamic textures. Extensive experiments on the Honda/UCSD, the CMU motion of body, the YouTube, and the DynTex datasets show that the proposed method consistently outperforms the state-of-the-art methods in dynamic texture recognition, and achieved the encouraging highest accuracy reported to date on the challenging YouTube face dataset. The encouraging experimental results show the effectiveness of the proposed method in video-based face recognition in human-machine system applications.""",IEEE Transactions on Human-Machine Systems
"""Heart Rate Detection From Facial Videos Using A Frequencyconstrained Multilayer Sparse Coding""",X. Liu; X. Yang; D. Wang; S. Fang;,2020,10.1109/ICIP40778.2020.9191243,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9191243,imaging Photoplethysmography;multilayer sparse coding;dictionary learning;,"""Imaging photoplethysmography (iPPG) can be used to detect heart rates from facial videos. However, it is sensitive to motion disturbances in realistic environments. To address this problem, a frequency-constrained multilayer sparse coding (FCMSC) algorithm is proposed in this paper. Specifically, FCMSC learns a dictionary about iPPG signals from a large set of clean iPPG signals in the training phase, and then reconstructs distorted iPPG signals with the learned dictionary in the testing phase. Compared to previous methods, FCMSC has stronger learning power due to its multilayer structure and better generalization ability because of its frequency constraint. A total of 3630 clips are cut out from the MAHNOB-HCI (a public video dataset) to train and test FCMSC. Experimental results show that FCMSC outperforms state-of-the-art methods in extracting heart rates from facial videos involving motion disturbances.""",2020 IEEE International Conference on Image Processing (ICIP)
"""A Computationally Efficient Neural Video Compression Accelerator Based on a Sparse CNN-Transformer Hybrid Network""",S. Zhang; W. Mao; H. Shi; Z. Wang;,2024,10.23919/DATE58400.2024.10546891,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10546891,Neural video compression;CNN-Transformer;fast algorithm;pruning;hardware accelerator;,"""Video compression is widely used in digital television, surveillance systems, and virtual reality. Real-time video decoding is crucial in practical scenarios. Recently, neural video compression (NVC) combines traditional coding with deep learning, achieving impressive compression efficiency. Nevertheless, the NVC models involve high computational costs and complex memory access patterns, challenging real-time hardware implementations. To relieve this burden, we propose an algorithm and hardware co-design framework named NVCA for video decoding on resource-limited devices. Firstly, a CNN-Transformer hybrid network is developed to improve compression performance by capturing multi-scale non-local features. In addition, we propose a fast algorithm-based sparse strategy that leverages the dual advantages of pruning and fast algorithms, sufficiently reducing computational complexity while maintaining video compression efficiency. Secondly, a reconfigurable sparse computing core is designed to flexibly support sparse convolutions and deconvolutions based on the fast algorithm-based sparse strategy. Furthermore, a novel heterogeneous layer chaining dataflow is incorporated to reduce off-chip memory traffic stemming from extensive inter-frame motion and residual information. Thirdly, the overall architecture of NVCA is designed and synthesized in TSMC 28nm CMOS technology. Extensive experiments demonstrate that our design provides superior coding quality and up to 22.7x decoding speed improvements over other video compression designs. Meanwhile, our design achieves up to 2.2x improvements in energy efficiency compared to prior accelerators.""","2024 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
"""A Sparsity-Assisted Fault Diagnosis Method Based on Nonconvex Sparse Regularization""",Y. Niu; J. Fei;,2021,10.1109/ACCESS.2021.3073072,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9404164,Fault diagnosis;sparse representation;generalized minimax-concave (GMC) penalty;fault feature extraction;,"""Sparse representation theory can be adopted for fault feature extraction and classification. Inspired by these two capabilities of sparse representation theory, this paper proposes a novel collaborative sparsity-assisted fault diagnosis (CSFD) method. Specifically, due to the repeatability and sparsity of fault feature signal in the whole signal, the feature extraction capability of sparse representation is utilized to extract fault features and construct a feature matrix. Subsequently, owing to the sparsity of fault classification problem itself, the classification capability of sparse representation is employed to achieve fault classification. Moreover, fault feature extraction is a key issue for fault diagnosis. Therefore, in order to improve the effectiveness of fault feature extraction, a RC-adjustment strategy with the ability to adaptively adjust regularization parameter is designed to assist the generalized minimax-concave (GMC) penalty for feature extraction. The advantage of the GMC penalty is that it can establish a nonconvex sparse regularization model with convex optimization solution. Finally, the proposed CSFD method is tested and verified by Case Western Reserve University (CWRU) bearing dataset and actual engineering dataset. Especially, the diagnostic accuracy of this method can reach 99.92%, which is nearly 7.68% higher than the traditional method based on L1 norm penalty. Enormous experiment results have thoroughly demonstrated the effectiveness of the proposed CSFD method for fault diagnosis.""",IEEE Access
"""The Unique Challenges of Testing Specialized Network-Based Data Acquisition Systems""",S. A. Kilpatrick; T. A. Newton;,2014,10.1109/AUTEST.2014.6935151,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6935151,Network;network-based;Ethernet;data acquisition system;system testing;,"""The increasing speeds and decreasing prices of network equipment have spurred much interest in applying networks to many different types of systems. A wide range of network protocols and tools already exist making its application attractive. Over the last decade, there has been a noticeable shift to introduce Internet Protocol (IP) networks into new areas such as data acquisition systems. These modern network-based test systems are composed of a large number of recognizable devices such as switches, routers, and recorders. While these networks contain components familiar to most, common enterprise network test tools are not always sufficient to test these specialized systems. Typically, these specialized network-centric data systems require tighter timing accuracies, shorter latency windows, and little to no acceptable packet loss. The unique features of IP networks present testing challenges that must be addressed to provide reliable verification of the data acquisition system. Packet latency, delay, jitter, and loss are all factors that must be accounted for. Intentionally introducing these types of impairments to the data system are essential in any full, end-toend system test. It is not only important to produce these types of errors, but to mimic the conditions that will exist in the real system and reliably reproduce them at any time. This paper describes the shortcomings of those common network tools for testing specialized network-based data acquisition systems and the custom workarounds that were necessary to provide the test capabilities that we required. We will share some of our lessons learned from real-life examples of testing specialized network equipment for a flight-test data acquisition system that would apply to any network-centric data system. The paper will also describe a portion of our lab setup that allows us to rapidly reconfigure test networks and deploy custom simulation tools to support multiple types of data acquisition systems simultaneously.""",2014 IEEE AUTOTEST
"""Defect Detection From Compressed 3-D Ultrasonic Frequency Measurements""",S. Semper; J. Kirchhof; C. Wagner; F. Krieg; F. Roemer; G. Del Galdo;,2019,10.23919/EUSIPCO.2019.8903133,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8903133,3D ultrasonic imaging;Sparse Signal Recovery;Compressive Sensing;SAFT;,"""In this paper, we propose a compressed sensing scheme for volumetric synthetic aperture measurements in ultrasonic nondestructive testing. The compression is achieved by limiting the measurement to a subset of the Fourier coefficients of the full measurement data, where we also address the issue of a suitable hardware architecture for the task. We present a theoretic analysis for one of the proposed schemes in terms of the Restricted Isometry Property and derive a scaling law for the lower bound of the number of necessary measurements for stable and efficient recovery. We verify our approach with reconstructions from measurement data of a steel specimen that was compressed synthetically in software. As a side result, our approach yields a variant of the 3-D Synthetic Aperture Focusing Technique which can deal with compressed data.""",2019 27th European Signal Processing Conference (EUSIPCO)
"""Retraction Notice: Instant Kirlian diagnostic system (IKiDs). A unique form of diagnosis""",V. Soultanov; J. Charalambous; E. White; D. Sapunar;,1998,10.1109/ICBEM.1998.10350032,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10350032,;,"""Medicine has long been challenged to detect disease before it can do irreversible damage, usually by means of invasive tests after the presentation of symptoms. This paper introduces a unique diagnostic tool, the Instant Kirlian Diagnostic System (IKiDS) which derives from the work of the Kirlians of Russia in 1939. It delineates both the disease process and its energetic precursors in the human energy field, allowing for treatment and lifestyle changes before real damage occurs. IKiDS utilises a sensor, high voltage electronics and computer software to ""capture"" an energy discharge from the fingertips which contains valuable diagnostic information. The research summary is based on analysis of over 1000 sets of prints. Whilst some diagnostic puzzles remain, it is clear that IKiDS has impressive diagnostic and evaluative capabilities. Furthermore, it is accurate, user friendly and offers a complete picture of the physical, mental and emotional condition in ""real time"". Future research directions are indicated.""",Proceedings of the 2nd International Conference on Bioelectromagnetism (Cat. No.98TH8269)
"""Improved OMP selecting sparse representation used with face recognition""",J. Zhang; K. Yan; Z. He;,2014,10.1109/ICSESS.2014.6933637,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6933637,orthogonal matching pursuit;sparse representation;image classification;,"""With the worldwide strengthening of anti-terrorism and other identity verification, the products based on face recognition are used in real life more and more. The recognition as an important ways has become the focus of academic research in the world. Face recognition accuracy can be improved by increasing the number of training samples, but increasing number will result in a large computing complexity. In recent years, the sparse representation becomes hot in face recognition. In this paper, we propose an energy constraint orthogonal matching pursuit (ECOMP) algorithm for sparse representation in face recognition. It selects a few training samples and hierarchical structure for face recognition. In this method, we re-select training samples by ECOMP, calculate the weight of all the selected training samples and find the sparse training samples which can recover the test sample. While the AR and the ORL database experimental results show that this method has better performance than other identification methods.""",2014 IEEE 5th International Conference on Software Engineering and Service Science
"""Global Overcomplete Dictionary-Based Sparse and Nonnegative Collaborative Representation for Hyperspectral Target Detection""",C. Li; D. Zhu; C. Wu; B. Du; L. Zhang;,2024,10.1109/TGRS.2024.3381719,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10478971,Collaborative representation;hyperspectral imagery (HSI);sparse representation;target detection;,"""The combined sparse and collaborative representation (CSCR)-based algorithm is one of the most effective methods among hyperspectral target detection (HTD) methods based on representation and dictionary learning. It encourages target atoms to compete with each other and background atoms to collaborate in the representation. However, this method suffers from several drawbacks. In sparse representation, an overcomplete dictionary is necessary, whereas, in collaborative representation, nonnegative coefficients are required. Besides, the local dual window approach may result in impure background dictionaries obtained from the outer window. To address these issues, we propose a novel approach for HTD, referred to as the global overcomplete dictionary-based sparse and nonnegative collaborative representation (GODSNCR) detector. First, a hierarchical density clustering algorithm is used to complete the dictionary atom extraction to construct a joint overcomplete dictionary to satisfy the dictionary overcompleteness problem required for sparse representation. Second, a nonnegative constraint on the coefficient matrix and a “sum to one” constraint for the joint representation are incorporated to make it more consistent with the physical meaning. Finally, the limitation of the local dual window approach is overcome by substituting the local background dictionary with a global background dictionary. Through the aforementioned strategies, we can use a joint overcomplete dictionary for achieving the sparse representation of targets and utilize a global background dictionary for the collaborative representation of background, the final detection results are obtained by calculating the residuals. The experimental results clearly demonstrate that the proposed algorithm has significant improvement in detection accuracy and strong robustness compared to other typical representation-based HTD methods. Our model will be available at https://github.com/Chenxing-Li/GODSNCR.""",IEEE Transactions on Geoscience and Remote Sensing
"""Parallel Algorithms for Testing Finite State Machines:Generating UIO Sequences""",R. M. Hierons; U. C. Türker;,2016,10.1109/TSE.2016.2539964,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7429774,Software engineering/software/program verification;software engineering/testing and debugging;software engineering/test design;finite state machine;unique input output sequence generation;general purpose graphics processing units;,"""This paper describes an efficient parallel algorithm that uses many-core GPUs for automatically deriving Unique Input Output sequences (UIOs) from Finite State Machines. The proposed algorithm uses the global scope of the GPU's global memory through coalesced memory access and minimises the transfer between CPU and GPU memory. The results of experiments indicate that the proposed method yields considerably better results compared to a single core UIO construction algorithm. Our algorithm is scalable and when multiple GPUs are added into the system the approach can handle FSMs whose size is larger than the memory available on a single GPU.""",IEEE Transactions on Software Engineering
"""Real-Time Ultrasound Open Platform with an Extendable Number of Channels""",D. Mazierli; A. Ramalli; E. Boni; F. Guidi; P. Tortoli;,2021,10.1109/IUS52206.2021.9593831,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9593831,3-D beamforming;high frame rate imaging;matrix array;multi-channel;open scanner;sparse array;synchronization;ultrasound;,"""Ultrasound open platforms are ideal instruments to experiment innovative modalities in transmission and reception. So far, the high number of channels, involved in applications based on 2-D arrays, has been managed through complex combinations of scanners and personal computers, at the expense of ease of use, portability, and real-time performance. This work presents a new flexible architecture, based on the full (hardware and software) synchronization of multiple ULA-OP 256 scanners, in a Master-Slave architecture, and controlled by a single computer. For the hardware synchronization, a Master scanner provides two digital signals (clock and transmission start) to the Slave scanners, while the software synchronization is managed by two purposely developed MATLAB classes that allow the user to interface the scanners as a single system. To demonstrate the effectiveness of the architecture, a 512-channel prototype, composed of two ULA-OP 256 connected to a host PC and to a sparse array, was implemented and tested through applications that demonstrate the high real-time processing power.""",2021 IEEE International Ultrasonics Symposium (IUS)
"""Epileptic Seizure Detection in EEG Signals Using Discriminative Stein Kernel-Based Sparse Representation""",C. Lei; S. Zheng; X. Zhang; D. Wang; H. Wu; H. Peng; B. Hu;,2022,10.1109/TIM.2021.3137159,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9656882,Discriminative Stein kernel (DSK);electroencephalogram (EEG);seizure detection;sparse representation (SR);symmetric positive definite (SPD) matrix;,"""The automatic seizure detection in electroencephalogram (EEG) signals is crucial for the monitoring, diagnosis, and treatment of epilepsy. In this study, an intelligent detection framework with the discriminative Stein kernel-based sparse representation (DSK-SR) is constructed to distinguish epileptic EEG signals. Specifically, in the scheme of DSK-SR, EEG samples are presented by symmetric positive definite (SPD) matrices in the form of covariance descriptors (CovDs). Taking into account the non-Euclidean geometry of the Riemannian manifold of SPD matrices, the traditional SR in Euclidean space cannot be applied in its original form on the manifold. To this end, the DSK defined on the manifold can permit us to embed the manifold into a high-dimensional reproducing kernel Hilbert space (RKHS) to perform SR. Then, test samples are sparsely coded over the training sets, and the classification decision is performed by assessing which class generates the minimal reconstructed residuals. Eventually, competitive experimental results on three widely recognized EEG datasets against the state-of-the-art methods demonstrate the efficacy of the proposed DSK-SR in identifying epileptic EEG signals, indicating its powerful application potential in the automatic seizure detection.""",IEEE Transactions on Instrumentation and Measurement
"""MRAM Based eFPGAs: Programming and Silicon Flows, Exploration Environments, MRAM Current State in Industry and Its Unique Potentials for FPGAs""",Y. Guillemenet; S. Z. Ahmed; L. Torres; A. Martheley; J. Eydoux; J. -B. Cuelle; L. Rougé; G. Sassatelli;,2009,10.1109/ReConFig.2009.25,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5382021,MRAM;FPGA;non volatility;Dynamic Reconfiguration;,"""The need of non volatility along with the added flexibility of un limited reprogramming like SRAM has lead to the concept of universal memories. MRAM (Magnetoresistive Random Access Memory) is one prominent member of them. At present only Flash is providing a limited bridge for that. Flash based FPGAs have several benefits being non volatile but unfortunately also loose many of the features which are only possible with SRAM based FPGAs. MRAMs have potential to bridge this gap. This paper will present a brief survey of our work in this regard for creating the entire eco system of software and hardware tool flows, MRAM layout work at 120nm, exploration environments to conduct complex experiments especially Dynamic Reconfiguration and Multi Context FPGAs. MRAM opens new opportunities for them compared to SRAM and Flash. It will discuss the current status of MRAM in industry and our current and future test chips road maps. Provide several references to industry and our published work for details about MRAMs and eFPGAs, to show why we think MRAM can be very interesting element for FPGAs.""",2009 International Conference on Reconfigurable Computing and FPGAs
"""Sparse Representation for 3D Face Recognition""",Z. Guo; Y. -y. Fan;,2013,10.1109/WCSE.2013.63,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6754312,3D face recognition;sparse representation;3D overcomplete dictionary;,"""The increasing availability of 3D facial data offers the potential to overcome the difficulties inherent with 2D face recognition, including the sensitivity to illumination conditions and head pose variations. In spite of their rapid development, many 3D face recognition algorithms in the literature still suffer from the intrinsic complexity in representing and processing 3D facial data. In this paper, we propose a novel sparse representation algorithm for 3D face recognition. The innovation of our approach lies in the strategy of constructing 3D over complete dictionary for 3D face such that 3D sparse representation can be directly used for 3D face recognition. We compare the proposed algorithm to six state-of-the-art algorithms in the FRGC2.0 database. Our results show that the proposed algorithm can substantially improve the efficiency of 3D face recognition.""",2013 Fourth World Congress on Software Engineering
"""Infrared Image Super-Resolution by Using Sparse Dictionary and Nonsubsampled Contourlet Transform""",K. Li; W. Wu; X. Yang; Y. Zhang; B. Yan; W. Lu; G. Jeon;,2015,10.1109/AITS.2015.20,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7396444,Infrared Images;Super-Resolution;Dictionary Learning;Sparse representation;NSCT;,"""Due to the limitation of hardware, Infrared (IR) image has low-resolution (LR) and poor visual quality. Infrared image super-resolution (SR) is a good solution for this problem. However, the conventional SR methods have some drawbacks. Firstly, the trained dictionary is an unstructured dictionary, which may lead to worse results. Secondly, the representation of the image is too simple to effectively represent image. To resolve these problems, in this paper, firstly, the sparse dictionary is introduced into the IR image SR to get better results. Secondly, nonsubsampled contour let transform (NSCT) is employed in the proposed method to obtain a better representation of IR image. The experiment results indicate that the subjective visual effect and objective evaluation are acquired excellent performance in the proposed method. Besides, this method is superior to other methods in the paper.""",2015 4th International Conference on Advanced Information Technology and Sensor Application (AITS)
"""Using the GAN Method, Analysis Several Characteristics of the ECG Signal in Order to Detect Cardiac Arrhythmia""",S. T. Sanamdikar; M. P. Borawake; A. A. Bamanikar;,2022,10.1109/I2CT54291.2022.9825337,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825337,Support Vector Regression;Kernel Principal Component Analysis;General Sparse Neural Network;and Generative Adversarial Network are all terms used to describe artificial neural networks;,"""Around the world, a large majority of individuals suffer from multiple cardiac diseases. As a consequence, understanding how the ECG signal operates is crucial for detecting a variety of heart conditions. The electrocardiogram (ECG) is a test that determines how strong the heart’s electrical system is. PQRST waves are a series of pulses that constitute a heartbeat in an ECG signal. In the characteristic extraction of ECG signals, the amplitude and time intervals of PQRST waves are determined for the learning of ECG signals. The PQRST segment’s amplitudes and time intervals can be utilized to determine the human heart’s proper functionality. In recent years, the bulk of methodologies and research for analyzing the ECG signal have been developed. Support Vector Regression, Kernel Principal Component Analysis, General Sparse Neural Network, and GAN techniques are compared in this article. Both of the other methods are outperformed by the Generative Adversarial Network method. However, each of the aforementioned approaches and strategies has its own set of advantages and disadvantages. The proposed system was created using Matlab software. In this study, the proposed technique was demonstrated using the MIT-BIH Arrhythmia record, which was personally annotated and validated.""",2022 IEEE 7th International conference for Convergence in Technology (I2CT)
"""Correlation and orbit determination of space objects based on sparse optical data""",A. Milani; G. Tommei; D. Farnocchia; A. Rossi; T. Schildknecht; R. Jehn;,2011,10.1111/j.1365-2966.2011.19392.x,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8163526,methods: analytical;methods: data analysis;catalogues;astrometry;celestial mechanics;,"""While building up a catalogue of Earth-orbiting objects, the available optical observations are typically sparse. In this case, no orbit determination is possible without previous correlation of observations obtained at different times. This correlation step is the most computationally intensive, and becomes more and more difficult as the number of objects to be discovered increases. In this paper, we tested two different algorithms, and the related prototype software, recently developed to solve the correlation problem for objects in geostationary orbit (GEO). The algorithms allow the accurate orbit determination by full least-squares solutions with all six orbital elements. The presence of a significant subpopulation of high area-to-mass ratio objects in the GEO region, strongly affected by non-gravitational perturbations, required to solve also for dynamical parameters describing these effects, that is to fit between six and eight free parameters for each orbit.The validation was based upon a set of real data, acquired from the European Space Agency (ESA) Space Debris Telescope (ESASDT) at the Teide Observatory (Canary Islands). We proved that it is possible to assemble a set of sparse observations into a set of objects with orbits. This would allow a survey strategy covering the region of interest in the sky just once per night. As a result, it would be possible to significantly reduce the requirements for a future telescope network, with respect to what would have been required with the previously known algorithms for correlation and orbit determination.""",Monthly Notices of the Royal Astronomical Society
"""Multiple 3-D Feature Fusion Framework for Hyperspectral Image Classification""",J. Zhu; J. Hu; S. Jia; X. Jia; Q. Li;,2018,10.1109/TGRS.2017.2769113,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8244247,Feature fusion;hyperspectral image classification;sparse representation;,"""Due to the 3-D nature of hyperspectral images, as well as the spatial properties (such as regularity and continuity) of land covers, many 3-D feature extraction operators have been designed to fully exploit the joint spatial-spectral information. However, the large amount of obtained features can suffer from the “curse of dimensionality” problem, especially for the small training sample set. Moreover, various spatial-spectral features can represent the characteristics of the hyperspectral image from different aspects. In this paper, a multiple 3-D feature fusion framework (M3DF3) has been proposed for hyperspectral image classification. First, we extend the 2-D Gabor surface feature into 3-D (3DSF) domains to comply with the spatial-spectral structure of the hyperspectral image, which is directly applied on the original hyperspectral image instead of the Gabor features. Second, three 3-D feature extraction methods, including the 3-D morphological profile, the 3-D local binary pattern, and the proposed 3DSF, that, respectively, characterize the hyperspectral image from three different angles, i.e., morphology, local dependence, and shape smoothness, are fused under a multitask sparse representation framework to take full advantage of the multiple 3-D features together. The proposed M3DF3 approach was fully tested on three real-world hyperspectral image data, i.e., the widely used Indian Pines, Pavia University, and Houston University. The results show that our method can achieve as high as 68.22%, 79.44%, and 72.84% accuracies, respectively, even when only few samples, i.e., three samples per class, are used for training.""",IEEE Transactions on Geoscience and Remote Sensing
"""A Parallel Memory Defect Detection Method based on Sparse-Value-Flow Graph""",R. Xu; X. Mao; L. Chen; Y. Yu;,2023,10.1109/JCC59055.2023.00014,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10229703,Static Analysis;Memory Defect;SVFG;Dependency Analysis;Task-level Parallel;,"""Memory vulnerability detection aims to identify software defects that can compromise memory safety. However, existing methods often struggle to achieve both high precision and efficiency. This paper presents a high-precision memory vulnerability detection approach based on value flow analysis and parallel computing. We first construct a static semantic representation called SVFG to enable precise detection of memory vulnerabilities such as null pointer dereference and use-after-free. We then perform dependency-aware path feasibility analysis using an SMT solver to reduce false positives. Finally, we develop a task-level parallel framework to accelerate the constraint solving process and improve efficiency.We evaluate our approach on the Juliet test set of over 2,000 test cases and 7 open-source projects. Experimental results show that our dependency-aware analysis can achieve 0.5%-2.05% false positive rates, outperforming traditional approaches and existing tools. Our task-level parallel framework can achieve up to 3.25x speedup with 4 computing nodes.Our study demonstrates that combining value flow analysis and parallel computing is a promising way to enable highly precise and efficient detection of memory vulnerabilities. For future work, we plan to integrate pointer analysis to support more complex codes, and optimize the granularity of parallelism to improve scalability. Overall, this paper presents a static analysis based method to address the inherent trade-off between precision and efficiency in memory vulnerability detection.""",2023 IEEE International Conference on Joint Cloud Computing (JCC)
"""Forecasting e-Journal Unique Visitors using Smoothed Long Short-Term Memory""",A. P. Wibawa; R. R. Ula; A. B. P. Utama; M. Y. Chuttur; A. Pranolo; Haviluddin;,2021,10.1109/ICEEIE52663.2021.9616628,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9616628,forecasting;unique visitors;e-journal;long short-term memory;data smoothing;,"""The number of unique visitors every day can be used as a benchmark to assess the success of an electronic journal. Unique visitors are visitors who use one IP in a certain period. With an increasing number of unique visitors every day, it can be inferred that scientific periodicals are increasingly in demand by the wider community, affecting the breadth of distribution and speeding up the journal accreditation system. Therefore, a model that can predict the number of unique visitors in electronic journals in the future can be beneficial for journal administrators. This paper evaluates Long Short-Term Memory (LSTM) performance when predicting the next day’s (T+1) time series for the number of unique visitors to an e-Journal website. The unique visitor data sample used was from January 1, 2018, to December 31, 2018. The distribution of data testing and training used was 80% and 20%. The data quality has been improved by smoothing the input data using exponential, mean, and median smoothing. The evaluation results show that the architecture with the best performance is model 2, namely the model mean + LSTM architecture 1-5-1 with a learning rate of 0.2 and a MAPE value of 0.08098. Therefore, we conclude that data smoothing with mean smoothing can improve Long Short-Term Memory performance for unique website visitor forecasting.""","2021 7th International Conference on Electrical, Electronics and Information Engineering (ICEEIE)"
"""Demo: Real-Time Implementation of Block Orthogonal Sparse Superposition Codes""",B. Lee; D. Han; N. Lee;,2022,10.1109/ICCWorkshops53468.2022.9915027,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9915027,;,"""Short-packet communication is a key enabler of various Internet of Things applications that require higher-level security. This proposal briefly reviews block orthogonal sparse superposition (BOSS) codes, which are applicable for secure short-packet transmissions. In addition, following the IEEE 802.11a Wi-Fi standards, we demonstrate the real-time performance of secure short packet transmission using a software-defined radio testbed to verify the feasibility of BOSS codes in a multi-path fading channel environment.""",2022 IEEE International Conference on Communications Workshops (ICC Workshops)
"""ALLARM: Optimizing sparse directories for thread-local data""",A. Roy; T. M. Jones;,2014,10.7873/DATE.2014.091,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6800292,;,"""Large-scale cache-coherent systems often impose unnecessary overhead on data that is thread-private for the whole of its lifetime. These include resources devoted to tracking the coherence state of the data, as well as unnecessary coherence messages sent out over the interconnect. In this paper we show how the memory allocation strategy for non-uniform memory access (NUMA) systems can be exploited to remove any coherence-related traffic for thread-local data, as well removing the need to track those cache lines in sparse directories. Our strategy is to allocate directory state only on a miss from a node in a different affinity domain from the directory. We call this ALLocAte on Remote Miss, or ALLARM. Our solution is entirely backward compatible with existing operating systems and software, and provides a means to scale cache coherence into the many-core era. On a mix of SPLASH2 and Parsec workloads, ALLARM is able to improve performance by 13% on average while reducing dynamic energy consumption by 9% in the on-chip network and 15% in the directory controller. This is achieved through a 46% reduction in the number of sparse directory entries evicted.""","2014 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
"""Optimization of a Solver for Computational Materials and Structures Problems on NVIDIA Volta and AMD Instinct GPUs""",M. Zubair; J. Warner; D. Wagner;,2019,10.1109/ScalA49573.2019.00007,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8948645,Iterative Solver;Sparse Matrices;Unstructured Grid;High performance computing;Optimization;GPUs;,"""Implementation of Finite Elements by NASA (ScIFEN) is a software package developed to solve complex computational materials and structures problems using the finite element method (FEM). In this paper, we describe optimization techniques to speed up the linear solver computation that occurs within the ScIFEN application. We consider GPUs from two different vendors, NVIDIA and AMD as our target platforms for optimization and highlight differences in performance and optimization techniques. The NVIDIA GPU Volta V100 is used in the Summit system deployed at Oak Ridge National Laboratory, and the new exascale system, Frontier, will be using AMD Radeon Instinct GPU. We evaluated the performance of various optimization techniques on test matrices, ranging in size from 100K to 4M, that are representative of ScIFEN applications. The linear solver computation is memory-bound on both GPUs. Our experiments show that on the NVIDIA GPU we obtained up to 79% of the theoretical peak bandwidth, while the AMD GPU achieved 59%. Overall, the NVIDIA V100 GPU outperforms the 1 AMD MI 25 GPU . We observed an overall speedup of up to 37X on an NVIDIA V100 compared to an Intel Skylake 12-core machine. The solver for a 4M degree of freedom system took under 2.5 seconds.""",2019 IEEE/ACM 10th Workshop on Latest Advances in Scalable Algorithms for Large-Scale Systems (ScalA)
"""Time optimal control of ground vehicles""",G. Max; B. Lantos;,2014,10.1109/SISY.2014.6923594,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6923594,Time optimal control;Vehicle model;Direct multiple shooting;Mixed integer nonlinear programming;Sparse gradient and Jacobian;,"""The paper deals with the time optimal control of automatically driven cars modeled with gear shift as discrete control input beside the continuous ones in a test path between corridors. The car is required to avoid a static obstacle or performing double lane change. The problem can be formulated as a mixed-integer optimal control problem (MIOCP). The resulting MIOCP is solved by reformulating it to static mixed-integer nonlinear program (MINLP) using time discretization and direct multiple shooting method. Non-commercial open software packages are applied that substantially use the gradients of the objective function and the Jacobians of the constraints exploiting sparsity. A novel algorithm and implementation is presented for computing the derivatives of the complex state trajectory joining equations. This algorithm was given in the form of matrix differential equations whose structure allowed to compute their solution using RK4 in matrix form. The elaborated method can be applied both for combustion engine and electric driven cars. It can form the basis to generate an offline database of a central general collision avoidance system (CAS) for varying path parameters on a grid which can support real time applications.""",2014 IEEE 12th International Symposium on Intelligent Systems and Informatics (SISY)
"""Dictionary Learning Algorithm based on Restricted Boltzmann Machine""",L. Lian;,2021,10.1109/DSA52907.2021.00082,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9623028,dictionary learning;restricted Boltzmann machine;sparse representation;dropout technique;,"""The traditional dictionary learning algorithm has a slow rate for convergence and poor sparse representation effect when it learns different kinds of training image, and the learning effect is worse if the image is disturbed by noise. In order to solve this problem, a dictionary learning algorithm based on restricted Boltzmann machine is proposed. The sparse representation and dictionary atoms are obtained by using the method of segmented training. The first neural network is composed of restricted Boltzmann machine. By fully learning the sparse representation of diversity images, the probability distribution of neurons of hidden layer in restricted Boltzmann machine are obtained. The second segment of neural network combined with dropout technique which restrain the number of the activated neurons. And the input layer is the non-zero elements of hidden layer in restricted Boltzmann machine. Finally the weight of the trained network is the optimal dictionary matrix. The simulation results show that the algorithm can improve the denoising effect and reconstruction accuracy of the test image.""",2021 8th International Conference on Dependable Systems and Their Applications (DSA)
"""An Aurora Image Classification Method based on Compressive Sensing and Distributed WKNN""",Y. Li; N. Jiang;,2018,10.1109/COMPSAC.2018.00055,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8377679,classification;sparse-representation;compressive-sensing;distributed-KNN;image-process;,"""Reasonable Aurora classification is particularly important for studying the relationship between aurora phenomena and the process of magnetosphere dynamics. With the development of computer science, image processing and pattern recognition technology, new approaches for Aurora classification are springing up. In this paper, we extract the LBP feature of images and use the distributed weighted KNN based on optimal discriminant dictionary for sparse representation as the classification method to discriminate the shape of aurora. The proposed method combines compressed sensing approaches and distributed computing technology, improving the accuracy and effectiveness of the existed sparse representation methods. The experimental results show that the proposed method significantly enhances the power of discrimination of aurora features, and consequently improve the accuracy and effectiveness of the classification of aurora images.""",2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)
"""Superpixel-level sparse representation-based classification for hyperspectral imagery""",S. Jia; B. Deng; X. Jia;,2016,10.1109/IGARSS.2016.7729854,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7729854,Hyperspectral imagery;superpixel;sparse representation-based classification;,"""Sparse representation-based classification (SRC) assigns a test sample to the class with minimal representation error via a sparse linear combination of all the training samples, which has successfully been applied to hyperspectral imagery (HSI). Meanwhile, spatial information, that means the adjacent pixels belong to the same class with a high probability, is a valuable complement to the spectral information. In this paper, we propose an efficient method for HSI classification by using superpixel based sparse representation-based classification (SP-SRC). One superpixel can be regarded as a small region consisting of a number of pixels with similar spectral characteristics. The novel method utilizes superpixel to exploit spatial information which can greatly improve classification accuracy. Specifically, SRC is firstly used to classifier the HSI. Then an efficient segmentation algorithm is adopted to divide the HSI into disjoint superpixels. Finally, each superpixel is used to fuse the results of the SRC classifier. Experimental results on the widely-used Indian Pines hyperspectral imagery have shown that the proposed SP-SRC approach could achieve better performance than the pixel-wise SRC method.""",2016 IEEE International Geoscience and Remote Sensing Symposium (IGARSS)
"""On using two-phase filtering in indexed approximate string matching with application to searching unique oligonucleotides""",H. Hyyro;,2001,10.1109/SPIRE.2001.989742,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=989742,;,"""We discuss using an indexing scheme to accelerate approximate search over a static text in the case of using unit cost edit distance as the measure of similarity between strings. First we generally consider the filtering criteria that can be used as a basis for the index, and then propose using filtering twice before the final checking phase. The last part consists of presenting an indexed approximate string matching application in bioinformatics, which is the search of unique oligonucleotides. We present practical comparisons and results for using different filtering schemes in this application. Our tests have involved a total of 15 different genomes, from which we present some results involving the largest two of these: The genome of Saccharomyces cerevisiae (baker's yeast) and a recent draft of the human genome, the latter being also the main target of the application.""",Proceedings Eighth Symposium on String Processing and Information Retrieval
"""IoT based Security Model to Enhance Blockchain Technology""",S. Awasthi; P. Johri; S. K. Khatri;,2018,10.1109/ICACCE.2018.8441720,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8441720,Blockchain;private key;Unique Id;block nodes;Validator;Bitcoin;Ethereum;,"""The rapid growth of blockchain technology can be felt in our everyday lives but despite these progressions security remains a concern for the Blockchain environment as it breached a diverse range of gadgets, noticeable measures of information, production network partners to security threats. IoT security and privacy are basic achievement factors for meeting the elevated standards of the innovation to change numerous parts of our general society and economy. The proposed IoT based Blockchain model handles most security dangers, while considering the asset imperatives of numerous IoT devices. Although this architecture is a lightweight, secure and fundamental approach, hypothetically it can be scaled further in areas without compromising its performance and user's anonymity.""",2018 International Conference on Advances in Computing and Communication Engineering (ICACCE)
"""Characterization and readout of MADPET-II detector modules: validation of a unique design concept for high resolution small animal PET""",D. P. McElroy; W. Pimpl; B. J. Pichler; M. Rafecas; T. Schuler; S. I. Ziegler;,2005,10.1109/TNS.2004.843114,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1417130,Avalanche photodiode (APD);biomedical nuclear imaging;list mode data;lutetium oxyorthosilicate (LSO);small animal positron emission tomography (PET);,"""MADPET-II is a novel high-resolution, high-sensitivity three-dimensional lutetium oxyorthosilicate avalanche photodiode small animal PET scanner with a unique detector design and readout that creates new possibilities for data processing and analysis. The scanner consists of a ring of dual-layer detector modules (O71 mm), each containing a 4/spl times/8 array of 2/spl times/2/spl times/6mm/sup 3/ (front) and 2/spl times/2/spl times/8 mm/sup 3/ (back) LSO crystals that are each optically isolated and coupled one-to-one to a monolithic 4/spl times/8 APD array. Signals from each channel are individually processed using fully-integrated 16-channel, low noise, charge sensitive preamplifiers and custom integrated electronics that include a four channel shaping amplifier, peak detector and non-delay line CFD. Analog-to-digital converters and time-to-digital converters digitise the pulse height and time-stamp each event, and data are stored exclusively in list mode format. Two 32-channel detector blocks were tested in a coincidence and in a dual-layer configuration. Coincidences were sorted post-acquisition in software, allowing for maximum flexibility in the data processing. Monte Carlo simulations have calculated the system spatial resolution to be 1.1 mm FWHM. Average energy resolution for a single detector block ranged from 20.2% FWHM to 23.9% FWHM. System pulse height linearity was measured, and all channels responded with R/sup 2/>0.9988 (Pearson correlation coefficient). Intrinsic uniformity for each detector block ranged from 2.0% to 4.8% (400 keV threshold). Overall system timing resolution was measured to be 10.2 ns FWHM, with individual LORs having time resolutions as low as 4.5 ns FWHM.""",IEEE Transactions on Nuclear Science
"""Spatial-Pooling Memristor Crossbar Converting Sensory Information to Sparse Distributed Representation of Cortical Neurons""",S. N. Truong; K. Van Pham; K. -S. Min;,2018,10.1109/TNANO.2018.2815624,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8315035,Spatial pooling;memristor crossbar;sensory information;sparse distributed representation;cortical neurons;,"""The spatial pooler in brain's neocortex models how the cortical neurons learn the feedforward connections and forms the cortical representation from various sensory information. To realize the spatial pooler's operation in hardware, we propose a new spatial-pooling memristor crossbar that converts the inputs from sensory organ into the sparse distributed representation (SDR) of cortical neurons, in this paper. The spatial pooling is composed of overlapping, inhibition, and learning steps. The proposed memristor crossbar can perform exactly the same functions of overlapping, inhibition, and learning with the spatial-pooling software algorithm. By converting the sensory information to SDR using the spatial-pooling memristor crossbar, we can have the following advantages for the cortical information processing: first, preserving the semantic similarity through the spatial pooling; second, changing synaptic weights continuously; third, keeping the sparsity of SDR as low as 2% like brain's neocortex; and fourth, improving the noise robustness. In this paper, these four properties of the spatial pooling in brain's neocortex have been simulated and verified in the memristor crossbar circuit. For verifying the spatial-pooling operation, the memristor crossbar has been tested for MNIST image recognition in this paper. The recognition rate is as high as 95% for the memristor crossbar with 4096 columns.""",IEEE Transactions on Nanotechnology
"""Application of total least squares (TLS) to the design of sparse signal representation dictionaries""",S. F. Cotter; B. D. Rao;,2002,10.1109/ACSSC.2002.1197319,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1197319,;,"""Sparse signal representation has been the subject of much research in recent years in a variety of applications. We address the problem of learning a dictionary of waveforms from a given set of data signals, which may then be used to provide efficient and meaningful signal decompositions. We motivate and develop a total least squares (TLS) based algorithm. Through a series of simulations using a known test-case dictionary, it is shown that the TLS algorithm gives a substantial performance improvement over a previously proposed least squares (LS) algorithm in correctly learning the generating dictionary vectors.""","Conference Record of the Thirty-Sixth Asilomar Conference on Signals, Systems and Computers, 2002."
"""Head Pose Estimation Using Sparse Representation""",B. Ma; T. Wang;,2010,10.1109/ICCEA.2010.226,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5445677,;,"""This paper proposes a novel method using sparse representation to improve the performance of head pose estimation. Sparse Representation Classifier (SRC) has been applied in face recognition and the related problem. In this paper, we first argue that SRC is efficient in head pose estimation. Then we propose the Block based Spare Representation Classifier (BSRC) method to reduce the influence of the background in the multi-view face images. The motivation of BSRC is that since the face shapes are different for the discretional head poses, the background in the multi-view face image is difficult to be eliminated by the traditional ways, such as feature extraction. As a result, the features of the multi-view face contain the noise of the background, which caused the descend of the performance of head pose estimation. In this paper, a face region is transformed into many images with the different blocks are discarded based on the assume that the block maybe the background at the specifically pose. By this way, there is an image in which the influence of the background can be reduced greatly. Specifically, SRC is applied on each image and the final class label of the input face region is decided by maximizing of Sparsity Concentration Index (SCI). The experimental results on the head pose database show the effectiveness of BSRC.""",2010 Second International Conference on Computer Engineering and Applications
"""A Sparse Model Based Detection of Copy Number Variations From Exome Sequencing Data""",J. Duan; M. Wan; H. -W. Deng; Y. -P. Wang;,2016,10.1109/TBME.2015.2464674,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7180343,Copy number variation (CNV);exome sequencing;sparse modeling;matrix approximation;iteratively reweighted least squares;Copy number variation (CNV);exome sequencing;iteratively reweighted least squares (IRLS);matrix approximation;sparse modeling;,"""Goal: Whole-exome sequencing provides a more cost-effective way than whole-genome sequencing for detecting genetic variants, such as copy number variations (CNVs). Although a number of approaches have been proposed to detect CNVs from whole-genome sequencing, a direct adoption of these approaches to whole-exome sequencing will often fail because exons are separately located along a genome. Therefore, an appropriate method is needed to target the specific features of exome sequencing data. Methods: In this paper, a novel sparse model based method is proposed to discover CNVs from multiple exome sequencing data. First, exome sequencing data are represented with a penalized matrix approximation, and technical variability and random sequencing errors are assumed to follow a generalized Gaussian distribution. Second, an iteratively reweighted least squares algorithm is used to estimate the solution. Results: The method is tested and validated on both synthetic and real data, and compared with other approaches including CoNIFER, XHMM, and cn.MOPS. The test demonstrates that the proposed method outperform other approaches. Conclusion: The proposed sparse model can detect CNVs from exome sequencing data with high power and precision. Significance: Sparse model can target the specific features of exome sequencing data. The software codes are freely available at  http://www.tulane.edu/ wyp/software/Exon_CNV.m""",IEEE Transactions on Biomedical Engineering
"""A Configurable 12–237 kS/s 12.8 mW Sparse-Approximation Engine for Mobile Data Aggregation of Compressively Sampled Physiological Signals""",F. Ren; D. Marković;,2016,10.1109/JSSC.2015.2480862,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7299281,Application-specific integrated circuits (ASICs);biomedical signal processing;compressed sensing;digital integrated circuits;energy efficiency;low-power design;minimization methods;parallel architecture;real-time systems;reconfigurable architecture;signal reconstruction;Application-specific integrated circuits (ASICs);biomedical signal processing;compressed sensing;digital integrated circuits;energy efficiency;low-power design;minimization methods;parallel architecture;real-time systems;reconfigurable architecture;signal reconstruction;,"""Compressive sensing (CS) is a promising technology for realizing low-power and cost-effective wireless sensor nodes (WSNs) in pervasive health systems for 24/7 health monitoring. Due to the high computational complexity (CC) of the reconstruction algorithms, software solutions cannot fulfill the energy efficiency needs for real-time processing. In this paper, we present a 12-237 kS/s 12.8 mW sparse-approximation (SA) engine chip that enables the energy-efficient data aggregation of compressively sampled physiological signals on mobile platforms. The SA engine chip integrated in 40 nm CMOS can support the simultaneous reconstruction of over 200 channels of physiological signals while consuming (1% of a smartphone's power budget. Such energyefficient reconstruction enables two-to-three times energy saving at the sensor nodes in a CS-based health monitoring system as compared to traditional Nyquist-based systems, while providing timely feedback and bringing signal intelligence closer to the user.""",IEEE Journal of Solid-State Circuits
"""A Protocol-based Intrusion Detection System using Dual Autoencoders""",Y. -L. Huang; C. -Y. Hung; H. -T. Hu;,2021,10.1109/QRS54544.2021.00084,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9724825,Intrusion Detection;Feature Extraction;Autoen-coder;Sparse Autoencoder;,"""This paper proposes a dual Autoencoder-based Intrusion Detection System (duAE-IDS) for the ever-changing network attacks. duAE-IDS is a protocol-based IDS, which divides traffic by its application-layer protocol. duAE-IDS determines the traffic's abnormality by considering both the criteria and the application-layer protocol. The criteria are obtained by training our neural network model (duAE model) with traffic containing only one type of application-layer protocol. duAE-IDS represents each traffic flow with 67 features with eight new features for TCP traffic to improve detection accuracy. duAE-Idsuses two sparse autoencoders and one 1D CNN to extract features from traffic for every application-layer protocol. We conduct several experiments to prove the abilities and flexibilities of duAE-IDS. We prove that duAE-Idstrained with the known datasets can reach an F1-score of 0.87 for detecting attack traffic in an unknown network. We can run duAE-Idsin any network without pre-collecting the traffic of the network.""","2021 IEEE 21st International Conference on Software Quality, Reliability and Security (QRS)"
"""Delineation of rock fragments by classification of image patches using compressed random features""",G. Bull; J. Gao; M. Antolovich;,2014,,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7294834,Compressed Sensing;Random Projections;Sparse Representation;Image Patches;Feature Extraction;Image Segmentation;Classification;,"""Monitoring of rock fragmentation is a commercially important problem for the mining industry. Existing analysis methods either resort to physically sieving rock samples, or using image analysis software. The currently available software systems for this problem typically work with 2D images and often require a significant amount of time by skilled human operators, particularly to accurately delineate rock fragments. Recent research into 3D image processing promises to overcome many of the issues with analysis of 2D images of rock fragments. However, for many mines it is not feasible to replace their existing image collection systems and there is still a need to improve on methods used for analysing 2D images. This paper proposes a method for delineation of rock fragments using compressed Haar-like features extracted from small image patches, with classification by a support vector machine. The optimum size of image patches and the numbers of compressed features have been determined empirically. Delineation results for images of rocks were superior to those obtained using the watershed algorithm with manually assigned markers. Using compressed features is demonstrated to improve the computational efficiently such that a machine learning solution is viable.""",2014 International Conference on Computer Vision Theory and Applications (VISAPP)
"""Whose move is it anyway? Authenticating smart wearable devices using unique head movement patterns""",S. Li; A. Ashok; Y. Zhang; C. Xu; J. Lindqvist; M. Gruteser;,2016,10.1109/PERCOM.2016.7456514,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7456514,;,"""In this paper, we present the design, implementation and evaluation of a user authentication system, Headbanger, for smart head-worn devices, through monitoring the user's unique head-movement patterns in response to an external audio stimulus. Compared to today's solutions, which primarily rely on indirect authentication mechanisms via the user's smartphone, thus cumbersome and susceptible to adversary intrusions, the proposed head-movement based authentication provides an accurate, robust, light-weight and convenient solution. Through extensive experimental evaluation with 95 participants, we show that our mechanism can accurately authenticate users with an average true acceptance rate of 95.57% while keeping the average false acceptance rate of 4.43%. We also show that even simple head-movement patterns are robust against imitation attacks. Finally, we demonstrate our authentication algorithm is rather light-weight: the overall processing latency on Google Glass is around 1.9 seconds.""",2016 IEEE International Conference on Pervasive Computing and Communications (PerCom)
"""Gaussian Bounds for Noise Correlation of Functions and Tight Analysis of Long Codes""",E. Mossel;,2008,10.1109/FOCS.2008.44,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4690950,|invariance;long codes;unique games;majority;predictability;Condorcet voting;,"""We derive tight bounds on the expected value of products of low influence functions defined on correlated probability spaces. The proofs are based on extending Fourier theory to an arbitrary number of correlated probability spaces, on a generalization of an invariance principle recently obtained with O'Donnell and Oleszkiewicz for multilinear polynomials with low influences and bounded degree and on properties of multi-dimensional Gaussian distributions. Let (Xi j : 1 les i les k,1 les j les n) be a matrix of random variables whose columns X1,..., Xn are independent and identically distributed and such that any two rows Xi, Xj for 1 les inej les k are independent. Assume further that the values that row Xi takes with non-zero probability are the same no matter how one conditions on the remaining rows X1,..., Xi-1Xi+1,..., Xk. Our results show that given k functions f1,... , fk taking values in [0,1] it holds that |E[Pii=1 k fi (Xi)] - Pii=1 k E[ fi (Xi)]| < epsi if all influences of the functions fi are smaller than tau(epsi, k) which is independent of n. In words: low influence functions of pairwise independent rows behave like independent random variables. The general statement of our result applies when the rows are not pairwise independent and when (some) of the variables do not have low influences for (some) functions. The results obtained here allow analyzing hyper-graph long-code tests. A number of applications in hardness of approximation assuming the Unique Games Conjecture were obtained using the results derived here in subsequent work by Raghavendra and jointly by Austrin and the author. Our results imply new results on voting schemes in social choice and in additive number theory. In particular we show that among all low influence functions, Majority is asymptotically the most predictable and is (almost) optimal in the context of Condorcet voting.""",2008 49th Annual IEEE Symposium on Foundations of Computer Science
"""A Noise-Resilient Super-Resolution Framework to Boost OCR Performance""",M. Sharma; A. Ray; S. Chaudhury; B. Lall;,2017,10.1109/ICDAR.2017.83,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8270014,Deep Learning;Stacked Sparse Deep Auto-encoder;Coupled Deep Convolutional Auto-encoder;Noise Resilient Super-Resolution;,"""Recognizing text from noisy low-resolution (LR) images is extremely challenging and is an open problem for the computer vision community. Super-resolving a noisy LR text image results in noisy High Resolution (HR) text image, as super-resolution (SR) leads to spatial correlation in the noise, and further cannot be de-noised successfully. Traditional noise-resilient text image super-resolution methods utilize a denoising algorithm prior to text SR but denoising process leads to loss of some high frequency details, and the output HR image has missing information (texture details and edges). This paper proposes a noise-resilient SR framework for text images and recognizes the text using a deep BLSTM network trained on high resolution images. The proposed end-to-end deep learning based framework for noise-resilient text image SR simultaneously perform image denoising and super-resolution as well as preserves missing details. Stacked sparse denoising auto-encoder (SSDA) is learned for LR text image denoising, and our proposed coupled deep convolutional auto-encoder (CDCA) is learned for text image super-resolution. The pretrained weights for both these networks serve as initial weights to the end-to-end framework during finetuning, and the network is jointly optimized for both the tasks. We tested on several Indian Language datasets and the OCR performance of the noise-resilient super-resolved images is at par with the original HR images.""",2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)
"""Performance of a fully parallel sparse solver""",M. T. Heath; P. Raghaven;,1994,10.1109/SHPCC.1994.296662,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=296662,;,"""The performance of a fully parallel direct solver for large sparse symmetric positive definite systems of linear equations is demonstrated. The solver is designed for distributed-memory message-passing parallel computer systems, particularly massively parallel machines. All phases of the computation, including symbolic processing as well as numeric factorization and triangular solution, are performed in parallel. A parallel Cartesian nested dissection algorithm is used to compute a fill-reducing ordering for the matrix and an appropriate partitioning of the problem across the processors. The separator tree resulting from nested dissection is used to identify and exploit large-grain parallelism in the remaining steps of the computation. The parallel performance of the solver is reported for a series of test problems on the Thinking Machines CM-5. The parallel efficiency and scalability of the solver, as well as the relative importance of the various phases of the computation, are investigated empirically.<>""",Proceedings of IEEE Scalable High Performance Computing Conference
"""Globally Convergent Algorithms for Estimating Generalized Gamma Distributions in Fast Signal and Image Processing""",K. -S. Song;,2008,10.1109/TIP.2008.926148,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4553725,Consistency;convexity;fast and globally convergent algorithms;fixed-point;generalized gamma distribution;image analysis and coding;scale-independent shape estimation;unique global root;,"""Many applications in real-time signal, image, and video processing require automatic algorithms for rapid characterizations of signals and images through fast estimation of their underlying statistical distributions. We present fast and globally convergent algorithms for estimating the three-parameter generalized gamma distribution (G Gamma D). The proposed method is based on novel scale-independent shape estimation (SISE) equations. We show that the SISE equations have a unique global root in their semi-infinite domains and the probability that the sample SISE equations have a unique global root tends to one. The consistency of the global root, its scale, and index shape estimators is obtained. Furthermore, we establish that, with probability tending to one, Newton-Raphson (NR) algorithms for solving the sample SISE equations converge globally to the unique root from any initial value in its given domain. In contrast to existing methods, another remarkable novelty is that the sample SISE equations are completely independent of gamma and polygamma functions and involve only elementary mathematical operations, making the algorithms well suited for real-time both hardware and software implementations. The SISE estimators also allow the maximum likelihood (ML) ratio procedure to be carried out for testing the generalized Gaussian distribution (GGD) versus the G Gamma D. Finally, the fast global convergence and accuracy of our algorithms for finite samples are demonstrated by both simulation studies and real image analysis.""",IEEE Transactions on Image Processing
